---
title: "Correlation: Myths and Alternative Views"
date: "2025-00-00"
categories: [statistical inference, correlation]
---

## Background

Correlation is endlessly fascinating. It may be the single topic that I have written the most on this blog. It offers significant depth. In the era of deep learning and black box models, correlation stands out as transparent and interpretable statistical quantity.

It is one of the most used (and misused) statistical concepts. It seems simple enough: a number between $-1$ and $1$ that tells you how strongly two variables are related. But lurking beneath that tidy number are a host of assumptions, limitations, and interpretations that often go unrecognized by even experienced analysts. In this article, we revisit two fascinating papers that try to untangle the myths and layers of meaning wrapped around correlation: van den Heuvel and Zhan (2022), and Rodgers and Nicewander (1988). Our goal is to sharpen our intuition and clear up misconceptions around three of the most popular correlation measures—Pearson's $r$, Spearman's $\rho$, and Kendall's $\tau$—while also exploring thirteen perspectives on what correlation actually tells us.

## Notation

Let $X$ and $Y$ be two random variables with realizations $(x_i, y_i)$ for $i = 1, \ldots, n$. We assume all variables are centered unless stated otherwise.

- **Pearson’s $r$** is defined as:
  $$r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2} \sqrt{\sum (y_i - \bar{y})^2}}$$
- **Spearman’s $\rho$** is Pearson’s $r$ computed on the ranks of the data.
- **Kendall’s $\tau$** is based on the number of concordant and discordant pairs:
  $$\tau = \frac{\#\text{concordant} - \#\text{discordant}}{\binom{n}{2}}$$

Concordant pairs of observations refer to pairs where the ranks of both variables move in the same direction. For example, if one observation is higher than another in both variables, they are concordant. Conversely, discordant pairs occur when the ranks of the variables move in opposite directions; one observation is higher in one variable but lower in the other. 

## A Closer Look

### Myths About Correlation Coefficients

van den Heuvel and Zhan (2022) outline several persistent myths about correlation, and it’s worth tackling them head-on. One of the biggest is that **Pearson’s $r$ measures a linear relationship**, while Spearman’s $\rho$ and Kendall’s $\tau$ measure monotonic relationships. That sounds tidy but is too simplistic. In fact, none of the three are pure tests of “linearity” or “monotonicity.” Their sensitivity to nonlinearity depends on the distributional form, heteroscedasticity, and outliers.

Another common misconception is that rank-based correlations are more “robust.” While it’s true that rank correlations are less sensitive to outliers in the marginal distributions, they can still behave poorly under certain forms of non-monotonic or heteroscedastic relationships. For instance, a $U$-shaped relationship will likely be missed by all three coefficients.

What’s most illuminating is the realization that none of these coefficients are silver bullets. They summarize different aspects of association but rarely tell the whole story. It’s a good idea to pair them with visualization and formal tests of fit or non-linearity.

### Thirteen Ways to Look at the Correlation Coefficient

Rodgers and Nicewander (1988) offer a brilliant framing of correlation by listing thirteen distinct ways to interpret Pearson’s $r$. Here’s a quick tour, each providing a slightly different angle:

1. **As a measure of standardized covariance**, it tells you how two variables co-vary after accounting for their units.
2. **As a regression slope between standardized variables**, it equals the slope of the line predicting $z$-scored $Y$ from $z$-scored $X$.
3. **As a symmetric regression slope**, since r is the same whether you regress $Y$ on $X$ or $X$ on $Y$, once both are standardized.
4. **As the cosine of the angle between two vectors**, showing their geometric alignment.
5. **As a function of sums of squares**, where it relates directly to the decomposition of total variance.
6. **As a measure of shared variance**, where $r^2$ tells you the proportion of variance explained.
7. **As a special case of canonical correlation**, when only one variable is in each set.
8. **As a maximum likelihood estimator**, under a bivariate normal model.
9. **As a test statistic**, r can be tested for significance under certain nulls.
10. **As an estimator sensitive to range restriction**, showing attenuation if $X$ or $Y$ is truncated.
11. **As an indicator of predictability**, but only under linear assumptions.
12. **As a metric dependent on scale**, since nonlinear transformations can dramatically change it.
13. **As a guide, not a truth**, because it tells part of the story but rarely all of it.

Each interpretation highlights a different trade-off or caveat. For example, the geometric view gives a great intuition, but the regression slope interpretation connects more directly to causal inference. And perhaps most importantly, several of these views are **not invariant to nonlinear transformations**, which matters a lot in real data.

## Bottom Line

- Pearson’s $r$, Spearman’s $\rho$, and Kendall’s $\tau$ measure different aspects of association—none is a catch-all indicator.

- The “monotonic vs. linear” framing is a helpful heuristic, but it breaks down in many real-world scenarios.

- Rodgers and Nicewander's thirteen perspectives on correlation reveal its multifaceted nature and limitations.

- Always visualize your data—correlation coefficients should not replace your eyes or your understanding of the domain.

## Where to Learn More

To dig deeper into the nuances of correlation, the papers discussed here are essential. You might also enjoy Bollen and Pearl's work on correlation vs. causation, or more modern texts on exploratory data analysis and robust statistics. Visualization tools like scatterplot matrices, partial residual plots, and nonparametric smoothers (e.g., loess) can complement numeric summaries.

## References

- van den Heuvel, E., & Zhan, Z. (2022). Myths about linear and monotonic associations: Pearson’s $r$, Spearman’s $\rho$, and Kendall’s $\tau$. *The American Statistician*, 76(1), 44–52.
- Lee Rodgers, J., & Nicewander, W. A. (1988). Thirteen ways to look at the correlation coefficient. *The American Statistician*, 42(1), 59–66.

