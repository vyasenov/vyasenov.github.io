---
title: "Why 'Significant' vs. 'Not Significant' Is a Statistical Trap"
date: "2025-04-24"
categories: [statistical inference, hypothesis testing]
---

## Background

We’ve all seen it—two estimates side by side, one with a $p$-value of 0.04 and the other with a $p$-value of 0.06. Cue the celebratory confetti for the “significant” effect, and a sad trombone for the “not significant” one. But wait! Are those results *really* different from each other? If one $p$-value dips below the magical 0.05 and the other doesn't, can we say that the two estimates differ in a meaningful way?

The short answer is: no. And that’s the central insight of Gelman and Stern (2006), a paper that gently but firmly calls out a persistent mistake in the way statistical results are interpreted. In this article, we’re going to unpack the logic, show the math, and make sure you never fall into this trap again—especially if you’re doing research at scale or comparing multiple models or subgroups.

## Notation

Let’s consider two estimated effects, say from two different subgroups or models:

- $\hat{\theta}_1$: Estimate from group 1 with standard error $SE_1$
- $\hat{\theta}_2$: Estimate from group 2 with standard error $SE_2$

Each of these is associated with a $p$-value from a test of the null hypothesis $H_0: \theta_i = 0$.

You might be tempted to look at the individual $p$-values:

- $p_1 < 0.05$ → statistically significant
- $p_2 > 0.05$ → not statistically significant

Then jump to the conclusion: “The effects are different!”  
But to formally test **whether** the effects differ, what you actually need is a test of:

$$
H_0: \theta_1 = \theta_2
$$

This is **not** the same as testing $\theta_1 = 0$ and $\theta_2 = 0$ separately.

## A Closer Look

### The Fallacy: "Significant vs. Not Significant"

This is the statistical equivalent of assuming that because one basketball team won a game by 1 point and another lost by 1 point, the winning team must be “better” than the losing one. That doesn’t necessarily follow—it could just be noise.

What we need to emphasize is this: **The difference between significant and not significant is not itself statistically significant.**

If $\hat{\theta}_1$ is significantly different from zero, and $\hat{\theta}_2$ is not, that tells us very little about whether $\theta_1 \neq \theta_2$. To evaluate that, we need to compare the two estimates *directly*.

### Formal Testing of Differences

So how do we actually test whether the estimates differ?

The key is to compute the standard error of the difference:

$$
SE_{\text{diff}} = \sqrt{SE_1^2 + SE_2^2}
$$

Then, compute the test statistic:

$$
Z = \frac{\hat{\theta}_1 - \hat{\theta}_2}{SE_{\text{diff}}}
$$

This is just a standard Wald test for the difference between two independent estimates. Under the null hypothesis $H_0: \theta_1 = \theta_2$, the $Z$-statistic is approximately standard normal, so you can compute a $p$-value from it directly.

The big insight? It’s entirely possible that:

- $\hat{\theta}_1$ is statistically significant ($p_1 < 0.05$)
- $\hat{\theta}_2$ is not ($p_2 > 0.05$)
- But $|Z|$ is small → there’s **no significant difference** between the two!

This happens all the time in subgroup analyses, A/B tests with interactions, and model comparisons.

### Intuition and Visualization

Here’s a helpful mental model. Imagine two normal distributions:

- One centered slightly above zero (say, $\hat{\theta}_1 = 1.96$)
- One centered slightly below zero (say, $\hat{\theta}_2 = 1.65$)

With standard errors of 1, the first just clears the 5% significance threshold, the second doesn’t. But the difference between the two estimates is a mere 0.31. That’s tiny relative to their standard errors, and certainly not enough to declare them “different.”

If you plotted the two distributions with their confidence intervals, you'd see huge overlap—so why treat them as meaningfully different?

## Bottom Line

- A statistically significant result and a non-significant result do not imply a significant difference between effects.

- To compare effects, test the difference *directly* using a formal hypothesis test.

- Misinterpreting significance this way leads to false conclusions in subgroup analyses and model comparisons.

- Always report and interpret confidence intervals for comparisons—not just $p$-values.

## Where to Learn More

For a deeper dive, the original Gelman and Stern (2006) paper is a classic. Andrew Gelman has also written extensively on this topic in his blog, often using real-life examples from scientific publications and the media. If you're looking for broader reading on the pitfalls of significance testing, check out "The Cult of Statistical Significance" by Ziliak and McCloskey, or "Statistical Rethinking" by Richard McElreath for a more Bayesian angle. Lastly, brushing up on basic hypothesis testing logic in texts like *Casella and Berger* or *Wasserman’s All of Statistics* is always a good idea.

## References

Gelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. *The American Statistician*, 60(4), 328–331.  

Ziliak, S. T., & McCloskey, D. N. (2008). *The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives*. University of Michigan Press.
