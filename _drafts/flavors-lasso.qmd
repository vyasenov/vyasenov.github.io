---
title: "Lasso Flavors: Variants, Objective Functions, and When to Use Them"
date: "2025-00-00"
categories: [lasso, regularization]
---

## Background

The Lasso (Least Absolute Shrinkage and Selection Operator), introduced by Tibshirani in 1996, has become one of the go-to tools for variable selection and shrinkage in regression problems. But the classic Lasso is just the starting point. Over the years, researchers have developed many **variants of Lasso**, each designed to address specific limitations or tailor the method to different kinds of data structures.

This article provides a tour of the most popular flavors of Lasso — from standard L1-penalized regression to modern adaptations like Adaptive Lasso, Elastic Net, Square-root Lasso, and more. For each version, we’ll lay out the objective function, describe when it’s applicable, and summarize its key characteristics.

## A Closer Look

### 1. Standard Lasso (Tibshirani, 1996)

**Objective Function:**
$$
\hat{\beta} = \arg \min_{\beta} \left( \frac{1}{2n} \| y - X \beta \|_2^2 + \lambda \| \beta \|_1 \right)
$$

- **Key feature:** Encourages sparsity by shrinking some coefficients exactly to zero.
- **When to use:** Variable selection with many predictors; handles high-dimensional data.
- **Characteristics:** Shrinkage bias; struggles with highly correlated predictors.

### 2. Adaptive Lasso (Zou, 2006)

**Objective Function:**
$$
\hat{\beta} = \arg \min_{\beta} \left( \frac{1}{2n} \| y - X \beta \|_2^2 + \lambda \sum_{j=1}^p w_j | \beta_j | \right)
$$
where $w_j = 1 / |\hat{\beta}_j^{\text{init}}|^\gamma$.

- **Key feature:** Oracle property (support recovery consistency).
- **When to use:** When initial estimates (like OLS or Ridge) are available.
- **Characteristics:** Bias reduction via adaptive weights.

### 3. Relaxed Lasso (Meinshausen, 2007)

- **Step 1:** Use Lasso for selection.
- **Step 2:** Refit OLS on the selected variables or apply partial shrinkage with a relaxation parameter $\phi$.

- **Key feature:** Separates selection from estimation.
- **When to use:** Reduce Lasso’s shrinkage bias after variable selection.
- **Characteristics:** Combines selection strength of Lasso with unbiased estimation of OLS.

### 4. Square-root Lasso / Scaled Lasso (Belloni, Chernozhukov, Wang, 2011)

**Objective Function:**
$$
\hat{\beta} = \arg \min_{\beta} \left( \frac{1}{\sqrt{n}} \| y - X \beta \|_2 + \lambda \| \beta \|_1 \right)
$$

- **Key feature:** Scale-invariant — does not require estimating error variance.
- **When to use:** Unknown or heteroskedastic error variance.
- **Characteristics:** Easier tuning; robust to variance misspecification.

### 5. Elastic Net (Zou and Hastie, 2005)

**Objective Function:**
$$
\hat{\beta} = \arg \min_{\beta} \left( \frac{1}{2n} \| y - X \beta \|_2^2 + \lambda_1 \| \beta \|_1 + \lambda_2 \| \beta \|_2^2 \right)
$$

- **Key feature:** Combines L1 and L2 penalties.
- **When to use:** Multicollinearity among predictors.
- **Characteristics:** Handles correlated variables better than standard Lasso.

### 6. Group Lasso (Yuan & Lin, 2006)

**Objective Function:**
$$
\hat{\beta} = \arg \min_{\beta} \left( \frac{1}{2n} \| y - X \beta \|_2^2 + \lambda \sum_{g=1}^G \| \beta^{(g)} \|_2 \right)
$$

- **Key feature:** Selects groups of variables together.
- **When to use:** Categorical variables or grouped data structures.
- **Characteristics:** Encourages sparsity at the group level, not individual coefficients.

### 7. Fused Lasso (Tibshirani et al., 2005)

**Objective Function:**
$$
\hat{\beta} = \arg \min_{\beta} \left( \frac{1}{2n} \| y - X \beta \|_2^2 + \lambda_1 \| \beta \|_1 + \lambda_2 \sum_{j=2}^p | \beta_j - \beta_{j-1} | \right)
$$

- **Key feature:** Penalizes differences between adjacent coefficients.
- **When to use:** Ordered features like time series or spatial data.
- **Characteristics:** Promotes both sparsity and smoothness.

### 8. Bayesian Lasso (Park & Casella, 2008)

- **Objective (via prior):**
$$
\beta_j \sim \text{Laplace}(0, \lambda^{-1}).
$$

- **Key feature:** Fully Bayesian formulation with posterior inference.
- **When to use:** When uncertainty quantification (credible intervals) is desired.
- **Characteristics:** Shrinkage through Laplace priors; allows full posterior analysis.

### 9. Graphical Lasso (Friedman et al., 2008)

**Objective Function:**
$$
\hat{\Theta} = \arg \min_{\Theta \succ 0} \left( -\log \det \Theta + \text{trace}(S \Theta) + \lambda \| \Theta \|_1 \right)
$$

- **Key feature:** Estimates sparse precision (inverse covariance) matrices.
- **When to use:** Gaussian graphical models (network structure estimation).
- **Characteristics:** Encourages sparsity in partial correlations.

### 10. Stability Selection (Meinshausen & Bühlmann, 2010)

- **Not an objective function per se** — combines subsampling with Lasso to control false discovery rate.
- **Key feature:** Improves selection stability and robustness.
- **When to use:** When worried about unstable variable selection.
- **Characteristics:** Reduces false positives; provides error control guarantees.

### Summary Table

| Variant               | Key Feature                                | Motivation                      |
|-----------------------|---------------------------------------------|----------------------------------|
| Standard Lasso        | L1 penalty, sparsity                        | Variable selection              |
| Adaptive Lasso        | Weighted penalties, oracle property        | Bias reduction                  |
| Elastic Net           | Combines L1 and L2 penalties                | Handles multicollinearity       |
| Group Lasso           | Group-wise selection                       | Grouped variables               |
| Fused Lasso           | Penalizes differences between coefficients | Time-series or spatial data     |
| Square-root Lasso     | Scale-free loss function                    | Unknown error variance          |
| Bayesian Lasso        | Laplace priors in a Bayesian framework     | Posterior inference, uncertainty|
| Graphical Lasso       | Penalizes inverse covariance matrix        | Gaussian graphical models       |
| Relaxed Lasso         | Selection and estimation separated         | Bias reduction after selection  |
| Stability Selection   | Adds subsampling for robustness            | Controls false discoveries      |

## Bottom Line

- Different Lasso variants address different modeling challenges like scaling, grouping, collinearity, and bias.

- Understanding your data structure and inference goals helps choose the right Lasso flavor.


- Most major machine learning libraries (R: `glmnet`, `grpreg`; Python: `scikit-learn`, `statsmodels`) provide support for these variants.

## Where to Learn More

Key papers: Tibshirani (1996), Zou (2006), Meinshausen (2007), Yuan and Lin (2006), Belloni et al. (2011), Park and Casella (2008), Friedman et al. (2008), Meinshausen and Bühlmann (2010).

## References

[TO ADD]