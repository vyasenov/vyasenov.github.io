{"title":"The Limits of Nonparametric Models","markdown":{"yaml":{"title":"The Limits of Nonparametric Models","date":"2025-01-22","categories":["statistical inference","nonparametric models"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nNonparametric statistics offers a powerful toolkit for data analysis when the underlying data-generating process is too complex or unknown to be captured by parametric models. Unlike parametric methods, which assume a specific form for the underlying distribution or functional relationship, nonparametric approaches let the data “speak for itself.”\n\nIn earlier articles, we explored parametric and semiparametric models, where the focus was on attaining the lowest possible variance. However, in nonparametric statistics, this goal is unsuitable, as prioritizing minimal variance often introduces significant bias. The spotlight falls instead on selecting the optimal tuning parameter: the bandwidth. This article explains the various considerations and mathematical details associated with selecting the optimal bandwidth value in Kernel Density (KD) and Kernel Regression (KR) estimation.\n\n## Notation\n\nWe begin by establishing the mathematical terminology and notations essential for our analysis.\n\n### Kernels\n\nA kernel function $K(\\cdot)$ is a non-negative, symmetric function that integrates to $1$ over its domain. It acts as a weighting function that determines how observations contribute to the estimate at any given point, effectively spreading the mass of each data point over a local neighborhood.\n\n### Kernel Density\n\nKD is sort of like creating a smooth histogram. The kernel density estimator for a sample of n observations $X_1, X_2, \\dots, X_n \\in \\mathbb{R}$ is given by:\n\n  $$\\hat{f}_h (x) = \\frac{1}{n h} \\sum_i K\\left(\\frac{x - X_i}{h}\\right),$$\n\nwhere $h > 0$ is the bandwidth, a positive scalar controlling the smoothness of the estimate. Its function is to determine the window length around x in which the observations get positive weight.\n\nThe value $\\hat{f}_h (x)$ will be large when there are many data points around $x$, and small otherwise. Some popular kernel functions like Epanechnikov, Gaussian or triangular assign higher weights to observations closer to $x$ and decaying importance to data further away.\n\nDensity estimation can also be performed in higher dimensions where the curse of dimensionality lurks in the background. In two dimensions, these estimates typically take the form of heat map-type figures.\n\n### Kernel Regression\n\nKernel regression, also known as local regression, is much like fitting a flexible smooth curve through data points. In this context, we have access to $n$ observations of an outcome variable $Y$. The objective is to estimate the conditional mean function at some point X=x:\n\n$$m(x) = \\mathbb{E}[Y \\mid X = x].$$\n\nAmong various kernel regression methods, the Nadaraya-Watson (NW) estimator is particularly popular. It is defined as:\n\n$$\\hat{m}_h (x) = \\frac{\\sum_i K\\left(\\frac{x - X_i}{h}\\right) Y_i}{\\sum_i K\\left(\\frac{x - X_i}{h}\\right)}.$$\n\nThe NW method fits a local constant around $x$ equal to the average $Y$ in that region (determined by the bandwidth). This approach is also known as local constant regression. More sophisticated variants include local linear and quadratic methods, which extend this fundamental idea but require more data. Across all these approaches, the bandwidth $h$ serves as the key parameter that determines the trade-off between bias and variance, a relationship we will explore in detail below.\n\n## A Closer Look\n\nIn practice, the choice of kernel is not as consequential. The primary focus of nonparametric statistics is selecting the optimal bandwidth $h$.\n\n### The Bias-Variance Tradeoff\n\nAt the heart of bandwidth selection is the bias-variance tradeoff. Intuitively:\n\n- Small bandwidth (“overfitting”): Captures fine details but also noise, leading to high variance and low bias.\n- Large bandwidth (“underfitting”): Smoothes over the noise but can miss important structure, leading to high bias and low variance.\n- For nonparametric curve estimation, the mean integrated squared error (MISE) is a common loss function for evaluating performance:\n\n$$\\text{MISE}(h) = \\mathbb{E}\\left[ \\int \\left( \\hat{f}_h(x) - f(x) \\right)^2 dx \\right].$$\n\nThis decomposes into bias and variance terms:\n\n $$\\text{MISE}(h) =  \\int \\text{Bias}^2(\\hat{f}_h(x)) dx + \\int \\text{Var}(\\hat{f}_h(x)) dx.$$\n\nMISE is analogous to the mean squared error (MSE) commonly used in machine learning, but integrated, or summed, across the support of the data. Minimizing the MISE yields the optimal bandwidth by balancing the bias-variance tradeoff, a fundamental concept in statistical estimation theory.\n\n### Optimal $h$ for Kernel Density\n\nSilverman’s rule of thumb provides a practical, closed-form approximation for $h$ in KDE, assuming the data is roughly Gaussian. The expression is:\n\n$$h^* = \\left( \\frac{4\\sigma^5}{3n} \\right) ^{\\frac{1}{5}}  = 1.06 \\sigma n^{-1/5},$$\n\nwhere $\\sigma$ is the standard deviation of the data. This formula emerges from theoretical analysis of the bias-variance tradeoff and serves as an effective initial bandwidth choice. While it performs well for many datasets, particularly those with approximately normal distributions, it may not be optimal for multimodal or heavily skewed distributions.\n\n### Optimal $h$ for Kernel Regression\n\nFor Nadaraya-Watson regression, several methods exist for selecting the bandwidth h. Cross-validation provides reliable results but can be computationally demanding. Theoretical analysis based on MISE minimization for common kernel functions suggests that the optimal bandwidth follows the relationship:\n\n$$h^* \\propto n^{-1/5}.$$\n\nThis expression highlights a fundamental characteristic of nonparametric methods: their convergence rate is slower than that of parametric methods. This represents one manifestation of the curse of dimensionality in nonparametric estimation. Furthermore, this relationship only specifies the rate at which the optimal bandwidth should decrease with sample size (proportionality to $n^{-1/5}$), rather than providing an exact value. Without the proportionality constant, its direct practical application is limited.\n\n## An Example\n\nLet’s illustrate bandwidth selection with a simple example.\n\n::: {.panel-tabset}\n\n### R\n\n```r\n# clear workspace\nrm(list=ls())\n\n# load data\nrequire(graphics)\n\n# plot\nwith(cars, {\n    plot(speed, dist)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 2), col = 2)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 5), col = 3)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 10), col = 4)\n})\n```\n\n### Python\n\n```python\n# same idea, but with the iris dataset\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom statsmodels.nonparametric.kernel_regression import KernelReg\n\n# Load iris dataset\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n# Choose two continuous variables\nx = df['sepal length (cm)'].values\ny = df['petal length (cm)'].values\n\n# Define a function to plot kernel smoothed lines with varying bandwidths\ndef plot_kernel_smoothing(x, y, bandwidths):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(x, y, alpha=0.5, label='Data', color='black')\n\n    x_grid = np.linspace(x.min(), x.max(), 200)\n    \n    colors = ['red', 'green', 'blue']\n    for bw, color in zip(bandwidths, colors):\n        kr = KernelReg(endog=[y], exog=[x], var_type='c', bw=[bw])\n        mean, _ = kr.fit(x_grid)\n        plt.plot(x_grid, mean, label=f'bandwidth={bw}', color=color)\n\n    plt.xlabel('Sepal Length (cm)')\n    plt.ylabel('Petal Length (cm)')\n    plt.title('Kernel Smoothing with Varying Bandwidths')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Call function with different bandwidths\nplot_kernel_smoothing(x, y, bandwidths=[0.2, 0.5, 1.0])\n```\n:::\n\nThis yields a simplified version of the following figure:\n\n![](../images/nonparametric.png)\n\nThe green line corresponds to the Nadaraya-Watson regression with an optimal bandwidth and features some degree of curvature. The red line has a smaller bandwidth (higher variance, lower bias), and the blue line is undersmoothed (higher bandwidth).\n\n## Bottom Line\n\n- Bandwidth selection is critical for nonparametric methods to balance bias and variance.\n\n- Silverman’s rule of thumb offers a simple yet effective starting point for KD.\n\n- For commonly used second-order kernels, the optimal bandwidth in KR scales as $n^{-1/5}$.\n\n- Practical implementation often relies on cross-validation or plug-in methods and is limited by the curse of dimensionality.\n\n## References\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.\n\nLi, Q., & Racine, J. S. (2023). Nonparametric econometrics: theory and practice. Princeton University Press.\n\nSilverman, B. W. (1986). Density Estimation for Statistics and Data Analysis. Wand, M. P., & Jones, M. C. (1995). Kernel Smoothing.\n\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media.\n","srcMarkdownNoYaml":"\n\n## Background\n\nNonparametric statistics offers a powerful toolkit for data analysis when the underlying data-generating process is too complex or unknown to be captured by parametric models. Unlike parametric methods, which assume a specific form for the underlying distribution or functional relationship, nonparametric approaches let the data “speak for itself.”\n\nIn earlier articles, we explored parametric and semiparametric models, where the focus was on attaining the lowest possible variance. However, in nonparametric statistics, this goal is unsuitable, as prioritizing minimal variance often introduces significant bias. The spotlight falls instead on selecting the optimal tuning parameter: the bandwidth. This article explains the various considerations and mathematical details associated with selecting the optimal bandwidth value in Kernel Density (KD) and Kernel Regression (KR) estimation.\n\n## Notation\n\nWe begin by establishing the mathematical terminology and notations essential for our analysis.\n\n### Kernels\n\nA kernel function $K(\\cdot)$ is a non-negative, symmetric function that integrates to $1$ over its domain. It acts as a weighting function that determines how observations contribute to the estimate at any given point, effectively spreading the mass of each data point over a local neighborhood.\n\n### Kernel Density\n\nKD is sort of like creating a smooth histogram. The kernel density estimator for a sample of n observations $X_1, X_2, \\dots, X_n \\in \\mathbb{R}$ is given by:\n\n  $$\\hat{f}_h (x) = \\frac{1}{n h} \\sum_i K\\left(\\frac{x - X_i}{h}\\right),$$\n\nwhere $h > 0$ is the bandwidth, a positive scalar controlling the smoothness of the estimate. Its function is to determine the window length around x in which the observations get positive weight.\n\nThe value $\\hat{f}_h (x)$ will be large when there are many data points around $x$, and small otherwise. Some popular kernel functions like Epanechnikov, Gaussian or triangular assign higher weights to observations closer to $x$ and decaying importance to data further away.\n\nDensity estimation can also be performed in higher dimensions where the curse of dimensionality lurks in the background. In two dimensions, these estimates typically take the form of heat map-type figures.\n\n### Kernel Regression\n\nKernel regression, also known as local regression, is much like fitting a flexible smooth curve through data points. In this context, we have access to $n$ observations of an outcome variable $Y$. The objective is to estimate the conditional mean function at some point X=x:\n\n$$m(x) = \\mathbb{E}[Y \\mid X = x].$$\n\nAmong various kernel regression methods, the Nadaraya-Watson (NW) estimator is particularly popular. It is defined as:\n\n$$\\hat{m}_h (x) = \\frac{\\sum_i K\\left(\\frac{x - X_i}{h}\\right) Y_i}{\\sum_i K\\left(\\frac{x - X_i}{h}\\right)}.$$\n\nThe NW method fits a local constant around $x$ equal to the average $Y$ in that region (determined by the bandwidth). This approach is also known as local constant regression. More sophisticated variants include local linear and quadratic methods, which extend this fundamental idea but require more data. Across all these approaches, the bandwidth $h$ serves as the key parameter that determines the trade-off between bias and variance, a relationship we will explore in detail below.\n\n## A Closer Look\n\nIn practice, the choice of kernel is not as consequential. The primary focus of nonparametric statistics is selecting the optimal bandwidth $h$.\n\n### The Bias-Variance Tradeoff\n\nAt the heart of bandwidth selection is the bias-variance tradeoff. Intuitively:\n\n- Small bandwidth (“overfitting”): Captures fine details but also noise, leading to high variance and low bias.\n- Large bandwidth (“underfitting”): Smoothes over the noise but can miss important structure, leading to high bias and low variance.\n- For nonparametric curve estimation, the mean integrated squared error (MISE) is a common loss function for evaluating performance:\n\n$$\\text{MISE}(h) = \\mathbb{E}\\left[ \\int \\left( \\hat{f}_h(x) - f(x) \\right)^2 dx \\right].$$\n\nThis decomposes into bias and variance terms:\n\n $$\\text{MISE}(h) =  \\int \\text{Bias}^2(\\hat{f}_h(x)) dx + \\int \\text{Var}(\\hat{f}_h(x)) dx.$$\n\nMISE is analogous to the mean squared error (MSE) commonly used in machine learning, but integrated, or summed, across the support of the data. Minimizing the MISE yields the optimal bandwidth by balancing the bias-variance tradeoff, a fundamental concept in statistical estimation theory.\n\n### Optimal $h$ for Kernel Density\n\nSilverman’s rule of thumb provides a practical, closed-form approximation for $h$ in KDE, assuming the data is roughly Gaussian. The expression is:\n\n$$h^* = \\left( \\frac{4\\sigma^5}{3n} \\right) ^{\\frac{1}{5}}  = 1.06 \\sigma n^{-1/5},$$\n\nwhere $\\sigma$ is the standard deviation of the data. This formula emerges from theoretical analysis of the bias-variance tradeoff and serves as an effective initial bandwidth choice. While it performs well for many datasets, particularly those with approximately normal distributions, it may not be optimal for multimodal or heavily skewed distributions.\n\n### Optimal $h$ for Kernel Regression\n\nFor Nadaraya-Watson regression, several methods exist for selecting the bandwidth h. Cross-validation provides reliable results but can be computationally demanding. Theoretical analysis based on MISE minimization for common kernel functions suggests that the optimal bandwidth follows the relationship:\n\n$$h^* \\propto n^{-1/5}.$$\n\nThis expression highlights a fundamental characteristic of nonparametric methods: their convergence rate is slower than that of parametric methods. This represents one manifestation of the curse of dimensionality in nonparametric estimation. Furthermore, this relationship only specifies the rate at which the optimal bandwidth should decrease with sample size (proportionality to $n^{-1/5}$), rather than providing an exact value. Without the proportionality constant, its direct practical application is limited.\n\n## An Example\n\nLet’s illustrate bandwidth selection with a simple example.\n\n::: {.panel-tabset}\n\n### R\n\n```r\n# clear workspace\nrm(list=ls())\n\n# load data\nrequire(graphics)\n\n# plot\nwith(cars, {\n    plot(speed, dist)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 2), col = 2)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 5), col = 3)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 10), col = 4)\n})\n```\n\n### Python\n\n```python\n# same idea, but with the iris dataset\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom statsmodels.nonparametric.kernel_regression import KernelReg\n\n# Load iris dataset\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n# Choose two continuous variables\nx = df['sepal length (cm)'].values\ny = df['petal length (cm)'].values\n\n# Define a function to plot kernel smoothed lines with varying bandwidths\ndef plot_kernel_smoothing(x, y, bandwidths):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(x, y, alpha=0.5, label='Data', color='black')\n\n    x_grid = np.linspace(x.min(), x.max(), 200)\n    \n    colors = ['red', 'green', 'blue']\n    for bw, color in zip(bandwidths, colors):\n        kr = KernelReg(endog=[y], exog=[x], var_type='c', bw=[bw])\n        mean, _ = kr.fit(x_grid)\n        plt.plot(x_grid, mean, label=f'bandwidth={bw}', color=color)\n\n    plt.xlabel('Sepal Length (cm)')\n    plt.ylabel('Petal Length (cm)')\n    plt.title('Kernel Smoothing with Varying Bandwidths')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Call function with different bandwidths\nplot_kernel_smoothing(x, y, bandwidths=[0.2, 0.5, 1.0])\n```\n:::\n\nThis yields a simplified version of the following figure:\n\n![](../images/nonparametric.png)\n\nThe green line corresponds to the Nadaraya-Watson regression with an optimal bandwidth and features some degree of curvature. The red line has a smaller bandwidth (higher variance, lower bias), and the blue line is undersmoothed (higher bandwidth).\n\n## Bottom Line\n\n- Bandwidth selection is critical for nonparametric methods to balance bias and variance.\n\n- Silverman’s rule of thumb offers a simple yet effective starting point for KD.\n\n- For commonly used second-order kernels, the optimal bandwidth in KR scales as $n^{-1/5}$.\n\n- Practical implementation often relies on cross-validation or plug-in methods and is limited by the curse of dimensionality.\n\n## References\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.\n\nLi, Q., & Racine, J. S. (2023). Nonparametric econometrics: theory and practice. Princeton University Press.\n\nSilverman, B. W. (1986). Density Estimation for Statistics and Data Analysis. Wand, M. P., & Jones, M. C. (1995). Kernel Smoothing.\n\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"filters":["code-insertion"],"output-file":"limits-nonparametric-models.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js","../code/back-to-top.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>\n<script src=\"../code/open-links-new-tab.js\"></script>  \n<script src=\"../code/back-to-top.js\"></script>\n<link href=\"https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Code+Pro&display=swap\" rel=\"stylesheet\">\n"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html",{"text":"<button id=\"back-to-top\" onclick=\"scrollToTop()\">↑</button>\n"}]},"insert-before-post":"_sharebuttons.md","title":"The Limits of Nonparametric Models","date":"2025-01-22","categories":["statistical inference","nonparametric models"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}