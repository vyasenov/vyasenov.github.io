{"title":"A Brief Introduction to Conformal Inference","markdown":{"yaml":{"title":"A Brief Introduction to Conformal Inference","date":"2023-12-20","categories":["machine learning"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nTraditional confidence intervals estimate the range in which a population parameter, such as a mean or regression coefficient, is likely to fall with a specified level of confidence (e.g., 95%). They reflect the uncertainty in estimating a fixed but unknown parameter based on observing only a sample of the population. But what if we are interested in measuring uncertainty in the context of making predictions? Can we use traditional methods, or do we need a new framework?\n\nPrediction intervals provide a range within which a future individual observation is expected to fall with a given probability. Since they account for both the uncertainty in estimating the mean and the inherent variability of individual observations, they are typically wider than confidence intervals. While a confidence interval narrows as sample size increases, a prediction interval remains relatively wide due to the irreducible noise in individual data points. For simplicity, I will use both terms interchangeably.\n\nConformal inference offers a formalized framework for building such prediction intervals in machine learning models. This article provides a gentle introduction to the main idea behind conformal inference, presenting a new way of thinking about uncertainty in the context of machine learning (i.e., prediction) models.\n\nAs a teaser, the basic idea behind the method rests on a simple result about sample quantiles. Let me explain.\n\n## Notation \n\nLet’s imagine a size n i.i.d. sample of an outcome variable $Y$ and a covariate vector $X$, $(X_1, Y_1) \\dots (X_n, Y_n)$. Conformal inference is concerned with building a “confidence interval” for a new outcome observation $Y_{n+1}$ from a new feature realization $X_{n+1}$.\n\nImportantly, this interval should be valid:\n\n- in finite samples (i.e., non-asymptotically),\n- without assumptions on the data generating process, and\n- for any estimator of the regression function, $\\mu(x)=E[Y \\mid X=x]$.\n\nIn mathematical notation, given a significance level \\alpha, we want to construct a confidence interval $CI(X_{n+1})$ satisfying the above properties and such that:\n\n  $$P(Y_{n+1} \\in CI(X_{n+1})) \\geq 1-\\alpha.$$\n\n## A Closer Look\n\n### Refresher on Sample Quantiles\n\nI will start with reviewing sample quantiles. Given an i.i.d. sample, $U_1, \\dots, U_n$, the ($1-\\alpha$)th quantile is the value $\\hat{q}_{1-\\alpha}$ such that approximately $(1-\\alpha)\\times100\\%$ of the data is smaller than it. For instance, the $95$th quantile (sometimes also called percentile) is the value for which $95\\%$ of the observations are at least as small.\n\nSo, given a new observation $U_{n+1}$, we know that:\n\n  $$P(U_{n+1}\\leq \\hat{q}_{1-\\alpha})\\geq 1-\\alpha.$$\n\n### The Naïve Approach\n\nLet’s turn back to the regression example with Y and X. We are given a new observation $X_{n+1}$ and our focus is on $Y_{n+1}$. Following the fact described above, a naïve way to construct a confidence interval for $Y_{n+1}$ is as follows:\n\n  $$CI^{\\text{naïve}}_{1-\\alpha}(X_{n+1}) = \\left[ \\hat{\\mu}(X_{n+1}) \\pm \\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha} \\right].$$\n\nHere $\\mu(\\cdot)$ is an estimate of the regression function $E[Y \\mid X]$, and $\\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha}$ is the $(1-\\alpha)$th quantile of empirical distribution function of the fitted residuals $\\mid Y-\\hat{\\mu}(X) \\mid$.\n\nPut simply, we can look at an interval around our best prediction for $Y_{n+1}$ (i.e., $\\hat{\\mu}(X_{n+1})$) defined by the residuals estimated on the original data.\n\nIt turns out this interval is too narrow. In a series of papers Vladimir Vovk and co-authors show that the empirical distribution function of the fitted residuals is often biased downward and hence this interval is invalid. This is where conformal inference comes in.\n\n### Conformal Inference\n\nConsider the following strategy. For each $y$ we fit a regression $\\hat{\\mu}_y $ on the sample $(Y_1, X_1),\\dots (Y_n, X_n), (y, X_{n+1})$. We calculate the residuals $R^y_i$ for $i=1,\\dots,n$ and $R^y_{n+1}$ and count the proportion of $R^y_i$’s smaller than $R^y_{n+1}$. Let’s call this number $\\sigma(y)$. That is,\n\n$$\\sigma(y) = \\frac{1}{n+1}\\sum_{i=1}^{n+1} I (R^y_i \\leq R^y_{n+1}),$$\n\nwhere $I(\\cdot)$ is the indicator function equal to one when the statement in the parenthesis is true and 0 if when it is not.\n\nThe test statistic $\\sigma({Y_{n+1}})$ is uniformly distributed over the set $\\{ \\frac{1}{n+1}, \\frac{2}{n+1},\\dots, 1\\}$, implying we can use $1-\\sigma({Y_{n+1}})$ as a valid p-value for testing the null that $Y_{n+1}=y.$ Then, using the sample quantiles logic outlined above we arrive at the following confidence interval for $Y_{n+1}$:\n\n  $$ CI^{\\text{conformal}}_{1-\\alpha}(X_{n+1}) \\approx \\{ y\\in \\mathbb{R} : \\sigma(y)\\leq 1-\\alpha \\}.$$\n\nThis is summarized in the following procedure:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. For each value $y$:\n- Fit the regression function $\\mu(\\cdot)$ on $(X_1, Y_1), \\dots, (X_n, Y_n), (X_{n+1}, y)$ using your favorite estimator/learner.\n- Calculate the $n+1$ residuals.\n- Calculate the proportion $\\sigma(y)$.\n2. Construct $CI = \\{y: \\sigma(y) \\leq (1-\\alpha)\\}$.\n:::\n\n*Software Package*: [conformalInference](https://github.com/ryantibs/conformal)\n\nTwo notes. First, conformal inference guarantees unconditional coverage. This is conceptually different and should not be confused with the conditional statement $P(Y_{n+1}\\in CI(x) \\mid X_{n+1}=x)\\geq 1-\\alpha$. The latter is stronger and more difficult to assert, requiring additional assumptions such as consistency of our estimator of $\\mu(\\cdot)$.\n\nSecond, this procedure can be computationally expensive. For a given value $X_{n+1}$ we need to fit a regression model and compute residuals for every $y$ which we consider including in the confidence interval. This is where split conformal inference comes in.\n\n### Split Conformal Inference\n\nSplit conformal inference is a modification of the original algorithm that requires significantly less computation power. The idea is to split the fitting and ranking steps, so that the former is done only once. Here is the algorithm.\n\n::: {.callout-note title=\"Algorithm:\"}\n\n1. Randomly split the data in two equal-sized bins.\n2. Get $\\hat{\\mu}$ on the first bin.\n3. Calculate the residuals for each observation in the second bin.\n4. Let $d$ be the $s$-th smallest residual, where $s=(\\frac{n}{2}+1)(1-\\alpha)$.\n5. Construct $CI^{\\text{split}}=[\\hat{\\mu}-d,\\hat{\\mu}+d]$.\n:::\n\nA downside of this splitting approach is the introduction of extra randomness. One way to mitigate this is to perform the split multiple times and construct a final confidence interval by taking the intersection of all intervals. The aggregation decreases the variability from a single data split and, as [this paper](https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1307116) shows, still remains valid. Similar random split aggregation [has also been used](https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.tm08647) in the context of statistical significance in high-dimensional models.\n\n## An Example\n\nI used the popular `iris` dataset to try out the `R` package `conformalInference`. Like most of my data demos, this is meant to be a mere illustration and you should not take the results seriously.\n\nThe outcome variable was `Sepal.Length`, and the matrix $X$ included `sepal.width`, `petal.length`, `petal.width`, `species_setosa`, `species_versicolor`, and `species_virginica`. Some of these were categorical in which case I converted them to a bunch of binary variables. I used the first $148$ observations to estimate the regression function $\\mu(X)$ using lasso and the $149$th row to form the prediction (i.e., the test set).\n\nHere is the code.\n\n```r\n# clear workspace and load packages\nrm(list=ls())\nlibrary(conformalInference)\nlibrary(glmnet)\nlibrary(tidyverse)\n\n# load the iris dataset\ndata <- iris\n\n# clean data\ncolnames(data) <- tolower(colnames(data))\n\n# one-hot encode the species variable\ndata <- data %>%\n  mutate(species_setosa = as.integer(species == 'setosa'),\n         species_versicolor = as.integer(species == 'versicolor'),\n         species_virginica = as.integer(species == 'virginica')) %>%\n  dplyr::select(-species)\n\n# check for missing values (none in iris, but included for completeness)\ndata <- na.omit(data)\n\n# split training/test data\ndata0 <- data %>% filter(row_number() == nrow(data))  # last row as test set\ndata <- data %>% filter(row_number() < nrow(data))   # remaining rows as training set\n\n# select variables X, Y\ny <- data$sepal.length  # target variable\nx <- data %>% dplyr::select(-sepal.length)  # predictors\nx <- as.matrix(x)\nx0 <- data0 %>% dplyr::select(-sepal.length)  # test predictors\nx0 <- as.matrix(x0)\nn <- nrow(x)\n\n# use lasso to estimate mu\nout.gnet = glmnet(x, y, nlambda=100, lambda.min.ratio=1e-3)\nlambda = min(out.gnet$lambda)\nfuns = lasso.funs(lambda=lambda)\n\n# run conformal inference\nout.conf = conformal.pred(x, y, x0, \n                          alpha=0.1,\n                          train.fun=funs$train, \n                          predict.fun=funs$predict, \n                          verb=TRUE)\n\n# run split conformal inference\nout.split = conformal.pred.split(x, y, x0, \n                                 alpha=0.1,\n                                 train.fun=funs$train, \n                                 predict.fun=funs$predict, \n                                 verb=TRUE)\n\n# print results\npaste('The lower bound is', out.conf$lo, 'and the upper bound is', out.conf$up)\n> [1] \"The lower bound is 5.89 and the upper bound is 6.68\"\nout.conf$pred\n>         [,1]\n> [1,] 6.316882\n\n# print results for split conformal inference\npaste('The lower bound is', out.split$lo, 'and the upper bound is', out.split$up)\n> [1] \"The lower bound is 5.74 and the upper bound is 6.93\"\nout.split$pred\n>          [,1]\n> [1,] 6.33556\n```\n\nThe actual age value in the test set was $6.2$ while the conformal inference approach computed a confidence interval ($5.88, 6.68$). The splitting algorithm gave similar results.\n\n## Bottom Line\n\n- Conformal inference offers a novel approach for constructing valid finite-sample prediction intervals in machine learning models.\n\n## Where to Learn More\n\nConformal inference in machine learning is an ongoing research topic and I do not know of any review papers or textbook treatments of the subject. If you are interested in learning more, check the paper referenced below.\n\n## References\n\nLei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094-1111.\n\nLei, J., Rinaldo, A., & Wasserman, L. (2015). A conformal prediction approach to explore functional data. Annals of Mathematics and Artificial Intelligence, 74, 29-43.\n\nShafer, G., & Vovk, V. (2008). A Tutorial on Conformal Prediction. Journal of Machine Learning Research, 9(3).\n\nVovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world (Vol. 29). New York: Springer.","srcMarkdownNoYaml":"\n\n## Background\n\nTraditional confidence intervals estimate the range in which a population parameter, such as a mean or regression coefficient, is likely to fall with a specified level of confidence (e.g., 95%). They reflect the uncertainty in estimating a fixed but unknown parameter based on observing only a sample of the population. But what if we are interested in measuring uncertainty in the context of making predictions? Can we use traditional methods, or do we need a new framework?\n\nPrediction intervals provide a range within which a future individual observation is expected to fall with a given probability. Since they account for both the uncertainty in estimating the mean and the inherent variability of individual observations, they are typically wider than confidence intervals. While a confidence interval narrows as sample size increases, a prediction interval remains relatively wide due to the irreducible noise in individual data points. For simplicity, I will use both terms interchangeably.\n\nConformal inference offers a formalized framework for building such prediction intervals in machine learning models. This article provides a gentle introduction to the main idea behind conformal inference, presenting a new way of thinking about uncertainty in the context of machine learning (i.e., prediction) models.\n\nAs a teaser, the basic idea behind the method rests on a simple result about sample quantiles. Let me explain.\n\n## Notation \n\nLet’s imagine a size n i.i.d. sample of an outcome variable $Y$ and a covariate vector $X$, $(X_1, Y_1) \\dots (X_n, Y_n)$. Conformal inference is concerned with building a “confidence interval” for a new outcome observation $Y_{n+1}$ from a new feature realization $X_{n+1}$.\n\nImportantly, this interval should be valid:\n\n- in finite samples (i.e., non-asymptotically),\n- without assumptions on the data generating process, and\n- for any estimator of the regression function, $\\mu(x)=E[Y \\mid X=x]$.\n\nIn mathematical notation, given a significance level \\alpha, we want to construct a confidence interval $CI(X_{n+1})$ satisfying the above properties and such that:\n\n  $$P(Y_{n+1} \\in CI(X_{n+1})) \\geq 1-\\alpha.$$\n\n## A Closer Look\n\n### Refresher on Sample Quantiles\n\nI will start with reviewing sample quantiles. Given an i.i.d. sample, $U_1, \\dots, U_n$, the ($1-\\alpha$)th quantile is the value $\\hat{q}_{1-\\alpha}$ such that approximately $(1-\\alpha)\\times100\\%$ of the data is smaller than it. For instance, the $95$th quantile (sometimes also called percentile) is the value for which $95\\%$ of the observations are at least as small.\n\nSo, given a new observation $U_{n+1}$, we know that:\n\n  $$P(U_{n+1}\\leq \\hat{q}_{1-\\alpha})\\geq 1-\\alpha.$$\n\n### The Naïve Approach\n\nLet’s turn back to the regression example with Y and X. We are given a new observation $X_{n+1}$ and our focus is on $Y_{n+1}$. Following the fact described above, a naïve way to construct a confidence interval for $Y_{n+1}$ is as follows:\n\n  $$CI^{\\text{naïve}}_{1-\\alpha}(X_{n+1}) = \\left[ \\hat{\\mu}(X_{n+1}) \\pm \\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha} \\right].$$\n\nHere $\\mu(\\cdot)$ is an estimate of the regression function $E[Y \\mid X]$, and $\\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha}$ is the $(1-\\alpha)$th quantile of empirical distribution function of the fitted residuals $\\mid Y-\\hat{\\mu}(X) \\mid$.\n\nPut simply, we can look at an interval around our best prediction for $Y_{n+1}$ (i.e., $\\hat{\\mu}(X_{n+1})$) defined by the residuals estimated on the original data.\n\nIt turns out this interval is too narrow. In a series of papers Vladimir Vovk and co-authors show that the empirical distribution function of the fitted residuals is often biased downward and hence this interval is invalid. This is where conformal inference comes in.\n\n### Conformal Inference\n\nConsider the following strategy. For each $y$ we fit a regression $\\hat{\\mu}_y $ on the sample $(Y_1, X_1),\\dots (Y_n, X_n), (y, X_{n+1})$. We calculate the residuals $R^y_i$ for $i=1,\\dots,n$ and $R^y_{n+1}$ and count the proportion of $R^y_i$’s smaller than $R^y_{n+1}$. Let’s call this number $\\sigma(y)$. That is,\n\n$$\\sigma(y) = \\frac{1}{n+1}\\sum_{i=1}^{n+1} I (R^y_i \\leq R^y_{n+1}),$$\n\nwhere $I(\\cdot)$ is the indicator function equal to one when the statement in the parenthesis is true and 0 if when it is not.\n\nThe test statistic $\\sigma({Y_{n+1}})$ is uniformly distributed over the set $\\{ \\frac{1}{n+1}, \\frac{2}{n+1},\\dots, 1\\}$, implying we can use $1-\\sigma({Y_{n+1}})$ as a valid p-value for testing the null that $Y_{n+1}=y.$ Then, using the sample quantiles logic outlined above we arrive at the following confidence interval for $Y_{n+1}$:\n\n  $$ CI^{\\text{conformal}}_{1-\\alpha}(X_{n+1}) \\approx \\{ y\\in \\mathbb{R} : \\sigma(y)\\leq 1-\\alpha \\}.$$\n\nThis is summarized in the following procedure:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. For each value $y$:\n- Fit the regression function $\\mu(\\cdot)$ on $(X_1, Y_1), \\dots, (X_n, Y_n), (X_{n+1}, y)$ using your favorite estimator/learner.\n- Calculate the $n+1$ residuals.\n- Calculate the proportion $\\sigma(y)$.\n2. Construct $CI = \\{y: \\sigma(y) \\leq (1-\\alpha)\\}$.\n:::\n\n*Software Package*: [conformalInference](https://github.com/ryantibs/conformal)\n\nTwo notes. First, conformal inference guarantees unconditional coverage. This is conceptually different and should not be confused with the conditional statement $P(Y_{n+1}\\in CI(x) \\mid X_{n+1}=x)\\geq 1-\\alpha$. The latter is stronger and more difficult to assert, requiring additional assumptions such as consistency of our estimator of $\\mu(\\cdot)$.\n\nSecond, this procedure can be computationally expensive. For a given value $X_{n+1}$ we need to fit a regression model and compute residuals for every $y$ which we consider including in the confidence interval. This is where split conformal inference comes in.\n\n### Split Conformal Inference\n\nSplit conformal inference is a modification of the original algorithm that requires significantly less computation power. The idea is to split the fitting and ranking steps, so that the former is done only once. Here is the algorithm.\n\n::: {.callout-note title=\"Algorithm:\"}\n\n1. Randomly split the data in two equal-sized bins.\n2. Get $\\hat{\\mu}$ on the first bin.\n3. Calculate the residuals for each observation in the second bin.\n4. Let $d$ be the $s$-th smallest residual, where $s=(\\frac{n}{2}+1)(1-\\alpha)$.\n5. Construct $CI^{\\text{split}}=[\\hat{\\mu}-d,\\hat{\\mu}+d]$.\n:::\n\nA downside of this splitting approach is the introduction of extra randomness. One way to mitigate this is to perform the split multiple times and construct a final confidence interval by taking the intersection of all intervals. The aggregation decreases the variability from a single data split and, as [this paper](https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1307116) shows, still remains valid. Similar random split aggregation [has also been used](https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.tm08647) in the context of statistical significance in high-dimensional models.\n\n## An Example\n\nI used the popular `iris` dataset to try out the `R` package `conformalInference`. Like most of my data demos, this is meant to be a mere illustration and you should not take the results seriously.\n\nThe outcome variable was `Sepal.Length`, and the matrix $X$ included `sepal.width`, `petal.length`, `petal.width`, `species_setosa`, `species_versicolor`, and `species_virginica`. Some of these were categorical in which case I converted them to a bunch of binary variables. I used the first $148$ observations to estimate the regression function $\\mu(X)$ using lasso and the $149$th row to form the prediction (i.e., the test set).\n\nHere is the code.\n\n```r\n# clear workspace and load packages\nrm(list=ls())\nlibrary(conformalInference)\nlibrary(glmnet)\nlibrary(tidyverse)\n\n# load the iris dataset\ndata <- iris\n\n# clean data\ncolnames(data) <- tolower(colnames(data))\n\n# one-hot encode the species variable\ndata <- data %>%\n  mutate(species_setosa = as.integer(species == 'setosa'),\n         species_versicolor = as.integer(species == 'versicolor'),\n         species_virginica = as.integer(species == 'virginica')) %>%\n  dplyr::select(-species)\n\n# check for missing values (none in iris, but included for completeness)\ndata <- na.omit(data)\n\n# split training/test data\ndata0 <- data %>% filter(row_number() == nrow(data))  # last row as test set\ndata <- data %>% filter(row_number() < nrow(data))   # remaining rows as training set\n\n# select variables X, Y\ny <- data$sepal.length  # target variable\nx <- data %>% dplyr::select(-sepal.length)  # predictors\nx <- as.matrix(x)\nx0 <- data0 %>% dplyr::select(-sepal.length)  # test predictors\nx0 <- as.matrix(x0)\nn <- nrow(x)\n\n# use lasso to estimate mu\nout.gnet = glmnet(x, y, nlambda=100, lambda.min.ratio=1e-3)\nlambda = min(out.gnet$lambda)\nfuns = lasso.funs(lambda=lambda)\n\n# run conformal inference\nout.conf = conformal.pred(x, y, x0, \n                          alpha=0.1,\n                          train.fun=funs$train, \n                          predict.fun=funs$predict, \n                          verb=TRUE)\n\n# run split conformal inference\nout.split = conformal.pred.split(x, y, x0, \n                                 alpha=0.1,\n                                 train.fun=funs$train, \n                                 predict.fun=funs$predict, \n                                 verb=TRUE)\n\n# print results\npaste('The lower bound is', out.conf$lo, 'and the upper bound is', out.conf$up)\n> [1] \"The lower bound is 5.89 and the upper bound is 6.68\"\nout.conf$pred\n>         [,1]\n> [1,] 6.316882\n\n# print results for split conformal inference\npaste('The lower bound is', out.split$lo, 'and the upper bound is', out.split$up)\n> [1] \"The lower bound is 5.74 and the upper bound is 6.93\"\nout.split$pred\n>          [,1]\n> [1,] 6.33556\n```\n\nThe actual age value in the test set was $6.2$ while the conformal inference approach computed a confidence interval ($5.88, 6.68$). The splitting algorithm gave similar results.\n\n## Bottom Line\n\n- Conformal inference offers a novel approach for constructing valid finite-sample prediction intervals in machine learning models.\n\n## Where to Learn More\n\nConformal inference in machine learning is an ongoing research topic and I do not know of any review papers or textbook treatments of the subject. If you are interested in learning more, check the paper referenced below.\n\n## References\n\nLei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094-1111.\n\nLei, J., Rinaldo, A., & Wasserman, L. (2015). A conformal prediction approach to explore functional data. Annals of Mathematics and Artificial Intelligence, 74, 29-43.\n\nShafer, G., & Vovk, V. (2008). A Tutorial on Conformal Prediction. Journal of Machine Learning Research, 9(3).\n\nVovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world (Vol. 29). New York: Springer."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"filters":["code-insertion"],"output-file":"conformal-inference.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html"]},"insert-before-post":"_sharebuttons.md","title":"A Brief Introduction to Conformal Inference","date":"2023-12-20","categories":["machine learning"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}