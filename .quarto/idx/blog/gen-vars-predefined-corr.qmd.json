{"title":"Generating Variables with Predefined Correlation","markdown":{"yaml":{"title":"Generating Variables with Predefined Correlation","date":"2024-12-20","categories":["correlation"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nSuppose you are working on a project where the relationship between two variables is influenced by an unobserved confounder, and you want to simulate data that reflects this dependency. Standard random number generators often assume independence between variables, making them unsuitable for this task. Instead, you need a method to introduce specific correlations into your data generation process.\n\nA powerful and efficient way to achieve this is through Cholesky decomposition. By decomposing a correlation matrix into its triangular components, you can transform independent random variables into correlated ones. This approach is versatile, efficient, and mathematically grounded, making it ideal for simulating realistic datasets with predefined (linear) relationships.\n\n## A Closer Look\n\n### The Algorithm\n\nAssume we want to generate a vector Y with n observations and p variables with a target correlation matrix $\\Sigma$. The algorithm to obtain $Y$ is as follows:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. **Start with Independent Variables**: Create a matrix $X$ of dimensions $n \\times p$, where each column is independently drawn from N(0,1):\n  $$ X = \\begin{bmatrix}x_{11} & x_{12} & \\cdots & x_{1p} \\\\x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\x_{n1} & x_{n2} & \\cdots & x_{np}.\\end{bmatrix} $$\n2. **Decompose the Target Matrix**: Perform Cholesky decomposition on the target correlation matrix $\\Sigma$ as:\n  $$\\Sigma = LL^T,$$\nwhere $L$ is a lower triangular matrix.\n3. **Transform the Independent Variables**: Multiply the independent variable matrix $X$ by $L$ to obtain the correlated variables:\n  $$Y = XL.$$\n:::\n\nHere $Y$ is an $n\\times p$ matrix where the columns have the desired correlation structure defined by $\\Sigma$. To ensure that $\\Sigma$ is a valid correlation matrix, it must be positive-definite. This condition guarantees the success of Cholesky decomposition and the correctness of the resulting correlated variables.\n\n### Mathematical Explanation\n\nLet’s examine how and why this approach works. We know that $\\Sigma = LL^T$ and $E(XX^T)=I$ by definition. We want to show that $E(YY^T)=LL^T$. Here is the simplest way to get there:\n\n\\begin{align*}\nE(YY^T) &= E((LX)(LX)^T) \\\\\n        &= E(LXX^TL^T) \\\\\n        &= LE(XX^T)L^T \\\\\n        &= LL^T.\n\\end{align*}\n\nThere you have it – the algorithm outlined above is mathematically grounded. The covariance matrix of $Y$ is indeed equal to $\\Sigma$. Let’s now look at an example.\n\n## An Example\n\nLet’s implement this in `R` and `python` with $p=3$ and $n=1,000$. Our target correlation matrix defines the desired relationships between the variables in $Y$. In our example, we have pairwise correlations equal to $0.8$ (b/w $y_1$ and $y_2$), $0.5$ (b/w $y_1$ and $y_3$), and $0.3$ (b/w $y_2$ and $y_3$).\n\n:::{.panel-tabset}\n\n## R \n\n```r\nrm(list=ls())\nset.seed(1988)\n\n# Generate X, independent standard normal variables\nn <- 1000 \np <- 3   \nx <- matrix(rnorm(n * p), nrow = n, ncol = p)\n\n# Define Sigma, the target correlation matrix\nsigma <- matrix(c(\n  1.0, 0.8, 0.5,\n  0.8, 1.0, 0.3,\n  0.5, 0.3, 1.0\n), nrow = p, byrow = TRUE)\n\n# Cholesky decomposition\nL <- t(chol(sigma))\ndiag <- diag(c(1,1,1))\ny <- t(diag %*% L %*% t(x))\n\n# Print the results\nprint(cor(y))\n          [,1]      [,2]      [,3]\n[1,] 1.0000000 0.7875707 0.5111323\n[2,] 0.7875707 1.0000000 0.3008518\n[3,] 0.5111323 0.3008518 1.0000000\n```\n\n## Python\n\n```python\nimport numpy as np\nnp.random.seed(1988)\n\n# Generate X, independent standard normal variables\nn = 1000\np = 3\nx = np.random.normal(size=(n, p))\n\n# Define Sigma, the target correlation matrix\nsigma = np.array([\n    [1.0, 0.8, 0.5],\n    [0.8, 1.0, 0.3],\n    [0.5, 0.3, 1.0]\n])\n\n# Cholesky decomposition\nL = np.linalg.cholesky(sigma)\ndiag = np.diag([1, 1, 1])\ny = (diag @ L @ x.T).T\n\n# Print results\n[[1.         0.78702913 0.48132289]\n [0.78702913 1.         0.27758356]\n [0.48132289 0.27758356 1.        ]]\n```\n\n:::\n\nUsing our notation above we have:\n\n$$\\Sigma = \\begin{bmatrix}1.0 & 0.8 & 0.5 \\\\0.8 & 1.0 & 0.3 \\\\ 0.5 & 0.3 &1.0\\end{bmatrix}. $$\n\nThe chol function in `R` decomposes the matrix into a lower triangular matrix. In our example:\n\n$$L^T = \\begin{bmatrix}1 & 0.8 & 0.5 \\\\0 & 0.6 & -0.17 \\\\0 & 0.0 & 0.85 \\end{bmatrix}. $$\n\nMultiplying the independent variables $X$ by the transpose of $L$ ensures the output $Y$ matches the specified correlation structure.\n\nThe `cor` function checks whether the generated data conforms to the target correlation matrix.\n\nThe two matrices match almost exactly. We can also visualize the three variables in a scatter plot matrix. Notice that higher correlation values (e.g., b/w $y_1$ and $y_2$) correspond to stronger linear associations between.\n\n![](../images/scatterplot_matrix.png)\n\n## Bottom Line\n\n- A common data practitioner’s need is to generate variables with a predefined correlation structure.\n\n- Cholesky decomposition offers a powerful and efficient way to achieve this.","srcMarkdownNoYaml":"\n\n## Background\n\nSuppose you are working on a project where the relationship between two variables is influenced by an unobserved confounder, and you want to simulate data that reflects this dependency. Standard random number generators often assume independence between variables, making them unsuitable for this task. Instead, you need a method to introduce specific correlations into your data generation process.\n\nA powerful and efficient way to achieve this is through Cholesky decomposition. By decomposing a correlation matrix into its triangular components, you can transform independent random variables into correlated ones. This approach is versatile, efficient, and mathematically grounded, making it ideal for simulating realistic datasets with predefined (linear) relationships.\n\n## A Closer Look\n\n### The Algorithm\n\nAssume we want to generate a vector Y with n observations and p variables with a target correlation matrix $\\Sigma$. The algorithm to obtain $Y$ is as follows:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. **Start with Independent Variables**: Create a matrix $X$ of dimensions $n \\times p$, where each column is independently drawn from N(0,1):\n  $$ X = \\begin{bmatrix}x_{11} & x_{12} & \\cdots & x_{1p} \\\\x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\x_{n1} & x_{n2} & \\cdots & x_{np}.\\end{bmatrix} $$\n2. **Decompose the Target Matrix**: Perform Cholesky decomposition on the target correlation matrix $\\Sigma$ as:\n  $$\\Sigma = LL^T,$$\nwhere $L$ is a lower triangular matrix.\n3. **Transform the Independent Variables**: Multiply the independent variable matrix $X$ by $L$ to obtain the correlated variables:\n  $$Y = XL.$$\n:::\n\nHere $Y$ is an $n\\times p$ matrix where the columns have the desired correlation structure defined by $\\Sigma$. To ensure that $\\Sigma$ is a valid correlation matrix, it must be positive-definite. This condition guarantees the success of Cholesky decomposition and the correctness of the resulting correlated variables.\n\n### Mathematical Explanation\n\nLet’s examine how and why this approach works. We know that $\\Sigma = LL^T$ and $E(XX^T)=I$ by definition. We want to show that $E(YY^T)=LL^T$. Here is the simplest way to get there:\n\n\\begin{align*}\nE(YY^T) &= E((LX)(LX)^T) \\\\\n        &= E(LXX^TL^T) \\\\\n        &= LE(XX^T)L^T \\\\\n        &= LL^T.\n\\end{align*}\n\nThere you have it – the algorithm outlined above is mathematically grounded. The covariance matrix of $Y$ is indeed equal to $\\Sigma$. Let’s now look at an example.\n\n## An Example\n\nLet’s implement this in `R` and `python` with $p=3$ and $n=1,000$. Our target correlation matrix defines the desired relationships between the variables in $Y$. In our example, we have pairwise correlations equal to $0.8$ (b/w $y_1$ and $y_2$), $0.5$ (b/w $y_1$ and $y_3$), and $0.3$ (b/w $y_2$ and $y_3$).\n\n:::{.panel-tabset}\n\n## R \n\n```r\nrm(list=ls())\nset.seed(1988)\n\n# Generate X, independent standard normal variables\nn <- 1000 \np <- 3   \nx <- matrix(rnorm(n * p), nrow = n, ncol = p)\n\n# Define Sigma, the target correlation matrix\nsigma <- matrix(c(\n  1.0, 0.8, 0.5,\n  0.8, 1.0, 0.3,\n  0.5, 0.3, 1.0\n), nrow = p, byrow = TRUE)\n\n# Cholesky decomposition\nL <- t(chol(sigma))\ndiag <- diag(c(1,1,1))\ny <- t(diag %*% L %*% t(x))\n\n# Print the results\nprint(cor(y))\n          [,1]      [,2]      [,3]\n[1,] 1.0000000 0.7875707 0.5111323\n[2,] 0.7875707 1.0000000 0.3008518\n[3,] 0.5111323 0.3008518 1.0000000\n```\n\n## Python\n\n```python\nimport numpy as np\nnp.random.seed(1988)\n\n# Generate X, independent standard normal variables\nn = 1000\np = 3\nx = np.random.normal(size=(n, p))\n\n# Define Sigma, the target correlation matrix\nsigma = np.array([\n    [1.0, 0.8, 0.5],\n    [0.8, 1.0, 0.3],\n    [0.5, 0.3, 1.0]\n])\n\n# Cholesky decomposition\nL = np.linalg.cholesky(sigma)\ndiag = np.diag([1, 1, 1])\ny = (diag @ L @ x.T).T\n\n# Print results\n[[1.         0.78702913 0.48132289]\n [0.78702913 1.         0.27758356]\n [0.48132289 0.27758356 1.        ]]\n```\n\n:::\n\nUsing our notation above we have:\n\n$$\\Sigma = \\begin{bmatrix}1.0 & 0.8 & 0.5 \\\\0.8 & 1.0 & 0.3 \\\\ 0.5 & 0.3 &1.0\\end{bmatrix}. $$\n\nThe chol function in `R` decomposes the matrix into a lower triangular matrix. In our example:\n\n$$L^T = \\begin{bmatrix}1 & 0.8 & 0.5 \\\\0 & 0.6 & -0.17 \\\\0 & 0.0 & 0.85 \\end{bmatrix}. $$\n\nMultiplying the independent variables $X$ by the transpose of $L$ ensures the output $Y$ matches the specified correlation structure.\n\nThe `cor` function checks whether the generated data conforms to the target correlation matrix.\n\nThe two matrices match almost exactly. We can also visualize the three variables in a scatter plot matrix. Notice that higher correlation values (e.g., b/w $y_1$ and $y_2$) correspond to stronger linear associations between.\n\n![](../images/scatterplot_matrix.png)\n\n## Bottom Line\n\n- A common data practitioner’s need is to generate variables with a predefined correlation structure.\n\n- Cholesky decomposition offers a powerful and efficient way to achieve this."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"filters":["code-insertion"],"output-file":"gen-vars-predefined-corr.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js","../code/back-to-top.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>\n<script src=\"../code/open-links-new-tab.js\"></script>  \n<script src=\"../code/back-to-top.js\"></script>\n<link href=\"https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Code+Pro&display=swap\" rel=\"stylesheet\">\n"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html",{"text":"<button id=\"back-to-top\" onclick=\"scrollToTop()\">↑</button>\n"}]},"insert-before-post":"_sharebuttons.md","title":"Generating Variables with Predefined Correlation","date":"2024-12-20","categories":["correlation"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}