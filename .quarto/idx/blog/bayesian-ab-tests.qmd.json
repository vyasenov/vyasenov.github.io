{"title":"Bayesian Analysis of Randomized Experiments: A Modern Approach","markdown":{"yaml":{"title":"Bayesian Analysis of Randomized Experiments: A Modern Approach","date":"2024-10-29","categories":["bayesian methods","experimentation"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nImagine you’re a data scientist evaluating an A/B test of a new recommendation algorithm. The results show a modest but promising 0.5% lift in conversion rate—up from $8\\%$ to $8.5\\%$ across $20,000$ users—along with a $p$-value of $0.06$. Given historical data, the improvement seems realistic, the cost to implement is low, and the projected revenue impact is substantial. Yet, the conventional analysis compels you to “fail to reject the null hypothesis” simply because the p-value doesn’t meet an arbitrary $0.05$ threshold.\n\nTraditional A/B testing approaches (randomized controlled trials or RCTs) rely primarily on frequentist methods like $t$-tests or chi-squared tests to detect differences between treatment and control groups. For more precision, analysts often adjust for covariates in linear models. After weeks or months of testing, however, these methods often boil down to a single outcome: the $p$-value. This binary interpretation—significant or not—can obscure valuable insights, as seen in the example above.\n\nBayesian statistics, by contrast, offers a more nuanced and potentially more informative alternative. In this article, we’ll walk through a practical example, discuss the implementation details, and dive into the Bayesian framework’s mathematical foundations for experimentation. One of the core advantages of the Bayesian approach is its ability to produce a full posterior distribution, offering a richer view of the experiment’s outcomes beyond just point estimates.\n\nThis article assumes a familiarity with Bayesian fundamentals like priors, posteriors, and conjugate distributions. If you’d like a refresher, see the References section below.\n\n## Notation\n\nLet’s establish our notation for a binary intervention randomized experiment:\n\n- $T \\in {0,1}$: Treatment indicator\n- $N_T$: Number of units in treatment group\n- $N_C$: Number of units in control group\n- $N = N_T + N_C$: Total sample size\n- $X_T$: Number of “successes” in treatment group\n- $X_C$: Number of “successes” in control group\n- $Y$: Success rate (e.g., conversion rate, employment status).\n\n## A Closer Look\n\nWe are interested in making inferences about the treatment effect, $\\tau$, of the intervention $T$ on the success rate $Y$. In Bayesian terms, this means we seek the posterior distribution of $\\tau$. Once we obtain this distribution or can draw observations from it, we can calculate various statistics to summarize the impact of the intervention, potentially providing a richer understanding of its effects.\n\nThe Bayesian methodology of analyzing experiments proceeds in four steps.\n\n### Step 1: Specify the Prior Distribution\n\nWe begin by choosing the prior distributions for Y. The prior distribution represents our beliefs about the parameter before seeing the data. It is usually informed by previous A/B testing experience combined with deep context knowledge. For example, we might believe that the treatment effect is around two percentage points ($\\hat{\\tau}=0.02$) with some uncertainty around it.\n\nGiven our binary outcome, the Beta distribution serves as a natural conjugate prior:\n  $$ Y_i \\sim \\text{Beta} (\\alpha_i, \\beta_i), \\hspace{.5cm} i \\in \\{T,C\\}.  $$\n\n### Step 2: Specify the Likelihood Function\n\nFor binary outcomes, we model the data using a Binomial distribution:\n\n  $$X_i|Y_i \\sim \\text{Binomial}(N_i, Y_i), \\hspace{.5cm} i\\in\\{T,C\\}.  $$\n\nThis choice reflects the inherent structure of our data: counting successes in a fixed number of trials. We are now ready to use the Bayes rule to arrive at the posterior distributions.\n\n### Step 3: Posterior Distributions Derivation\n\nThanks to conjugacy between Beta and Binomial distributions, we obtain closed-form posterior distributions:\n\n  $$ Y_i | X_i \\sim \\text{Beta}(\\alpha_i+X_i, \\beta_i+N_i-X_i), \\hspace{.5cm} i\\in\\{T,C\\}  .$$\n\nThis mathematical convenience allows us to avoid more complex numerical methods like MCMC sampling. We can now even plot the two posterior distributions and visually asses their differences.\n\n### Step 4: Inference and Decision Making\n\nThe Bayesian framework enables rich analysis beyond traditional frequentist-style hypothesis testing. Here are some examples:\n\n*Probability of Positive Impact*\n\nWe can calculate the probability that the outcomes for the treatment group are higher than those of the control group. In closed form, we have:\n\n  $$ P(Y_T>Y_C|X_T,X_C) = \\frac{1}{N}\\sum_i\\mathbf{1}(Y_{T,i}>Y_{C,i}),$$\n\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. This is simply share of observations for which $Y_T >Y_C$.\n\n*Average Treatment Effect*\n\nAnother potential example of an object of interest is the Average Treatment Effect (ATE):\n\n  $$ ATE = \\frac{1}{N_T}\\sum_{i \\in T}Y_{T,i} -  \\frac{1}{N_C}\\sum_{i \\in C}Y_{C,i} \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} $$\n\n  $$ ATE \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} $$\n\nTo summarize, here is the high-level algorithm:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Specify the Prior Distribution\n2. Specify the Likelihood Function\n3. Posterior Distributions Derivation\n4. Inference and Decision Making\n:::\n\n\nAnd that’s it. You see how with the cost of additional assumptions we get much more than a single $p$-value, and that’s where much of the appeal of this approach lies.\n\n### Advantages\n\nOverall, Bayesian thinking entails some compelling advantages:\n\n- **Incorporation of Prior Information**. It allows you to incorporate prior knowledge or expert opinion into the analysis. This is particularly useful when historical data or domain expertise is available.\n- **Probabilistic Interpretation**: It provides direct probabilistic interpretations of the results, such as the probability that one variant is better than another. This may be more intuitive than frequentist p-values, which are often misinterpreted.\n- **Handling of Small Sample Sizes**: It tends to perform better with small sample sizes because they incorporate prior distributions, which can regularize estimates and prevent overfitting.\n- **Continuous Learning**: As new data comes in, Bayesian methods provide a natural way to update the posterior distribution, leading to continuous learning and adaptation.\n\nThe downsides are mostly related to the choice of prior distribution. In settings where there is a lack of expert knowledge, this Bayesian approach to experimentation might not be very attractive. Computation can also be an issue in more complex settings.\n\n## An Example\n\nLet’s code an example in `R` and `python`. We start with generating some fake data and selecting parameters for the prior distributions.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Data: Number of successes and total observations for T and C\nn_T <- 1000\nx_T <- 70\nn_C <- 900\nx_C <- 50\n\n# Prior parameters for the Beta distribution\nalpha_T <- 1\nbeta_T <- 1\nalpha_C <- 1\nbeta_C <- 1\n\n# Posterior parameters\nposterior_alpha_T <- alpha_T + x_T\nposterior_beta_T <- beta_T + n_T - x_T\nposterior_alpha_C <- alpha_C + x_C\nposterior_beta_C <- beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T <- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C <- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Sample from the posterior distributions\nposterior_obs_T <- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C <- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Estimate the probability that T is better than C\nprob_T_better <- mean(posterior_obs_T > posterior_obs_C)\nprint(paste(\"Probability that T is better than C:\", round(prob_T_better, digit=3)))\n\n# Estimate the average treatment effect\ntreatment_effect <- mean(posterior_obs_T - posterior_obs_C)\nprint(paste(\"Average change in Y b/w T and C:\", round(treatment_effect, digit=3)))\n\ntreatment_effect_limit <- (alpha_T + x_T)/(alpha_T + beta_T + n_T) - (alpha_C + x_C)/(alpha_C + beta_C + n_C)\nprint(treatment_effect_limit)\n\n# we can even plot both posteriors distributions.\ndf <- data.frame(\n  y = c(posterior_obs_T, posterior_obs_C),\n  group = factor(rep(c(\"T\", \"C\"), each = 10000))\n)\n\nggplot(df, aes(x = y, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Posterior Distributions of Outcomes\",\n       x = \"Y = 1\", y = \"Density\") +\n  theme_minimal()\n```\n\n### Python\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.random.seed(1988)\n\n# Data: Number of successes and total observations for T and C\nn_T = 1000\nx_T = 70\nn_C = 900\nx_C = 50\n\n# Prior parameters for the Beta distribution\nalpha_T = 1\nbeta_T = 1\nalpha_C = 1\nbeta_C = 1\n\n# Posterior parameters\nposterior_alpha_T = alpha_T + x_T\nposterior_beta_T = beta_T + n_T - x_T\nposterior_alpha_C = alpha_C + x_C\nposterior_beta_C = beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T = np.random.beta(posterior_alpha_T, posterior_beta_T, 10000)\nposterior_obs_C = np.random.beta(posterior_alpha_C, posterior_beta_C, 10000)\n\n# Estimate the probability that T is better than C\nprob_T_better = np.mean(posterior_obs_T > posterior_obs_C)\nprint(f\"Probability that T is better than C: {prob_T_better:.3f}\")\n\n# Estimate the average treatment effect\ntreatment_effect = np.mean(posterior_obs_T - posterior_obs_C)\nprint(f\"Average change in Y b/w T and C: {treatment_effect:.3f}\")\n\n# Plot posterior distributions\nplt.figure(figsize=(8, 6))\nsns.kdeplot(posterior_obs_T, fill=True, label=\"T\", alpha=0.5)\nsns.kdeplot(posterior_obs_C, fill=True, label=\"C\", alpha=0.5)\nplt.title(\"Posterior Distributions of Outcomes\")\nplt.xlabel(\"Y = 1\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n```\n\n::::\n\nHere are the two posterior distributions.\n\n![](../images/bayesAB_posteriors.png)\n\nThere is also a specialized `bayesAB` package in `R`. It produces some cool charts, so I definitely recommend giving it a try.\n\n*Software Package*: [bayesAB](https://www.rdocumentation.org/packages/bayesAB/).\n\n## Bottom Line\n\n- Bayesian inference offers a compelling alternative to the traditional methods based on frequentist statistics.\n\n- The main idea rests on incorporating prior information on the success rates in the treatment and control group as a starting point.\n\n- Advantages of bayesian methods include incorporating prior information, providing probabilistic interpretations, handling small sample sizes better, and enabling continuous learning.\n\n- The main challenge is the choice of prior distribution, which can be difficult without expert knowledge.\n\n## Where to Learn More\n\nAccessible introductions to the world of Bayesian inference are common. An example is Will Kurt’s book “[Bayesian Statistics the Fun Way](https://www.amazon.com/Bayesian-Statistics-Fun-Will-Kurt/dp/1593279566)“. See also the papers I cite below. As almost everything else, Google is also a great starting point.\n\n## References\n\nDeng, A. (2015, May). Objective bayesian two sample hypothesis testing for online controlled experiments. In Proceedings of the 24th International Conference on World Wide Web (pp. 923-928).\n\nKamalbasha, S., & Eugster, M. J. (2021). Bayesian A/B testing for business decisions. In Data Science–Analytics and Applications: Proceedings of the 3rd International Data Science Conference–iDSC2020 (pp. 50-57). Springer Fachmedien Wiesbaden.\n\nKurt, Will. Bayesian statistics the fun way: understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. No Starch Press, 2019.\n\nStevens, N. T., & Hagar, L. (2022). Comparative probability metrics: using posterior probabilities to account for practical equivalence in A/B tests. The American Statistician, 76(3), 224-237.","srcMarkdownNoYaml":"\n\n## Background\n\nImagine you’re a data scientist evaluating an A/B test of a new recommendation algorithm. The results show a modest but promising 0.5% lift in conversion rate—up from $8\\%$ to $8.5\\%$ across $20,000$ users—along with a $p$-value of $0.06$. Given historical data, the improvement seems realistic, the cost to implement is low, and the projected revenue impact is substantial. Yet, the conventional analysis compels you to “fail to reject the null hypothesis” simply because the p-value doesn’t meet an arbitrary $0.05$ threshold.\n\nTraditional A/B testing approaches (randomized controlled trials or RCTs) rely primarily on frequentist methods like $t$-tests or chi-squared tests to detect differences between treatment and control groups. For more precision, analysts often adjust for covariates in linear models. After weeks or months of testing, however, these methods often boil down to a single outcome: the $p$-value. This binary interpretation—significant or not—can obscure valuable insights, as seen in the example above.\n\nBayesian statistics, by contrast, offers a more nuanced and potentially more informative alternative. In this article, we’ll walk through a practical example, discuss the implementation details, and dive into the Bayesian framework’s mathematical foundations for experimentation. One of the core advantages of the Bayesian approach is its ability to produce a full posterior distribution, offering a richer view of the experiment’s outcomes beyond just point estimates.\n\nThis article assumes a familiarity with Bayesian fundamentals like priors, posteriors, and conjugate distributions. If you’d like a refresher, see the References section below.\n\n## Notation\n\nLet’s establish our notation for a binary intervention randomized experiment:\n\n- $T \\in {0,1}$: Treatment indicator\n- $N_T$: Number of units in treatment group\n- $N_C$: Number of units in control group\n- $N = N_T + N_C$: Total sample size\n- $X_T$: Number of “successes” in treatment group\n- $X_C$: Number of “successes” in control group\n- $Y$: Success rate (e.g., conversion rate, employment status).\n\n## A Closer Look\n\nWe are interested in making inferences about the treatment effect, $\\tau$, of the intervention $T$ on the success rate $Y$. In Bayesian terms, this means we seek the posterior distribution of $\\tau$. Once we obtain this distribution or can draw observations from it, we can calculate various statistics to summarize the impact of the intervention, potentially providing a richer understanding of its effects.\n\nThe Bayesian methodology of analyzing experiments proceeds in four steps.\n\n### Step 1: Specify the Prior Distribution\n\nWe begin by choosing the prior distributions for Y. The prior distribution represents our beliefs about the parameter before seeing the data. It is usually informed by previous A/B testing experience combined with deep context knowledge. For example, we might believe that the treatment effect is around two percentage points ($\\hat{\\tau}=0.02$) with some uncertainty around it.\n\nGiven our binary outcome, the Beta distribution serves as a natural conjugate prior:\n  $$ Y_i \\sim \\text{Beta} (\\alpha_i, \\beta_i), \\hspace{.5cm} i \\in \\{T,C\\}.  $$\n\n### Step 2: Specify the Likelihood Function\n\nFor binary outcomes, we model the data using a Binomial distribution:\n\n  $$X_i|Y_i \\sim \\text{Binomial}(N_i, Y_i), \\hspace{.5cm} i\\in\\{T,C\\}.  $$\n\nThis choice reflects the inherent structure of our data: counting successes in a fixed number of trials. We are now ready to use the Bayes rule to arrive at the posterior distributions.\n\n### Step 3: Posterior Distributions Derivation\n\nThanks to conjugacy between Beta and Binomial distributions, we obtain closed-form posterior distributions:\n\n  $$ Y_i | X_i \\sim \\text{Beta}(\\alpha_i+X_i, \\beta_i+N_i-X_i), \\hspace{.5cm} i\\in\\{T,C\\}  .$$\n\nThis mathematical convenience allows us to avoid more complex numerical methods like MCMC sampling. We can now even plot the two posterior distributions and visually asses their differences.\n\n### Step 4: Inference and Decision Making\n\nThe Bayesian framework enables rich analysis beyond traditional frequentist-style hypothesis testing. Here are some examples:\n\n*Probability of Positive Impact*\n\nWe can calculate the probability that the outcomes for the treatment group are higher than those of the control group. In closed form, we have:\n\n  $$ P(Y_T>Y_C|X_T,X_C) = \\frac{1}{N}\\sum_i\\mathbf{1}(Y_{T,i}>Y_{C,i}),$$\n\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. This is simply share of observations for which $Y_T >Y_C$.\n\n*Average Treatment Effect*\n\nAnother potential example of an object of interest is the Average Treatment Effect (ATE):\n\n  $$ ATE = \\frac{1}{N_T}\\sum_{i \\in T}Y_{T,i} -  \\frac{1}{N_C}\\sum_{i \\in C}Y_{C,i} \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} $$\n\n  $$ ATE \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} $$\n\nTo summarize, here is the high-level algorithm:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Specify the Prior Distribution\n2. Specify the Likelihood Function\n3. Posterior Distributions Derivation\n4. Inference and Decision Making\n:::\n\n\nAnd that’s it. You see how with the cost of additional assumptions we get much more than a single $p$-value, and that’s where much of the appeal of this approach lies.\n\n### Advantages\n\nOverall, Bayesian thinking entails some compelling advantages:\n\n- **Incorporation of Prior Information**. It allows you to incorporate prior knowledge or expert opinion into the analysis. This is particularly useful when historical data or domain expertise is available.\n- **Probabilistic Interpretation**: It provides direct probabilistic interpretations of the results, such as the probability that one variant is better than another. This may be more intuitive than frequentist p-values, which are often misinterpreted.\n- **Handling of Small Sample Sizes**: It tends to perform better with small sample sizes because they incorporate prior distributions, which can regularize estimates and prevent overfitting.\n- **Continuous Learning**: As new data comes in, Bayesian methods provide a natural way to update the posterior distribution, leading to continuous learning and adaptation.\n\nThe downsides are mostly related to the choice of prior distribution. In settings where there is a lack of expert knowledge, this Bayesian approach to experimentation might not be very attractive. Computation can also be an issue in more complex settings.\n\n## An Example\n\nLet’s code an example in `R` and `python`. We start with generating some fake data and selecting parameters for the prior distributions.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Data: Number of successes and total observations for T and C\nn_T <- 1000\nx_T <- 70\nn_C <- 900\nx_C <- 50\n\n# Prior parameters for the Beta distribution\nalpha_T <- 1\nbeta_T <- 1\nalpha_C <- 1\nbeta_C <- 1\n\n# Posterior parameters\nposterior_alpha_T <- alpha_T + x_T\nposterior_beta_T <- beta_T + n_T - x_T\nposterior_alpha_C <- alpha_C + x_C\nposterior_beta_C <- beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T <- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C <- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Sample from the posterior distributions\nposterior_obs_T <- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C <- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Estimate the probability that T is better than C\nprob_T_better <- mean(posterior_obs_T > posterior_obs_C)\nprint(paste(\"Probability that T is better than C:\", round(prob_T_better, digit=3)))\n\n# Estimate the average treatment effect\ntreatment_effect <- mean(posterior_obs_T - posterior_obs_C)\nprint(paste(\"Average change in Y b/w T and C:\", round(treatment_effect, digit=3)))\n\ntreatment_effect_limit <- (alpha_T + x_T)/(alpha_T + beta_T + n_T) - (alpha_C + x_C)/(alpha_C + beta_C + n_C)\nprint(treatment_effect_limit)\n\n# we can even plot both posteriors distributions.\ndf <- data.frame(\n  y = c(posterior_obs_T, posterior_obs_C),\n  group = factor(rep(c(\"T\", \"C\"), each = 10000))\n)\n\nggplot(df, aes(x = y, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Posterior Distributions of Outcomes\",\n       x = \"Y = 1\", y = \"Density\") +\n  theme_minimal()\n```\n\n### Python\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.random.seed(1988)\n\n# Data: Number of successes and total observations for T and C\nn_T = 1000\nx_T = 70\nn_C = 900\nx_C = 50\n\n# Prior parameters for the Beta distribution\nalpha_T = 1\nbeta_T = 1\nalpha_C = 1\nbeta_C = 1\n\n# Posterior parameters\nposterior_alpha_T = alpha_T + x_T\nposterior_beta_T = beta_T + n_T - x_T\nposterior_alpha_C = alpha_C + x_C\nposterior_beta_C = beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T = np.random.beta(posterior_alpha_T, posterior_beta_T, 10000)\nposterior_obs_C = np.random.beta(posterior_alpha_C, posterior_beta_C, 10000)\n\n# Estimate the probability that T is better than C\nprob_T_better = np.mean(posterior_obs_T > posterior_obs_C)\nprint(f\"Probability that T is better than C: {prob_T_better:.3f}\")\n\n# Estimate the average treatment effect\ntreatment_effect = np.mean(posterior_obs_T - posterior_obs_C)\nprint(f\"Average change in Y b/w T and C: {treatment_effect:.3f}\")\n\n# Plot posterior distributions\nplt.figure(figsize=(8, 6))\nsns.kdeplot(posterior_obs_T, fill=True, label=\"T\", alpha=0.5)\nsns.kdeplot(posterior_obs_C, fill=True, label=\"C\", alpha=0.5)\nplt.title(\"Posterior Distributions of Outcomes\")\nplt.xlabel(\"Y = 1\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n```\n\n::::\n\nHere are the two posterior distributions.\n\n![](../images/bayesAB_posteriors.png)\n\nThere is also a specialized `bayesAB` package in `R`. It produces some cool charts, so I definitely recommend giving it a try.\n\n*Software Package*: [bayesAB](https://www.rdocumentation.org/packages/bayesAB/).\n\n## Bottom Line\n\n- Bayesian inference offers a compelling alternative to the traditional methods based on frequentist statistics.\n\n- The main idea rests on incorporating prior information on the success rates in the treatment and control group as a starting point.\n\n- Advantages of bayesian methods include incorporating prior information, providing probabilistic interpretations, handling small sample sizes better, and enabling continuous learning.\n\n- The main challenge is the choice of prior distribution, which can be difficult without expert knowledge.\n\n## Where to Learn More\n\nAccessible introductions to the world of Bayesian inference are common. An example is Will Kurt’s book “[Bayesian Statistics the Fun Way](https://www.amazon.com/Bayesian-Statistics-Fun-Will-Kurt/dp/1593279566)“. See also the papers I cite below. As almost everything else, Google is also a great starting point.\n\n## References\n\nDeng, A. (2015, May). Objective bayesian two sample hypothesis testing for online controlled experiments. In Proceedings of the 24th International Conference on World Wide Web (pp. 923-928).\n\nKamalbasha, S., & Eugster, M. J. (2021). Bayesian A/B testing for business decisions. In Data Science–Analytics and Applications: Proceedings of the 3rd International Data Science Conference–iDSC2020 (pp. 50-57). Springer Fachmedien Wiesbaden.\n\nKurt, Will. Bayesian statistics the fun way: understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. No Starch Press, 2019.\n\nStevens, N. T., & Hagar, L. (2022). Comparative probability metrics: using posterior probabilities to account for practical equivalence in A/B tests. The American Statistician, 76(3), 224-237."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"bayesian-ab-tests.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Bayesian Analysis of Randomized Experiments: A Modern Approach","date":"2024-10-29","categories":["bayesian methods","experimentation"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}