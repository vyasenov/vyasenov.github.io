{"title":"Lord’s Paradox: A Simple Illustration","markdown":{"yaml":{"title":"Lord’s Paradox: A Simple Illustration","date":"2022-12-18","categories":["correlation","paradox"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nLord’s paradox presents a fascinating challenge in causal inference and statistics. It highlights how different statistical methods applied to the same data can lead to contradictory conclusions. The paradox typically arises when comparing changes over time between two groups in the context of adjusting for baseline characteristics. Despite its theoretical nature, the phenomena is deeply relevant to practical data analysis, especially in observational studies where causation is challenging to establish. Let’s look at an example.\n\n## A Closer Look\n\n### Mean Differences Over Time\n\nTo explore Lord’s paradox, consider the following scenario: Suppose we have two groups of individuals—$A$ and $B$—with their body weights measured at two time points: before and after a specific intervention. Let the weight at the initial time point be denoted as $W_{\\text{pre}}$, and the weight at the final time point be $W_{\\text{post}}$. We are interested in whether the intervention caused a change in weight between the two groups.\n\nOne approach to analyze this is to compute the (unadjusted) mean weight change for each group over time:\n\n$$\\Delta = \\Delta^A - \\Delta^B.$$\n\nIf this quantity is statistically significant, we might conclude that the intervention had a differential effect.\n\n### Controlling for Baseline Characteristics\n\nAn alternative approach involves adjusting for baseline weight $W_{\\text{pre}}$ using, for example, a regression model:\n\n  $$W_{\\text{post}}=\\beta_1 + \\beta_2 G_A + \\beta_3 W_{\\text{pre}} + \\epsilon,$$\n\nwhere $G$ is a binary indicator for group $A$ membership and $\\epsilon$ is an error term. Here, $\\beta_2$ captures the group difference in $W_{\\text{post}}$, linearly controlling for baseline body weight.\n\nSurprisingly, these two approaches can yield conflicting results. For example, the former method might suggest no difference, while the regression adjustment indicates a significant group effect.\n\n### Explanation\n\nThis contradiction arises because the two methods implicitly address different causal questions.\n\n- **Method 1** asks: “Do Groups $A$ and $B$ gain/lose different amounts of weight?”\n- **Method 2** asks: “Given the same initial weight, does any of the groups end up at different final weights?”\nThe regression approach adjusts for baseline differences, assuming $W_{\\text{pre}}$ is a confounder.\n\nThe key insight is that the choice of analysis reflects underlying assumptions about the causal structure of the data. If $W_{\\text{pre}}$ is affected by group membership (e.g., due to selection bias), then adjusting for it may introduce bias rather than remove it. However, in practice we most often should control for baseline characteristics as they are critical in balancing the treatment and control groups.\n\n### The Simpson’s Paradox Once Again\n\nI recently illustrated the more commonly discussed Simpson’s paradox. Interestingly, a [2008 paper](https://link.springer.com/article/10.1186/1742-7622-5-2) claims that two phenomena are closely related, with the Lord’s paradox being a “continuous version” of Simpson’s paradox.\n\n## An Example\n\nLet’s look at some code illustrating Lord’s paradox in `R` and `python`. We start with simulating a dataset where two groups have identical distributions of $W_{\\text{pre}}$ and $W_{\\text{post}}$, yet differing relationships between the two variables.\n\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\nn <- 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup <- factor(rep(c(\"A\", \"B\"), each = n/2))\n\n# Initial weight (pre). # Group A starts with higher average weight\nweight_pre <- numeric(n)\nweight_pre[group == \"A\"] <- rnorm(n/2, mean = 75, sd = 10)\nweight_pre[group == \"B\"] <- rnorm(n/2, mean = 65, sd = 10)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain <- rnorm(n, mean = 10, sd = 5)\nweight_post <- weight_pre + gain\n\n# Create data frame\ndata <- data.frame(group = group, pre = weight_pre, post = weight_post)\n\n# Analysis 1: Compare change scores between groups`\nt.test(post - pre ~ group, data = data)\n> p-value = 0.6107\n\n# Analysis 2: Regression Adjustment`\nmodel <- lm(post ~ group + pre, data = data)\nsummary(model)\n> p-value = 0.08428742\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Set seed for reproducibility\nnp.random.seed(1988)\nn = 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup = np.array([\"A\"] * (n // 2) + [\"B\"] * (n // 2))\n\n# Initial weight (pre). Group A starts with higher average weight\nweight_pre = np.zeros(n)\nweight_pre[group == \"A\"] = np.random.normal(loc=75, scale=10, size=n // 2)\nweight_pre[group == \"B\"] = np.random.normal(loc=65, scale=10, size=n // 2)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain = np.random.normal(loc=10, scale=5, size=n)\nweight_post = weight_pre + gain\n\n# Create DataFrame\ndata = pd.DataFrame({\"group\": group, \"pre\": weight_pre, \"post\": weight_post})\n\n# Analysis 1: Compare change scores between groups\ndata[\"change\"] = data[\"post\"] - data[\"pre\"]\ngroup_a_change = data[data[\"group\"] == \"A\"][\"change\"]\ngroup_b_change = data[data[\"group\"] == \"B\"][\"change\"]\nt_stat, p_value = ttest_ind(group_a_change, group_b_change)\nprint(f\"Analysis 1: p-value = {p_value:.4f}\")\n\n# Analysis 2: Regression Adjustment\nmodel = smf.ols(\"post ~ group + pre\", data=data).fit()\nprint(model.summary())\n```\n\n::::\n\nThe former approach shows lack of statistically significant differences in the body weight after the intervention between the two groups ($p$-value =$ 0.6107$). The results from the latter method do show meaningful differences ($p$-value = $0.0842$).\n\nThis illustrates the core of Lord’s paradox – the statistical approach chosen can lead to different interpretations of the same underlying phenomenon.\n\n## Bottom Line\n\n- Lord’s paradox underscores the importance of aligning statistical methods with causal assumptions.\n\n- Different methods answer different questions and may yield contradictory results if applied blindly.\n\n- Careful consideration of the data-generating process and the role of potential confounders is crucial in choosing the appropriate analytical approach.\n\n## References\n\nLord, E. M. (1967). A paradox in the interpretation of group comparisons. Psychological Bulletin, 68, 304–305. doi:10.1037/h0025105\n\nLord, F. M. (1969). Statistical adjustments when comparing preexisting groups. Psychological Bulletin, 72, 336–337. doi:10.1037/h0028108\n\nLord, E. M. (1975). Lord’s paradox. In S. B. Anderson, S. Ball, R. T. Murphy, & Associates, Encyclopedia of Educational Evaluation (pp. 232–236). San Francisco, CA: Jossey-Bass.\n\nTu, Y. K., Gunnell, D., & Gilthorpe, M. S. (2008). Simpson’s Paradox, Lord’s Paradox, and Suppression Effects are the same phenomenon–the reversal paradox. Emerging themes in epidemiology, 5, 1-9.","srcMarkdownNoYaml":"\n\n## Background\n\nLord’s paradox presents a fascinating challenge in causal inference and statistics. It highlights how different statistical methods applied to the same data can lead to contradictory conclusions. The paradox typically arises when comparing changes over time between two groups in the context of adjusting for baseline characteristics. Despite its theoretical nature, the phenomena is deeply relevant to practical data analysis, especially in observational studies where causation is challenging to establish. Let’s look at an example.\n\n## A Closer Look\n\n### Mean Differences Over Time\n\nTo explore Lord’s paradox, consider the following scenario: Suppose we have two groups of individuals—$A$ and $B$—with their body weights measured at two time points: before and after a specific intervention. Let the weight at the initial time point be denoted as $W_{\\text{pre}}$, and the weight at the final time point be $W_{\\text{post}}$. We are interested in whether the intervention caused a change in weight between the two groups.\n\nOne approach to analyze this is to compute the (unadjusted) mean weight change for each group over time:\n\n$$\\Delta = \\Delta^A - \\Delta^B.$$\n\nIf this quantity is statistically significant, we might conclude that the intervention had a differential effect.\n\n### Controlling for Baseline Characteristics\n\nAn alternative approach involves adjusting for baseline weight $W_{\\text{pre}}$ using, for example, a regression model:\n\n  $$W_{\\text{post}}=\\beta_1 + \\beta_2 G_A + \\beta_3 W_{\\text{pre}} + \\epsilon,$$\n\nwhere $G$ is a binary indicator for group $A$ membership and $\\epsilon$ is an error term. Here, $\\beta_2$ captures the group difference in $W_{\\text{post}}$, linearly controlling for baseline body weight.\n\nSurprisingly, these two approaches can yield conflicting results. For example, the former method might suggest no difference, while the regression adjustment indicates a significant group effect.\n\n### Explanation\n\nThis contradiction arises because the two methods implicitly address different causal questions.\n\n- **Method 1** asks: “Do Groups $A$ and $B$ gain/lose different amounts of weight?”\n- **Method 2** asks: “Given the same initial weight, does any of the groups end up at different final weights?”\nThe regression approach adjusts for baseline differences, assuming $W_{\\text{pre}}$ is a confounder.\n\nThe key insight is that the choice of analysis reflects underlying assumptions about the causal structure of the data. If $W_{\\text{pre}}$ is affected by group membership (e.g., due to selection bias), then adjusting for it may introduce bias rather than remove it. However, in practice we most often should control for baseline characteristics as they are critical in balancing the treatment and control groups.\n\n### The Simpson’s Paradox Once Again\n\nI recently illustrated the more commonly discussed Simpson’s paradox. Interestingly, a [2008 paper](https://link.springer.com/article/10.1186/1742-7622-5-2) claims that two phenomena are closely related, with the Lord’s paradox being a “continuous version” of Simpson’s paradox.\n\n## An Example\n\nLet’s look at some code illustrating Lord’s paradox in `R` and `python`. We start with simulating a dataset where two groups have identical distributions of $W_{\\text{pre}}$ and $W_{\\text{post}}$, yet differing relationships between the two variables.\n\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\nn <- 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup <- factor(rep(c(\"A\", \"B\"), each = n/2))\n\n# Initial weight (pre). # Group A starts with higher average weight\nweight_pre <- numeric(n)\nweight_pre[group == \"A\"] <- rnorm(n/2, mean = 75, sd = 10)\nweight_pre[group == \"B\"] <- rnorm(n/2, mean = 65, sd = 10)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain <- rnorm(n, mean = 10, sd = 5)\nweight_post <- weight_pre + gain\n\n# Create data frame\ndata <- data.frame(group = group, pre = weight_pre, post = weight_post)\n\n# Analysis 1: Compare change scores between groups`\nt.test(post - pre ~ group, data = data)\n> p-value = 0.6107\n\n# Analysis 2: Regression Adjustment`\nmodel <- lm(post ~ group + pre, data = data)\nsummary(model)\n> p-value = 0.08428742\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Set seed for reproducibility\nnp.random.seed(1988)\nn = 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup = np.array([\"A\"] * (n // 2) + [\"B\"] * (n // 2))\n\n# Initial weight (pre). Group A starts with higher average weight\nweight_pre = np.zeros(n)\nweight_pre[group == \"A\"] = np.random.normal(loc=75, scale=10, size=n // 2)\nweight_pre[group == \"B\"] = np.random.normal(loc=65, scale=10, size=n // 2)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain = np.random.normal(loc=10, scale=5, size=n)\nweight_post = weight_pre + gain\n\n# Create DataFrame\ndata = pd.DataFrame({\"group\": group, \"pre\": weight_pre, \"post\": weight_post})\n\n# Analysis 1: Compare change scores between groups\ndata[\"change\"] = data[\"post\"] - data[\"pre\"]\ngroup_a_change = data[data[\"group\"] == \"A\"][\"change\"]\ngroup_b_change = data[data[\"group\"] == \"B\"][\"change\"]\nt_stat, p_value = ttest_ind(group_a_change, group_b_change)\nprint(f\"Analysis 1: p-value = {p_value:.4f}\")\n\n# Analysis 2: Regression Adjustment\nmodel = smf.ols(\"post ~ group + pre\", data=data).fit()\nprint(model.summary())\n```\n\n::::\n\nThe former approach shows lack of statistically significant differences in the body weight after the intervention between the two groups ($p$-value =$ 0.6107$). The results from the latter method do show meaningful differences ($p$-value = $0.0842$).\n\nThis illustrates the core of Lord’s paradox – the statistical approach chosen can lead to different interpretations of the same underlying phenomenon.\n\n## Bottom Line\n\n- Lord’s paradox underscores the importance of aligning statistical methods with causal assumptions.\n\n- Different methods answer different questions and may yield contradictory results if applied blindly.\n\n- Careful consideration of the data-generating process and the role of potential confounders is crucial in choosing the appropriate analytical approach.\n\n## References\n\nLord, E. M. (1967). A paradox in the interpretation of group comparisons. Psychological Bulletin, 68, 304–305. doi:10.1037/h0025105\n\nLord, F. M. (1969). Statistical adjustments when comparing preexisting groups. Psychological Bulletin, 72, 336–337. doi:10.1037/h0028108\n\nLord, E. M. (1975). Lord’s paradox. In S. B. Anderson, S. Ball, R. T. Murphy, & Associates, Encyclopedia of Educational Evaluation (pp. 232–236). San Francisco, CA: Jossey-Bass.\n\nTu, Y. K., Gunnell, D., & Gilthorpe, M. S. (2008). Simpson’s Paradox, Lord’s Paradox, and Suppression Effects are the same phenomenon–the reversal paradox. Emerging themes in epidemiology, 5, 1-9."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"filters":["code-insertion"],"output-file":"paradox-lord.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js","../code/back-to-top.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>\n<script src=\"../code/open-links-new-tab.js\"></script>  \n<script src=\"../code/back-to-top.js\"></script>\n<link href=\"https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Code+Pro&display=swap\" rel=\"stylesheet\">\n"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html",{"text":"<button id=\"back-to-top\" onclick=\"scrollToTop()\">↑</button>\n"}]},"insert-before-post":"_sharebuttons.md","title":"Lord’s Paradox: A Simple Illustration","date":"2022-12-18","categories":["correlation","paradox"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}