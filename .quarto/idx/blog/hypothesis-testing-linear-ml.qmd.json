{"title":"Hypothesis Testing in Linear Machine Learning Models","markdown":{"yaml":{"title":"Hypothesis Testing in Linear Machine Learning Models","date":"2022-11-06","categories":["hypothesis testing","machine learning"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nMachine learning models are an indispensable part of data science. They are incredibly good at what they are designed for – making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, $p$-values, or anything else related to statistical significance. Why?\n\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have *already* selected the most strongly correlated variables.\n\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The $t$-stats and $p$-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\n\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\n\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models – Ridge, Elastic net, SCAD, etc.\n\n## Notation \n\nAs a reminder, $\\beta^{lasso}$ is the solution to:\n\n  $$\\min_{\\beta} \\frac{1}{2} || Y-x\\beta|| ^2_2 + \\lambda ||\\beta||_1. $$\n\nWe are trying to predict a vector $Y\\in \\mathbb{R}$ with a set of features $X\\in \\mathbb{R}^{pxn}$ with $p\\leq n$, and $\\lambda$ is a tuning parameter. When needed, I will use $j$ to index individual columns (i.e., variables) of $X$.\n\n## A Closer Look\n\n### Two Types of Models and Parameters\n\nWe first need to discuss an important subtlety. There are two distinct ways of thinking about performing hypothesis testing in ML models. The traditional view is that we have a true linear model which includes all variables:\n\n\\begin{equation} Y=X\\beta_0+\\epsilon. \\end{equation} \n\nWe are interested in testing whether $\\beta_0=0$ – that is, inference on the full model. This is certainly an intuitive target. The interpretation is that this model encapsulates all relevant causal variables for $Y$. Importantly, even if a given variable $X_j $ is not selected, it still belongs to the model and has a meaningful interpretation.\n\nThere is an alternative way to think about the problem. Imagine we run a variable selection algorithm (e.g., lasso), which selects a subset $M=\\{1,\\dots,p\\}$ of all available predictors ($X$), $M<p$ leading to the alternative model:\n\n\\begin{equation} Y=X_M\\beta_M+u. \\end{equation} \n\nNow we are interested in testing whether $\\beta_M=0$ – that is, inference on the selected model. Unlike the scenario above, here $\\beta_{Mj}$ is interpreted as the change in $Y$ for a unit change in $X_j$ when all other variables in $X_M$ (as opposed to all of $X$) are kept constant.\n\nWhich of the two targets is more intuitive?\n\nStatisticians argue vehemently about this. Some claim that the full model interpretation is inherently problematic. It is too naïve and perhaps even arrogant to think that (*i*) mother nature can be explained by a linear equation, (*ii*) we can measure and include the full set of relevant predictors. On top of this, there are also [technical issues](https://doi.org/10.1017/S0266466605050036) with this interpretation beyond the scope of this post.\n\nTo overcome these challenges, relatively recently, statisticians developed the idea of inference on the selected model. This introduces major technical challenges, however.\n\n### The Naïve Approach: What *Not* to Do\n\nFirst things first – here is what we should not do.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Run a Lasso regression.\n2. Run OLS regression on the subset of selected variables.\n3. Perform statistical inference with the estimated $t$-stats, confidence intervals, and $p$-values.\n:::\n\nThis is bad practice. Can you see why?\n\nIt is simply because we ignored the fact that we already peeked at the data when we ran the Lasso regression. Lasso already chose the variables that are strongly correlated with the outcome. Intuitively, we will need to inflate the $p$-values to account for the data exploration in the first step.\n\nIt [turns out](https://doi.org/10.1017/S0266466605050036) that, in general, both the finite- and large-sample distributions of these parameters are non-Gaussian and depend on unknown parameters in weird ways. Consequently, the calculated $t$-stats and $p$-values are all wrong, and there is little hope that anything simple can be done to save this approach. And no, the standard bootstrap cannot help us either.\n\nBut are there special cases when this approach might work? A recent paper titled “*[In Defense of the Indefensible](https://projecteuclid.org/journals/statistical-science/volume-36/issue-4/In-Defense-of-the-Indefensible--A-Very-Na%C3%AFve-Approach/10.1214/20-STS815.short): A Very Naïve Approach to High-Dimensional Inference*” argues that under very strict assumptions on $X$ and $\\lambda$, this method is actually kosher. The reason it works is that in the magical world of those assumptions, the set of variables that Lasso chooses is deterministic, and not random (hence circumventing the issue described above). The resulting estimator is unbiased and asymptotically normal – hence hypothesis testing is trivial.\n\nHere is what we should do instead.\n\n### The Classical Approach: Inference on the Full Model\n\nRoughly speaking, there are at least four ways we can go about doing hypothesis testing for $\\beta$ in equation (1).\n\n#### Data Split\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Split our data into two equal parts.\n2. Run a Lasso regression on the first part.\n3. Run OLS on the second part with the selected variables from Step 2.\n4. Perform inference using the computed $t$-stats, $p$-values, and confidence intervals.\n:::\n\nThis is simple and intuitive. The problem is that in small samples, the $p$-values can be quite sensitive to how we split the data in the first step. This is clearly undesirable, as we will be getting different results every time we run this algorithm for no apparent reason.\n\n#### Multi Split\n\nThis is a modification of the Data Split approach designed to solve the sensitivity issue and increase power.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Repeat $B$ times:\n  i. Reshuffle data.\n  ii. Run the Data Split method.\n  iii. Save the $p$-values.\n2. Aggregate the B $p$-values into a single final one for each variable.\n:::\n\nInstead of splitting the data into two parts only once, we can do it many times, and each time, we get a $p$-value for every variable. The aggregation goes a long way to solving the instability of the simple data split approach. There is a lot of clever mathematics behind it. For example, there is a complicated expression for aggregating the $p$-values rather than taking a simple average.\n\n*Software Package*: [hdi](https://www.rdocumentation.org/packages/hdi/).\n\n#### Bias Correction\n\nThis approach tackles the problem from a very different angle. The idea is to directly remove the bias from the naïve Lasso procedure without any subsampling or data splitting. Somewhat magically, the resulting estimator is unbiased and asymptotically normally distributed – statistical inference is then straightforward.\n\nThere are [multiple versions](https://projecteuclid.org/journals/annals-of-statistics/volume-42/issue-3/On-asymptotically-optimal-confidence-regions-and-tests-for-high-dimensional/10.1214/14-AOS1221.full) of this idea, but the general form of these estimators is:\n\n  $$\\hat{\\beta}^{\\text{bias cor}} = \\hat{\\beta}^{lasso} + \\hat{\\Theta} X'\\epsilon^{lasso},$$\n\nWhere $\\hat{\\beta}^{lasso}$ is the lasso estimator and $\\epsilon^{lasso}$ are the residuals. The missing piece is the $\\hat{\\Theta}$ matrix; there are several ways to estimate it depending on the setting. In its simplest form, $\\hat{\\Theta}$ is the inverse of the sample variance-covariance matrix. Other examples include the matrix, which minimizes an error term related to the bias as well as the variance of its Gaussian component. Similar bias-correction methods have been developed for Ridge regression as well.\n\n*Software Package*: [hdi](https://www.rdocumentation.org/packages/hdi/).\n\n#### Bootstrap\n\nAs in many other complicated settings for statistical inference, the bootstrap can come to the rescue. Still, the plain vanilla bootstrap will not do. Instead, here is the general idea of the leading version of the bootstrap estimator for Lasso:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Run a Lasso regression.\n2. Keep only $\\beta^{lasso}$‘s larger than some magical threshold.\n3. Compute the associated residuals and center them around $0$.\n4. Repeat B times:\n- draw random samples of these centered residuals,\n- compute new responses $\\dot{Y}$ by adding them to the predictions $X'\\beta^{lasso}$, and\n- obtain $\\beta^{lasso}$ coefficients from Lasso regressions on these new responses $\\dot{Y}$.\n5. Use the distribution of the obtained coefficients to conduct statistical inference.\n:::\n\nThis idea can be generalized to other settings and, for instance, be combined with the bias-corrected estimator.\n\nThis wraps up our discussion of methods for performing hypothesis testing on equation (1) (i.e., the full model). We now move on to a more challenging topic – inference on equation (2) (i.e., the selected model).\n\n### The Novel Approach: Inference on the Selected Model\n\n#### PoSI (Post Selection Inference)\n\nThe goal of the [PoSI method](https://www.jstor.org/stable/23566582) is to construct confidence intervals that are valid regardless of the variable selection method and the selected submodel. The benefit is that we would be reaching the correct conclusion even if we did not select the true model. This luxury comes at the expense of often being too conservative (i.e., confidence intervals are “too wide”). Let me explain how this is done.\n\nTo take a step back, most confidence intervals in statistics take the form:\n\n  $$\\hat{\\beta} \\pm m \\times \\hat{SE}(\\hat{\\beta}).$$\n\nEvery data scientist has seen a similar formula before. The question is usually one about choosing the constant $m$. When we work with two-sided hypotheses tests and large samples, we often use $m = 1.96$ because this is roughly the $97.5$th percentile of the $t$-distribution with many degrees of freedom.  This gives a $2.5\\%$ false positive error on both tails of the distribution ($5\\%$ in total) and hence the associated 95% confidence intervals. The larger m, the wider or more conservative the confidence interval.\n\nThere are a few ways to choose the constant m in the PoSI world. Vaguely speaking, PoSI says we should select this constant to equal the $97.5$th percentile of a distribution related to the largest $t$-statistic among all possible models. This is usually approximated with Monte Carlo simulations. Interestingly, we do not need the response variable to approximate the value.\n\n  $$m = \\max_{\\text{ \\{all models and vars\\} }} |t|.$$\n\nAnother and even more conservative choice for $m$ is the Scheffe constant.\n\n  $$m^{scheffe} = \\sqrt{rank(X) \\times F(rank(X),  n-p)},$$\n\nwhere $F(\\dot)$ denotes the $95$th percentile of the $F$ distribution with the respective degrees of freedom.\n\nUnfortunately, you guessed correctly that this method does not scale well. In some sense, it is a “brute force” method that scans through all possible model combinations and all variables within each model and picks the most conservative value. The authors recommend this procedure for datasets with roughly $p<20$. This rules out many practical applications where machine learning is most useful.\n\n*Software Package*: [PoSI](https://www.rdocumentation.org/packages/PoSI)\n\n#### EPoSI (Exact PoSI)\n\nOk, the name of [this approach](https://doi.org/10.1214/15-AOS1371) is not super original. The “E” here stands for “exact.” Unlike its cousin, this approach is valid only in the selected submodel. Because we cover fewer scenarios, the intervals will generally be narrower than the PoSI ones. EPoSI produces valid finite sample (as opposed to asymptotic) confidence intervals and $p$-values. Like all methods described here, the math behind this is extremely technical. So, I will give you only a high-level description of how this works.\n\nThe idea is first to get the conditional distribution of $\\beta$ given the selected model. A bit magically, it turns out it is a [truncated normal distribution](https://en.wikipedia.org/wiki/Truncated_normal_distribution). Really, who would have guessed this? Do you even remember truncated probability distributions (hint: they are just like regular PDFs but bounded from at least one side. This requires further scaling so that the density area sums to 1.)?\n\nTo dig one layer deeper, the authors show that the selection of Lasso predictors can be recast as a “polyhedral region” of the form:\n\n  $$ AY\\leq b. $$\n\nIn English, for fixed $X$ and $\\lambda$, the set of alternative outcome values $Y^*$ which yields the same set of selected predictors, can be expressed by the simple inequality above. In it, $A$ and $b$ depend do not depend on $Y$. Under this new result, the distribution of $\\hat{\\beta}_M^{\\text{EPoSI}}$ is now well-understood and tractable, thus enabling valid hypothesis testing.\n\nThen, we can use the conditional distribution function to construct a whimsical test statistic that is uniformly distributed on the $[0,1]$ interval. And we can finally build confidence intervals based on that statistic.\n\nSelective inference is currently among the hottest topic in all of statistics. There have been a myriad of extensions and improvements on the original paper. Still, this literature is painstakingly technical. What is the polyhedral selection property or a union of polytopes?\n\n*Software Package*: [selectiveInference](https://www.rdocumentation.org/packages/selectiveInference).\n\n## An Example\n\nI used the popular Titanic dataset ($n=889$) to illustrate some of the methods I discussed above. Refer to the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, `survived`, indicated whether the passenger survived the disaster (mean=$0.382$), while the predictors included demographic characteristics (e.g., `age`, `gender`) as well as some information about the travel ticket (e.g., `cabin number`, `fare`).\n\nUnlike in Monte Carlo simulations, I do not know the ground truth here, so this exercise is not informative about which approaches work well and which do not. Rather, it just serves as an illustrative example.\n\nHere is a table displaying the number of statistically significant variables with $p < .05$ for various inference methods.\n\n<div style=\"max-width: 400px; margin: 0 auto;\">\n|        |            |            |              |       |        |\n|-------------------------------------------------------------------|---------|------------|------------|--------------|-------|--------|\n|                                                                   | Naive   | Data Split | Multi Split | Bias Correct. | PoSI  | EPoSI  |\n| $\\#$ vars $p < .05$                                                    | 7       | 5          | 2          | 3            | 2     | 2      |\n\n</div>\n\nAs expected, the naive method results in the smallest $p$-values and hence the highest number of significant predictors – seven. Data Split knocks down two of those seven variables, resulting in five significant ones. The rest are more conservative, leaving only two or three features with $p < .05$.\n\nBelow is the table with $p$-values for all variables and each method.\n\n<div style=\"max-width: 600px; margin: 0 auto;\">\n| Table 2: $p$-values |         |            |            |              |       |        |\n|-------------------|---------|------------|------------|--------------|-------|--------|\n|                   | Naive   | Data Split | Multi Split | Bias Correct. | PoSI  | EPoSI  |\n|                   | 0.00    | 0.00       | 0.00       | 0.00         | 0.01  | 1.00   |\n| `age`               | 0.00    | 0.04       | 1.00       | 0.01         | 1.00  | 1.00   |\n| `sibsp`             | 0.02    | 0.03       | 1.00       | 0.21         | 1.00  | 1.00   |\n| `parch`             | 0.32    | 0.64       | 1.00       | 1.00         | 1.00  | 1.00   |\n| `fare`              | 0.20    | 0.21       | 1.00       | 1.00         | 1.00  | 1.00   |\n| `male`              | 0.00    | 0.00       | 0.00       | 0.00         | 0.01  | 0.00   |\n| `embarkedS`         | 0.00    | 0.01       | 1.00       | 0.06         | -     | 1.00   |\n| `cabinA`            | 0.39    | -          | 1.00       | 1.00         | 1.00  | 1.00   |\n| `cabinB`            | 0.35    | -          | 1.00       | 1.00         | 1.00  | 1.00   |\n| `cabinD`            | 0.05    | 0.27       | 1.00       | 0.44         | 1.00  | -      |\n| `cabinE`            | 0.00    | 0.64       | 1.00       | 0.06         | 1.00  | 1.00   |\n| `cabinF`            | 0.02    | 0.52       | 1.00       | 0.21         | 1.00  | 1.00   |\n| `embarkedC`         | -       | -          | 1.00       | 0.11         | 1.00  | 1.00   |\n| `embarkedQ`         | -       | -          | 1.00       | 1.00         | 1.00  | 0.00   |\n| `cabinC`            | -       | -          | 1.00       | 1.00         | 1.00  | -      |\n</div>\n\nYou can find the code for this exercise in [this GitHub repo](https://github.com/vyasenov/inference-with-lasso).\n\n## Bottom Line\n\n- Machine learning methods mine through datasets to find strong correlations between the response and the features. Quantifying this strength is an open and challenging problem.\n\n- The naive approach to hypothesis testing is usually invalid.\n\n- There are two main approaches that work – inference on the full model or on the selected model. The latter poses more technical challenges than the former.\n\n- If we are interested in the full model, the Multi Split approach is general enough to cover a wide range of models and settings.\n\n- If we believe you have to focus on the selected model, EPoSI is the state-of-the-art.\n\n- [Simulation](https://doi.org/10.1214/14-STS507) [exercises](https://www.jstor.org/stable/24780819) usually show no clear winner, as none of the methods consistently outperforms the rest.\n\n## Where to Learn More\n\n[Taylor and Tibshirani (2015)](https://doi.org/10.1073/pnas.1507583112) give a non-technical introduction to the problem space along with a description of the POSI method – a great read but focused on a single approach. [Other](https://doi.org/10.1214/14-STS507) [studies](https://www.jstor.org/stable/24780819add) both provide a relatively accessible overview of the various methods for statistical inference on the full model. For technical readers, [Zhang et al. (2022)](https://doi.org/10.1214/22-SS135) provide an excellent up-to-date review of the literature, which I used extensively.\n\n## References\n\nBerk, R., Brown, L., Buja, A., Zhang, K., & Zhao, L. (2013). Valid post-selection inference. The Annals of Statistics, 802-837.\n\nBühlmann, P. (2013). Statistical significance in high-dimensional linear models. Bernoulli, 19(4), 1212-1242.\n\nDezeure, R., Bühlmann, P., Meier, L., & Meinshausen, N. (2015). High-dimensional inference: confidence intervals, p-values and R-software hdi. Statistical Science, 533-558.\n\nJavanmard, A., & Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15(1), 2869-2909.\n\nLee, J. D., Sun, D. L., Sun, Y., & Taylor, J. E. (2016). Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3), 907-927.\n\nLeeb, H., & Pötscher, B. M. (2005). Model selection and inference: Facts and fiction. Econometric Theory, 21(1), 21-59.\n\nLeeb, H., Pötscher, B. M., & Ewald, K. (2015). On various confidence intervals post-model-selection. Statistical Science, 30(2), 216-227.\n\nMeinshausen, N., Meier, L., & Bühlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\n\nTaylor, J., & Tibshirani, R. J. (2015). Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112(25), 7629-7634.\n\nVan de Geer, S., Bühlmann, P., Ritov, Y. A., & Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. The Annals of Statistics, 42(3), 1166-1202.\n\nWasserman, L., & Roeder, K. (2009). High dimensional variable selection. Annals of Statistics, 37(5A), 2178.\n\nZhang, D., Khalili, A., & Asgharian, M. (2022). Post-model-selection inference in linear regression models: An integrated review. Statistics Surveys, 16, 86-136.\n\nZhang, C. H., & Zhang, S. S. (2014). Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 217-242.\n\nZhao, S., Witten, D., & Shojaie, A. (2021). In defense of the indefensible: A very naive approach to high-dimensional inference. Statistical Science, 36(4), 562-577.","srcMarkdownNoYaml":"\n\n## Background\n\nMachine learning models are an indispensable part of data science. They are incredibly good at what they are designed for – making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, $p$-values, or anything else related to statistical significance. Why?\n\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have *already* selected the most strongly correlated variables.\n\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The $t$-stats and $p$-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\n\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\n\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models – Ridge, Elastic net, SCAD, etc.\n\n## Notation \n\nAs a reminder, $\\beta^{lasso}$ is the solution to:\n\n  $$\\min_{\\beta} \\frac{1}{2} || Y-x\\beta|| ^2_2 + \\lambda ||\\beta||_1. $$\n\nWe are trying to predict a vector $Y\\in \\mathbb{R}$ with a set of features $X\\in \\mathbb{R}^{pxn}$ with $p\\leq n$, and $\\lambda$ is a tuning parameter. When needed, I will use $j$ to index individual columns (i.e., variables) of $X$.\n\n## A Closer Look\n\n### Two Types of Models and Parameters\n\nWe first need to discuss an important subtlety. There are two distinct ways of thinking about performing hypothesis testing in ML models. The traditional view is that we have a true linear model which includes all variables:\n\n\\begin{equation} Y=X\\beta_0+\\epsilon. \\end{equation} \n\nWe are interested in testing whether $\\beta_0=0$ – that is, inference on the full model. This is certainly an intuitive target. The interpretation is that this model encapsulates all relevant causal variables for $Y$. Importantly, even if a given variable $X_j $ is not selected, it still belongs to the model and has a meaningful interpretation.\n\nThere is an alternative way to think about the problem. Imagine we run a variable selection algorithm (e.g., lasso), which selects a subset $M=\\{1,\\dots,p\\}$ of all available predictors ($X$), $M<p$ leading to the alternative model:\n\n\\begin{equation} Y=X_M\\beta_M+u. \\end{equation} \n\nNow we are interested in testing whether $\\beta_M=0$ – that is, inference on the selected model. Unlike the scenario above, here $\\beta_{Mj}$ is interpreted as the change in $Y$ for a unit change in $X_j$ when all other variables in $X_M$ (as opposed to all of $X$) are kept constant.\n\nWhich of the two targets is more intuitive?\n\nStatisticians argue vehemently about this. Some claim that the full model interpretation is inherently problematic. It is too naïve and perhaps even arrogant to think that (*i*) mother nature can be explained by a linear equation, (*ii*) we can measure and include the full set of relevant predictors. On top of this, there are also [technical issues](https://doi.org/10.1017/S0266466605050036) with this interpretation beyond the scope of this post.\n\nTo overcome these challenges, relatively recently, statisticians developed the idea of inference on the selected model. This introduces major technical challenges, however.\n\n### The Naïve Approach: What *Not* to Do\n\nFirst things first – here is what we should not do.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Run a Lasso regression.\n2. Run OLS regression on the subset of selected variables.\n3. Perform statistical inference with the estimated $t$-stats, confidence intervals, and $p$-values.\n:::\n\nThis is bad practice. Can you see why?\n\nIt is simply because we ignored the fact that we already peeked at the data when we ran the Lasso regression. Lasso already chose the variables that are strongly correlated with the outcome. Intuitively, we will need to inflate the $p$-values to account for the data exploration in the first step.\n\nIt [turns out](https://doi.org/10.1017/S0266466605050036) that, in general, both the finite- and large-sample distributions of these parameters are non-Gaussian and depend on unknown parameters in weird ways. Consequently, the calculated $t$-stats and $p$-values are all wrong, and there is little hope that anything simple can be done to save this approach. And no, the standard bootstrap cannot help us either.\n\nBut are there special cases when this approach might work? A recent paper titled “*[In Defense of the Indefensible](https://projecteuclid.org/journals/statistical-science/volume-36/issue-4/In-Defense-of-the-Indefensible--A-Very-Na%C3%AFve-Approach/10.1214/20-STS815.short): A Very Naïve Approach to High-Dimensional Inference*” argues that under very strict assumptions on $X$ and $\\lambda$, this method is actually kosher. The reason it works is that in the magical world of those assumptions, the set of variables that Lasso chooses is deterministic, and not random (hence circumventing the issue described above). The resulting estimator is unbiased and asymptotically normal – hence hypothesis testing is trivial.\n\nHere is what we should do instead.\n\n### The Classical Approach: Inference on the Full Model\n\nRoughly speaking, there are at least four ways we can go about doing hypothesis testing for $\\beta$ in equation (1).\n\n#### Data Split\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Split our data into two equal parts.\n2. Run a Lasso regression on the first part.\n3. Run OLS on the second part with the selected variables from Step 2.\n4. Perform inference using the computed $t$-stats, $p$-values, and confidence intervals.\n:::\n\nThis is simple and intuitive. The problem is that in small samples, the $p$-values can be quite sensitive to how we split the data in the first step. This is clearly undesirable, as we will be getting different results every time we run this algorithm for no apparent reason.\n\n#### Multi Split\n\nThis is a modification of the Data Split approach designed to solve the sensitivity issue and increase power.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Repeat $B$ times:\n  i. Reshuffle data.\n  ii. Run the Data Split method.\n  iii. Save the $p$-values.\n2. Aggregate the B $p$-values into a single final one for each variable.\n:::\n\nInstead of splitting the data into two parts only once, we can do it many times, and each time, we get a $p$-value for every variable. The aggregation goes a long way to solving the instability of the simple data split approach. There is a lot of clever mathematics behind it. For example, there is a complicated expression for aggregating the $p$-values rather than taking a simple average.\n\n*Software Package*: [hdi](https://www.rdocumentation.org/packages/hdi/).\n\n#### Bias Correction\n\nThis approach tackles the problem from a very different angle. The idea is to directly remove the bias from the naïve Lasso procedure without any subsampling or data splitting. Somewhat magically, the resulting estimator is unbiased and asymptotically normally distributed – statistical inference is then straightforward.\n\nThere are [multiple versions](https://projecteuclid.org/journals/annals-of-statistics/volume-42/issue-3/On-asymptotically-optimal-confidence-regions-and-tests-for-high-dimensional/10.1214/14-AOS1221.full) of this idea, but the general form of these estimators is:\n\n  $$\\hat{\\beta}^{\\text{bias cor}} = \\hat{\\beta}^{lasso} + \\hat{\\Theta} X'\\epsilon^{lasso},$$\n\nWhere $\\hat{\\beta}^{lasso}$ is the lasso estimator and $\\epsilon^{lasso}$ are the residuals. The missing piece is the $\\hat{\\Theta}$ matrix; there are several ways to estimate it depending on the setting. In its simplest form, $\\hat{\\Theta}$ is the inverse of the sample variance-covariance matrix. Other examples include the matrix, which minimizes an error term related to the bias as well as the variance of its Gaussian component. Similar bias-correction methods have been developed for Ridge regression as well.\n\n*Software Package*: [hdi](https://www.rdocumentation.org/packages/hdi/).\n\n#### Bootstrap\n\nAs in many other complicated settings for statistical inference, the bootstrap can come to the rescue. Still, the plain vanilla bootstrap will not do. Instead, here is the general idea of the leading version of the bootstrap estimator for Lasso:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Run a Lasso regression.\n2. Keep only $\\beta^{lasso}$‘s larger than some magical threshold.\n3. Compute the associated residuals and center them around $0$.\n4. Repeat B times:\n- draw random samples of these centered residuals,\n- compute new responses $\\dot{Y}$ by adding them to the predictions $X'\\beta^{lasso}$, and\n- obtain $\\beta^{lasso}$ coefficients from Lasso regressions on these new responses $\\dot{Y}$.\n5. Use the distribution of the obtained coefficients to conduct statistical inference.\n:::\n\nThis idea can be generalized to other settings and, for instance, be combined with the bias-corrected estimator.\n\nThis wraps up our discussion of methods for performing hypothesis testing on equation (1) (i.e., the full model). We now move on to a more challenging topic – inference on equation (2) (i.e., the selected model).\n\n### The Novel Approach: Inference on the Selected Model\n\n#### PoSI (Post Selection Inference)\n\nThe goal of the [PoSI method](https://www.jstor.org/stable/23566582) is to construct confidence intervals that are valid regardless of the variable selection method and the selected submodel. The benefit is that we would be reaching the correct conclusion even if we did not select the true model. This luxury comes at the expense of often being too conservative (i.e., confidence intervals are “too wide”). Let me explain how this is done.\n\nTo take a step back, most confidence intervals in statistics take the form:\n\n  $$\\hat{\\beta} \\pm m \\times \\hat{SE}(\\hat{\\beta}).$$\n\nEvery data scientist has seen a similar formula before. The question is usually one about choosing the constant $m$. When we work with two-sided hypotheses tests and large samples, we often use $m = 1.96$ because this is roughly the $97.5$th percentile of the $t$-distribution with many degrees of freedom.  This gives a $2.5\\%$ false positive error on both tails of the distribution ($5\\%$ in total) and hence the associated 95% confidence intervals. The larger m, the wider or more conservative the confidence interval.\n\nThere are a few ways to choose the constant m in the PoSI world. Vaguely speaking, PoSI says we should select this constant to equal the $97.5$th percentile of a distribution related to the largest $t$-statistic among all possible models. This is usually approximated with Monte Carlo simulations. Interestingly, we do not need the response variable to approximate the value.\n\n  $$m = \\max_{\\text{ \\{all models and vars\\} }} |t|.$$\n\nAnother and even more conservative choice for $m$ is the Scheffe constant.\n\n  $$m^{scheffe} = \\sqrt{rank(X) \\times F(rank(X),  n-p)},$$\n\nwhere $F(\\dot)$ denotes the $95$th percentile of the $F$ distribution with the respective degrees of freedom.\n\nUnfortunately, you guessed correctly that this method does not scale well. In some sense, it is a “brute force” method that scans through all possible model combinations and all variables within each model and picks the most conservative value. The authors recommend this procedure for datasets with roughly $p<20$. This rules out many practical applications where machine learning is most useful.\n\n*Software Package*: [PoSI](https://www.rdocumentation.org/packages/PoSI)\n\n#### EPoSI (Exact PoSI)\n\nOk, the name of [this approach](https://doi.org/10.1214/15-AOS1371) is not super original. The “E” here stands for “exact.” Unlike its cousin, this approach is valid only in the selected submodel. Because we cover fewer scenarios, the intervals will generally be narrower than the PoSI ones. EPoSI produces valid finite sample (as opposed to asymptotic) confidence intervals and $p$-values. Like all methods described here, the math behind this is extremely technical. So, I will give you only a high-level description of how this works.\n\nThe idea is first to get the conditional distribution of $\\beta$ given the selected model. A bit magically, it turns out it is a [truncated normal distribution](https://en.wikipedia.org/wiki/Truncated_normal_distribution). Really, who would have guessed this? Do you even remember truncated probability distributions (hint: they are just like regular PDFs but bounded from at least one side. This requires further scaling so that the density area sums to 1.)?\n\nTo dig one layer deeper, the authors show that the selection of Lasso predictors can be recast as a “polyhedral region” of the form:\n\n  $$ AY\\leq b. $$\n\nIn English, for fixed $X$ and $\\lambda$, the set of alternative outcome values $Y^*$ which yields the same set of selected predictors, can be expressed by the simple inequality above. In it, $A$ and $b$ depend do not depend on $Y$. Under this new result, the distribution of $\\hat{\\beta}_M^{\\text{EPoSI}}$ is now well-understood and tractable, thus enabling valid hypothesis testing.\n\nThen, we can use the conditional distribution function to construct a whimsical test statistic that is uniformly distributed on the $[0,1]$ interval. And we can finally build confidence intervals based on that statistic.\n\nSelective inference is currently among the hottest topic in all of statistics. There have been a myriad of extensions and improvements on the original paper. Still, this literature is painstakingly technical. What is the polyhedral selection property or a union of polytopes?\n\n*Software Package*: [selectiveInference](https://www.rdocumentation.org/packages/selectiveInference).\n\n## An Example\n\nI used the popular Titanic dataset ($n=889$) to illustrate some of the methods I discussed above. Refer to the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, `survived`, indicated whether the passenger survived the disaster (mean=$0.382$), while the predictors included demographic characteristics (e.g., `age`, `gender`) as well as some information about the travel ticket (e.g., `cabin number`, `fare`).\n\nUnlike in Monte Carlo simulations, I do not know the ground truth here, so this exercise is not informative about which approaches work well and which do not. Rather, it just serves as an illustrative example.\n\nHere is a table displaying the number of statistically significant variables with $p < .05$ for various inference methods.\n\n<div style=\"max-width: 400px; margin: 0 auto;\">\n|        |            |            |              |       |        |\n|-------------------------------------------------------------------|---------|------------|------------|--------------|-------|--------|\n|                                                                   | Naive   | Data Split | Multi Split | Bias Correct. | PoSI  | EPoSI  |\n| $\\#$ vars $p < .05$                                                    | 7       | 5          | 2          | 3            | 2     | 2      |\n\n</div>\n\nAs expected, the naive method results in the smallest $p$-values and hence the highest number of significant predictors – seven. Data Split knocks down two of those seven variables, resulting in five significant ones. The rest are more conservative, leaving only two or three features with $p < .05$.\n\nBelow is the table with $p$-values for all variables and each method.\n\n<div style=\"max-width: 600px; margin: 0 auto;\">\n| Table 2: $p$-values |         |            |            |              |       |        |\n|-------------------|---------|------------|------------|--------------|-------|--------|\n|                   | Naive   | Data Split | Multi Split | Bias Correct. | PoSI  | EPoSI  |\n|                   | 0.00    | 0.00       | 0.00       | 0.00         | 0.01  | 1.00   |\n| `age`               | 0.00    | 0.04       | 1.00       | 0.01         | 1.00  | 1.00   |\n| `sibsp`             | 0.02    | 0.03       | 1.00       | 0.21         | 1.00  | 1.00   |\n| `parch`             | 0.32    | 0.64       | 1.00       | 1.00         | 1.00  | 1.00   |\n| `fare`              | 0.20    | 0.21       | 1.00       | 1.00         | 1.00  | 1.00   |\n| `male`              | 0.00    | 0.00       | 0.00       | 0.00         | 0.01  | 0.00   |\n| `embarkedS`         | 0.00    | 0.01       | 1.00       | 0.06         | -     | 1.00   |\n| `cabinA`            | 0.39    | -          | 1.00       | 1.00         | 1.00  | 1.00   |\n| `cabinB`            | 0.35    | -          | 1.00       | 1.00         | 1.00  | 1.00   |\n| `cabinD`            | 0.05    | 0.27       | 1.00       | 0.44         | 1.00  | -      |\n| `cabinE`            | 0.00    | 0.64       | 1.00       | 0.06         | 1.00  | 1.00   |\n| `cabinF`            | 0.02    | 0.52       | 1.00       | 0.21         | 1.00  | 1.00   |\n| `embarkedC`         | -       | -          | 1.00       | 0.11         | 1.00  | 1.00   |\n| `embarkedQ`         | -       | -          | 1.00       | 1.00         | 1.00  | 0.00   |\n| `cabinC`            | -       | -          | 1.00       | 1.00         | 1.00  | -      |\n</div>\n\nYou can find the code for this exercise in [this GitHub repo](https://github.com/vyasenov/inference-with-lasso).\n\n## Bottom Line\n\n- Machine learning methods mine through datasets to find strong correlations between the response and the features. Quantifying this strength is an open and challenging problem.\n\n- The naive approach to hypothesis testing is usually invalid.\n\n- There are two main approaches that work – inference on the full model or on the selected model. The latter poses more technical challenges than the former.\n\n- If we are interested in the full model, the Multi Split approach is general enough to cover a wide range of models and settings.\n\n- If we believe you have to focus on the selected model, EPoSI is the state-of-the-art.\n\n- [Simulation](https://doi.org/10.1214/14-STS507) [exercises](https://www.jstor.org/stable/24780819) usually show no clear winner, as none of the methods consistently outperforms the rest.\n\n## Where to Learn More\n\n[Taylor and Tibshirani (2015)](https://doi.org/10.1073/pnas.1507583112) give a non-technical introduction to the problem space along with a description of the POSI method – a great read but focused on a single approach. [Other](https://doi.org/10.1214/14-STS507) [studies](https://www.jstor.org/stable/24780819add) both provide a relatively accessible overview of the various methods for statistical inference on the full model. For technical readers, [Zhang et al. (2022)](https://doi.org/10.1214/22-SS135) provide an excellent up-to-date review of the literature, which I used extensively.\n\n## References\n\nBerk, R., Brown, L., Buja, A., Zhang, K., & Zhao, L. (2013). Valid post-selection inference. The Annals of Statistics, 802-837.\n\nBühlmann, P. (2013). Statistical significance in high-dimensional linear models. Bernoulli, 19(4), 1212-1242.\n\nDezeure, R., Bühlmann, P., Meier, L., & Meinshausen, N. (2015). High-dimensional inference: confidence intervals, p-values and R-software hdi. Statistical Science, 533-558.\n\nJavanmard, A., & Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15(1), 2869-2909.\n\nLee, J. D., Sun, D. L., Sun, Y., & Taylor, J. E. (2016). Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3), 907-927.\n\nLeeb, H., & Pötscher, B. M. (2005). Model selection and inference: Facts and fiction. Econometric Theory, 21(1), 21-59.\n\nLeeb, H., Pötscher, B. M., & Ewald, K. (2015). On various confidence intervals post-model-selection. Statistical Science, 30(2), 216-227.\n\nMeinshausen, N., Meier, L., & Bühlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\n\nTaylor, J., & Tibshirani, R. J. (2015). Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112(25), 7629-7634.\n\nVan de Geer, S., Bühlmann, P., Ritov, Y. A., & Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. The Annals of Statistics, 42(3), 1166-1202.\n\nWasserman, L., & Roeder, K. (2009). High dimensional variable selection. Annals of Statistics, 37(5A), 2178.\n\nZhang, D., Khalili, A., & Asgharian, M. (2022). Post-model-selection inference in linear regression models: An integrated review. Statistics Surveys, 16, 86-136.\n\nZhang, C. H., & Zhang, S. S. (2014). Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 217-242.\n\nZhao, S., Witten, D., & Shojaie, A. (2021). In defense of the indefensible: A very naive approach to high-dimensional inference. Statistical Science, 36(4), 562-577."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"filters":["code-insertion"],"output-file":"hypothesis-testing-linear-ml.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html"]},"insert-before-post":"_sharebuttons.md","title":"Hypothesis Testing in Linear Machine Learning Models","date":"2022-11-06","categories":["hypothesis testing","machine learning"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}