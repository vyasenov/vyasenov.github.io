{"title":"Lasso for Heterogeneous Treatment Effects Estimation","markdown":{"yaml":{"title":"Lasso for Heterogeneous Treatment Effects Estimation","date":"2023-06-30","categories":["heterogeneous treatment effects","causal inference"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nLasso is one of my favorite machine learning algorithms. It is so simple, elegant, and powerful. My feelings aside, Lasso indeed has a lot to offer. While, admittedly, it is outperformed by more complex, black-box type methods (e.g., boosting or neural networks), it has several advantages: interpretability, [computational efficiency](https://projecteuclid.org/journals/annals-of-statistics/volume-32/issue-2/Least-angle-regression/10.1214/009053604000000067.full), and flexibility. Even when it comes to accuracy, [theory](https://www.tandfonline.com/doi/abs/10.1198/016214501753382273) tells us that under appropriate assumptions, Lasso can uncover the true submodel and we can even derive bounds on its prediction loss.\n\nIn this article I will briefly describe two ways researchers use Lasso to detect heterogeneous treatment effects. The underlying idea is to throw a bunch of covariates in the model and let the $L1$ regularization do the difficult job of identifying which ones are important for treatment effect heterogeneity.\n\n## Notation\n\nAs always, let’s start with some notation. Let $T$ denote a binary treatment indicator, $Y(0), Y(1)$ be the potential outcomes under each treatment state ($Y$ is the observed one), and $X$ be a covariate vector. Lastly, $p$ is the share of units in the treatment group, $p=\\frac{1}{N}\\sum T$, where $N$ is the sample size.\n\nThe Lasso coefficient vector is commonly expressed as the solution to the following problem:\n\n $$\\min_\\beta \\{ \\frac{1}{N}||y-X\\beta||_2^2 + \\lambda||\\beta||_1 \\},$$\n\nwhere $\\lambda$ is the regularization parameter governing the variance-bias trade-off.\n\nWe are interested in the heterogeneous treatment effect given $X (HTE(X))$:\n\n $$HTE(X) = E[Y(1)-Y(0)|X=x].$$\n\nThat is, $HTE(X)$ is the average treatment effect for units with covariate levels $X=x$.\n\nMore precisely, our goal is identifying which variables in $X$ divide the population of interest such that there are meaningful treatment effect differences across these groups. For instance, in the case of estimating the impact of school quality on test scores, $X$ might be students’ gender (e.g., girls benefit more than boys), or in the context of online A/B testing, $X$ might denote previous product engagement (e.g., tenured users benefit more from a new feature than inexperienced users).\n\nBroadly speaking, there are two main approaches to using Lasso to solve this problem — (i) a linear model with interactions between $T$ and $X$, and (ii) directly regressing the imputed unit-level treatment effects on $X$.\n\n## A Closer Look\n\n### Heterogeneous Treatment Effects in Linear Models\n\nIn a low-dimensional world where regularization is not necessary and researchers are interested in $HTE$s, they often use a linear model in which the treatment variable is interacted with the covariates. Statistically significant interaction coefficients identify the $X$ variables for which the treatment has a differential impact.\n\nMathematically, when all variables are properly interacted, this is analogous to splitting the sample into subgroups based on $X$ and running OLS on each group separately. This is feasible and convenient when $X$ is binary or categorical, but not when it is continuous. The advantage of this approach is that linear regression produces $p$-values which can be (mis)used to determine statistical significance of these interaction variables.\n\nThe OLS model is then:\n\n$$Y = \\beta_1 T + \\beta_2 X + \\beta_3 X \\times T + \\epsilon,$$\n\nwhere $\\epsilon$ is the error term. The attention here falls on the coefficient vector $\\beta_3$ which identifies whether the treatment has had a differential impact on units with a particular characteristic $X$.\n\n### Lasso with Treatment Variable Interactions\n\nIn high-dimensional settings with a wide $X$, this is not feasible. Instead, we can use an algorithm to pick out the variables in $X$ that are important for treatment effect heterogeneity.\n\n[Imai and Ratkovic (2013)](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-7/issue-1/Estimating-treatment-effect-heterogeneity-in-randomized-program-evaluation/10.1214/12-AOAS593.full) show how to adapt the Lasso to this setting. It turns out we should not simply throw this model into the Lasso loss function. Can you guess why? Some variables might be predictive of the baseline outcome while others only of the treatment effect heterogeneity. The trick is to have two separate Lasso constraints — $\\lambda_1$ and $\\lambda_2$.\n\nSo, the loss function looks something like this:\n\n $$\\min_\\beta \\{ \\frac{1}{N}||y-f(X,T)\\beta_1 - X\\times T \\beta_2 ||_2^2 + \\lambda_1||\\beta_1||_1 + \\lambda_2||\\beta_2||_1 \\}.$$\n\nIn their actual implementation, this objective is embedded in a support vector machine (SVM) framework, where the authors replace the squared loss with a weighted hinge loss and treat treatment effect estimation as a classification task. The outcome variable is used to weight the classification errors, allowing the method to focus on correctly identifying individuals for whom the treatment has the most impact.\n\nA simplified version of the algorithm is as follows:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Generate interaction variables, $\\tilde{X}(T)$, by interacting covariates with the treatment indicator.\n2. Set up a prediction model for $Y$ using $T$, $X$, and $\\tilde{X}(T)$, and fit it using an *outcome-weighted support vector machine (SVM)* with *separate Lasso penalties* on the main effect and interaction terms.\n3. Identify variables with non-zero interaction coefficients as drivers of treatment effect heterogeneity.\n:::\n\n\nTwo notes. First, this requires some assurance that we are not overfitting, so that some form of sample splitting or cross validation is necessary. On top of this, [Athey and Imbens (2017)](https://www.aeaweb.org/articles?id=10.1257/jep.31.2.3) suggest comparing these results with post-lasso OLS estimates to further guard against overfitting. Second, multiple testing is an issue as is the case with ML algorithms more generally. (You can check my [earlier post](https://vyasenov.github.io/blog/hypothesis-testing-linear-ml.html) on multiple hypothesis testing in linear machine learning models.) Options to take care of this include [sample splitting](https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.tm08647) and [bootstrap](https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10159), among others.\n\n*Software Package*: [FindIt](https://github.com/kosukeimai/FindIt).\n\n### Lasso with Knockoffs\n\nAn alternative approach directly regresses the unit-level treatment effects on $X$. To get there, we first model the outcome function and impute the missing potential outcome for each unit. See my [previous post](https://vyasenov.github.io/blog/flavors-ml-methods-ci.html) on using Machine Learning tools for causal inference for more information on how we might do that.\n\nThis approach was developed by [Xie et al. (2018)](https://dl.acm.org/doi/abs/10.1145/3219819.3219860?casa_token=prn79eBrsBMAAAAA:hWN_3s9f9DcTlWu4DtQlnU4bdJfchGOWFabAGa20TaAeJ660LWtC0rdcj5Gf62BSQmtER-T6wMDq). They recognized the multiple hypothesis issue and suggested using [knockoffs](https://projecteuclid.org/journals/annals-of-statistics/volume-43/issue-5/Controlling-the-false-discovery-rate-via-knockoffs/10.1214/15-AOS1337.full) to control the False Discovery Rate.  This is still not completely kosher, as it does not account for the fact that the outcome variable is estimated in an earlier step. Oh well. My guess and hope are that this is more of a theoretical concern, and empirically the inference we get is still “correct.”\n\nHere is a simplified version of their algorithm:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Transform the outcome variable $\\tilde{Y}=Y\\times\\frac{(T-p)}{p(1-p)}$.\n2. Calculate unit-level treatment effects, $\\hat{Y}(1)-\\hat{Y}(0)$.\n3. Generate the difference $Y^*=\\tilde{Y}-\\frac{1}{N}\\sum \\hat{Y}(1)-\\hat{Y}(0)$.\n4. Run Lasso of $Y^*$ on $X$ and $X^*$ (the knockoff counterparts of $X$).\n5. Follow the knockoff method to obtain the set of significant variables.\n:::\n\nRemember that $\\tilde{Y}$ has the special property that $E[\\tilde{Y} \\mid X=x]=HTE(X)$ under the unconfoundedness assumption.\n\nInterestingly, in the special case when $p=1/2$ the algebra reduces further which provides computation scaling advantages. [Tian et al. (2014)](https://www.tandfonline.com/doi/abs/10.1080/01621459.2014.951443) showed this result first.\n\n## An Example\n\nI used the popular Titanic dataset ($n=889$) to illustrate the latter method. As such, it is a mere depiction of the approach, and its results should certainly not be taken seriously.\n\nThe outcome variable was survived, and the treatment variable was male. As is well known, women were much more likely to survive than men ($74\\%$ vs $19\\%$ in this sample). So, I analyzed whether the gender difference in survival was impacted by other factors. I included the following covariates – `pclass` (ticket class), `age`, `sibsp` (number of siblings aboard), parch (number of parents aboard), `fare`, `embarked` (port of Embarkation), and `cabin`. Some of these were categorical in which case I converted them to a bunch of binary variables.\n\nThe knockoff filter identified a single variable as having a significant impact on the treatment effect – `pclass`. For the more affluent passengers (i.e., those in the higher ticket classes), this gender difference in survival was much smaller.\n\nYou can find the code in [this GitHub repo](https://github.com/vyasenov/lasso-knockoffs-hte).\n\n## Bottom Line\n\n- The core idea behind using Lasso in HTE estimation is to leverage $L_1$ regularization to select which covariates explain differences in treatment responses.\n\n- There are two main ways researchers use Lasso to estimate HTEs. Use a linear model with all covariates interacted with the treatment indicator, and apply Lasso with two separate regularization constraints. Directly regress unit-level treatment effects on the covariates.\n\n- I am not aware of simulation studies comparing both approaches.\n\n- While Lasso is among the simplest and most popular machine learning algorithms, more suitable methods may exist for estimating HTEs.\n\n## Where to Learn More\n\n[Hu (2022)](https://www.sciencedirect.com/science/article/pii/S0049089X22001211) is an excellent summary of the several ways researchers use Machine Learning to uncover heterogeneity in treatment effect estimation.\n\n## References\n\nBarber, R. F., & Candès, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics: 2055-2085.\n\nBarber, R. F., & Candès, E. J. (2019). A knockoff filter for high-dimensional selective inference. The Annals of Statistics, 47(5), 2504-2537.\n\nChatterjee, A., & Lahiri, S. N. (2011). Bootstrapping lasso estimators. Journal of the American Statistical Association, 106(494), 608-625.\n\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\n\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. The Annals of Applied Statistics (2013): 443-470.\n\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\n\nMeinshausen, N., Meier, L., & Bühlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\n\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.\n\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\n\nXie, Y., Chen, N., & Shi, X. (2018). False discovery rate controlled heterogeneous treatment effect detection for online controlled experiments. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 876-885).","srcMarkdownNoYaml":"\n\n## Background\n\nLasso is one of my favorite machine learning algorithms. It is so simple, elegant, and powerful. My feelings aside, Lasso indeed has a lot to offer. While, admittedly, it is outperformed by more complex, black-box type methods (e.g., boosting or neural networks), it has several advantages: interpretability, [computational efficiency](https://projecteuclid.org/journals/annals-of-statistics/volume-32/issue-2/Least-angle-regression/10.1214/009053604000000067.full), and flexibility. Even when it comes to accuracy, [theory](https://www.tandfonline.com/doi/abs/10.1198/016214501753382273) tells us that under appropriate assumptions, Lasso can uncover the true submodel and we can even derive bounds on its prediction loss.\n\nIn this article I will briefly describe two ways researchers use Lasso to detect heterogeneous treatment effects. The underlying idea is to throw a bunch of covariates in the model and let the $L1$ regularization do the difficult job of identifying which ones are important for treatment effect heterogeneity.\n\n## Notation\n\nAs always, let’s start with some notation. Let $T$ denote a binary treatment indicator, $Y(0), Y(1)$ be the potential outcomes under each treatment state ($Y$ is the observed one), and $X$ be a covariate vector. Lastly, $p$ is the share of units in the treatment group, $p=\\frac{1}{N}\\sum T$, where $N$ is the sample size.\n\nThe Lasso coefficient vector is commonly expressed as the solution to the following problem:\n\n $$\\min_\\beta \\{ \\frac{1}{N}||y-X\\beta||_2^2 + \\lambda||\\beta||_1 \\},$$\n\nwhere $\\lambda$ is the regularization parameter governing the variance-bias trade-off.\n\nWe are interested in the heterogeneous treatment effect given $X (HTE(X))$:\n\n $$HTE(X) = E[Y(1)-Y(0)|X=x].$$\n\nThat is, $HTE(X)$ is the average treatment effect for units with covariate levels $X=x$.\n\nMore precisely, our goal is identifying which variables in $X$ divide the population of interest such that there are meaningful treatment effect differences across these groups. For instance, in the case of estimating the impact of school quality on test scores, $X$ might be students’ gender (e.g., girls benefit more than boys), or in the context of online A/B testing, $X$ might denote previous product engagement (e.g., tenured users benefit more from a new feature than inexperienced users).\n\nBroadly speaking, there are two main approaches to using Lasso to solve this problem — (i) a linear model with interactions between $T$ and $X$, and (ii) directly regressing the imputed unit-level treatment effects on $X$.\n\n## A Closer Look\n\n### Heterogeneous Treatment Effects in Linear Models\n\nIn a low-dimensional world where regularization is not necessary and researchers are interested in $HTE$s, they often use a linear model in which the treatment variable is interacted with the covariates. Statistically significant interaction coefficients identify the $X$ variables for which the treatment has a differential impact.\n\nMathematically, when all variables are properly interacted, this is analogous to splitting the sample into subgroups based on $X$ and running OLS on each group separately. This is feasible and convenient when $X$ is binary or categorical, but not when it is continuous. The advantage of this approach is that linear regression produces $p$-values which can be (mis)used to determine statistical significance of these interaction variables.\n\nThe OLS model is then:\n\n$$Y = \\beta_1 T + \\beta_2 X + \\beta_3 X \\times T + \\epsilon,$$\n\nwhere $\\epsilon$ is the error term. The attention here falls on the coefficient vector $\\beta_3$ which identifies whether the treatment has had a differential impact on units with a particular characteristic $X$.\n\n### Lasso with Treatment Variable Interactions\n\nIn high-dimensional settings with a wide $X$, this is not feasible. Instead, we can use an algorithm to pick out the variables in $X$ that are important for treatment effect heterogeneity.\n\n[Imai and Ratkovic (2013)](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-7/issue-1/Estimating-treatment-effect-heterogeneity-in-randomized-program-evaluation/10.1214/12-AOAS593.full) show how to adapt the Lasso to this setting. It turns out we should not simply throw this model into the Lasso loss function. Can you guess why? Some variables might be predictive of the baseline outcome while others only of the treatment effect heterogeneity. The trick is to have two separate Lasso constraints — $\\lambda_1$ and $\\lambda_2$.\n\nSo, the loss function looks something like this:\n\n $$\\min_\\beta \\{ \\frac{1}{N}||y-f(X,T)\\beta_1 - X\\times T \\beta_2 ||_2^2 + \\lambda_1||\\beta_1||_1 + \\lambda_2||\\beta_2||_1 \\}.$$\n\nIn their actual implementation, this objective is embedded in a support vector machine (SVM) framework, where the authors replace the squared loss with a weighted hinge loss and treat treatment effect estimation as a classification task. The outcome variable is used to weight the classification errors, allowing the method to focus on correctly identifying individuals for whom the treatment has the most impact.\n\nA simplified version of the algorithm is as follows:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Generate interaction variables, $\\tilde{X}(T)$, by interacting covariates with the treatment indicator.\n2. Set up a prediction model for $Y$ using $T$, $X$, and $\\tilde{X}(T)$, and fit it using an *outcome-weighted support vector machine (SVM)* with *separate Lasso penalties* on the main effect and interaction terms.\n3. Identify variables with non-zero interaction coefficients as drivers of treatment effect heterogeneity.\n:::\n\n\nTwo notes. First, this requires some assurance that we are not overfitting, so that some form of sample splitting or cross validation is necessary. On top of this, [Athey and Imbens (2017)](https://www.aeaweb.org/articles?id=10.1257/jep.31.2.3) suggest comparing these results with post-lasso OLS estimates to further guard against overfitting. Second, multiple testing is an issue as is the case with ML algorithms more generally. (You can check my [earlier post](https://vyasenov.github.io/blog/hypothesis-testing-linear-ml.html) on multiple hypothesis testing in linear machine learning models.) Options to take care of this include [sample splitting](https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.tm08647) and [bootstrap](https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10159), among others.\n\n*Software Package*: [FindIt](https://github.com/kosukeimai/FindIt).\n\n### Lasso with Knockoffs\n\nAn alternative approach directly regresses the unit-level treatment effects on $X$. To get there, we first model the outcome function and impute the missing potential outcome for each unit. See my [previous post](https://vyasenov.github.io/blog/flavors-ml-methods-ci.html) on using Machine Learning tools for causal inference for more information on how we might do that.\n\nThis approach was developed by [Xie et al. (2018)](https://dl.acm.org/doi/abs/10.1145/3219819.3219860?casa_token=prn79eBrsBMAAAAA:hWN_3s9f9DcTlWu4DtQlnU4bdJfchGOWFabAGa20TaAeJ660LWtC0rdcj5Gf62BSQmtER-T6wMDq). They recognized the multiple hypothesis issue and suggested using [knockoffs](https://projecteuclid.org/journals/annals-of-statistics/volume-43/issue-5/Controlling-the-false-discovery-rate-via-knockoffs/10.1214/15-AOS1337.full) to control the False Discovery Rate.  This is still not completely kosher, as it does not account for the fact that the outcome variable is estimated in an earlier step. Oh well. My guess and hope are that this is more of a theoretical concern, and empirically the inference we get is still “correct.”\n\nHere is a simplified version of their algorithm:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Transform the outcome variable $\\tilde{Y}=Y\\times\\frac{(T-p)}{p(1-p)}$.\n2. Calculate unit-level treatment effects, $\\hat{Y}(1)-\\hat{Y}(0)$.\n3. Generate the difference $Y^*=\\tilde{Y}-\\frac{1}{N}\\sum \\hat{Y}(1)-\\hat{Y}(0)$.\n4. Run Lasso of $Y^*$ on $X$ and $X^*$ (the knockoff counterparts of $X$).\n5. Follow the knockoff method to obtain the set of significant variables.\n:::\n\nRemember that $\\tilde{Y}$ has the special property that $E[\\tilde{Y} \\mid X=x]=HTE(X)$ under the unconfoundedness assumption.\n\nInterestingly, in the special case when $p=1/2$ the algebra reduces further which provides computation scaling advantages. [Tian et al. (2014)](https://www.tandfonline.com/doi/abs/10.1080/01621459.2014.951443) showed this result first.\n\n## An Example\n\nI used the popular Titanic dataset ($n=889$) to illustrate the latter method. As such, it is a mere depiction of the approach, and its results should certainly not be taken seriously.\n\nThe outcome variable was survived, and the treatment variable was male. As is well known, women were much more likely to survive than men ($74\\%$ vs $19\\%$ in this sample). So, I analyzed whether the gender difference in survival was impacted by other factors. I included the following covariates – `pclass` (ticket class), `age`, `sibsp` (number of siblings aboard), parch (number of parents aboard), `fare`, `embarked` (port of Embarkation), and `cabin`. Some of these were categorical in which case I converted them to a bunch of binary variables.\n\nThe knockoff filter identified a single variable as having a significant impact on the treatment effect – `pclass`. For the more affluent passengers (i.e., those in the higher ticket classes), this gender difference in survival was much smaller.\n\nYou can find the code in [this GitHub repo](https://github.com/vyasenov/lasso-knockoffs-hte).\n\n## Bottom Line\n\n- The core idea behind using Lasso in HTE estimation is to leverage $L_1$ regularization to select which covariates explain differences in treatment responses.\n\n- There are two main ways researchers use Lasso to estimate HTEs. Use a linear model with all covariates interacted with the treatment indicator, and apply Lasso with two separate regularization constraints. Directly regress unit-level treatment effects on the covariates.\n\n- I am not aware of simulation studies comparing both approaches.\n\n- While Lasso is among the simplest and most popular machine learning algorithms, more suitable methods may exist for estimating HTEs.\n\n## Where to Learn More\n\n[Hu (2022)](https://www.sciencedirect.com/science/article/pii/S0049089X22001211) is an excellent summary of the several ways researchers use Machine Learning to uncover heterogeneity in treatment effect estimation.\n\n## References\n\nBarber, R. F., & Candès, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics: 2055-2085.\n\nBarber, R. F., & Candès, E. J. (2019). A knockoff filter for high-dimensional selective inference. The Annals of Statistics, 47(5), 2504-2537.\n\nChatterjee, A., & Lahiri, S. N. (2011). Bootstrapping lasso estimators. Journal of the American Statistical Association, 106(494), 608-625.\n\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\n\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. The Annals of Applied Statistics (2013): 443-470.\n\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\n\nMeinshausen, N., Meier, L., & Bühlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\n\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.\n\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\n\nXie, Y., Chen, N., & Shi, X. (2018). False discovery rate controlled heterogeneous treatment effect detection for online controlled experiments. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 876-885)."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"filters":["code-insertion"],"output-file":"lasso-heterogeneous-effects.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js","../code/back-to-top.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>\n<script src=\"../code/open-links-new-tab.js\"></script>  \n<script src=\"../code/back-to-top.js\"></script>\n<link href=\"https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Code+Pro&display=swap\" rel=\"stylesheet\">\n"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html",{"text":"<button id=\"back-to-top\" onclick=\"scrollToTop()\">↑</button>\n"}]},"insert-before-post":"_sharebuttons.md","title":"Lasso for Heterogeneous Treatment Effects Estimation","date":"2023-06-30","categories":["heterogeneous treatment effects","causal inference"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}