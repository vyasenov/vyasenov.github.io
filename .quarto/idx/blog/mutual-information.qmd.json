{"title":"Mutual Information: What, Why, How, and When","markdown":{"yaml":{"title":"Mutual Information: What, Why, How, and When","date":"2025-01-02","categories":["correlation"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nWhen exploring dependencies between variables, the data scientist’s toolbox often relies on correlation measures to reveal relationships and potential patterns. But what if we’re looking to capture relationships beyond linear correlations? Mutual Information (MI) quantifies the “amount of information” shared between variables in a more general sense. It measures how much we know about $Y$ by observing $X$. This approach goes beyond linear patterns and can help us uncover more complex relationships within data.\n\nMI can be especially helpful for applications such as feature selection and unsupervised learning. For readers familiar with various types of correlation metrics, Mutual Information provides an additional lens to interpret relationships within data.\n\nIn this article, I’ll guide you through the concept of Mutual Information, its definition, properties, and usage, as well as comparisons with other dependency measures. We’ll explore MI’s mathematical foundations, its advantages and limitations, and its applications, concluding with an example demonstrating MI’s behavior alongside more traditional measures like Pearson’s correlation.\n\n## Notation\n\nBefore diving deeper, let’s establish our notation:\n\n- Random variables will be denoted by capital letters ($X$, $Y$).\n- Lowercase letters ($x$,$y$) represent specific values of these variables.\n- $p(x)$ denotes the probability mass/density function of $X$.\n- $p(x,y)$ represents the joint probability mass/density function of $X$ and $Y$.\n- $p(x\\mid y)$ is the conditional probability of $X$ given $Y$.\n- $H(X)$ represents the entropy of random variable $X$.\n\nThese should not surprise anyone, as they are standard conventions in most statistics textbooks.\n\n## A Closer Look\n\n### Refresher on Entropy\n\nMutual Information is related to the notion of entropy, a measure of uncertainty or randomness in a random variable. In less formal terms, entropy quantifies how “surprising” or “unpredictable” the outcomes of $X$ are.\n\nFormally, for a discrete random variable $X$ with possible outcomes $x_1, x_2, \\dots, x_n$ and associated probabilities $p(x_1), p(x_2), \\dots, p(x_n)$, entropy $H(X)$ is defined as:\n\n  $$H(X) = -\\sum_{i=1}^n p(x_i) \\log p(x_i).$$\n\nFor continuous variables we switch the summation with an integral.\n\nEntropy equals $0$ when there’s no uncertainty, such as when $X$ always takes a single outcome. High entropy means greater uncertainty (many possible outcomes, all equally likely), while low entropy indicates less uncertainty, as some outcomes are much more likely than others. In essence, entropy tells us how much “information” is gained on average when observing the variable’s realization.\n\n### Mathematical Definitions of MI\n\nMI can be defined in multiple ways. Perhaps the most intuitive definition of MI between two random variables $X$ and $Y$ is that it measures the difference between the joint distribution and the product of their marginals. If two random variables are independent, their joint distribution is the product of their marginals $p(x,y)=p(x)p(y)$. In some sense, the discrepancy between these two objects (i.e., the two sides of the equality) measures the strength of association between $X$ and $Y$ (i.e., their “independence”).\n\nFormally, MI is the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the joint distribution and the product of the two marginals:\n\n  $$ MI(X,Y) = D_{KL} (p(X,Y) || p(X)p(Y)). $$\n\nLet’s now examine MI from a different angle. We can express mutual information as follows:\n\n $$ MI(X,Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$$\n\nAgain, for continuous variables, the sums become integrals. \n\nAlternatively, MI can also be expressed in terms of entropy. It equals the sum of the entropy of $X$ and $Y$ taking away their joint entropy:\n\n  $$ \\begin{align*}MI(X,Y) & = H(X) - H(X|Y) \\\\ & = H(Y) - H(Y|X) \\\\ & = H(X) + H(Y) - H(X,Y).\\end{align*} $$\n\nYou can compute MI in `R` with the `infotheo` package.\n\n*Software Package*: [infotheo](https://www.rdocumentation.org/packages/infotheo).\n\n### Properties\n\nMI has several intriguing properties:\n\n- **Non-negativity**: $MI(X;Y) \\geq 0$. Mutual Information is always non-negative, as it measures the amount of information one variable provides about the other. Higher values correspond to stronger association.\n- **Symmetry**: $MI(X,Y) = MI(Y,X)$. This symmetry implies that the information $X$ provides about $Y$ is the same as what Y provides about X.\n- **Independence**: Similarly to Chatterjee’s correlation coefficient, $MI(X,Y) = 0$ if and only if $X$ and $Y$ are independent.\n- **Scale-invarance**: Mutual information is scale invariant. If you apply a scaling transformation to the variables, their MI will not be affected.\n\n### Conditional MI\n\nConditional Mutual Information (CMI) extends the concept of MI to measure dependency between $X$ and $Y$ given a third variable $Z$. CMI is useful for investigating how much information $X$ and $Y$ share independently of $Z$. It is defined as:\n\n  $$MI(X,Y|Z) = \\sum_{x,y,z} p(x,y,z) \\log \\left(\\frac{p(x,y|z)}{p(x|z)p(y|z)}\\right). $$\n\nThis can be valuable in causal inference, where understanding dependencies conditioned on specific variables aids in interpreting relationships within complex models. CMI is also particularly useful for feature selection when accounting for redundancy among already included features. Let’s explore this idea in greater detail.\n\n### Feature Selection with MI\n\nIn machine learning, MI serves as a useful metric for feature selection (Brown et al. 2012, Vergara and Estévez 2014). Consider an outcome variable $Y$ and a set of features $X\\in\\mathbb{R}^p$ with n i.i.d. observations. By evaluating the MI between each feature and the target variable, one can retain features with the highest information content, passing a certain threshold. This is somewhat primitive since it assumes independence across features. More sophisticated approaches take feature dependency into account.\n\nFor instance, methods like Minimum Redundancy Maximum Relevance (mRMR, Peng et al. 2005) aim to maximize the relevance of features to the target while minimizing redundancy among features. Here is a concise version of the mRMR algorithm:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Calculate MI between each feature and the target (relevance).\n2. Calculate MI between features (redundancy).\n3. Select features that maximize relevance while minimizing redundancy:\n$$ \\text{mRMR} = \\max_{X_i} \\left[MI(X_i;Y) - \\frac{1}{|S|} \\sum_{X_j \\in S} MI(X_i;X_j)\\right], $$\n\nwhere $S$ is the set of already selected features.\n:::\n\n### Pros and Cons\n\nLike any other statistical tool, mutual Information has several advantages and limitations. On the positive side, it captures both linear and nonlinear relationships, is scale-invariant, and works with both continuous and discrete variables, making it a theoretically well-founded measure. However, it requires density estimation for continuous variables, can be computationally intensive for large datasets, and its results can be sensitive to binning choices for continuous variables. Additionally, there is no standard normalization for MI.\n\n## An Example\n\nLet’s implement MI calculation in `R` and `python` and compare it with traditional correlation measures using the `iris` dataset.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nlibrary(infotheo)\nlibrary(corrplot)\nlibrary(dplyr)\ndata(iris)\n\n# Calculate mutual information matrix\nmi_matrix <- mutinformation(discretize(iris[,1:4]))\n\n# Calculate correlation matrix\ncor_matrix <- cor(iris[,1:4])\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value <- mi_matrix[1,3]\ncor_value <- cor_matrix[1,3]\n\n# Print results\nprint(paste(\"Mutual Information:\", round(mi_value, 3)))\nprint(paste(\"Pearson Correlation:\", round(cor_value, 3)))\n\n>[1] \"Mutual Information: 0.585\"\n>[1] \"Pearson Correlation: 0.872\"\n```\n\n### Python\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.metrics import mutual_info_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load dataset\niris = sns.load_dataset('iris')\n\n# Discretize the dataset (except the target variable)\nX = iris.iloc[:, :-1]\nest = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\nX_discretized = est.fit_transform(X)\n\n# Calculate mutual information matrix\nmi_matrix = np.zeros((X.shape[1], X.shape[1]))\nfor i in range(X.shape[1]):\n    for j in range(X.shape[1]):\n        mi_matrix[i, j] = mutual_info_score(X_discretized[:, i], X_discretized[:, j])\n\n# Calculate correlation matrix\ncor_matrix = X.corr()\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value = mi_matrix[0, 2]  # Sepal.Length vs Petal.Length\ncor_value = cor_matrix.iloc[0, 2]  # Sepal.Length vs Petal.Length\n\n# Print results\nprint(f\"Mutual Information: {mi_value:.3f}\")\nprint(f\"Pearson Correlation: {cor_value:.3f}\")\n\n> Mutual Information: 0.905\n> Pearson Correlation: 0.872\n```\n\n::::\n\n![](../images/mutual_info_overview.png)\n\nThe left matrix displays the MI results and the right one shows the standard (Pearson) correlation values. The default scales are different, so one should compare the values and not the colors. Indeed, the variable pairs with negative linear correlation also have the lowest MI values.\n\nThis example demonstrates how MI can capture nonlinear relationships that might be missed by traditional correlation measures.\n\n## Bottom Line\n\n- Mutual Information provides a comprehensive measure of statistical dependence, capturing both linear and nonlinear relationships.\n\n- Unlike correlation coefficients, MI works naturally with both continuous and categorical variables.\n\n- MI serves as the foundation for sophisticated feature selection algorithms like mRMR.\n\n## Where to Learn More\n\n[Wikipedia](https://en.wikipedia.org/wiki/Mutual_information) is a great place to start and learning the basics. Brown et al. (2012) and Vergara and Estévez (2014) are the go-to resources for conditional MI and using MI for feature selection.\n\n## References\n\nBrown, G., Pocock, A., Zhao, M. J., & Luján, M. (2012). Conditional likelihood maximisation: a unifying framework for information theoretic feature selection. JMLR.\n\nCover, T. M., & Thomas, J. A. (2006). Elements of information theory (2nd ed.). Wiley-Interscience.\n\nKraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical Review E, 69(6).\n\nPeng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE TPAMI.\n\nRoss, B. C. (2014). Mutual information between discrete and continuous data sets. PloS one, 9(2).\n\nVergara, J. R., & Estévez, P. A. (2014). A review of feature selection methods based on mutual information. Neural Computing and Applications, 24(1).","srcMarkdownNoYaml":"\n\n## Introduction \n\nWhen exploring dependencies between variables, the data scientist’s toolbox often relies on correlation measures to reveal relationships and potential patterns. But what if we’re looking to capture relationships beyond linear correlations? Mutual Information (MI) quantifies the “amount of information” shared between variables in a more general sense. It measures how much we know about $Y$ by observing $X$. This approach goes beyond linear patterns and can help us uncover more complex relationships within data.\n\nMI can be especially helpful for applications such as feature selection and unsupervised learning. For readers familiar with various types of correlation metrics, Mutual Information provides an additional lens to interpret relationships within data.\n\nIn this article, I’ll guide you through the concept of Mutual Information, its definition, properties, and usage, as well as comparisons with other dependency measures. We’ll explore MI’s mathematical foundations, its advantages and limitations, and its applications, concluding with an example demonstrating MI’s behavior alongside more traditional measures like Pearson’s correlation.\n\n## Notation\n\nBefore diving deeper, let’s establish our notation:\n\n- Random variables will be denoted by capital letters ($X$, $Y$).\n- Lowercase letters ($x$,$y$) represent specific values of these variables.\n- $p(x)$ denotes the probability mass/density function of $X$.\n- $p(x,y)$ represents the joint probability mass/density function of $X$ and $Y$.\n- $p(x\\mid y)$ is the conditional probability of $X$ given $Y$.\n- $H(X)$ represents the entropy of random variable $X$.\n\nThese should not surprise anyone, as they are standard conventions in most statistics textbooks.\n\n## A Closer Look\n\n### Refresher on Entropy\n\nMutual Information is related to the notion of entropy, a measure of uncertainty or randomness in a random variable. In less formal terms, entropy quantifies how “surprising” or “unpredictable” the outcomes of $X$ are.\n\nFormally, for a discrete random variable $X$ with possible outcomes $x_1, x_2, \\dots, x_n$ and associated probabilities $p(x_1), p(x_2), \\dots, p(x_n)$, entropy $H(X)$ is defined as:\n\n  $$H(X) = -\\sum_{i=1}^n p(x_i) \\log p(x_i).$$\n\nFor continuous variables we switch the summation with an integral.\n\nEntropy equals $0$ when there’s no uncertainty, such as when $X$ always takes a single outcome. High entropy means greater uncertainty (many possible outcomes, all equally likely), while low entropy indicates less uncertainty, as some outcomes are much more likely than others. In essence, entropy tells us how much “information” is gained on average when observing the variable’s realization.\n\n### Mathematical Definitions of MI\n\nMI can be defined in multiple ways. Perhaps the most intuitive definition of MI between two random variables $X$ and $Y$ is that it measures the difference between the joint distribution and the product of their marginals. If two random variables are independent, their joint distribution is the product of their marginals $p(x,y)=p(x)p(y)$. In some sense, the discrepancy between these two objects (i.e., the two sides of the equality) measures the strength of association between $X$ and $Y$ (i.e., their “independence”).\n\nFormally, MI is the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the joint distribution and the product of the two marginals:\n\n  $$ MI(X,Y) = D_{KL} (p(X,Y) || p(X)p(Y)). $$\n\nLet’s now examine MI from a different angle. We can express mutual information as follows:\n\n $$ MI(X,Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$$\n\nAgain, for continuous variables, the sums become integrals. \n\nAlternatively, MI can also be expressed in terms of entropy. It equals the sum of the entropy of $X$ and $Y$ taking away their joint entropy:\n\n  $$ \\begin{align*}MI(X,Y) & = H(X) - H(X|Y) \\\\ & = H(Y) - H(Y|X) \\\\ & = H(X) + H(Y) - H(X,Y).\\end{align*} $$\n\nYou can compute MI in `R` with the `infotheo` package.\n\n*Software Package*: [infotheo](https://www.rdocumentation.org/packages/infotheo).\n\n### Properties\n\nMI has several intriguing properties:\n\n- **Non-negativity**: $MI(X;Y) \\geq 0$. Mutual Information is always non-negative, as it measures the amount of information one variable provides about the other. Higher values correspond to stronger association.\n- **Symmetry**: $MI(X,Y) = MI(Y,X)$. This symmetry implies that the information $X$ provides about $Y$ is the same as what Y provides about X.\n- **Independence**: Similarly to Chatterjee’s correlation coefficient, $MI(X,Y) = 0$ if and only if $X$ and $Y$ are independent.\n- **Scale-invarance**: Mutual information is scale invariant. If you apply a scaling transformation to the variables, their MI will not be affected.\n\n### Conditional MI\n\nConditional Mutual Information (CMI) extends the concept of MI to measure dependency between $X$ and $Y$ given a third variable $Z$. CMI is useful for investigating how much information $X$ and $Y$ share independently of $Z$. It is defined as:\n\n  $$MI(X,Y|Z) = \\sum_{x,y,z} p(x,y,z) \\log \\left(\\frac{p(x,y|z)}{p(x|z)p(y|z)}\\right). $$\n\nThis can be valuable in causal inference, where understanding dependencies conditioned on specific variables aids in interpreting relationships within complex models. CMI is also particularly useful for feature selection when accounting for redundancy among already included features. Let’s explore this idea in greater detail.\n\n### Feature Selection with MI\n\nIn machine learning, MI serves as a useful metric for feature selection (Brown et al. 2012, Vergara and Estévez 2014). Consider an outcome variable $Y$ and a set of features $X\\in\\mathbb{R}^p$ with n i.i.d. observations. By evaluating the MI between each feature and the target variable, one can retain features with the highest information content, passing a certain threshold. This is somewhat primitive since it assumes independence across features. More sophisticated approaches take feature dependency into account.\n\nFor instance, methods like Minimum Redundancy Maximum Relevance (mRMR, Peng et al. 2005) aim to maximize the relevance of features to the target while minimizing redundancy among features. Here is a concise version of the mRMR algorithm:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Calculate MI between each feature and the target (relevance).\n2. Calculate MI between features (redundancy).\n3. Select features that maximize relevance while minimizing redundancy:\n$$ \\text{mRMR} = \\max_{X_i} \\left[MI(X_i;Y) - \\frac{1}{|S|} \\sum_{X_j \\in S} MI(X_i;X_j)\\right], $$\n\nwhere $S$ is the set of already selected features.\n:::\n\n### Pros and Cons\n\nLike any other statistical tool, mutual Information has several advantages and limitations. On the positive side, it captures both linear and nonlinear relationships, is scale-invariant, and works with both continuous and discrete variables, making it a theoretically well-founded measure. However, it requires density estimation for continuous variables, can be computationally intensive for large datasets, and its results can be sensitive to binning choices for continuous variables. Additionally, there is no standard normalization for MI.\n\n## An Example\n\nLet’s implement MI calculation in `R` and `python` and compare it with traditional correlation measures using the `iris` dataset.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nlibrary(infotheo)\nlibrary(corrplot)\nlibrary(dplyr)\ndata(iris)\n\n# Calculate mutual information matrix\nmi_matrix <- mutinformation(discretize(iris[,1:4]))\n\n# Calculate correlation matrix\ncor_matrix <- cor(iris[,1:4])\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value <- mi_matrix[1,3]\ncor_value <- cor_matrix[1,3]\n\n# Print results\nprint(paste(\"Mutual Information:\", round(mi_value, 3)))\nprint(paste(\"Pearson Correlation:\", round(cor_value, 3)))\n\n>[1] \"Mutual Information: 0.585\"\n>[1] \"Pearson Correlation: 0.872\"\n```\n\n### Python\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.metrics import mutual_info_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load dataset\niris = sns.load_dataset('iris')\n\n# Discretize the dataset (except the target variable)\nX = iris.iloc[:, :-1]\nest = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\nX_discretized = est.fit_transform(X)\n\n# Calculate mutual information matrix\nmi_matrix = np.zeros((X.shape[1], X.shape[1]))\nfor i in range(X.shape[1]):\n    for j in range(X.shape[1]):\n        mi_matrix[i, j] = mutual_info_score(X_discretized[:, i], X_discretized[:, j])\n\n# Calculate correlation matrix\ncor_matrix = X.corr()\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value = mi_matrix[0, 2]  # Sepal.Length vs Petal.Length\ncor_value = cor_matrix.iloc[0, 2]  # Sepal.Length vs Petal.Length\n\n# Print results\nprint(f\"Mutual Information: {mi_value:.3f}\")\nprint(f\"Pearson Correlation: {cor_value:.3f}\")\n\n> Mutual Information: 0.905\n> Pearson Correlation: 0.872\n```\n\n::::\n\n![](../images/mutual_info_overview.png)\n\nThe left matrix displays the MI results and the right one shows the standard (Pearson) correlation values. The default scales are different, so one should compare the values and not the colors. Indeed, the variable pairs with negative linear correlation also have the lowest MI values.\n\nThis example demonstrates how MI can capture nonlinear relationships that might be missed by traditional correlation measures.\n\n## Bottom Line\n\n- Mutual Information provides a comprehensive measure of statistical dependence, capturing both linear and nonlinear relationships.\n\n- Unlike correlation coefficients, MI works naturally with both continuous and categorical variables.\n\n- MI serves as the foundation for sophisticated feature selection algorithms like mRMR.\n\n## Where to Learn More\n\n[Wikipedia](https://en.wikipedia.org/wiki/Mutual_information) is a great place to start and learning the basics. Brown et al. (2012) and Vergara and Estévez (2014) are the go-to resources for conditional MI and using MI for feature selection.\n\n## References\n\nBrown, G., Pocock, A., Zhao, M. J., & Luján, M. (2012). Conditional likelihood maximisation: a unifying framework for information theoretic feature selection. JMLR.\n\nCover, T. M., & Thomas, J. A. (2006). Elements of information theory (2nd ed.). Wiley-Interscience.\n\nKraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical Review E, 69(6).\n\nPeng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE TPAMI.\n\nRoss, B. C. (2014). Mutual information between discrete and continuous data sets. PloS one, 9(2).\n\nVergara, J. R., & Estévez, P. A. (2014). A review of feature selection methods based on mutual information. Neural Computing and Applications, 24(1)."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"filters":["code-insertion"],"output-file":"mutual-information.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html"]},"insert-before-post":"_sharebuttons.md","title":"Mutual Information: What, Why, How, and When","date":"2025-01-02","categories":["correlation"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}