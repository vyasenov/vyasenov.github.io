{"title":"Stratified Sampling with Continuous Variables","markdown":{"yaml":{"title":"Stratified Sampling with Continuous Variables","date":"2024-12-18","categories":["randomized experiments","causal inference"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nStratified sampling is a foundational technique in survey design, ensuring that observations capture key characteristics of a population. By dividing the data into distinct strata and sampling from each, stratified sampling often results in more efficient estimates than simple random sampling. Strata are typically defined by categorical variables such as classrooms, villages, or user types. This method is particularly advantageous when some strata are rare but carry critical information, as it ensures their representation in the sample. It is also often employed to tackle spillover effects or manage survey costs more effectively.\n\nWhile straightforward for categorical variables (like geographic region), continuous variables—such as income or churn score—pose greater challenges for stratified sampling. The primary issue lies in the curse of dimensionality: attempting to create strata across multiple continuous variables results in an explosion of possible combinations, making effective sampling impractical. For example, stratifying a population based on income at every possible dollar amount is absurd.\n\nIn this article, I present two solutions to the problem of stratified sampling with continuous variables.\n\n## A Closer Look\n\n### The Traditional Method: Equal-Sized Binning\n\nThis approach involves dividing the continuous variable(s) into intervals or bins. For example, churn score, a single continuous variable $X$, can be divided into quantiles (e.g., quartiles or deciles), ensuring each bin contains approximately the same number of observations/users.\n\nLet’s focus on the case of building ten equally-sized strata. Mathematically, for a continuous variable $X$, the decile-based binning can be defined as:\n\n  $$\\text{Bin}_i = \\{x \\in X : Q_{10\\times (i-1)+1} \\leq x < Q_{10\\times i}\\} \\text{ for } i \\in \\{1,\\dots,10\\}, $$\n\nwhere $Q_k$ represents the $k$-th quantile of $X$. This approach splits in the first ten percentiles (i.e., minimum value to the $10$th percentile) into a single stratum, the next ten percentiles ($10$th to $20$th) into another stratum, and so on.\n\nWhen dealing with multiple variables, this method extends to either marginally stratify each variable or jointly stratify them. However, joint stratification across multiple variables can also fall prey to the curse of dimensionality.\n\n### The Modern Method: Unsupervised Clustering\n\nAn alternative approach uses unsupervised clustering algorithms, such as $k$-means or hierarchical clustering, to group observations into clusters, treating these clusters as strata. Unlike binning, clustering leverages the distribution of the data to form natural groupings.\n\nFormally, let $X$ be a matrix of n observations across $p$ continuous variables. One class of clustering algorithms aims to assign each observation $i$ to one of $k$ clusters:\n\n$$\\text{minimize} \\quad \\sum_{j=1}^k \\sum_{i \\in \\mathcal{C}_j} \\text{distance}(X_i - \\mu_j), $$\n\nwhere $\\mu_j=\\frac{1}{|S_j|}\\sum_{i\\in \\mathcal{C}_j} X$​ is the centroid of cluster $\\mathcal{C}_j$ of $size |S_j|$.\n\nCommonly, $\\text{distance}(X_i, \\mu_j)=\\|X_i - \\mu_j\\|^2$ which leads to $k$-means clustering. Unlike in the binning approach, here we are not restricting each strata to have the same number of observations.\n\n## Pros and Cons\n\nUnsurprisingly, each method comes with its trade-offs. Traditional binning is simple and interpretable but can struggle with multivariate dependencies. Clustering accounts for multivariate relationships between variables, avoids imposing arbitrary bin thresholds, and may results in more natural groupings. However, it can be computationally expensive and sensitive to the choice of algorithm and hyperparameters (e.g., $k$ in $k$-means).\n\nOne can also imagine a hybrid approach. Begin with a dimensionality reduction method like PCA and then perform binning on the first few principal components.\n\n## An Example\n\nHere is `R` and `python` code illustrating both types of approaches on the popular `iris` dataset. We are interested in creating strata based on the `SepalLenght` variable. We begin with the traditional binning approach.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\ndata(iris)\n\n#Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris$SepalLengthBin <- cut(iris$Sepal.Length, \n                           breaks = quantile(iris$Sepal.Length, probs = seq(0, 1, 0.25)), include.lowest = TRUE)\n\n#Inspect the resulting strata\ntable(iris$SepalLengthBin)\n> [4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] \n>       41        39        35        35\n\n#Perform k-means clustering on two continuous variables\niris_cluster <- kmeans(iris[, c(\"Sepal.Length\", \"Petal.Length\")], centers = 4)\n\n#Assign clusters as strata\niris$Cluster <- as.factor(iris_cluster$cluster)\n\n#Inspect the resulting strata\ntable(iris$Cluster) \n> 1  2  3  4 \n> 50 15 54 31 \n```\n\n### Python\n\n```python\n# Load libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nnp.random.seed(1988)\n\n# Load the iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris['SepalLengthBin'] = pd.qcut(iris['Sepal.Length'], q=4, labels=False)\n\n# Inspect the resulting strata\nprint(\"Quantile Bins (Sepal.Length):\")\nprint(iris['SepalLengthBin'].value_counts())\n> SepalLengthBin\n> 0    41\n> 1    39\n> 3    35\n> 2    35\n\n# Perform k-means clustering on two continuous variables\nkmeans = KMeans(n_clusters=4, random_state=1988)\niris['Cluster'] = kmeans.fit_predict(iris[['Sepal.Length', 'Petal.Length']])\n\n# Assign clusters as strata\niris['Cluster'] = iris['Cluster'].astype('category')\n\n# Inspect the resulting strata\nprint(\"\\nCluster Sizes:\")\nprint(iris['Cluster'].value_counts())\n> Cluster\n> 2    50\n> 3    50\n> 0    28\n> 1    22\n```\n\n::::\n\nHere we also have four clusters, but their size ranges from $25$ to $50$ observations each.\n\n## Bottom Line\n\n- Stratified sampling with continuous variables requires balancing simplicity and sophistication.\n\n- Traditional binning remains a practical choice for single continuous variables or very few categorical ones.\n\n- Clustering provides a robust alternative, enabling stratification with multiple continuous variables at the cost of adding complexity and tuning parameters.","srcMarkdownNoYaml":"\n\n## Background\n\nStratified sampling is a foundational technique in survey design, ensuring that observations capture key characteristics of a population. By dividing the data into distinct strata and sampling from each, stratified sampling often results in more efficient estimates than simple random sampling. Strata are typically defined by categorical variables such as classrooms, villages, or user types. This method is particularly advantageous when some strata are rare but carry critical information, as it ensures their representation in the sample. It is also often employed to tackle spillover effects or manage survey costs more effectively.\n\nWhile straightforward for categorical variables (like geographic region), continuous variables—such as income or churn score—pose greater challenges for stratified sampling. The primary issue lies in the curse of dimensionality: attempting to create strata across multiple continuous variables results in an explosion of possible combinations, making effective sampling impractical. For example, stratifying a population based on income at every possible dollar amount is absurd.\n\nIn this article, I present two solutions to the problem of stratified sampling with continuous variables.\n\n## A Closer Look\n\n### The Traditional Method: Equal-Sized Binning\n\nThis approach involves dividing the continuous variable(s) into intervals or bins. For example, churn score, a single continuous variable $X$, can be divided into quantiles (e.g., quartiles or deciles), ensuring each bin contains approximately the same number of observations/users.\n\nLet’s focus on the case of building ten equally-sized strata. Mathematically, for a continuous variable $X$, the decile-based binning can be defined as:\n\n  $$\\text{Bin}_i = \\{x \\in X : Q_{10\\times (i-1)+1} \\leq x < Q_{10\\times i}\\} \\text{ for } i \\in \\{1,\\dots,10\\}, $$\n\nwhere $Q_k$ represents the $k$-th quantile of $X$. This approach splits in the first ten percentiles (i.e., minimum value to the $10$th percentile) into a single stratum, the next ten percentiles ($10$th to $20$th) into another stratum, and so on.\n\nWhen dealing with multiple variables, this method extends to either marginally stratify each variable or jointly stratify them. However, joint stratification across multiple variables can also fall prey to the curse of dimensionality.\n\n### The Modern Method: Unsupervised Clustering\n\nAn alternative approach uses unsupervised clustering algorithms, such as $k$-means or hierarchical clustering, to group observations into clusters, treating these clusters as strata. Unlike binning, clustering leverages the distribution of the data to form natural groupings.\n\nFormally, let $X$ be a matrix of n observations across $p$ continuous variables. One class of clustering algorithms aims to assign each observation $i$ to one of $k$ clusters:\n\n$$\\text{minimize} \\quad \\sum_{j=1}^k \\sum_{i \\in \\mathcal{C}_j} \\text{distance}(X_i - \\mu_j), $$\n\nwhere $\\mu_j=\\frac{1}{|S_j|}\\sum_{i\\in \\mathcal{C}_j} X$​ is the centroid of cluster $\\mathcal{C}_j$ of $size |S_j|$.\n\nCommonly, $\\text{distance}(X_i, \\mu_j)=\\|X_i - \\mu_j\\|^2$ which leads to $k$-means clustering. Unlike in the binning approach, here we are not restricting each strata to have the same number of observations.\n\n## Pros and Cons\n\nUnsurprisingly, each method comes with its trade-offs. Traditional binning is simple and interpretable but can struggle with multivariate dependencies. Clustering accounts for multivariate relationships between variables, avoids imposing arbitrary bin thresholds, and may results in more natural groupings. However, it can be computationally expensive and sensitive to the choice of algorithm and hyperparameters (e.g., $k$ in $k$-means).\n\nOne can also imagine a hybrid approach. Begin with a dimensionality reduction method like PCA and then perform binning on the first few principal components.\n\n## An Example\n\nHere is `R` and `python` code illustrating both types of approaches on the popular `iris` dataset. We are interested in creating strata based on the `SepalLenght` variable. We begin with the traditional binning approach.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\ndata(iris)\n\n#Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris$SepalLengthBin <- cut(iris$Sepal.Length, \n                           breaks = quantile(iris$Sepal.Length, probs = seq(0, 1, 0.25)), include.lowest = TRUE)\n\n#Inspect the resulting strata\ntable(iris$SepalLengthBin)\n> [4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] \n>       41        39        35        35\n\n#Perform k-means clustering on two continuous variables\niris_cluster <- kmeans(iris[, c(\"Sepal.Length\", \"Petal.Length\")], centers = 4)\n\n#Assign clusters as strata\niris$Cluster <- as.factor(iris_cluster$cluster)\n\n#Inspect the resulting strata\ntable(iris$Cluster) \n> 1  2  3  4 \n> 50 15 54 31 \n```\n\n### Python\n\n```python\n# Load libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nnp.random.seed(1988)\n\n# Load the iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris['SepalLengthBin'] = pd.qcut(iris['Sepal.Length'], q=4, labels=False)\n\n# Inspect the resulting strata\nprint(\"Quantile Bins (Sepal.Length):\")\nprint(iris['SepalLengthBin'].value_counts())\n> SepalLengthBin\n> 0    41\n> 1    39\n> 3    35\n> 2    35\n\n# Perform k-means clustering on two continuous variables\nkmeans = KMeans(n_clusters=4, random_state=1988)\niris['Cluster'] = kmeans.fit_predict(iris[['Sepal.Length', 'Petal.Length']])\n\n# Assign clusters as strata\niris['Cluster'] = iris['Cluster'].astype('category')\n\n# Inspect the resulting strata\nprint(\"\\nCluster Sizes:\")\nprint(iris['Cluster'].value_counts())\n> Cluster\n> 2    50\n> 3    50\n> 0    28\n> 1    22\n```\n\n::::\n\nHere we also have four clusters, but their size ranges from $25$ to $50$ observations each.\n\n## Bottom Line\n\n- Stratified sampling with continuous variables requires balancing simplicity and sophistication.\n\n- Traditional binning remains a practical choice for single continuous variables or very few categorical ones.\n\n- Clustering provides a robust alternative, enabling stratification with multiple continuous variables at the cost of adding complexity and tuning parameters."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"filters":["code-insertion"],"output-file":"stratified-sampling-cont-var.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js","../code/back-to-top.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>\n<script src=\"../code/open-links-new-tab.js\"></script>  \n<script src=\"../code/back-to-top.js\"></script>\n<link href=\"https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Code+Pro&display=swap\" rel=\"stylesheet\">\n"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html",{"text":"<button id=\"back-to-top\" onclick=\"scrollToTop()\">↑</button>\n"}]},"insert-before-post":"_sharebuttons.md","title":"Stratified Sampling with Continuous Variables","date":"2024-12-18","categories":["randomized experiments","causal inference"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}