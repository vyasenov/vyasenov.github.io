{"title":"Weights in Statistical Analyses","markdown":{"yaml":{"title":"Weights in Statistical Analyses","date":"2024-09-18","categories":["weights","statistical inference"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nWeights in statistical analyses offer a way to assign varying importance to observations in a dataset. Although powerful, they can be quite confusing due to the various types of weights available. In this article, I will unpack the details behind the most common types of weights used in data science.\n\nMost types of statistical analyses can be performed with weights. These include calculating summary statistics, regression models, bootstrap, etc. Even maximum likelihood estimation is minimally affected. In this article, I will keep it simple and only discuss mean and variance estimation.\n\nThe two primary types of weights encountered in practice are sampling weights and frequency weights. I will explore each in turn and conclude with a comparative example.\n\nTo begin, let’s imagine a well-behaved random variable X of which we have an iid sample of size n. I will use w to denote the relevant weighting variable.\n\n## A Closer Look\n\nMean estimation does not depend on the type of weights. We have:\n\n$$ \\bar{X} = \\frac{\\sum_i wX}{\\sum_i w}. $$\n\nVariance estimation depends on the weight type.\n\nRemember that we estimate the standard error of $\\bar{X}$ as:\n\n$$ SE(\\bar{X})=\\frac{s}{\\sqrt{n}}, \\hspace{.3cm} \\text{where} \\hspace{.3cm} s=\\sqrt{\\frac{1}{N-1}\\sum_i(X-\\bar{X})^2} $$\n\nis an estimate of the standard deviation of $X$. I will explain below how estimation of $SE(\\bar{X})$ differs for sampling and frequency weights. Importantly, s_w will denote the weighted version of this standard error.\n\n### Sampling Weights\n\nSampling weights measure the inverse probability of an observation entering the sample. They are particularly relevant in surveys when some units in the population are sampled more frequently than others. These weights adjust for sampling discrepancies to ensure that survey estimates are representative of the entire population. Sampling weights are also sometimes called probability weights.\n\nIntuitively, if we want to use the survey to accurately represent the population, we should assign higher importance to observations that are less likely to be sampled and lower importance to those more likely to appear in our data. This is exactly what sampling weights achieve. Sampling weights are, thus, inversely proportional to the probability of selection. Therefore, a smaller weight indicates a higher probability of being sampled, and vice versa.\n\nFor example, households in rural areas might be less likely to enter a survey due to the increased resources required to reach remote locations. Conversely, urban households are typically easier to sample and thus more likely to appear in the data.\n\nThis weighting approach is also common in causal inference analyses using propensity score methods. Similarly, in machine learning, weighting is often employed to adjust for unbalanced samples in the context of rare events (e.g., fraud detection, cancer diagnosis).\n\nLet’s now get back to the discussion on variance estimation. With sampling weights, we have:\n\n$$ SE^{\\text{sampling}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{S}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\frac{(\\sum_i w)^2}{\\sum_i w^2}}}$$\n\nThe numerator is the weighted version of the sum of squared deviations from the mean, emphasizing observations with higher weights. The denominator normalizes the standard error based on the total sum of the weights. When the weights differ greatly—high weights make some observations much more influential, increasing uncertainty about the population mean and so the sampling-weighted standard error is often larger.\n\nI will move on to describing frequency weights.\n\n*Software Package*: [survey](https://www.rdocumentation.org/packages/survey)\n\n### Frequency Weights\n\nFrequency weights measure the number of times an observation should be counted in the analysis. They are most relevant when there are multiple identical observations or when duplicating observations is possible. Naturally, higher weights correspond to observations that appear more frequently. Frequency weights are common in aggregated datasets. For instance, with market- or city-level data, we might want to weight the rows by market size, thus assigning more importance to larger units.\n\nWe can gain intuition from a linear algebra perspective. In a regression context, the design matrix X must be of full rank (to ensure invertibility), which implies no two rows can be identical. We thus need to collapse X to keep only distinct rows and record the number of times each row appears in the original data. This record constitutes the frequency weights.\n\nThe formula for estimating the standard error of X with frequency weights is:\n\n$$ SE^{\\text{frequency}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{F}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\sum_i w}}.$$\n\nHere $n_{\\text{eff}}$ is the effective sample size (i.e., the sum of all weights) and $s_w$ is the weighted standard deviation of $X$.\n\nThe frequency-weighted standard error is typically the smaller because it increases the effective sample size without introducing variability in the contribution of different observations. In contrast, the sampling-weighted standard error could be larger if some observations are given much higher weights, increasing the variability and lowering the precision.\n\n*Software Package*: [survey](https://www.rdocumentation.org/packages/survey).\n\n### One More Thing\n\nThere is actually one more type of weights which are not so commonly used in practice, precision weights. They represent the precision ($1/\\text{variance}$) of observations. A weight of 5, for example, reflects 5 times the precision of a weight of 1, originally based on averaging 5 replicate observations. Precision weights often come up in statistical theory in places such as [Generalized Least Squares](https://en.wikipedia.org/wiki/Generalized_least_squares) where they promise efficiency gains (i.e., lower variance) relative to traditional Ordinary Least Squares estimation. In practice, precision weights are in fact frequency weights normalized to sum to n. Lastly, [Stata’s user guide](https://www.stata.com/manuals/u.pdf) refers to them as analytic weights.\n\n## An Example\n\nLet’s see all of this in practice. We begin by creating a fake dataset of a variable $X$ with both sampling and frequency weights. The weights are randomly created and hence have no underlying meaning, so think of this example as a tutorial, without a strong focus on the results.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\n# clear workspace and load libraries\nlibrary(survey)\nrm(list=ls())\nset.seed(681)\n\n# generate fake data\nn <- 100000\ndata <- data.frame(\n  x = rnorm(n),\n  prob_selection = runif(n, .1, .9),\n  freq_weight = rpois(n, 3)\n)\ndata$samp_weight <- 1 / data$prob_selection\n\n# calculate the average value of $X$ using both types of weights.\ndesign_unweight <- svydesign(ids = ~1, data = data, weights = ~1)\ndesign_samp <- svydesign(ids = ~1, data = data, weights = ~samp_weight)\ndesign_freq <- svydesign(ids = ~1, data = data, weights = ~freq_weight)\n\nmean_unweight <- svymean(~x, design_unweight)\nmean_samp <- svymean(~x, design_samp)\nmean_freq <- svymean(~x, design_freq)\n\n# print results\nprint(round(mean_unweight, digits=3))\n>    mean     SE\n> x -0.002 0.0032\nprint(round(mean_samp, digits=3))\n>  mean     SE\n> x    0 0.0038\nprint(round(mean_freq, digits=3))\n>    mean     SE\n> x -0.002 0.0037\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.weightstats import DescrStatsW\n\n# Set seed for reproducibility\nnp.random.seed(681)\n\n# Generate fake data\nn = 100000\ndata = pd.DataFrame({\n    \"x\": np.random.normal(size=n),\n    \"prob_selection\": np.random.uniform(0.1, 0.9, size=n),\n    \"freq_weight\": np.random.poisson(3, size=n)\n})\ndata[\"samp_weight\"] = 1 / data[\"prob_selection\"]\n\n# Calculate the average value of X using both types of weights\n# Unweighted mean\nmean_unweight = DescrStatsW(data[\"x\"]).mean, DescrStatsW(data[\"x\"]).std_mean\n\n# Sampling-weighted mean\nmean_samp = DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).std_mean\n\n# Frequency-weighted mean\nmean_freq = DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).std_mean\n\n# Print results\nprint(\"Unweighted Mean and SE:\", np.round(mean_unweight, 3))\n> Unweighted Mean and SE: [-0.004  0.003]\nprint(\"Sampling-Weighted Mean and SE:\", np.round(mean_samp, 3))\n> Sampling-Weighted Mean and SE: [-0.002  0.002]\nprint(\"Frequency-Weighted Mean and SE:\", np.round(mean_freq, 3))\n> Frequency-Weighted Mean and SE: [-0.004  0.002]\n```\n\n::::\n\nThe unweighted and frequency-weighted means match exactly, (I am not sure why the sampling-weighted mean is slightly lower.) while the variances are different. The variance with frequency weights is lower than that of sampling weights.\n\n## Bottom Line\n\n- Weights are one of the most confusing aspects of working with data.\n\n- Sampling and frequency weights are the most common types of weights found in practice.\n\n- The former measure the inverse probability of being sampled, while the latter represent the number of times an observation enters the sample.\n\n- While weighting usually does not impact point estimates (e.g., regression coefficients, means), incorrect usage of weights can lead to inaccurate confidence intervals and p-values.\n\n- This is relevant only if *(i)* your dataset contains weights, and *(ii)* you are interested in population-level statistics.\n\n## Where to Learn More\n\nGoogle is a great starting place. [Lumley’s blog post](http://notstatschat.rbind.io/2020/08/04/weights-in-statistics/) titled Weights in Statistics was incredibly helpful in preparing this article. Stata’s manuals which are publicly available contain more detailed information on various types of weighting schemes. See also Solon et al. (2015) for using weights in causal inference.\n\n## References\n\nDupraz, Y. (2013). Using Weights in Stata. Memo, 54(2)\n\nLumley, T. (2020), Weights in Statistics, [Blog Post](https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/)\n\nSolon, G., Haider, S. J., & Wooldridge, J. M. (2015). What are we weighting for?. Journal of Human resources, 50(2), 301-316.\n\n[Stata User’s Guide](https://www.stata.com/manuals/u.pdf) (2023) ","srcMarkdownNoYaml":"\n\n## Background\n\nWeights in statistical analyses offer a way to assign varying importance to observations in a dataset. Although powerful, they can be quite confusing due to the various types of weights available. In this article, I will unpack the details behind the most common types of weights used in data science.\n\nMost types of statistical analyses can be performed with weights. These include calculating summary statistics, regression models, bootstrap, etc. Even maximum likelihood estimation is minimally affected. In this article, I will keep it simple and only discuss mean and variance estimation.\n\nThe two primary types of weights encountered in practice are sampling weights and frequency weights. I will explore each in turn and conclude with a comparative example.\n\nTo begin, let’s imagine a well-behaved random variable X of which we have an iid sample of size n. I will use w to denote the relevant weighting variable.\n\n## A Closer Look\n\nMean estimation does not depend on the type of weights. We have:\n\n$$ \\bar{X} = \\frac{\\sum_i wX}{\\sum_i w}. $$\n\nVariance estimation depends on the weight type.\n\nRemember that we estimate the standard error of $\\bar{X}$ as:\n\n$$ SE(\\bar{X})=\\frac{s}{\\sqrt{n}}, \\hspace{.3cm} \\text{where} \\hspace{.3cm} s=\\sqrt{\\frac{1}{N-1}\\sum_i(X-\\bar{X})^2} $$\n\nis an estimate of the standard deviation of $X$. I will explain below how estimation of $SE(\\bar{X})$ differs for sampling and frequency weights. Importantly, s_w will denote the weighted version of this standard error.\n\n### Sampling Weights\n\nSampling weights measure the inverse probability of an observation entering the sample. They are particularly relevant in surveys when some units in the population are sampled more frequently than others. These weights adjust for sampling discrepancies to ensure that survey estimates are representative of the entire population. Sampling weights are also sometimes called probability weights.\n\nIntuitively, if we want to use the survey to accurately represent the population, we should assign higher importance to observations that are less likely to be sampled and lower importance to those more likely to appear in our data. This is exactly what sampling weights achieve. Sampling weights are, thus, inversely proportional to the probability of selection. Therefore, a smaller weight indicates a higher probability of being sampled, and vice versa.\n\nFor example, households in rural areas might be less likely to enter a survey due to the increased resources required to reach remote locations. Conversely, urban households are typically easier to sample and thus more likely to appear in the data.\n\nThis weighting approach is also common in causal inference analyses using propensity score methods. Similarly, in machine learning, weighting is often employed to adjust for unbalanced samples in the context of rare events (e.g., fraud detection, cancer diagnosis).\n\nLet’s now get back to the discussion on variance estimation. With sampling weights, we have:\n\n$$ SE^{\\text{sampling}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{S}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\frac{(\\sum_i w)^2}{\\sum_i w^2}}}$$\n\nThe numerator is the weighted version of the sum of squared deviations from the mean, emphasizing observations with higher weights. The denominator normalizes the standard error based on the total sum of the weights. When the weights differ greatly—high weights make some observations much more influential, increasing uncertainty about the population mean and so the sampling-weighted standard error is often larger.\n\nI will move on to describing frequency weights.\n\n*Software Package*: [survey](https://www.rdocumentation.org/packages/survey)\n\n### Frequency Weights\n\nFrequency weights measure the number of times an observation should be counted in the analysis. They are most relevant when there are multiple identical observations or when duplicating observations is possible. Naturally, higher weights correspond to observations that appear more frequently. Frequency weights are common in aggregated datasets. For instance, with market- or city-level data, we might want to weight the rows by market size, thus assigning more importance to larger units.\n\nWe can gain intuition from a linear algebra perspective. In a regression context, the design matrix X must be of full rank (to ensure invertibility), which implies no two rows can be identical. We thus need to collapse X to keep only distinct rows and record the number of times each row appears in the original data. This record constitutes the frequency weights.\n\nThe formula for estimating the standard error of X with frequency weights is:\n\n$$ SE^{\\text{frequency}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{F}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\sum_i w}}.$$\n\nHere $n_{\\text{eff}}$ is the effective sample size (i.e., the sum of all weights) and $s_w$ is the weighted standard deviation of $X$.\n\nThe frequency-weighted standard error is typically the smaller because it increases the effective sample size without introducing variability in the contribution of different observations. In contrast, the sampling-weighted standard error could be larger if some observations are given much higher weights, increasing the variability and lowering the precision.\n\n*Software Package*: [survey](https://www.rdocumentation.org/packages/survey).\n\n### One More Thing\n\nThere is actually one more type of weights which are not so commonly used in practice, precision weights. They represent the precision ($1/\\text{variance}$) of observations. A weight of 5, for example, reflects 5 times the precision of a weight of 1, originally based on averaging 5 replicate observations. Precision weights often come up in statistical theory in places such as [Generalized Least Squares](https://en.wikipedia.org/wiki/Generalized_least_squares) where they promise efficiency gains (i.e., lower variance) relative to traditional Ordinary Least Squares estimation. In practice, precision weights are in fact frequency weights normalized to sum to n. Lastly, [Stata’s user guide](https://www.stata.com/manuals/u.pdf) refers to them as analytic weights.\n\n## An Example\n\nLet’s see all of this in practice. We begin by creating a fake dataset of a variable $X$ with both sampling and frequency weights. The weights are randomly created and hence have no underlying meaning, so think of this example as a tutorial, without a strong focus on the results.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\n# clear workspace and load libraries\nlibrary(survey)\nrm(list=ls())\nset.seed(681)\n\n# generate fake data\nn <- 100000\ndata <- data.frame(\n  x = rnorm(n),\n  prob_selection = runif(n, .1, .9),\n  freq_weight = rpois(n, 3)\n)\ndata$samp_weight <- 1 / data$prob_selection\n\n# calculate the average value of $X$ using both types of weights.\ndesign_unweight <- svydesign(ids = ~1, data = data, weights = ~1)\ndesign_samp <- svydesign(ids = ~1, data = data, weights = ~samp_weight)\ndesign_freq <- svydesign(ids = ~1, data = data, weights = ~freq_weight)\n\nmean_unweight <- svymean(~x, design_unweight)\nmean_samp <- svymean(~x, design_samp)\nmean_freq <- svymean(~x, design_freq)\n\n# print results\nprint(round(mean_unweight, digits=3))\n>    mean     SE\n> x -0.002 0.0032\nprint(round(mean_samp, digits=3))\n>  mean     SE\n> x    0 0.0038\nprint(round(mean_freq, digits=3))\n>    mean     SE\n> x -0.002 0.0037\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.weightstats import DescrStatsW\n\n# Set seed for reproducibility\nnp.random.seed(681)\n\n# Generate fake data\nn = 100000\ndata = pd.DataFrame({\n    \"x\": np.random.normal(size=n),\n    \"prob_selection\": np.random.uniform(0.1, 0.9, size=n),\n    \"freq_weight\": np.random.poisson(3, size=n)\n})\ndata[\"samp_weight\"] = 1 / data[\"prob_selection\"]\n\n# Calculate the average value of X using both types of weights\n# Unweighted mean\nmean_unweight = DescrStatsW(data[\"x\"]).mean, DescrStatsW(data[\"x\"]).std_mean\n\n# Sampling-weighted mean\nmean_samp = DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).std_mean\n\n# Frequency-weighted mean\nmean_freq = DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).std_mean\n\n# Print results\nprint(\"Unweighted Mean and SE:\", np.round(mean_unweight, 3))\n> Unweighted Mean and SE: [-0.004  0.003]\nprint(\"Sampling-Weighted Mean and SE:\", np.round(mean_samp, 3))\n> Sampling-Weighted Mean and SE: [-0.002  0.002]\nprint(\"Frequency-Weighted Mean and SE:\", np.round(mean_freq, 3))\n> Frequency-Weighted Mean and SE: [-0.004  0.002]\n```\n\n::::\n\nThe unweighted and frequency-weighted means match exactly, (I am not sure why the sampling-weighted mean is slightly lower.) while the variances are different. The variance with frequency weights is lower than that of sampling weights.\n\n## Bottom Line\n\n- Weights are one of the most confusing aspects of working with data.\n\n- Sampling and frequency weights are the most common types of weights found in practice.\n\n- The former measure the inverse probability of being sampled, while the latter represent the number of times an observation enters the sample.\n\n- While weighting usually does not impact point estimates (e.g., regression coefficients, means), incorrect usage of weights can lead to inaccurate confidence intervals and p-values.\n\n- This is relevant only if *(i)* your dataset contains weights, and *(ii)* you are interested in population-level statistics.\n\n## Where to Learn More\n\nGoogle is a great starting place. [Lumley’s blog post](http://notstatschat.rbind.io/2020/08/04/weights-in-statistics/) titled Weights in Statistics was incredibly helpful in preparing this article. Stata’s manuals which are publicly available contain more detailed information on various types of weighting schemes. See also Solon et al. (2015) for using weights in causal inference.\n\n## References\n\nDupraz, Y. (2013). Using Weights in Stata. Memo, 54(2)\n\nLumley, T. (2020), Weights in Statistics, [Blog Post](https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/)\n\nSolon, G., Haider, S. J., & Wooldridge, J. M. (2015). What are we weighting for?. Journal of Human resources, 50(2), 301-316.\n\n[Stata User’s Guide](https://www.stata.com/manuals/u.pdf) (2023) "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"filters":["code-insertion"],"output-file":"weights-statistics.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js","../code/back-to-top.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>\n<script src=\"../code/open-links-new-tab.js\"></script>  \n<script src=\"../code/back-to-top.js\"></script>\n<link href=\"https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Code+Pro&display=swap\" rel=\"stylesheet\">\n"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html",{"text":"<button id=\"back-to-top\" onclick=\"scrollToTop()\">↑</button>\n"}]},"insert-before-post":"_sharebuttons.md","title":"Weights in Statistical Analyses","date":"2024-09-18","categories":["weights","statistical inference"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}