{"title":"The Variance of Propensity Score Matching Estimators","markdown":{"yaml":{"title":"The Variance of Propensity Score Matching Estimators","date":"2023-03-30","categories":["propensity score","causal inference"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nPropensity score matching (PSM) is among the most popular methods for estimating causal effects with observational data. It lends its fame to both its power and simplicity. Under a certain set of commonly invoked assumptions, controlling for the propensity score alone (as opposed to the full set of covariates) is enough to remove bias between the treatment and control groups.  The underlying idea of matching on the propensity score is incredibly simple and most data scientists can quickly estimate a treatment effect even from scratch.\n\nWhile estimating treatment effects is common and straightforward, calculating their variance along with the associated confidence intervals and p-values can be more challenging.  So, how exactly do you compute the variance of PSM methods? In this article I will describe the most popular ways of doing that. My goal is to cover the intuition and I will spare the technical details. Interested readers can refer to the References section below for more detailed expositions.\n\nStatistical methods based on the propensity score function come in different flavors (e.g., weighting, matching, subclassification, etc.), but today I will focus on $1:1$ or $1:N$ matching. There are also several potential estimands of interest, such as ATE (average treatment effect), ATT (ATE for the treatment group) but I will talk specifically about the latter.  \n\n## A Closer Look\n\nIn brief, the idea behind treatment effect estimation with PSM is:\n\n- Estimate the propensity score (i.e., the probability of being in the treatment group).\n- For each user in the treatment group find the user(s) in the control group with the most similar predicted propensity score value.\n- Analyze the outcome differences in each pair to arrive at a final ATT (or ATE) estimate.\n\nThroughout the article, I will assume a standard, well-behaved setting with SUTVA, unconfoundedness, and overlap assumptions in place. Similarly, I will ignore many important aspects of PSM methods such as trimming, propensity score and even treatment effect estimation. I will also use standard notation (e.g., $Y$ and $D$ denote outcome and treatment; and $\\tau$ is the ATT) without setting up the entire framework. \n\nWe can classify the methods for estimating the PSM variance in three distinct buckets.\n\n### Asymptotic Approximations\n\nRecall the [Law of Total Variance](https://en.wikipedia.org/wiki/Law_of_total_variance) stating that, given a conditioning variable $X$, we can always decompose a random variable $Y$’s variance, into two components – the expectation of the conditional variance and the variance of the conditional expectation:\n$$Var(Y) = Var(E(Y|X)) + E(Var(Y|X)).$$\n\n[Abadie and Imbens (2006)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00655.x) use this idea to derive the asymptotic variance formula for one-to-one and one-to-many matching estimators. They provide an analytical expression for the variance as $n \\to \\infty$ (and hence, this is an approximation) using the matching covariates as conditioning variables. I will not be going into their work in detail, but a couple of notes are worth discussing.\n\nThe original formulation assumes the true propensity score function is known. This is rarely true and in practice we rely on an estimate of it. Practitioners then replace this true propensity score with the estimated one. This is OK, but not great – it brings me to the second issue.\n\nOne needs to account for the fact that this propensity score is estimated. As such, it comes with uncertainty, which, if ignored, can potentially understate the variance of your estimator. Hello, type 1 errors!\n\nIn follow-up work, [Abadie and Imbens (2016)](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11293) propose correction which accounts for this uncertainty. Interestingly, in a work that I will discuss below, [Bodory et al. (2020)](https://www.tandfonline.com/doi/abs/10.1080/07350015.2018.1476247) report that in practice this correction might increase or decrease the ATT variance.\n\n### Approximation Based on Weights\n\nWe can express all treatment effect estimators as a difference of weighted means:\n\n$$\\hat{\\tau} = \\frac{1}{n} \\sum\\hat{w} D Y - \\frac{1}{n} \\sum\\hat{w} (1-D) Y,$$\n\nwhere the weights $\\hat{w}$ may depend on the propensity score.\n\n[Lechner (2002)](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-985X.0asp2) suggested calculating the variance of PSM based on this formula, assuming the weights are non-stochastic. In other words, the approach ignores the fact that the propensity score is estimated in a first step. The PSM variance is then equal to the sum of two terms – the one for the treated and the one for the control.\n\n### Bootstrap\n\nBootstrap is the go-to method for estimating variances in complex settings where analytical expressions are too messy or even do not exist. PSM seems like a natural environment where the bootstrap can help. Not so fast!\n\nAbadie and Imbens (2006) show that the standard nonparametric [bootstrap is inconsistent](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA6474) for non-smooth estimators (such as propensity score matching) with fixed number of matches and continuous covariates. Nevertheless, practitioners routinely ignore this result. While the theory says we should not use the bootstrap here, the extent to which this is of practical relevance is unclear. For instance, if this only makes a difference in the fourth decimal, we might be willing to sacrifice some bias for ease of computation.\n\nThere are at least two ways one can go around using the bootstrap here. The first one is the standard plain vanilla approach.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Randomly draw $B$ samples of size $n$. Alternatively, you can also sample directly from the [matched pairs](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.6276) which works better in certain cases.\n2. Compute the ATT each time.\n3. Compute the confidence interval:\n  $$ \\hat{\\tau}\\pm \\sqrt{\\hat{V}(\\hat{\\tau}) \\times c}, $$\n\nwhere $\\hat{V}(\\tau) = \\frac{1}{B-1}\\sum_{b=1}^B(\\hat{\\tau}^b-\\frac{1}{B}\\sum_b\\hat{\\tau}^b)^2$ is the bootstrap variance of $\\hat{\\tau}$ and $c$ is the critical value associated with a confidence level $\\alpha$.\n:::\n\nAlternatively, in this last step we can directly use the $\\frac{\\alpha}{2}$ and $1-\\frac{\\alpha}{2}$ quantiles of the bootstrap distribution of $\\hat{\\tau}$.\n\nThe second approach uses the well-known fact that the bootstrap behaves better when it uses so called asymptotically pivotal statistics – i.e., ones which asymptotic distribution does not depend on unknown quantities. In this setting a candidate would be the $t$-statistic which, as we know, under certain conditions has a standard normal asymptotic distribution. We proceed in three steps:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Compute the $t$-stat in the main sample using one of the variance approximations.\n2. Draw $B$ random samples of size $n$ and we compute the recentered $t$-stat with respect to $\\hat{\\tau}$ in the main sample,\n  $$T^b = \\frac{\\hat{\\tau}^b - \\hat{\\tau}}{\\sqrt{\\hat{V}(\\tau^b)}}.$$\n3. The $p$-value is the share of (absolute value) bootstrap $t$-stats larger than the absolute value of the $t$-stat in the main sample\n  $$p{\\text{-value}} = 1 - \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|\\leq |T|) = \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|> |T|),$$\n\nwhere $T$ is the $t$-stat from the main sample.\n:::\n\nThere are also [newer](https://www.dropbox.com/s/e4n2wct32uopsyi/PSM-Bootstrap-10.pdf?dl=0) ([wild](https://doc.rero.ch/record/261179/files/WP_SES_470.pdf)) [bootstrap](https://www.tandfonline.com/doi/abs/10.1080/01621459.2016.1231613) ideas which are still making their way into the mainstream.\n\n### Monte Carlo Simulations\n\nBodory et al. (2020) compare the finite sample performance of several of these methods (plus others). Their analysis is more thorough in the sense that they include other ATT estimators besides PSM, such as PS weighting or radius matching.\n\nOverall, their results do not indicate a clear winner which performs best in all scenarios. Interestingly, even some of the bootstrap methods often outperform the asymptotic approximations of Abadie and Imbens (2006) or Lechner (2002).\n\n## Where to Learn More\n\nI based this post on Bodory et al. (2020)’s paper, so that is a natural place to start. Many other studies have focused on the performance of propensity score treatment effect estimators (as opposed to their variance) – [Huber et al. (2013)](https://www.sciencedirect.com/science/article/abs/pii/S0304407613000390) and [Busso et al. (2014)](https://direct.mit.edu/rest/article-abstract/96/5/885/58201/New-Evidence-on-the-Finite-Sample-Properties-of) are great starting points. Lastly, [Imbens (2015)](https://jhr.uwpress.org/content/50/2/373.short) is a great resource for an overview of matching methods more generally along with some of the problems they solve as well as the pitfalls they entail.\n\n## Bottom Line\n\n- There is no shortage of methods when it comes to estimating the variance of propensity score matching estimators.\n\n- Abadie and Imbens (2006) showed that the standard bootstrap is inconsistent in this setting with continuous covariates and formally derived an asymptotic approximation of the true variance.\n\n- Monte Carlo simulations, however, show that bootstrap methods offer a good tradeoff between simplicity and performance.\n\n## References\n\nAbadie, A., & Imbens, G. W. (2006). Large sample properties of matching estimators for average treatment effects. Econometrica, 74(1), 235-267.\n\nAbadie, A., & Imbens, G. W. (2008). On the failure of the bootstrap for matching estimators. Econometrica, 76(6), 1537-1557.\n\nAustin, P. C., & Small, D. S. (2014). The use of bootstrapping when using propensity‐score matching without replacement: a simulation study. Statistics in medicine, 33(24), 4306-4319.\n\nBodory, H., Camponovo, L., Huber, M., & Lechner, M. (2020). The finite sample performance of inference methods for propensity score matching and weighting estimators. Journal of Business & Economic Statistics, 38(1), 183-200.\n\nBusso, M., DiNardo, J., & McCrary, J. (2014). New evidence on the finite sample properties of propensity score reweighting and matching estimators. Review of Economics and Statistics, 96(5), 885-897.\n\nCaliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of propensity score matching. Journal of economic surveys, 22(1), 31-72.\n\nHuber, M., Lechner, M., & Wunsch, C. (2013). The performance of estimators based on the propensity score. Journal of Econometrics, 175(1), 1-21.\n\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\n\nLechner, M. (2002). Some practical issues in the evaluation of heterogeneous labour market programmes by matching methods. Journal of the Royal Statistical Society: Series A (Statistics in Society), 165(1), 59-82.","srcMarkdownNoYaml":"\n\n## Background\n\nPropensity score matching (PSM) is among the most popular methods for estimating causal effects with observational data. It lends its fame to both its power and simplicity. Under a certain set of commonly invoked assumptions, controlling for the propensity score alone (as opposed to the full set of covariates) is enough to remove bias between the treatment and control groups.  The underlying idea of matching on the propensity score is incredibly simple and most data scientists can quickly estimate a treatment effect even from scratch.\n\nWhile estimating treatment effects is common and straightforward, calculating their variance along with the associated confidence intervals and p-values can be more challenging.  So, how exactly do you compute the variance of PSM methods? In this article I will describe the most popular ways of doing that. My goal is to cover the intuition and I will spare the technical details. Interested readers can refer to the References section below for more detailed expositions.\n\nStatistical methods based on the propensity score function come in different flavors (e.g., weighting, matching, subclassification, etc.), but today I will focus on $1:1$ or $1:N$ matching. There are also several potential estimands of interest, such as ATE (average treatment effect), ATT (ATE for the treatment group) but I will talk specifically about the latter.  \n\n## A Closer Look\n\nIn brief, the idea behind treatment effect estimation with PSM is:\n\n- Estimate the propensity score (i.e., the probability of being in the treatment group).\n- For each user in the treatment group find the user(s) in the control group with the most similar predicted propensity score value.\n- Analyze the outcome differences in each pair to arrive at a final ATT (or ATE) estimate.\n\nThroughout the article, I will assume a standard, well-behaved setting with SUTVA, unconfoundedness, and overlap assumptions in place. Similarly, I will ignore many important aspects of PSM methods such as trimming, propensity score and even treatment effect estimation. I will also use standard notation (e.g., $Y$ and $D$ denote outcome and treatment; and $\\tau$ is the ATT) without setting up the entire framework. \n\nWe can classify the methods for estimating the PSM variance in three distinct buckets.\n\n### Asymptotic Approximations\n\nRecall the [Law of Total Variance](https://en.wikipedia.org/wiki/Law_of_total_variance) stating that, given a conditioning variable $X$, we can always decompose a random variable $Y$’s variance, into two components – the expectation of the conditional variance and the variance of the conditional expectation:\n$$Var(Y) = Var(E(Y|X)) + E(Var(Y|X)).$$\n\n[Abadie and Imbens (2006)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00655.x) use this idea to derive the asymptotic variance formula for one-to-one and one-to-many matching estimators. They provide an analytical expression for the variance as $n \\to \\infty$ (and hence, this is an approximation) using the matching covariates as conditioning variables. I will not be going into their work in detail, but a couple of notes are worth discussing.\n\nThe original formulation assumes the true propensity score function is known. This is rarely true and in practice we rely on an estimate of it. Practitioners then replace this true propensity score with the estimated one. This is OK, but not great – it brings me to the second issue.\n\nOne needs to account for the fact that this propensity score is estimated. As such, it comes with uncertainty, which, if ignored, can potentially understate the variance of your estimator. Hello, type 1 errors!\n\nIn follow-up work, [Abadie and Imbens (2016)](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11293) propose correction which accounts for this uncertainty. Interestingly, in a work that I will discuss below, [Bodory et al. (2020)](https://www.tandfonline.com/doi/abs/10.1080/07350015.2018.1476247) report that in practice this correction might increase or decrease the ATT variance.\n\n### Approximation Based on Weights\n\nWe can express all treatment effect estimators as a difference of weighted means:\n\n$$\\hat{\\tau} = \\frac{1}{n} \\sum\\hat{w} D Y - \\frac{1}{n} \\sum\\hat{w} (1-D) Y,$$\n\nwhere the weights $\\hat{w}$ may depend on the propensity score.\n\n[Lechner (2002)](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-985X.0asp2) suggested calculating the variance of PSM based on this formula, assuming the weights are non-stochastic. In other words, the approach ignores the fact that the propensity score is estimated in a first step. The PSM variance is then equal to the sum of two terms – the one for the treated and the one for the control.\n\n### Bootstrap\n\nBootstrap is the go-to method for estimating variances in complex settings where analytical expressions are too messy or even do not exist. PSM seems like a natural environment where the bootstrap can help. Not so fast!\n\nAbadie and Imbens (2006) show that the standard nonparametric [bootstrap is inconsistent](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA6474) for non-smooth estimators (such as propensity score matching) with fixed number of matches and continuous covariates. Nevertheless, practitioners routinely ignore this result. While the theory says we should not use the bootstrap here, the extent to which this is of practical relevance is unclear. For instance, if this only makes a difference in the fourth decimal, we might be willing to sacrifice some bias for ease of computation.\n\nThere are at least two ways one can go around using the bootstrap here. The first one is the standard plain vanilla approach.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Randomly draw $B$ samples of size $n$. Alternatively, you can also sample directly from the [matched pairs](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.6276) which works better in certain cases.\n2. Compute the ATT each time.\n3. Compute the confidence interval:\n  $$ \\hat{\\tau}\\pm \\sqrt{\\hat{V}(\\hat{\\tau}) \\times c}, $$\n\nwhere $\\hat{V}(\\tau) = \\frac{1}{B-1}\\sum_{b=1}^B(\\hat{\\tau}^b-\\frac{1}{B}\\sum_b\\hat{\\tau}^b)^2$ is the bootstrap variance of $\\hat{\\tau}$ and $c$ is the critical value associated with a confidence level $\\alpha$.\n:::\n\nAlternatively, in this last step we can directly use the $\\frac{\\alpha}{2}$ and $1-\\frac{\\alpha}{2}$ quantiles of the bootstrap distribution of $\\hat{\\tau}$.\n\nThe second approach uses the well-known fact that the bootstrap behaves better when it uses so called asymptotically pivotal statistics – i.e., ones which asymptotic distribution does not depend on unknown quantities. In this setting a candidate would be the $t$-statistic which, as we know, under certain conditions has a standard normal asymptotic distribution. We proceed in three steps:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Compute the $t$-stat in the main sample using one of the variance approximations.\n2. Draw $B$ random samples of size $n$ and we compute the recentered $t$-stat with respect to $\\hat{\\tau}$ in the main sample,\n  $$T^b = \\frac{\\hat{\\tau}^b - \\hat{\\tau}}{\\sqrt{\\hat{V}(\\tau^b)}}.$$\n3. The $p$-value is the share of (absolute value) bootstrap $t$-stats larger than the absolute value of the $t$-stat in the main sample\n  $$p{\\text{-value}} = 1 - \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|\\leq |T|) = \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|> |T|),$$\n\nwhere $T$ is the $t$-stat from the main sample.\n:::\n\nThere are also [newer](https://www.dropbox.com/s/e4n2wct32uopsyi/PSM-Bootstrap-10.pdf?dl=0) ([wild](https://doc.rero.ch/record/261179/files/WP_SES_470.pdf)) [bootstrap](https://www.tandfonline.com/doi/abs/10.1080/01621459.2016.1231613) ideas which are still making their way into the mainstream.\n\n### Monte Carlo Simulations\n\nBodory et al. (2020) compare the finite sample performance of several of these methods (plus others). Their analysis is more thorough in the sense that they include other ATT estimators besides PSM, such as PS weighting or radius matching.\n\nOverall, their results do not indicate a clear winner which performs best in all scenarios. Interestingly, even some of the bootstrap methods often outperform the asymptotic approximations of Abadie and Imbens (2006) or Lechner (2002).\n\n## Where to Learn More\n\nI based this post on Bodory et al. (2020)’s paper, so that is a natural place to start. Many other studies have focused on the performance of propensity score treatment effect estimators (as opposed to their variance) – [Huber et al. (2013)](https://www.sciencedirect.com/science/article/abs/pii/S0304407613000390) and [Busso et al. (2014)](https://direct.mit.edu/rest/article-abstract/96/5/885/58201/New-Evidence-on-the-Finite-Sample-Properties-of) are great starting points. Lastly, [Imbens (2015)](https://jhr.uwpress.org/content/50/2/373.short) is a great resource for an overview of matching methods more generally along with some of the problems they solve as well as the pitfalls they entail.\n\n## Bottom Line\n\n- There is no shortage of methods when it comes to estimating the variance of propensity score matching estimators.\n\n- Abadie and Imbens (2006) showed that the standard bootstrap is inconsistent in this setting with continuous covariates and formally derived an asymptotic approximation of the true variance.\n\n- Monte Carlo simulations, however, show that bootstrap methods offer a good tradeoff between simplicity and performance.\n\n## References\n\nAbadie, A., & Imbens, G. W. (2006). Large sample properties of matching estimators for average treatment effects. Econometrica, 74(1), 235-267.\n\nAbadie, A., & Imbens, G. W. (2008). On the failure of the bootstrap for matching estimators. Econometrica, 76(6), 1537-1557.\n\nAustin, P. C., & Small, D. S. (2014). The use of bootstrapping when using propensity‐score matching without replacement: a simulation study. Statistics in medicine, 33(24), 4306-4319.\n\nBodory, H., Camponovo, L., Huber, M., & Lechner, M. (2020). The finite sample performance of inference methods for propensity score matching and weighting estimators. Journal of Business & Economic Statistics, 38(1), 183-200.\n\nBusso, M., DiNardo, J., & McCrary, J. (2014). New evidence on the finite sample properties of propensity score reweighting and matching estimators. Review of Economics and Statistics, 96(5), 885-897.\n\nCaliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of propensity score matching. Journal of economic surveys, 22(1), 31-72.\n\nHuber, M., Lechner, M., & Wunsch, C. (2013). The performance of estimators based on the propensity score. Journal of Econometrics, 175(1), 1-21.\n\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\n\nLechner, M. (2002). Some practical issues in the evaluation of heterogeneous labour market programmes by matching methods. Journal of the Royal Statistical Society: Series A (Statistics in Society), 165(1), 59-82."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"filters":["code-insertion"],"output-file":"variance-ps-matching.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html"]},"insert-before-post":"_sharebuttons.md","title":"The Variance of Propensity Score Matching Estimators","date":"2023-03-30","categories":["propensity score","causal inference"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}