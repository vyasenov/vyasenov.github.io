{"title":"The Three Classes of Statistical Models","markdown":{"yaml":{"title":"The Three Classes of Statistical Models","date":"2025-01-12","categories":["statistical models"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nStatistical modeling is among the most exciting elements of working with data. When mentoring junior data scientists, I never fail to see the spark in their eyes when our projects have overcome the data gathering and cleaning stages and have reached that precious Rubicon.\n\nA major goal in the craft of statistical modeling is extracting the most information from the data at hand. Statisticians and econometricians call this efficiency or precision. In simple terms, this means achieving the lowest possible variance from a class of available estimators. As an example, imagine two findings—both centered at $3\\%$ (e.g., the treatment effect of an intervention of interest)—but one with a $95\\%$ confidence interval of $[2\\%, 4\\%]$, and the other with $[-7\\%, 13\\%]$. The former is clearly more informative and useful than the latter.\n\nIn a series of four articles, I will dive deeper into the topic of efficiency with a varying degree of detail and complexity. As a first step in this exploration, I want to demystify the distinction between the three classes of statistical models: parametric, semiparametric, and nonparametric. The goal of this article is to explore in depth the concepts of parameterizing a model and the distinctions between these three groups.\n\n## A Closer Look\n\n### Parametric Models\n\nParametric models impose strong assumptions about the underlying data-generating process, making them less flexible but highly interpretable and data-efficient. They require fewer data points to estimate the target accurately compared to semiparametric or nonparametric models. Consequently, they are often the default choice in practice.\n\nA classic example is density estimation. Suppose you observe that your data appears symmetric around its mean. You might assume the data follows a normal (Gaussian) distribution. In this case, specifying the mean $\\mu$ and variance $\\sigma^2$ fully characterizes the distribution, allowing you to estimate the density parametrically. And then, you can make all kinds of calculations related to your variable, such as what is the probability that it will take values greater than a given number $c$.\n\nFormally, a parametric model describes the data-generating process entirely using a finite-dimensional parameter vector. As yet another example, in the omnipresent linear model:\n\n$$Y = X\\beta + \\epsilon,$$\n\nthe parameter \\beta specifies the entire relationship between $X$ and $Y$. This imposes a strong assumption: the relationship is linear, meaning a unit change in $X$ consistently results in a $\\beta$ change in $Y$, regardless of $X$‘s magnitude (small or large). While convenient, this assumption may not hold in all practical scenarios, as real-world relationships often are complex and deviate from linearity.\n\n### Semiparametric Models\n\nSemiparametric models combine the structure of parametric models with the flexibility of nonparametric methods. They specify certain aspects of the data-generating process parametrically while leaving other aspects unspecified or modeled flexibly.\n\nConsider the partially linear model:\n\n$$Y = X \\beta + g(Z) + \\epsilon,$$\n\nwhere $\\beta$ is a parametric component describing the linear effect of $X$, while $g(Z)$ is an unknown and potentially complex function capturing the nonparametric effect of a covariate matrix $Z$. Here, the model imposes linearity on $X$‘s effect but allows $Z$‘s effect to be fully flexible.\n\nSemiparametric models are highly versatile, balancing the interpretability of parametric models with the adaptability of nonparametric ones. However, estimating $\\beta$ efficiently while accounting for the unknown $g(Z)$ poses challenges, often requiring further assumptions.\n\nIn practice, semiparametric models play a crucial role in causal inference, particularly when working with observational cross-sectional data (see Imbens and Rubin 2015). A common strategy involves estimating propensity scores using parametric models (like logistic regression) and then employing nonparametric techniques such as kernel weighting, matching, or stratification to estimate treatment effects. This hybrid approach helps address confounding while maintaining computational tractability and interpretability.\n\n### Nonparametric Models\n\nNonparametric models make minimal assumptions about the underlying data-generating process, allowing the data to “speak for itself”. Unlike parametric and semiparametric models, these ones do not specify a finite-dimensional parameter vector or impose rigid structural assumptions. This flexibility makes them highly robust to model misspecification but often requires larger datasets to achieve accurate estimates.\n\nLet’s get back to the density estimation example. Histograms are a form of nonparametric models as they do not assume specific underlying functional form. Instead of fitting a parametric distribution (e.g., normal), you might use a kernel density estimator:\n\n  $$\\hat{f}(x)=\\frac{1}{nh}\\sum_i K \\left( \\frac{x-X_i}{h} \\right),$$\n\nwhere $K(\\cdot)$ is a kernel function, and $h$ is a bandwidth parameter controlling the smoothness of the estimate. А kernel function assigns varying weight to observations around a data point. They are non-negative, symmetric around zero, and sum to one. Commonly employed kernel functions include:\n\n- Gaussian $K(\\cdot)=\\frac{1}{\\sqrt{2 \\pi}}e^{-0.5x^2}$,\n- Epanechnikov $K(\\cdot)=\\frac{3}{4}(1-x^2)$,\n- Rectangular: $K(\\cdot)=0.5$.\n\nThis approach does not rely on assumptions about the data’s shape, allowing it to adapt to various distributions.\n\nNonparametric regression provides another illustration. In it, the relationship between $X$ and $Y$ is modeled as:\n\n$$Y=m(X)+\\epsilon,$$\n\nwhere $m(X)$ is an unknown function estimated directly from the data using methods like local polynomial regression, splines. Unlike the linearity assumption in parametric models, nonparametric regression allows $m(X)$ to capture complex, nonlinear relationships. A commonly used variant of this is [LOESS regression](https://en.wikipedia.org/wiki/Local_regression) often overlayed in bivariate scatterplots.\n\nMany popular machine learning (ML) methods are also nonparametric. In this context it is important to distinguish between parameters which impose assumptions on the data and hyperparameters which serve to tune the algorithm. Tree-based techniques exemplify the nonparametric nature of ML: gradient boosting machines, random forests, and individual decision trees can all grow more complex as they encounter more data, creating flexible models that capture intricate patterns. Think even of unsupervised clustering models including $k$-means and hierarchical clustering.\n\nWhile nonparametric models are powerful, their flexibility comes at a cost: they can overfit small datasets and often require careful tuning (e.g., choosing the kernel bandwidth) to balance bias and variance. Nonetheless, they are invaluable for exploratory analysis and applications where minimal assumptions are desired.\n\nDid I also mention the curse of dimensionality?! You will quickly fall into its trap with even moderate number of variables. Yes, there is just so much you can do without adding some structure to your models.\n\n## An Example\n\nWe illustrate the distinctions between parametric, semiparametric, and nonparametric models using a toy example. We generate $1,000$ observations of\n\n$$Y=sin(X)+\\epsilon,$$\n\nwhere $X$ is uniformly distributed and \\epsilon is normally distributed noise. We then model the relationship between $Y$ and $X$ using three approaches. \n\n:::: {.panel-tabset}\n\n### R\n\n```r\n# clear workspace and load libraries\nrm(list=ls())\nset.seed(1988)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nx <- runif(1000, 0, 10)  # Uniformly distributed x\ny <- sin(x) + rnorm(1000, mean = 0, sd = 0.3)  # Non-linear relationship with noise\ndata <- data.frame(x = x, y = y)\n\n# Fit models\nlinear_model <- lm(y ~ x, data = data)\nloess_model <- loess(y ~ x, data = data, span = 0.3)\n\n# Piecewise linear model\nsplit_point <- median(data$x)\ndata <- data %>%\n  mutate(split_group = ifelse(x <= split_point, \"first_half\", \"second_half\"))\n\nlinear1 <- lm(y ~ x, data = filter(data, split_group == \"first_half\"))\nlinear2 <- lm(y ~ x, data = filter(data, split_group == \"second_half\"))\n\n# Predictions\ndata <- data %>%\n  mutate(pred_linear = predict(linear_model, newdata = data),\n         pred_loess = predict(loess_model, newdata = data))\n\npiecewise_preds <- bind_rows(\n  data.frame(x = filter(data, split_group == \"first_half\")$x,\n             y = predict(linear1, newdata = filter(data, split_group == \"first_half\"))),\n  data.frame(x = filter(data, split_group == \"second_half\")$x,\n             y = predict(linear2, newdata = filter(data, split_group == \"second_half\")))\n) %>%\n  arrange(x)\n\n# Plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  geom_line(aes(y = pred_linear, color = \"Parametric (Linear)\"), linewidth = 1) +\n  geom_line(aes(y = pred_loess, color = \"Nonparametric (Loess)\"), linewidth = 1) +\n  geom_line(data = piecewise_preds, aes(x = x, y = y, color = \"Semiparametric (Piecewise Linear)\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Parametric (Linear)\" = \"red\",\n                                \"Nonparametric (Loess)\" = \"blue\",\n                                \"Semiparametric (Piecewise Linear)\" = \"green\")) +\n  labs(title = \"Bivariate Scatterplot with Regression Fits\",\n       x = \"X\", y = \"Y\", color = \"Model Type\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = c(0.8, 0.2))\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nnp.random.seed(1988)\n\nx = np.random.uniform(0, 10, 1000)\ny = np.sin(x) + np.random.normal(0, 0.3, 1000)\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Linear model\nlinear_model = LinearRegression()\nlinear_model.fit(data[['x']], data['y'])\ndata['pred_linear'] = linear_model.predict(data[['x']])\n\n# Loess model\nloess_result = lowess(data['y'], data['x'], frac=0.3, return_sorted=True)\ndata['pred_loess'] = np.interp(data['x'], loess_result[:, 0], loess_result[:, 1])\n\n# Piecewise linear\nsplit_point = np.median(data['x'])\ndata['split_group'] = np.where(data['x'] <= split_point, 'first_half', 'second_half')\n\nfirst_half = data[data['split_group'] == 'first_half'].copy()\nsecond_half = data[data['split_group'] == 'second_half'].copy()\n\nlinear1 = LinearRegression()\nlinear1.fit(first_half[['x']], first_half['y'])\nfirst_half['pred_piecewise'] = linear1.predict(first_half[['x']])\n\nlinear2 = LinearRegression()\nlinear2.fit(second_half[['x']], second_half['y'])\nsecond_half['pred_piecewise'] = linear2.predict(second_half[['x']])\n\npiecewise_preds = pd.concat([first_half[['x', 'pred_piecewise']],\n                             second_half[['x', 'pred_piecewise']]]).sort_values('x')\n\n# Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='x', y='y', data=data, alpha=0.6, label='Data')\nplt.plot(data.sort_values('x')['x'], data.sort_values('x')['pred_linear'], color='red', label='Parametric (Linear)', linewidth=1)\nplt.plot(data.sort_values('x')['x'], data.sort_values('x')['pred_loess'], color='blue', label='Nonparametric (Loess)', linewidth=1)\nplt.plot(piecewise_preds['x'], piecewise_preds['pred_piecewise'], color='green', label='Semiparametric (Piecewise Linear)', linewidth=1)\n\nplt.title('Bivariate Scatterplot with Regression Fits', fontsize=14)\nplt.xlabel('X', fontsize=12)\nplt.ylabel('Y', fontsize=12)\nplt.legend(title='Model Type', loc='lower right')\nplt.grid(True)\nplt.show()\n```\n\n::::\n\nA parametric model, such as linear regression (although it could also be quadratic or a higher order polynomial), provides a simple, interpretable approximation but may miss crucial aspects of the true relationship. A “semiparametric” model, like piecewise linear regression, offers greater flexibility by allowing for changes in slope, capturing some curvature while maintaining a degree of interpretability. (One can cast this piecewise linear model as a parametric one, but for simplicity’s sake let’s go with this uncommon and imprecise definition of semiparametric.) Finally, a nonparametric model, such as LOESS, provides the most flexible representation, closely following the underlying sinusoidal pattern but potentially leading to overfitting.\n\n![](../images/three_model_classes.png)\n\nYou should not be surprised. This example demonstrates how the choice of model class significantly impacts the flexibility and interpretability of the fitted relationship. Do not take this example too seriously, it merely serves to illustrate the varying degree of complexity of statistical models.\n\n## Bottom Line\n\n- Parametric models impose the strongest assumptions and require the least amount of data. These are most models employed in practice. Think of linear regression.\n\n- Semiparametric models strike balance between flexibility and interpretation while allowing for flexible relationships in the data. Think of (non-Augmented) Inverse Propensity Score Weighting.\n\n- Nonparametric models are flexible and data-hungry. They allow for flexible associations between your variables. Think of a histrogram or kernel density.\n","srcMarkdownNoYaml":"\n\n## Background\n\nStatistical modeling is among the most exciting elements of working with data. When mentoring junior data scientists, I never fail to see the spark in their eyes when our projects have overcome the data gathering and cleaning stages and have reached that precious Rubicon.\n\nA major goal in the craft of statistical modeling is extracting the most information from the data at hand. Statisticians and econometricians call this efficiency or precision. In simple terms, this means achieving the lowest possible variance from a class of available estimators. As an example, imagine two findings—both centered at $3\\%$ (e.g., the treatment effect of an intervention of interest)—but one with a $95\\%$ confidence interval of $[2\\%, 4\\%]$, and the other with $[-7\\%, 13\\%]$. The former is clearly more informative and useful than the latter.\n\nIn a series of four articles, I will dive deeper into the topic of efficiency with a varying degree of detail and complexity. As a first step in this exploration, I want to demystify the distinction between the three classes of statistical models: parametric, semiparametric, and nonparametric. The goal of this article is to explore in depth the concepts of parameterizing a model and the distinctions between these three groups.\n\n## A Closer Look\n\n### Parametric Models\n\nParametric models impose strong assumptions about the underlying data-generating process, making them less flexible but highly interpretable and data-efficient. They require fewer data points to estimate the target accurately compared to semiparametric or nonparametric models. Consequently, they are often the default choice in practice.\n\nA classic example is density estimation. Suppose you observe that your data appears symmetric around its mean. You might assume the data follows a normal (Gaussian) distribution. In this case, specifying the mean $\\mu$ and variance $\\sigma^2$ fully characterizes the distribution, allowing you to estimate the density parametrically. And then, you can make all kinds of calculations related to your variable, such as what is the probability that it will take values greater than a given number $c$.\n\nFormally, a parametric model describes the data-generating process entirely using a finite-dimensional parameter vector. As yet another example, in the omnipresent linear model:\n\n$$Y = X\\beta + \\epsilon,$$\n\nthe parameter \\beta specifies the entire relationship between $X$ and $Y$. This imposes a strong assumption: the relationship is linear, meaning a unit change in $X$ consistently results in a $\\beta$ change in $Y$, regardless of $X$‘s magnitude (small or large). While convenient, this assumption may not hold in all practical scenarios, as real-world relationships often are complex and deviate from linearity.\n\n### Semiparametric Models\n\nSemiparametric models combine the structure of parametric models with the flexibility of nonparametric methods. They specify certain aspects of the data-generating process parametrically while leaving other aspects unspecified or modeled flexibly.\n\nConsider the partially linear model:\n\n$$Y = X \\beta + g(Z) + \\epsilon,$$\n\nwhere $\\beta$ is a parametric component describing the linear effect of $X$, while $g(Z)$ is an unknown and potentially complex function capturing the nonparametric effect of a covariate matrix $Z$. Here, the model imposes linearity on $X$‘s effect but allows $Z$‘s effect to be fully flexible.\n\nSemiparametric models are highly versatile, balancing the interpretability of parametric models with the adaptability of nonparametric ones. However, estimating $\\beta$ efficiently while accounting for the unknown $g(Z)$ poses challenges, often requiring further assumptions.\n\nIn practice, semiparametric models play a crucial role in causal inference, particularly when working with observational cross-sectional data (see Imbens and Rubin 2015). A common strategy involves estimating propensity scores using parametric models (like logistic regression) and then employing nonparametric techniques such as kernel weighting, matching, or stratification to estimate treatment effects. This hybrid approach helps address confounding while maintaining computational tractability and interpretability.\n\n### Nonparametric Models\n\nNonparametric models make minimal assumptions about the underlying data-generating process, allowing the data to “speak for itself”. Unlike parametric and semiparametric models, these ones do not specify a finite-dimensional parameter vector or impose rigid structural assumptions. This flexibility makes them highly robust to model misspecification but often requires larger datasets to achieve accurate estimates.\n\nLet’s get back to the density estimation example. Histograms are a form of nonparametric models as they do not assume specific underlying functional form. Instead of fitting a parametric distribution (e.g., normal), you might use a kernel density estimator:\n\n  $$\\hat{f}(x)=\\frac{1}{nh}\\sum_i K \\left( \\frac{x-X_i}{h} \\right),$$\n\nwhere $K(\\cdot)$ is a kernel function, and $h$ is a bandwidth parameter controlling the smoothness of the estimate. А kernel function assigns varying weight to observations around a data point. They are non-negative, symmetric around zero, and sum to one. Commonly employed kernel functions include:\n\n- Gaussian $K(\\cdot)=\\frac{1}{\\sqrt{2 \\pi}}e^{-0.5x^2}$,\n- Epanechnikov $K(\\cdot)=\\frac{3}{4}(1-x^2)$,\n- Rectangular: $K(\\cdot)=0.5$.\n\nThis approach does not rely on assumptions about the data’s shape, allowing it to adapt to various distributions.\n\nNonparametric regression provides another illustration. In it, the relationship between $X$ and $Y$ is modeled as:\n\n$$Y=m(X)+\\epsilon,$$\n\nwhere $m(X)$ is an unknown function estimated directly from the data using methods like local polynomial regression, splines. Unlike the linearity assumption in parametric models, nonparametric regression allows $m(X)$ to capture complex, nonlinear relationships. A commonly used variant of this is [LOESS regression](https://en.wikipedia.org/wiki/Local_regression) often overlayed in bivariate scatterplots.\n\nMany popular machine learning (ML) methods are also nonparametric. In this context it is important to distinguish between parameters which impose assumptions on the data and hyperparameters which serve to tune the algorithm. Tree-based techniques exemplify the nonparametric nature of ML: gradient boosting machines, random forests, and individual decision trees can all grow more complex as they encounter more data, creating flexible models that capture intricate patterns. Think even of unsupervised clustering models including $k$-means and hierarchical clustering.\n\nWhile nonparametric models are powerful, their flexibility comes at a cost: they can overfit small datasets and often require careful tuning (e.g., choosing the kernel bandwidth) to balance bias and variance. Nonetheless, they are invaluable for exploratory analysis and applications where minimal assumptions are desired.\n\nDid I also mention the curse of dimensionality?! You will quickly fall into its trap with even moderate number of variables. Yes, there is just so much you can do without adding some structure to your models.\n\n## An Example\n\nWe illustrate the distinctions between parametric, semiparametric, and nonparametric models using a toy example. We generate $1,000$ observations of\n\n$$Y=sin(X)+\\epsilon,$$\n\nwhere $X$ is uniformly distributed and \\epsilon is normally distributed noise. We then model the relationship between $Y$ and $X$ using three approaches. \n\n:::: {.panel-tabset}\n\n### R\n\n```r\n# clear workspace and load libraries\nrm(list=ls())\nset.seed(1988)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nx <- runif(1000, 0, 10)  # Uniformly distributed x\ny <- sin(x) + rnorm(1000, mean = 0, sd = 0.3)  # Non-linear relationship with noise\ndata <- data.frame(x = x, y = y)\n\n# Fit models\nlinear_model <- lm(y ~ x, data = data)\nloess_model <- loess(y ~ x, data = data, span = 0.3)\n\n# Piecewise linear model\nsplit_point <- median(data$x)\ndata <- data %>%\n  mutate(split_group = ifelse(x <= split_point, \"first_half\", \"second_half\"))\n\nlinear1 <- lm(y ~ x, data = filter(data, split_group == \"first_half\"))\nlinear2 <- lm(y ~ x, data = filter(data, split_group == \"second_half\"))\n\n# Predictions\ndata <- data %>%\n  mutate(pred_linear = predict(linear_model, newdata = data),\n         pred_loess = predict(loess_model, newdata = data))\n\npiecewise_preds <- bind_rows(\n  data.frame(x = filter(data, split_group == \"first_half\")$x,\n             y = predict(linear1, newdata = filter(data, split_group == \"first_half\"))),\n  data.frame(x = filter(data, split_group == \"second_half\")$x,\n             y = predict(linear2, newdata = filter(data, split_group == \"second_half\")))\n) %>%\n  arrange(x)\n\n# Plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  geom_line(aes(y = pred_linear, color = \"Parametric (Linear)\"), linewidth = 1) +\n  geom_line(aes(y = pred_loess, color = \"Nonparametric (Loess)\"), linewidth = 1) +\n  geom_line(data = piecewise_preds, aes(x = x, y = y, color = \"Semiparametric (Piecewise Linear)\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Parametric (Linear)\" = \"red\",\n                                \"Nonparametric (Loess)\" = \"blue\",\n                                \"Semiparametric (Piecewise Linear)\" = \"green\")) +\n  labs(title = \"Bivariate Scatterplot with Regression Fits\",\n       x = \"X\", y = \"Y\", color = \"Model Type\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = c(0.8, 0.2))\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nnp.random.seed(1988)\n\nx = np.random.uniform(0, 10, 1000)\ny = np.sin(x) + np.random.normal(0, 0.3, 1000)\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Linear model\nlinear_model = LinearRegression()\nlinear_model.fit(data[['x']], data['y'])\ndata['pred_linear'] = linear_model.predict(data[['x']])\n\n# Loess model\nloess_result = lowess(data['y'], data['x'], frac=0.3, return_sorted=True)\ndata['pred_loess'] = np.interp(data['x'], loess_result[:, 0], loess_result[:, 1])\n\n# Piecewise linear\nsplit_point = np.median(data['x'])\ndata['split_group'] = np.where(data['x'] <= split_point, 'first_half', 'second_half')\n\nfirst_half = data[data['split_group'] == 'first_half'].copy()\nsecond_half = data[data['split_group'] == 'second_half'].copy()\n\nlinear1 = LinearRegression()\nlinear1.fit(first_half[['x']], first_half['y'])\nfirst_half['pred_piecewise'] = linear1.predict(first_half[['x']])\n\nlinear2 = LinearRegression()\nlinear2.fit(second_half[['x']], second_half['y'])\nsecond_half['pred_piecewise'] = linear2.predict(second_half[['x']])\n\npiecewise_preds = pd.concat([first_half[['x', 'pred_piecewise']],\n                             second_half[['x', 'pred_piecewise']]]).sort_values('x')\n\n# Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='x', y='y', data=data, alpha=0.6, label='Data')\nplt.plot(data.sort_values('x')['x'], data.sort_values('x')['pred_linear'], color='red', label='Parametric (Linear)', linewidth=1)\nplt.plot(data.sort_values('x')['x'], data.sort_values('x')['pred_loess'], color='blue', label='Nonparametric (Loess)', linewidth=1)\nplt.plot(piecewise_preds['x'], piecewise_preds['pred_piecewise'], color='green', label='Semiparametric (Piecewise Linear)', linewidth=1)\n\nplt.title('Bivariate Scatterplot with Regression Fits', fontsize=14)\nplt.xlabel('X', fontsize=12)\nplt.ylabel('Y', fontsize=12)\nplt.legend(title='Model Type', loc='lower right')\nplt.grid(True)\nplt.show()\n```\n\n::::\n\nA parametric model, such as linear regression (although it could also be quadratic or a higher order polynomial), provides a simple, interpretable approximation but may miss crucial aspects of the true relationship. A “semiparametric” model, like piecewise linear regression, offers greater flexibility by allowing for changes in slope, capturing some curvature while maintaining a degree of interpretability. (One can cast this piecewise linear model as a parametric one, but for simplicity’s sake let’s go with this uncommon and imprecise definition of semiparametric.) Finally, a nonparametric model, such as LOESS, provides the most flexible representation, closely following the underlying sinusoidal pattern but potentially leading to overfitting.\n\n![](../images/three_model_classes.png)\n\nYou should not be surprised. This example demonstrates how the choice of model class significantly impacts the flexibility and interpretability of the fitted relationship. Do not take this example too seriously, it merely serves to illustrate the varying degree of complexity of statistical models.\n\n## Bottom Line\n\n- Parametric models impose the strongest assumptions and require the least amount of data. These are most models employed in practice. Think of linear regression.\n\n- Semiparametric models strike balance between flexibility and interpretation while allowing for flexible relationships in the data. Think of (non-Augmented) Inverse Propensity Score Weighting.\n\n- Nonparametric models are flexible and data-hungry. They allow for flexible associations between your variables. Think of a histrogram or kernel density.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"filters":["code-insertion"],"output-file":"three-classes-stat-models.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js","../code/back-to-top.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>\n<script src=\"../code/open-links-new-tab.js\"></script>  \n<script src=\"../code/back-to-top.js\"></script>\n<link href=\"https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Code+Pro&display=swap\" rel=\"stylesheet\">\n"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html",{"text":"<button id=\"back-to-top\" onclick=\"scrollToTop()\">↑</button>\n"}]},"insert-before-post":"_sharebuttons.md","title":"The Three Classes of Statistical Models","date":"2025-01-12","categories":["statistical models"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}