{"title":"The Limits of Parametric Models: The Cramér-Rao Bound","markdown":{"yaml":{"title":"The Limits of Parametric Models: The Cramér-Rao Bound","date":"2025-01-12","categories":["statistical inference","parametric models"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nObtaining the lowest possible variance is a primary goal for anyone working with statistical models. Efficiency (or precision), as is the jargon, is a cornerstone of statistics and econometrics, guiding us toward estimators that extract the maximum possible information from the data. It can make or break a data project.\n\nThe Cramér-Rao lower bound (CRLB) plays a pivotal role in this context by establishing a theoretical limit on the variance of unbiased estimators. Unbiased estimators are those that yield the true answer (on average), rendering them a highly attractive class of methods. The CRLB highlights the best achievable precision for parameter estimation based on the Fisher information in the data. This article explores the theoretical foundation of the CRLB, its computation, and its implications for practical estimation.\n\nIn what follows, I am concerned with unbiased estimators, a common practice that should not be taken for granted. As a counterexample, consider the [James-Stein estimator](http://yasenov.com/2025/01/steins-paradox-a-simple-illustration/) —a biased but attractive technique.\n\n## Notation\n\nBefore diving in, let’s establish a unified notation to structure the mathematical discussion:\n\n- Let X denote the observed data, with $X_1, X_2, \\dots, X_n$ being n independent and identically distributed (i.i.d.) observations.\n- The model governing the data is characterized by a (finite-dimensional) parameter $\\theta \\in \\mathbb{R}^d$ which we aim to estimate.\n- The likelihood of the data is $f(x; \\theta)$, fully specified by the parameter $\\theta$.\n\n## A Closer Look\n\nThe Cramér-Rao lower bound provides a theoretical benchmark for how precise an unbiased estimator can be. It sets the minimum variance that any unbiased estimator of a parameter $\\theta$ can achieve, given a specific data-generating process.\n\n### The CRLB Formula\n\nFor a parameter $\\theta$ in a parametric model with likelihood $f(x; \\theta)$, the CRLB is expressed as:\n\n  $$\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{I(\\theta)},$$\n\nwhere $I(\\theta)$ is the Fisher information (FI), defined as:\n\n  $$I(\\theta) = \\mathbb{E}\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 \\right].$$\n\n### Intuition\nTo understand the CRLB, we must delve into the concept of Fisher information named after one of the modern fathers of statistics R.A. Fisher. Intuitively, FI quantifies how much information the observed data carries about the parameter $\\theta$.\n\nThink of the likelihood function $f(x; \\theta)$ as describing the probability of observing a given dataset $x$ for a particular value of $\\theta$. If the likelihood changes sharply with $\\theta$ (i.e., $\\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta)$ is large), small changes in $\\theta$ lead to noticeable differences in the likelihood. This variability reflects high information: the data can “pinpoint” $\\theta$ with greater precision. Conversely, if the likelihood changes slowly with $\\theta$, the data offers less information about its true value.\n\nMathematically, the Fisher information $I(\\theta)$ is the variance of the the partial derivative\n\n$$\\frac{\\partial}{\\partial \\theta} logf(x;\\theta),$$\n\nwhich we refer to as the score function. This score measures how sensitive the likelihood function is to changes in $\\theta$. Higher variance in the score corresponds to more precise information about $\\theta$.\n\n### Practical Application\n\nThe CRLB provides a benchmark for evaluating the performance of estimators. For example, if you propose an unbiased estimator $\\hat{\\theta}$, you can compare its variance to the CRLB. If $\\text{Var}(\\hat{\\theta}) = \\frac{1}{I(\\theta)}$, we say the estimator is efficient. However, if the variance is higher, there may be room to improve the estimation method.\n\nMoreover, the CRLB also offers insight into the difficulty of estimating a parameter. If $I(\\theta)$ is “small”, so that the bound on the variance is high, then no unbiased estimator can achieve high precision with the available data. It is possible to develop a biased estimator for $\\theta$ with lower variance, but it is not clear why you would do that.\n\n## An Example\n\nImagine you are estimating the mean \\mu of a normal distribution, where $X \\sim N(\\mu, \\sigma^2)$, and $\\sigma^2$ is known. The likelihood for a single observation $x_i$ is:\n\n$$f(x_i;\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2 \\sigma^2}}.$$\n\nUsing the Fisher information definition given above, taking the derivative and simplifying, we find:\n\n$$I(\\mu)=  \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 = \\frac{1}{\\sigma^2}.$$\n\nFor n independent observations, this expression becomes:\n\n$$I(\\mu)=\\frac{n}{\\sigma^2}.$$\n\nThe CRLB for the variance of any unbiased estimator of \\mu is:\n\n$$\\text{Var}(\\hat{\\mu})\\geq \\frac{\\sigma^2}{n}$$\n\nThis result aligns with our intuition: as n increases, the precision of our estimate improves. In other words, more data leads to more informative results.\n\n## Where to Learn More\n\nAny graduate econometrics textbook will do. Personally, my grad school nightmares were induced by Greene’s textbook (cited below). It can be dry but certainly contains what you need to know.\n\n## Bottom Line\n\n- The CRLB establishes a theoretical lower limit on the variance of unbiased estimators, serving as a benchmark for efficiency.\n\n- Fisher information measures the sensitivity of the likelihood to changes in the parameter $\\theta$, linking the amount of information in the data to the precision of estimation.\n\n- Efficient estimators achieve the CRLB and are optimal under the given model assumptions.\n\n## References\n\nGreene, William H. “Econometric analysis”. New Jersey: Prentice Hall (2000): 201-215.","srcMarkdownNoYaml":"\n\n## Background\n\nObtaining the lowest possible variance is a primary goal for anyone working with statistical models. Efficiency (or precision), as is the jargon, is a cornerstone of statistics and econometrics, guiding us toward estimators that extract the maximum possible information from the data. It can make or break a data project.\n\nThe Cramér-Rao lower bound (CRLB) plays a pivotal role in this context by establishing a theoretical limit on the variance of unbiased estimators. Unbiased estimators are those that yield the true answer (on average), rendering them a highly attractive class of methods. The CRLB highlights the best achievable precision for parameter estimation based on the Fisher information in the data. This article explores the theoretical foundation of the CRLB, its computation, and its implications for practical estimation.\n\nIn what follows, I am concerned with unbiased estimators, a common practice that should not be taken for granted. As a counterexample, consider the [James-Stein estimator](http://yasenov.com/2025/01/steins-paradox-a-simple-illustration/) —a biased but attractive technique.\n\n## Notation\n\nBefore diving in, let’s establish a unified notation to structure the mathematical discussion:\n\n- Let X denote the observed data, with $X_1, X_2, \\dots, X_n$ being n independent and identically distributed (i.i.d.) observations.\n- The model governing the data is characterized by a (finite-dimensional) parameter $\\theta \\in \\mathbb{R}^d$ which we aim to estimate.\n- The likelihood of the data is $f(x; \\theta)$, fully specified by the parameter $\\theta$.\n\n## A Closer Look\n\nThe Cramér-Rao lower bound provides a theoretical benchmark for how precise an unbiased estimator can be. It sets the minimum variance that any unbiased estimator of a parameter $\\theta$ can achieve, given a specific data-generating process.\n\n### The CRLB Formula\n\nFor a parameter $\\theta$ in a parametric model with likelihood $f(x; \\theta)$, the CRLB is expressed as:\n\n  $$\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{I(\\theta)},$$\n\nwhere $I(\\theta)$ is the Fisher information (FI), defined as:\n\n  $$I(\\theta) = \\mathbb{E}\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 \\right].$$\n\n### Intuition\nTo understand the CRLB, we must delve into the concept of Fisher information named after one of the modern fathers of statistics R.A. Fisher. Intuitively, FI quantifies how much information the observed data carries about the parameter $\\theta$.\n\nThink of the likelihood function $f(x; \\theta)$ as describing the probability of observing a given dataset $x$ for a particular value of $\\theta$. If the likelihood changes sharply with $\\theta$ (i.e., $\\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta)$ is large), small changes in $\\theta$ lead to noticeable differences in the likelihood. This variability reflects high information: the data can “pinpoint” $\\theta$ with greater precision. Conversely, if the likelihood changes slowly with $\\theta$, the data offers less information about its true value.\n\nMathematically, the Fisher information $I(\\theta)$ is the variance of the the partial derivative\n\n$$\\frac{\\partial}{\\partial \\theta} logf(x;\\theta),$$\n\nwhich we refer to as the score function. This score measures how sensitive the likelihood function is to changes in $\\theta$. Higher variance in the score corresponds to more precise information about $\\theta$.\n\n### Practical Application\n\nThe CRLB provides a benchmark for evaluating the performance of estimators. For example, if you propose an unbiased estimator $\\hat{\\theta}$, you can compare its variance to the CRLB. If $\\text{Var}(\\hat{\\theta}) = \\frac{1}{I(\\theta)}$, we say the estimator is efficient. However, if the variance is higher, there may be room to improve the estimation method.\n\nMoreover, the CRLB also offers insight into the difficulty of estimating a parameter. If $I(\\theta)$ is “small”, so that the bound on the variance is high, then no unbiased estimator can achieve high precision with the available data. It is possible to develop a biased estimator for $\\theta$ with lower variance, but it is not clear why you would do that.\n\n## An Example\n\nImagine you are estimating the mean \\mu of a normal distribution, where $X \\sim N(\\mu, \\sigma^2)$, and $\\sigma^2$ is known. The likelihood for a single observation $x_i$ is:\n\n$$f(x_i;\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2 \\sigma^2}}.$$\n\nUsing the Fisher information definition given above, taking the derivative and simplifying, we find:\n\n$$I(\\mu)=  \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 = \\frac{1}{\\sigma^2}.$$\n\nFor n independent observations, this expression becomes:\n\n$$I(\\mu)=\\frac{n}{\\sigma^2}.$$\n\nThe CRLB for the variance of any unbiased estimator of \\mu is:\n\n$$\\text{Var}(\\hat{\\mu})\\geq \\frac{\\sigma^2}{n}$$\n\nThis result aligns with our intuition: as n increases, the precision of our estimate improves. In other words, more data leads to more informative results.\n\n## Where to Learn More\n\nAny graduate econometrics textbook will do. Personally, my grad school nightmares were induced by Greene’s textbook (cited below). It can be dry but certainly contains what you need to know.\n\n## Bottom Line\n\n- The CRLB establishes a theoretical lower limit on the variance of unbiased estimators, serving as a benchmark for efficiency.\n\n- Fisher information measures the sensitivity of the likelihood to changes in the parameter $\\theta$, linking the amount of information in the data to the precision of estimation.\n\n- Efficient estimators achieve the CRLB and are optimal under the given model assumptions.\n\n## References\n\nGreene, William H. “Econometric analysis”. New Jersey: Prentice Hall (2000): 201-215."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"filters":["code-insertion"],"output-file":"limits-parametric-models.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html"]},"insert-before-post":"_sharebuttons.md","title":"The Limits of Parametric Models: The Cramér-Rao Bound","date":"2025-01-12","categories":["statistical inference","parametric models"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}