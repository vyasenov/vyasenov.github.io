{"title":"Nonlinear Correlations and Chatterjee’s Coefficient","markdown":{"yaml":{"title":"Nonlinear Correlations and Chatterjee’s Coefficient","date":"2024-04-12","categories":["correlation"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nMuch of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford [published a paper](http://tandfonline.com/doi/abs/10.1080/01621459.2020.1758115) outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\n\nIn this article I will go over the basics of Chatterjee’s correlation measure. Before we get there, let’s first review some of the more traditional approaches in assessing bivariate relationship strength.\n\nFor simplicity, let’s assume away ties. Let’s also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant.\n\n## A Closer Look\n\n### Linear Relationships\n\nWhen we simply say “correlation” we refer to the so-called Pearson correlation coefficient. Virtually everyone working with data is familiar with it. Given a random sample $(X_1,Y_1),\\dots,(X_n, Y_n)$ of two random variables $X$ and $Y$ it is computed by:\n\n  $$corr^{P}(X,Y) = \\frac{ \\sum_i(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}},$$\n\nwhere an upper bar denotes a sample mean.\n\nThis coefficient lives in the $[-1,1]$ interval. Larger values indicate stronger relationship between $X$ and $Y$, be it positive or negative. At the extreme, the Pearson coefficient will equal $1$ when all observations can be perfectly lined up on a upward sloping line. Yes, you guessed it – when it equals $-1$ the line is sloping down.\n\nYou can easily calculate the Pearson correlation in `R` and `python`:\n\n:::: {.panel-tabset}\n\n### R\n\n```r\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'pearson', alternative='two.sided').\n```\n\n### Python\n\n```python\nimport numpy as np\nfrom scipy.stats import pearsonr\n\npearson_corr, pearson_pval = pearsonr(x, y)\n```\n\n:::\n\nThis measure, while widely popular, suffers from a few shortcomings. First, outliers have an outsized impact in skewing its value.  Sample means vulnerable to outliers are a key ingredient in the calculation, rendering the measure sensitive to data anomalies. Second, it is designed to detect only linear relationships. Two variables might have a strong but non-linear relationship which this measure will not detect. Lastly, it is not transformation-invariant, meaning that applying a monotone transformation to either of the variables will change the correlation value.\n\nLet’s discuss some improvements to the Pearson correlation measure. Enter Spearman correlation.\n\n### Monotone Relationships\n\nSpearman correlation is the Pearson correlation among the ranks of $X$ and $Y$:\n\n$$corr^{S}(X,Y) = corr^{P}(R(X),R(Y)),$$\n\nwhere $R(\\cdot)$ denotes an observation’s rank (or order) in the sample.\n\nSpearman correlation is thus a rank correlation measure. As such, it quantifies how well the relationship between $X$ and $Y$ can be described using a monotone (and not necessarily a linear) function. It is therefore a more flexible measure of association. Again, intuitively, Spearman correlation will take on a large positive value when the $X$ and $Y$ observations have similar ranks. This value will be negative when the ranks tend to go in opposite directions.\n\nSpearman correlation addresses some of the shortcomings associated with Pearson correlation. It is not easily influenced by outliers, and it does not change if we apply a monotone transformation of $X$ and/or $Y$. These benefits come at the expense of a loss in interpretation and potential issues when tied ranks are common, a scenario I ignore here.\n\nCalculating it is just as simple:\n\n:::: {.panel-tabset}\n\n### R\n\n```r\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n```\n\n### Python\n\n```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\nspearman_corr, spearman_pval = spearmanr(x, y)\n```\n\n:::\n\n\nSpearman correlation is not the only rank correlation coefficient out there. А popular alternative is the Kendall rank coefficient which is computed slightly differently. Let’s define a pair of observations $(X_i, Y_i)$ and $(X_j, Y_j)$ to be agreeing (the technical term is concordant) if the differences $(X_i - X_j)$ and $(Y_i - Y_j)$ have the same sign (i.e., either both $X_i > X_j$ and $Y_i > Y_j $or both $X_i < X_j$ and $Y_i < Y_j$ ).\n\nThen, Kendall’s coefficient is expressed as:\n\n$$corr^{K} = \\frac{\\text{number of agreeing pairs} - \\text{number of disagreeing pairs}} {\\text{total number of pairs}}.$$\n\nSo, it quantifies the degree of agreement between the ranks of $X$ and $Y$. Like the other coeffients described above, its range is $[-1, 1]$ and values away from zero indicate stronger relationship.\n\nAgain, this coefficient is similarly computed in `R` and `python`:\n\n:::: {.panel-tabset}\n\n### R\n\n```r\ncor(x,y, method = 'kendall')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n```\n\n### Python\n\n```python\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\nkendall_corr, kendall_pval = kendalltau(x, y)\n```\n\n:::\n\nKendall’s measure improves on some of the shortcomings baked in the Spearman’s coefficients – it has a clearer interpretation and it is less sensitive to rank ties.\n\nHowever, none of these rank correlation coefficients can detect non-monotonic relationships. For instance, $X$ and $Y$ can have a parabola- or wave-like pattern when plotted against each other. We would like a correlation measure flexible enough to capture such non-linear relationships.\n\nThis is where Chatterjee’s coefficient comes in.\n\n### More General Relationships\n\nChatterjee recently proposed [a new correlation coefficient](https://www.tandfonline.com/doi/abs/10.1080/01621459.2020.1758115) designed to detect non-monotonic relationships. He discovered a novel estimator of a population quantity first proposed by [Dette et al. (2013)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2011.00767.x).\n\nLet’s start with the formula. The Chatterjee correlation coefficient is calculated as follows:\n\n$$corr^{C}(X,Y) = 1-\\frac{3\\sum_{i=1}^{n-1}|R(Y_{k:R(X_k)=i+1}) - R(Y_{k:R(X_k)=i})|}{n^2-1},$$\n\nwhere n is the sample size. This looks complicated, so let’s try to simplify the numerator. Let’s sort the data in an ascending order of $X$ so that we have $(X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})$, where $X_{(1)}<X_{(2)}<\\dots <X_{(n)}$. Also, denote $R(Y_i)$ be the rank of $Y_{(i)}$. Then:\n\n  $$corr^{C}(X,Y) = 1 - \\frac{3\\sum_{i=1}^{n-1}|R(Y_{i+1})-R(Y_i)|}{n^2-1}.$$\n\nSo, this new coefficient is a scaled version of the sum of the absolute differences in the consecutive ranks of $Y$ when ordered by $X$. It is perhaps best to think about Chatterjee’s method as a measure of dependence and not strictly a correlation coefficient.\n\nThere are some major difference comapred to the previous correlation measures. Chatterjee’s correlation coefficient lies in the $[0,1]$ interval. It is equal to zero if and only if $X$ and $Y$ are independent and to one if one of them is a function of the other. Unlike the coefficients descibed above, it is not symmetric in $X$ and $Y$, meaning $corr^{C}(X,Y) \\neq corr^{C}(Y,X)$. This is understandable since we are interested in whether $X$ is a function of $Y$, which does not imply the opposite. The author also develops asymptotic theory for calculating p-values although some researchers have [raised concerns](https://academic.oup.com/biomet/article-abstract/109/2/317/6259083) about the coefficient’s power.\n\nHere is a sample code to calculate its value:\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\n\nn <- 1000\nx <- runif(n) \ny <- 5 * sin(x) + rnorm(n)\n\ndata <- data.frame(x=x, y=y)\ndata$R <- rank(data$y)\ndata <- data[order(data$x), ]\n\n1 - 3 * sum(abs(diff(data$R))) / (n^2-1)\n>[1] 0.4093024\n```\n\n### Python\n```python\nimport numpy as np\n\nnp.random.seed(1988)\nn = 1000\nx = np.random.uniform(size=n)\ny = 5 * np.sin(x) + np.random.normal(size=n)\n\ndata = np.array(sorted(zip(x, y), key=lambda pair: pair[0]))\nranks = np.argsort(np.argsort(data[:, 1]))  # Rank of y\nchatterjee_corr = 1 - 3 * np.sum(np.abs(np.diff(ranks))) / (n**2 - 1)\nprint(f\"Chatterjee's correlation: {chatterjee_corr:.4f}\")\n> Chatterjee's correlation: 0.4050\n```\n\n:::\n\n\n*Software Package*: [XICOR](https://www.rdocumentation.org/packages/XICOR/versions/0.4.1).\n\nThere you have it. You are now well-equipped to dive deeper into your datasets and find new exciting relationships.\n\n## Bottom Line\n\n- There are numerous ways of measuring association between two variables.\n\n- The most common methods measure only linear or monotonic relationships. These are often useful but do not capture more complex, non-linear associations.\n\n- A new correlation measure, Chatterjee’s coeffient, is designed to go beyond monotonicty and assess more general bivariate relationships.\n\n## Where to Learn More\n\nWikipedia has detailed entries on correlation, rank correlation, and Kendall’s coefficient which I found helpful. The R bloggers platform has articles exploring the Chatterjee’s correlation coefficient in detail. The more technically oriented folks will find Chatterjee’s original paper helpful.\n\n## References\n\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\n\nDette, H., Siburg, K. F., & Stoimenov, P. A. (2013). A Copula‐Based Non‐parametric Measure of Regression Dependence. Scandinavian Journal of Statistics, 40(1), 21-41.\n\nShi, H., Drton, M., & Han, F. (2022). On the power of Chatterjee’s rank correlation. Biometrika, 109(2), 317-333.\n\nhttps://www.r-bloggers.com/2021/12/exploring-the-xi-correlation-coefficient/","srcMarkdownNoYaml":"\n\n## Background\n\nMuch of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford [published a paper](http://tandfonline.com/doi/abs/10.1080/01621459.2020.1758115) outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\n\nIn this article I will go over the basics of Chatterjee’s correlation measure. Before we get there, let’s first review some of the more traditional approaches in assessing bivariate relationship strength.\n\nFor simplicity, let’s assume away ties. Let’s also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant.\n\n## A Closer Look\n\n### Linear Relationships\n\nWhen we simply say “correlation” we refer to the so-called Pearson correlation coefficient. Virtually everyone working with data is familiar with it. Given a random sample $(X_1,Y_1),\\dots,(X_n, Y_n)$ of two random variables $X$ and $Y$ it is computed by:\n\n  $$corr^{P}(X,Y) = \\frac{ \\sum_i(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}},$$\n\nwhere an upper bar denotes a sample mean.\n\nThis coefficient lives in the $[-1,1]$ interval. Larger values indicate stronger relationship between $X$ and $Y$, be it positive or negative. At the extreme, the Pearson coefficient will equal $1$ when all observations can be perfectly lined up on a upward sloping line. Yes, you guessed it – when it equals $-1$ the line is sloping down.\n\nYou can easily calculate the Pearson correlation in `R` and `python`:\n\n:::: {.panel-tabset}\n\n### R\n\n```r\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'pearson', alternative='two.sided').\n```\n\n### Python\n\n```python\nimport numpy as np\nfrom scipy.stats import pearsonr\n\npearson_corr, pearson_pval = pearsonr(x, y)\n```\n\n:::\n\nThis measure, while widely popular, suffers from a few shortcomings. First, outliers have an outsized impact in skewing its value.  Sample means vulnerable to outliers are a key ingredient in the calculation, rendering the measure sensitive to data anomalies. Second, it is designed to detect only linear relationships. Two variables might have a strong but non-linear relationship which this measure will not detect. Lastly, it is not transformation-invariant, meaning that applying a monotone transformation to either of the variables will change the correlation value.\n\nLet’s discuss some improvements to the Pearson correlation measure. Enter Spearman correlation.\n\n### Monotone Relationships\n\nSpearman correlation is the Pearson correlation among the ranks of $X$ and $Y$:\n\n$$corr^{S}(X,Y) = corr^{P}(R(X),R(Y)),$$\n\nwhere $R(\\cdot)$ denotes an observation’s rank (or order) in the sample.\n\nSpearman correlation is thus a rank correlation measure. As such, it quantifies how well the relationship between $X$ and $Y$ can be described using a monotone (and not necessarily a linear) function. It is therefore a more flexible measure of association. Again, intuitively, Spearman correlation will take on a large positive value when the $X$ and $Y$ observations have similar ranks. This value will be negative when the ranks tend to go in opposite directions.\n\nSpearman correlation addresses some of the shortcomings associated with Pearson correlation. It is not easily influenced by outliers, and it does not change if we apply a monotone transformation of $X$ and/or $Y$. These benefits come at the expense of a loss in interpretation and potential issues when tied ranks are common, a scenario I ignore here.\n\nCalculating it is just as simple:\n\n:::: {.panel-tabset}\n\n### R\n\n```r\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n```\n\n### Python\n\n```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\nspearman_corr, spearman_pval = spearmanr(x, y)\n```\n\n:::\n\n\nSpearman correlation is not the only rank correlation coefficient out there. А popular alternative is the Kendall rank coefficient which is computed slightly differently. Let’s define a pair of observations $(X_i, Y_i)$ and $(X_j, Y_j)$ to be agreeing (the technical term is concordant) if the differences $(X_i - X_j)$ and $(Y_i - Y_j)$ have the same sign (i.e., either both $X_i > X_j$ and $Y_i > Y_j $or both $X_i < X_j$ and $Y_i < Y_j$ ).\n\nThen, Kendall’s coefficient is expressed as:\n\n$$corr^{K} = \\frac{\\text{number of agreeing pairs} - \\text{number of disagreeing pairs}} {\\text{total number of pairs}}.$$\n\nSo, it quantifies the degree of agreement between the ranks of $X$ and $Y$. Like the other coeffients described above, its range is $[-1, 1]$ and values away from zero indicate stronger relationship.\n\nAgain, this coefficient is similarly computed in `R` and `python`:\n\n:::: {.panel-tabset}\n\n### R\n\n```r\ncor(x,y, method = 'kendall')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n```\n\n### Python\n\n```python\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\nkendall_corr, kendall_pval = kendalltau(x, y)\n```\n\n:::\n\nKendall’s measure improves on some of the shortcomings baked in the Spearman’s coefficients – it has a clearer interpretation and it is less sensitive to rank ties.\n\nHowever, none of these rank correlation coefficients can detect non-monotonic relationships. For instance, $X$ and $Y$ can have a parabola- or wave-like pattern when plotted against each other. We would like a correlation measure flexible enough to capture such non-linear relationships.\n\nThis is where Chatterjee’s coefficient comes in.\n\n### More General Relationships\n\nChatterjee recently proposed [a new correlation coefficient](https://www.tandfonline.com/doi/abs/10.1080/01621459.2020.1758115) designed to detect non-monotonic relationships. He discovered a novel estimator of a population quantity first proposed by [Dette et al. (2013)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2011.00767.x).\n\nLet’s start with the formula. The Chatterjee correlation coefficient is calculated as follows:\n\n$$corr^{C}(X,Y) = 1-\\frac{3\\sum_{i=1}^{n-1}|R(Y_{k:R(X_k)=i+1}) - R(Y_{k:R(X_k)=i})|}{n^2-1},$$\n\nwhere n is the sample size. This looks complicated, so let’s try to simplify the numerator. Let’s sort the data in an ascending order of $X$ so that we have $(X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})$, where $X_{(1)}<X_{(2)}<\\dots <X_{(n)}$. Also, denote $R(Y_i)$ be the rank of $Y_{(i)}$. Then:\n\n  $$corr^{C}(X,Y) = 1 - \\frac{3\\sum_{i=1}^{n-1}|R(Y_{i+1})-R(Y_i)|}{n^2-1}.$$\n\nSo, this new coefficient is a scaled version of the sum of the absolute differences in the consecutive ranks of $Y$ when ordered by $X$. It is perhaps best to think about Chatterjee’s method as a measure of dependence and not strictly a correlation coefficient.\n\nThere are some major difference comapred to the previous correlation measures. Chatterjee’s correlation coefficient lies in the $[0,1]$ interval. It is equal to zero if and only if $X$ and $Y$ are independent and to one if one of them is a function of the other. Unlike the coefficients descibed above, it is not symmetric in $X$ and $Y$, meaning $corr^{C}(X,Y) \\neq corr^{C}(Y,X)$. This is understandable since we are interested in whether $X$ is a function of $Y$, which does not imply the opposite. The author also develops asymptotic theory for calculating p-values although some researchers have [raised concerns](https://academic.oup.com/biomet/article-abstract/109/2/317/6259083) about the coefficient’s power.\n\nHere is a sample code to calculate its value:\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\n\nn <- 1000\nx <- runif(n) \ny <- 5 * sin(x) + rnorm(n)\n\ndata <- data.frame(x=x, y=y)\ndata$R <- rank(data$y)\ndata <- data[order(data$x), ]\n\n1 - 3 * sum(abs(diff(data$R))) / (n^2-1)\n>[1] 0.4093024\n```\n\n### Python\n```python\nimport numpy as np\n\nnp.random.seed(1988)\nn = 1000\nx = np.random.uniform(size=n)\ny = 5 * np.sin(x) + np.random.normal(size=n)\n\ndata = np.array(sorted(zip(x, y), key=lambda pair: pair[0]))\nranks = np.argsort(np.argsort(data[:, 1]))  # Rank of y\nchatterjee_corr = 1 - 3 * np.sum(np.abs(np.diff(ranks))) / (n**2 - 1)\nprint(f\"Chatterjee's correlation: {chatterjee_corr:.4f}\")\n> Chatterjee's correlation: 0.4050\n```\n\n:::\n\n\n*Software Package*: [XICOR](https://www.rdocumentation.org/packages/XICOR/versions/0.4.1).\n\nThere you have it. You are now well-equipped to dive deeper into your datasets and find new exciting relationships.\n\n## Bottom Line\n\n- There are numerous ways of measuring association between two variables.\n\n- The most common methods measure only linear or monotonic relationships. These are often useful but do not capture more complex, non-linear associations.\n\n- A new correlation measure, Chatterjee’s coeffient, is designed to go beyond monotonicty and assess more general bivariate relationships.\n\n## Where to Learn More\n\nWikipedia has detailed entries on correlation, rank correlation, and Kendall’s coefficient which I found helpful. The R bloggers platform has articles exploring the Chatterjee’s correlation coefficient in detail. The more technically oriented folks will find Chatterjee’s original paper helpful.\n\n## References\n\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\n\nDette, H., Siburg, K. F., & Stoimenov, P. A. (2013). A Copula‐Based Non‐parametric Measure of Regression Dependence. Scandinavian Journal of Statistics, 40(1), 21-41.\n\nShi, H., Drton, M., & Han, F. (2022). On the power of Chatterjee’s rank correlation. Biometrika, 109(2), 317-333.\n\nhttps://www.r-bloggers.com/2021/12/exploring-the-xi-correlation-coefficient/"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"flavors-correlation-coef.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Nonlinear Correlations and Chatterjee’s Coefficient","date":"2024-04-12","categories":["correlation"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}