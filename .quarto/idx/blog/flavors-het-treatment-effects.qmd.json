{"title":"The Alphabet of Learners for Heterogeneous Treatment Effects","markdown":{"yaml":{"title":"The Alphabet of Learners for Heterogeneous Treatment Effects","date":"2023-07-28","categories":["machine learning","randomized experiments","heterogeneous treatment effects"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nNumerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\n\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\n\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods – the so-called $S$-, $T$-, $X$- and $R$-learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as [the lasso](https://yasenov.com/2023/06/using-lasso-to-estimate-heterogeneous-treatment-effects/), gradient boosting, or even neural networks. In some clever sense, $S-T-X-R$ is the $A-B-C$ of heterogeneous treatment effect estimation.\n\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The [CausalML](https://causalml.readthedocs.io/en/latest/index.html) Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them.\n\n## Notation\n\nAs usual, let’s begin by setting some mathematical notation. I use D to denote a binary treatment indicator, $Y$ is the observed outcome and $X$ is a covariate of interest. The potential outcomes under each treatment state are $Y(0)$ and $Y(1)$, and $p$ is the (conditional) probability of assignment into the treatment (i.e., the propensity score).\n\nThe average treatment effect is then the mean difference in potential outcomes across all units:\n\n  $$ATE = E[Y(1)-E(0)].$$\n\nInterest is, instead, in the ATE for units with values $X=x$ which I refer to as the heterogeneous treatment effect, $HTE(X)$:\n\n  $$HTE(X) = E[Y(1)-E(0)|X].$$\n\nIt is also helpful to define the conditional outcome functions under each treatment state:\n\n  $$\\mu(X,d) = E[Y(d)|X].$$\n\nIt then follows that $HTE(X)$ can also be expressed as:\n\n  $$HTE(X) = \\mu(X,1) - \\mu(X,0).$$\n\n## A Closer Look\n\nI will now briefly describe the four learners for heterogeneous treatment effects in ascending order of complexity.\n\n### $S$(ingle) Learner\n\nThe idea behind the $S$-learner is to estimate a single outcome function $\\mu(X,D)$ and then calculate $HTE(X)$ by taking the difference in the predicted values between the units in the treatment and control groups.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Use the entire sample to estimate $\\hat{\\mu}(X,D)$.\n2. Compute $\\hat{HTE}(X)=\\hat{\\mu}(X,1)-\\hat{\\mu}(X,0)$.\n:::\n\n\nThis is intuitive and fine. But the problem is that the treatment variable D might be excluded in step 1 when it is not highly correlated with the outcome. (Note that in observational data this does not necessarily imply a null treatment effect.) In this case, we cannot even move to step 2.\n\n### $T$(wo) Learner\n\nThe $T$-learner solves the above problem by forcing the response models to include $D$. The idea is to first estimate two separate (conditional) outcome functions – one for the treatment and one for the control and proceed similarly.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Use the observations in the control group to estimate $\\hat{\\mu}(X,0)$ and the ones in the treatment effect for $\\hat{\\mu}(X,1)$.\n2. Then $\\hat{HTE}(X)=\\hat{\\mu}(X,1)- \\hat{\\mu}(X,0)$\n:::\n\nThis is better, but still not great. A potential problem is when there are different number of observations in the treatment and control groups. For instance, in the common case where the treatment group is much smaller, their $HTE(X)$ will be estimated much less precisely than the that of the control group. When combining the two to arrive at a final estimate of $HTE(X)$ the former should get a smaller weight than the latter. This is because the ML algorithms optimize for learning the $\\mu(\\cdot)$ functions, and not the $HTE(X)$ function directly.\n\n### $X$ Learner\n\nThe [$X$-learner](https://www.pnas.org/doi/abs/10.1073/pnas.1804597116) is designed to overcome the above concern. The procedure starts similarly to the $T$-learner but then weighs differently the $HTE(X)$’s for the treatment and control groups.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Use the observations in the control group to estimate $\\hat{\\mu}(X,0)$ and the ones in the treatment effect for $\\hat{\\mu}(X,1)$.\n2. Estimate the unit-level treatment effect for the observations in the control group, $\\hat{\\mu}(X,1)-Y$, and for the treatment group, $Y-\\hat{\\mu}(X,0)$.\n3. Combine both estimates by weighing them using the predicted propensity score, $\\hat{p}$:\n  $$HTE(X)=\\hat{p} \\times HTE(X|D=1) + (1-\\hat{p})\\times HTE(X|D=0).$$\n:::\n\nHere $\\hat{p}$ balances the uncertainty associated with the $HTE(X)$‘s in each group and hence, this approach is particularly effective when there is a significant difference in the number of units between the two groups.\n\n### $R$(obinson) Learner\n\nTo avoid getting too deep into technical details, I will not be describing the entire algorithm. The $R$-learner models both the outcome, and the propensity score and begins by computing unit-level predicted outcomes and treatment probabilities using cross-fitting and leave-one-out estimation. The key innovation is plugging these into a novel loss function featuring squared deviations of these predictions as well as a regularization term. This ensures “optimality” in learning the treatment effect heterogeneity.\n\n## Bottom Line\n\n- ML methods offer a promising way of determining which groups of units experience differential response to treatments.\n\n- I summarized four such model-agnostic methods – the $S$-, $T$-, $X$-, and $R$-learners.\n\n- Compared to the simpler $S$- and $T$- learners, the $X$- and $R$-learners solve some common issues and are more attractive options in most settings.\n\n## Where to Learn More\n\nI have previously written on how ML can be useful in causal inference, more generally and how we can use the Lasso to estimate heterogeneous treatment effects. [Hu (2022)](https://www.sciencedirect.com/science/article/pii/S0049089X22001211) offers a detailed summary of a bunch of ML methods for HTE estimation. A Statistical Odds and Ends [blog post](https://statisticaloddsandends.wordpress.com/2022/05/20/t-learners-s-learners-and-x-learners/) describes the $S$-, $T$- and $X$-learners and contains useful advice on when each of them is preferable. Chapter 21 of [Causal Inference for the Brave and True](https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html) also discusses this material and provides useful examples.\n\n## References\n\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\n\nKünzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165. Nie,\n\nXie, N., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.","srcMarkdownNoYaml":"\n\n## Background\n\nNumerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\n\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\n\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods – the so-called $S$-, $T$-, $X$- and $R$-learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as [the lasso](https://yasenov.com/2023/06/using-lasso-to-estimate-heterogeneous-treatment-effects/), gradient boosting, or even neural networks. In some clever sense, $S-T-X-R$ is the $A-B-C$ of heterogeneous treatment effect estimation.\n\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The [CausalML](https://causalml.readthedocs.io/en/latest/index.html) Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them.\n\n## Notation\n\nAs usual, let’s begin by setting some mathematical notation. I use D to denote a binary treatment indicator, $Y$ is the observed outcome and $X$ is a covariate of interest. The potential outcomes under each treatment state are $Y(0)$ and $Y(1)$, and $p$ is the (conditional) probability of assignment into the treatment (i.e., the propensity score).\n\nThe average treatment effect is then the mean difference in potential outcomes across all units:\n\n  $$ATE = E[Y(1)-E(0)].$$\n\nInterest is, instead, in the ATE for units with values $X=x$ which I refer to as the heterogeneous treatment effect, $HTE(X)$:\n\n  $$HTE(X) = E[Y(1)-E(0)|X].$$\n\nIt is also helpful to define the conditional outcome functions under each treatment state:\n\n  $$\\mu(X,d) = E[Y(d)|X].$$\n\nIt then follows that $HTE(X)$ can also be expressed as:\n\n  $$HTE(X) = \\mu(X,1) - \\mu(X,0).$$\n\n## A Closer Look\n\nI will now briefly describe the four learners for heterogeneous treatment effects in ascending order of complexity.\n\n### $S$(ingle) Learner\n\nThe idea behind the $S$-learner is to estimate a single outcome function $\\mu(X,D)$ and then calculate $HTE(X)$ by taking the difference in the predicted values between the units in the treatment and control groups.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Use the entire sample to estimate $\\hat{\\mu}(X,D)$.\n2. Compute $\\hat{HTE}(X)=\\hat{\\mu}(X,1)-\\hat{\\mu}(X,0)$.\n:::\n\n\nThis is intuitive and fine. But the problem is that the treatment variable D might be excluded in step 1 when it is not highly correlated with the outcome. (Note that in observational data this does not necessarily imply a null treatment effect.) In this case, we cannot even move to step 2.\n\n### $T$(wo) Learner\n\nThe $T$-learner solves the above problem by forcing the response models to include $D$. The idea is to first estimate two separate (conditional) outcome functions – one for the treatment and one for the control and proceed similarly.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Use the observations in the control group to estimate $\\hat{\\mu}(X,0)$ and the ones in the treatment effect for $\\hat{\\mu}(X,1)$.\n2. Then $\\hat{HTE}(X)=\\hat{\\mu}(X,1)- \\hat{\\mu}(X,0)$\n:::\n\nThis is better, but still not great. A potential problem is when there are different number of observations in the treatment and control groups. For instance, in the common case where the treatment group is much smaller, their $HTE(X)$ will be estimated much less precisely than the that of the control group. When combining the two to arrive at a final estimate of $HTE(X)$ the former should get a smaller weight than the latter. This is because the ML algorithms optimize for learning the $\\mu(\\cdot)$ functions, and not the $HTE(X)$ function directly.\n\n### $X$ Learner\n\nThe [$X$-learner](https://www.pnas.org/doi/abs/10.1073/pnas.1804597116) is designed to overcome the above concern. The procedure starts similarly to the $T$-learner but then weighs differently the $HTE(X)$’s for the treatment and control groups.\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Use the observations in the control group to estimate $\\hat{\\mu}(X,0)$ and the ones in the treatment effect for $\\hat{\\mu}(X,1)$.\n2. Estimate the unit-level treatment effect for the observations in the control group, $\\hat{\\mu}(X,1)-Y$, and for the treatment group, $Y-\\hat{\\mu}(X,0)$.\n3. Combine both estimates by weighing them using the predicted propensity score, $\\hat{p}$:\n  $$HTE(X)=\\hat{p} \\times HTE(X|D=1) + (1-\\hat{p})\\times HTE(X|D=0).$$\n:::\n\nHere $\\hat{p}$ balances the uncertainty associated with the $HTE(X)$‘s in each group and hence, this approach is particularly effective when there is a significant difference in the number of units between the two groups.\n\n### $R$(obinson) Learner\n\nTo avoid getting too deep into technical details, I will not be describing the entire algorithm. The $R$-learner models both the outcome, and the propensity score and begins by computing unit-level predicted outcomes and treatment probabilities using cross-fitting and leave-one-out estimation. The key innovation is plugging these into a novel loss function featuring squared deviations of these predictions as well as a regularization term. This ensures “optimality” in learning the treatment effect heterogeneity.\n\n## Bottom Line\n\n- ML methods offer a promising way of determining which groups of units experience differential response to treatments.\n\n- I summarized four such model-agnostic methods – the $S$-, $T$-, $X$-, and $R$-learners.\n\n- Compared to the simpler $S$- and $T$- learners, the $X$- and $R$-learners solve some common issues and are more attractive options in most settings.\n\n## Where to Learn More\n\nI have previously written on how ML can be useful in causal inference, more generally and how we can use the Lasso to estimate heterogeneous treatment effects. [Hu (2022)](https://www.sciencedirect.com/science/article/pii/S0049089X22001211) offers a detailed summary of a bunch of ML methods for HTE estimation. A Statistical Odds and Ends [blog post](https://statisticaloddsandends.wordpress.com/2022/05/20/t-learners-s-learners-and-x-learners/) describes the $S$-, $T$- and $X$-learners and contains useful advice on when each of them is preferable. Chapter 21 of [Causal Inference for the Brave and True](https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html) also discusses this material and provides useful examples.\n\n## References\n\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\n\nKünzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165. Nie,\n\nXie, N., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"filters":["code-insertion"],"output-file":"flavors-het-treatment-effects.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js","../code/back-to-top.js"],"theme":{"light":"cosmo","dark":"cyborg"},"header-includes":["<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=680ee8d89f7a510019a96bcf&product=inline-share-buttons' async='async'></script>\n<script src=\"../code/open-links-new-tab.js\"></script>  \n<script src=\"../code/back-to-top.js\"></script>\n<link href=\"https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Code+Pro&display=swap\" rel=\"stylesheet\">\n"],"page-layout":"full","includes":{"after-body":["../_includes/comments.html",{"text":"<button id=\"back-to-top\" onclick=\"scrollToTop()\">↑</button>\n"}]},"insert-before-post":"_sharebuttons.md","title":"The Alphabet of Learners for Heterogeneous Treatment Effects","date":"2023-07-28","categories":["machine learning","randomized experiments","heterogeneous treatment effects"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}