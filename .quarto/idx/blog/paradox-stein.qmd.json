{"title":"Stein’s Paradox: A Simple Illustration","markdown":{"yaml":{"title":"Stein’s Paradox: A Simple Illustration","date":"2025-01-10","categories":["statistical inference","paradox"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nIn the realm of statistics, few findings are as counterintuitive and fascinating as Stein’s paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein’s paradox is not merely about memorizing an oddity—it’s about recognizing the delicate balance inherent in statistical decision-making.\n\nIn essence, Stein’s paradox asserts that when estimating the means of multiple variables simultaneously, it’s possible to achieve better results compared to relying solely on the sample averages.\n\nLet’s now delve deeper into this statement and unravel the underlying mechanisms.\n\n## A Closer Look\n\n### Refresher on Mean Squared Error (MSE)\n\nTo understand the paradox, let’s begin by quantify what we mean by “better” estimation. The MSE of an estimator $\\hat{\\mu}$ is defined as its expected squared error (or loss):\n\n  $$\\text{MSE}(\\hat{\\mu}) = \\mathbb{E}\\left[ ( \\hat{\\mu} - \\mu )^2 \\right],$$\n\nwhere $\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_p)$ is the true vector of means. All else equal, the lower MSE the better.\n\n### Mathematical Formulation\n\nStein’s paradox arises in the context of estimating multiple parameters simultaneously. Suppose you’re estimating the mean $\\mu_i$ of several independent normal distributions:\n\n$$X_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, p,$$\n\nwhere $X_i$ are observed values, $\\mu_i$ are the unknown means, and $\\sigma^2$ is known. We assume non-zero covariance between the variables.\n\nA natural approach is to estimate each \\mu_i using its corresponding sample mean $X_i$, which is the maximum likelihood estimator (MLE). However, in dimensions $p \\geq 3$, this seemingly reasonable approach is dominated by an alternative method.\n\nThe surprise? Shrinking the individual estimates toward a common value—such as the overall mean—produces an estimator with uniformly lower expected squared error.\n\nThe MSE of the MLE is equal to:\n\n$$R(\\hat{\\mu}_\\text{MLE}) = p\\sigma^2.$$\n\nNow consider the biased(!) James-Stein (JS) estimator:\n\n$$\\hat{\\mu}_\\text{JS} = \\left( 1 - \\frac{(p - 2)\\sigma^2}{\\lVert X \\rVert^2} \\right) X,$$\n\nwhere $\\lVert X \\rVert^2 = \\sum_i X_i^2$ is the squared norm of the observed data. The shrinkage factor $1 - \\frac{(p - 2)\\sigma^2}{|X|^2}$ pulls the estimates toward zero (or any other pre-specified point).\n\nRemarkably, the James-Stein estimator has lower MSE than the MLE for $p \\geq 3$:\n\n$$\\text{MSE}(\\hat{\\mu}_{\\text{JS}}) < \\text{MSE}(\\hat{\\mu}_{\\text{MLE}}).$$\n\nThis is the mystery at its core.\n\n### Explanation\n\nThis result holds because the James-Stein estimator balances variance reduction and bias introduction in a way that minimizes overall MSE. The MLE, in contrast, does not account for the possibility of shared structure among the parameters. The counterintuitive nature of this result stems from the independence of the $X_i$‘s. Intuition suggests that pooling information across independent observations should not improve estimation, yet the James-Stein estimator demonstrates otherwise.\n\n## An Example\n\nLet’s emulate this paradox in `R` and `python` in a setting with $p=5$.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\n\np <- 5  # Number of means\nn <- 1000  # Number of simulations\nsigma <- 1\nmu <- rnorm(p, mean = 5, sd = 2)  # True means\n\n# results storage\nmse_mle <- numeric(n)  \nmse_js <- numeric(n)\n\n# create a fake dataset and compute the MSEs of both the MLE and JS estimator. \n# repeat this 1,000 times and take the average loss for each estimator.\nfor (sim in 1:n) {\n  # Simulate observations\n  X <- rnorm(p, mean = mu, sd = sigma)\n\n  # MLE estimator and its MSE\n  mle <- X\n  mse_mle[sim] <- sum((mle - mu)^2)\n\n  # James-Stein estimator and its MSE\n  shrinkage <- max(0, 1 - ((p - 2) * sigma^2) / sum(X^2))\n  js <- shrinkage * X\n  mse_js[sim] <- sum((js - mu)^2)\n}\n\n# print the results.\ncat(\"Average MSE of MLE:\", mean(mse_mle), \"\\n\")\n> Average MSE of MLE: 5.125081 \ncat(\"Average MSE of James-Stein:\", mean(mse_js), \"\\n\")\n> Average MSE of James-Stein: 5.055019 \n```\n\n### Python\n\n```python\nimport numpy as np\nnp.random.seed(1988)\n\n# Parameters\np = 5  # Number of means\nn = 1000  # Number of simulations\nsigma = 1\nmu = np.random.normal(loc=5, scale=2, size=p)  # True means\n\n# Results storage\nmse_mle = np.zeros(n)\nmse_js = np.zeros(n)\n\n# Simulate data and compute MSEs for MLE and James-Stein estimator\nfor sim in range(n):\n    # Simulate observations\n    X = np.random.normal(loc=mu, scale=sigma, size=p)\n\n    # MLE estimator and its MSE\n    mle = X\n    mse_mle[sim] = np.sum((mle - mu) ** 2)\n\n    # James-Stein estimator and its MSE\n    shrinkage = max(0, 1 - ((p - 2) * sigma**2) / np.sum(X**2))\n    js = shrinkage * X\n    mse_js[sim] = np.sum((js - mu) ** 2)\n\n# Print the results\nprint(\"Average MSE of MLE:\", np.mean(mse_mle).round(3))\n> Average MSE of MLE: 4.998\nprint(\"Average MSE of James-Stein:\", np.mean(mse_js).round(3))\n> Average MSE of James-Stein: 4.951\n```\n\n::::\n\nIn this example, average MSE of the James-Stein estimator ($5.06$) is consistently lower than that of the MLE ($5.13$), illustrating the paradox in action.\n\n## Bottom Line\n\n- Stein’s paradox shows that shrinkage estimators can outperform the MLE in dimensions $p \\geq 3$, even when the underlying variables are independent.\n\n- The James-Stein estimator achieves lower MSE by balancing variance reduction and bias introduction.\n\n- Understanding this result highlights the power of shrinkage techniques in high-dimensional statistics.\n\n## References\n\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press.","srcMarkdownNoYaml":"\n\n## Background\n\nIn the realm of statistics, few findings are as counterintuitive and fascinating as Stein’s paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein’s paradox is not merely about memorizing an oddity—it’s about recognizing the delicate balance inherent in statistical decision-making.\n\nIn essence, Stein’s paradox asserts that when estimating the means of multiple variables simultaneously, it’s possible to achieve better results compared to relying solely on the sample averages.\n\nLet’s now delve deeper into this statement and unravel the underlying mechanisms.\n\n## A Closer Look\n\n### Refresher on Mean Squared Error (MSE)\n\nTo understand the paradox, let’s begin by quantify what we mean by “better” estimation. The MSE of an estimator $\\hat{\\mu}$ is defined as its expected squared error (or loss):\n\n  $$\\text{MSE}(\\hat{\\mu}) = \\mathbb{E}\\left[ ( \\hat{\\mu} - \\mu )^2 \\right],$$\n\nwhere $\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_p)$ is the true vector of means. All else equal, the lower MSE the better.\n\n### Mathematical Formulation\n\nStein’s paradox arises in the context of estimating multiple parameters simultaneously. Suppose you’re estimating the mean $\\mu_i$ of several independent normal distributions:\n\n$$X_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, p,$$\n\nwhere $X_i$ are observed values, $\\mu_i$ are the unknown means, and $\\sigma^2$ is known. We assume non-zero covariance between the variables.\n\nA natural approach is to estimate each \\mu_i using its corresponding sample mean $X_i$, which is the maximum likelihood estimator (MLE). However, in dimensions $p \\geq 3$, this seemingly reasonable approach is dominated by an alternative method.\n\nThe surprise? Shrinking the individual estimates toward a common value—such as the overall mean—produces an estimator with uniformly lower expected squared error.\n\nThe MSE of the MLE is equal to:\n\n$$R(\\hat{\\mu}_\\text{MLE}) = p\\sigma^2.$$\n\nNow consider the biased(!) James-Stein (JS) estimator:\n\n$$\\hat{\\mu}_\\text{JS} = \\left( 1 - \\frac{(p - 2)\\sigma^2}{\\lVert X \\rVert^2} \\right) X,$$\n\nwhere $\\lVert X \\rVert^2 = \\sum_i X_i^2$ is the squared norm of the observed data. The shrinkage factor $1 - \\frac{(p - 2)\\sigma^2}{|X|^2}$ pulls the estimates toward zero (or any other pre-specified point).\n\nRemarkably, the James-Stein estimator has lower MSE than the MLE for $p \\geq 3$:\n\n$$\\text{MSE}(\\hat{\\mu}_{\\text{JS}}) < \\text{MSE}(\\hat{\\mu}_{\\text{MLE}}).$$\n\nThis is the mystery at its core.\n\n### Explanation\n\nThis result holds because the James-Stein estimator balances variance reduction and bias introduction in a way that minimizes overall MSE. The MLE, in contrast, does not account for the possibility of shared structure among the parameters. The counterintuitive nature of this result stems from the independence of the $X_i$‘s. Intuition suggests that pooling information across independent observations should not improve estimation, yet the James-Stein estimator demonstrates otherwise.\n\n## An Example\n\nLet’s emulate this paradox in `R` and `python` in a setting with $p=5$.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(1988)\n\np <- 5  # Number of means\nn <- 1000  # Number of simulations\nsigma <- 1\nmu <- rnorm(p, mean = 5, sd = 2)  # True means\n\n# results storage\nmse_mle <- numeric(n)  \nmse_js <- numeric(n)\n\n# create a fake dataset and compute the MSEs of both the MLE and JS estimator. \n# repeat this 1,000 times and take the average loss for each estimator.\nfor (sim in 1:n) {\n  # Simulate observations\n  X <- rnorm(p, mean = mu, sd = sigma)\n\n  # MLE estimator and its MSE\n  mle <- X\n  mse_mle[sim] <- sum((mle - mu)^2)\n\n  # James-Stein estimator and its MSE\n  shrinkage <- max(0, 1 - ((p - 2) * sigma^2) / sum(X^2))\n  js <- shrinkage * X\n  mse_js[sim] <- sum((js - mu)^2)\n}\n\n# print the results.\ncat(\"Average MSE of MLE:\", mean(mse_mle), \"\\n\")\n> Average MSE of MLE: 5.125081 \ncat(\"Average MSE of James-Stein:\", mean(mse_js), \"\\n\")\n> Average MSE of James-Stein: 5.055019 \n```\n\n### Python\n\n```python\nimport numpy as np\nnp.random.seed(1988)\n\n# Parameters\np = 5  # Number of means\nn = 1000  # Number of simulations\nsigma = 1\nmu = np.random.normal(loc=5, scale=2, size=p)  # True means\n\n# Results storage\nmse_mle = np.zeros(n)\nmse_js = np.zeros(n)\n\n# Simulate data and compute MSEs for MLE and James-Stein estimator\nfor sim in range(n):\n    # Simulate observations\n    X = np.random.normal(loc=mu, scale=sigma, size=p)\n\n    # MLE estimator and its MSE\n    mle = X\n    mse_mle[sim] = np.sum((mle - mu) ** 2)\n\n    # James-Stein estimator and its MSE\n    shrinkage = max(0, 1 - ((p - 2) * sigma**2) / np.sum(X**2))\n    js = shrinkage * X\n    mse_js[sim] = np.sum((js - mu) ** 2)\n\n# Print the results\nprint(\"Average MSE of MLE:\", np.mean(mse_mle).round(3))\n> Average MSE of MLE: 4.998\nprint(\"Average MSE of James-Stein:\", np.mean(mse_js).round(3))\n> Average MSE of James-Stein: 4.951\n```\n\n::::\n\nIn this example, average MSE of the James-Stein estimator ($5.06$) is consistently lower than that of the MLE ($5.13$), illustrating the paradox in action.\n\n## Bottom Line\n\n- Stein’s paradox shows that shrinkage estimators can outperform the MLE in dimensions $p \\geq 3$, even when the underlying variables are independent.\n\n- The James-Stein estimator achieves lower MSE by balancing variance reduction and bias introduction.\n\n- Understanding this result highlights the power of shrinkage techniques in high-dimensional statistics.\n\n## References\n\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"paradox-stein.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Stein’s Paradox: A Simple Illustration","date":"2025-01-10","categories":["statistical inference","paradox"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}