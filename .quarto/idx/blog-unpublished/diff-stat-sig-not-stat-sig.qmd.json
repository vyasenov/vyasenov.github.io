{"title":"Why 'Significant' vs. 'Not Significant' Is a Statistical Trap","markdown":{"yaml":{"title":"Why 'Significant' vs. 'Not Significant' Is a Statistical Trap","date":"2025-04-24","categories":["statistical inference","hypothesis testing"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nWe’ve all seen it—two estimates side by side, one with a $p$-value of 0.04 and the other with a $p$-value of 0.06. Cue the celebratory confetti for the “significant” effect, and a sad trombone for the “not significant” one. But wait! Are those results *really* different from each other? If one $p$-value dips below the magical 0.05 and the other doesn't, can we say that the two estimates differ in a meaningful way?\n\nThe short answer is: no. And that’s the central insight of Gelman and Stern (2006), a paper that gently but firmly calls out a persistent mistake in the way statistical results are interpreted. In this article, we’re going to unpack the logic, show the math, and make sure you never fall into this trap again—especially if you’re doing research at scale or comparing multiple models or subgroups.\n\n## Notation\n\nLet’s consider two estimated effects, say from two different subgroups or models:\n\n- $\\hat{\\theta}_1$: Estimate from group 1 with standard error $SE_1$\n- $\\hat{\\theta}_2$: Estimate from group 2 with standard error $SE_2$\n\nEach of these is associated with a $p$-value from a test of the null hypothesis $H_0: \\theta_i = 0$.\n\nYou might be tempted to look at the individual $p$-values:\n\n- $p_1 < 0.05$ → statistically significant\n- $p_2 > 0.05$ → not statistically significant\n\nThen jump to the conclusion: “The effects are different!”  \nBut to formally test **whether** the effects differ, what you actually need is a test of:\n\n$$\nH_0: \\theta_1 = \\theta_2\n$$\n\nThis is **not** the same as testing $\\theta_1 = 0$ and $\\theta_2 = 0$ separately.\n\n## A Closer Look\n\n### The Fallacy: \"Significant vs. Not Significant\"\n\nThis is the statistical equivalent of assuming that because one basketball team won a game by 1 point and another lost by 1 point, the winning team must be “better” than the losing one. That doesn’t necessarily follow—it could just be noise.\n\nWhat we need to emphasize is this: **The difference between significant and not significant is not itself statistically significant.**\n\nIf $\\hat{\\theta}_1$ is significantly different from zero, and $\\hat{\\theta}_2$ is not, that tells us very little about whether $\\theta_1 \\neq \\theta_2$. To evaluate that, we need to compare the two estimates *directly*.\n\n### Formal Testing of Differences\n\nSo how do we actually test whether the estimates differ?\n\nThe key is to compute the standard error of the difference:\n\n$$\nSE_{\\text{diff}} = \\sqrt{SE_1^2 + SE_2^2}\n$$\n\nThen, compute the test statistic:\n\n$$\nZ = \\frac{\\hat{\\theta}_1 - \\hat{\\theta}_2}{SE_{\\text{diff}}}\n$$\n\nThis is just a standard Wald test for the difference between two independent estimates. Under the null hypothesis $H_0: \\theta_1 = \\theta_2$, the $Z$-statistic is approximately standard normal, so you can compute a $p$-value from it directly.\n\nThe big insight? It’s entirely possible that:\n\n- $\\hat{\\theta}_1$ is statistically significant ($p_1 < 0.05$)\n- $\\hat{\\theta}_2$ is not ($p_2 > 0.05$)\n- But $|Z|$ is small → there’s **no significant difference** between the two!\n\nThis happens all the time in subgroup analyses, A/B tests with interactions, and model comparisons.\n\n### Intuition and Visualization\n\nHere’s a helpful mental model. Imagine two normal distributions:\n\n- One centered slightly above zero (say, $\\hat{\\theta}_1 = 1.96$)\n- One centered slightly below zero (say, $\\hat{\\theta}_2 = 1.65$)\n\nWith standard errors of 1, the first just clears the 5% significance threshold, the second doesn’t. But the difference between the two estimates is a mere 0.31. That’s tiny relative to their standard errors, and certainly not enough to declare them “different.”\n\nIf you plotted the two distributions with their confidence intervals, you'd see huge overlap—so why treat them as meaningfully different?\n\n## Bottom Line\n\n- A statistically significant result and a non-significant result do not imply a significant difference between effects.\n\n- To compare effects, test the difference *directly* using a formal hypothesis test.\n\n- Misinterpreting significance this way leads to false conclusions in subgroup analyses and model comparisons.\n\n- Always report and interpret confidence intervals for comparisons—not just $p$-values.\n\n## Where to Learn More\n\nFor a deeper dive, the original Gelman and Stern (2006) paper is a classic. Andrew Gelman has also written extensively on this topic in his blog, often using real-life examples from scientific publications and the media. If you're looking for broader reading on the pitfalls of significance testing, check out \"The Cult of Statistical Significance\" by Ziliak and McCloskey, or \"Statistical Rethinking\" by Richard McElreath for a more Bayesian angle. Lastly, brushing up on basic hypothesis testing logic in texts like *Casella and Berger* or *Wasserman’s All of Statistics* is always a good idea.\n\n## References\n\nGelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. *The American Statistician*, 60(4), 328–331.  \n\nZiliak, S. T., & McCloskey, D. N. (2008). *The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives*. University of Michigan Press.\n","srcMarkdownNoYaml":"\n\n## Background\n\nWe’ve all seen it—two estimates side by side, one with a $p$-value of 0.04 and the other with a $p$-value of 0.06. Cue the celebratory confetti for the “significant” effect, and a sad trombone for the “not significant” one. But wait! Are those results *really* different from each other? If one $p$-value dips below the magical 0.05 and the other doesn't, can we say that the two estimates differ in a meaningful way?\n\nThe short answer is: no. And that’s the central insight of Gelman and Stern (2006), a paper that gently but firmly calls out a persistent mistake in the way statistical results are interpreted. In this article, we’re going to unpack the logic, show the math, and make sure you never fall into this trap again—especially if you’re doing research at scale or comparing multiple models or subgroups.\n\n## Notation\n\nLet’s consider two estimated effects, say from two different subgroups or models:\n\n- $\\hat{\\theta}_1$: Estimate from group 1 with standard error $SE_1$\n- $\\hat{\\theta}_2$: Estimate from group 2 with standard error $SE_2$\n\nEach of these is associated with a $p$-value from a test of the null hypothesis $H_0: \\theta_i = 0$.\n\nYou might be tempted to look at the individual $p$-values:\n\n- $p_1 < 0.05$ → statistically significant\n- $p_2 > 0.05$ → not statistically significant\n\nThen jump to the conclusion: “The effects are different!”  \nBut to formally test **whether** the effects differ, what you actually need is a test of:\n\n$$\nH_0: \\theta_1 = \\theta_2\n$$\n\nThis is **not** the same as testing $\\theta_1 = 0$ and $\\theta_2 = 0$ separately.\n\n## A Closer Look\n\n### The Fallacy: \"Significant vs. Not Significant\"\n\nThis is the statistical equivalent of assuming that because one basketball team won a game by 1 point and another lost by 1 point, the winning team must be “better” than the losing one. That doesn’t necessarily follow—it could just be noise.\n\nWhat we need to emphasize is this: **The difference between significant and not significant is not itself statistically significant.**\n\nIf $\\hat{\\theta}_1$ is significantly different from zero, and $\\hat{\\theta}_2$ is not, that tells us very little about whether $\\theta_1 \\neq \\theta_2$. To evaluate that, we need to compare the two estimates *directly*.\n\n### Formal Testing of Differences\n\nSo how do we actually test whether the estimates differ?\n\nThe key is to compute the standard error of the difference:\n\n$$\nSE_{\\text{diff}} = \\sqrt{SE_1^2 + SE_2^2}\n$$\n\nThen, compute the test statistic:\n\n$$\nZ = \\frac{\\hat{\\theta}_1 - \\hat{\\theta}_2}{SE_{\\text{diff}}}\n$$\n\nThis is just a standard Wald test for the difference between two independent estimates. Under the null hypothesis $H_0: \\theta_1 = \\theta_2$, the $Z$-statistic is approximately standard normal, so you can compute a $p$-value from it directly.\n\nThe big insight? It’s entirely possible that:\n\n- $\\hat{\\theta}_1$ is statistically significant ($p_1 < 0.05$)\n- $\\hat{\\theta}_2$ is not ($p_2 > 0.05$)\n- But $|Z|$ is small → there’s **no significant difference** between the two!\n\nThis happens all the time in subgroup analyses, A/B tests with interactions, and model comparisons.\n\n### Intuition and Visualization\n\nHere’s a helpful mental model. Imagine two normal distributions:\n\n- One centered slightly above zero (say, $\\hat{\\theta}_1 = 1.96$)\n- One centered slightly below zero (say, $\\hat{\\theta}_2 = 1.65$)\n\nWith standard errors of 1, the first just clears the 5% significance threshold, the second doesn’t. But the difference between the two estimates is a mere 0.31. That’s tiny relative to their standard errors, and certainly not enough to declare them “different.”\n\nIf you plotted the two distributions with their confidence intervals, you'd see huge overlap—so why treat them as meaningfully different?\n\n## Bottom Line\n\n- A statistically significant result and a non-significant result do not imply a significant difference between effects.\n\n- To compare effects, test the difference *directly* using a formal hypothesis test.\n\n- Misinterpreting significance this way leads to false conclusions in subgroup analyses and model comparisons.\n\n- Always report and interpret confidence intervals for comparisons—not just $p$-values.\n\n## Where to Learn More\n\nFor a deeper dive, the original Gelman and Stern (2006) paper is a classic. Andrew Gelman has also written extensively on this topic in his blog, often using real-life examples from scientific publications and the media. If you're looking for broader reading on the pitfalls of significance testing, check out \"The Cult of Statistical Significance\" by Ziliak and McCloskey, or \"Statistical Rethinking\" by Richard McElreath for a more Bayesian angle. Lastly, brushing up on basic hypothesis testing logic in texts like *Casella and Berger* or *Wasserman’s All of Statistics* is always a good idea.\n\n## References\n\nGelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. *The American Statistician*, 60(4), 328–331.  \n\nZiliak, S. T., & McCloskey, D. N. (2008). *The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives*. University of Michigan Press.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"diff-stat-sig-not-stat-sig.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Why 'Significant' vs. 'Not Significant' Is a Statistical Trap","date":"2025-04-24","categories":["statistical inference","hypothesis testing"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}