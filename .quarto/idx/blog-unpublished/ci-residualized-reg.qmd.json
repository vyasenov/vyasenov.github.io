{"title":"Causal Inference with Residualized Regressions","markdown":{"yaml":{"title":"Causal Inference with Residualized Regressions","date":"2025-04-24","categories":["causal inference","linear model"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nThe Frisch-Waugh-Lovell (FWL) theorem offers an elegant alternative to standard multiple regression when estimating causal effects. Instead of running a full regression with treatment and control variables, FWL demonstrates that we can obtain identical treatment coefficient estimates by first \"residualizing\" both the outcome and treatment variables with respect to controls, then regressing these residuals against each other. This approach provides both computational advantages and conceptual clarity.\n\nThis theorem effectively decomposes multiple regression into simpler components, helping researchers understand what happens \"under the hood\" of regression models. By residualizing variables—removing the components explained by control variables—we can isolate and estimate treatment effects more intuitively, essentially peeling away confounding layers to reveal the core relationship of interest.\n\nFWL proves particularly valuable in high-dimensional settings and causal inference applications, offering researchers a powerful framework for understanding causal relationships. Whether working with observational studies, quasi-experiments, or complex datasets, this approach allows analysts to conceptualize how controlling for confounders isolates the treatment-outcome relationship they truly want to measure.\n\nA related and often underappreciated issue is how standard errors behave under the FWL theorem. A recent paper by Peng Ding (2021) shows that using FWL residualization in regression with random regressors can lead to incorrect standard errors unless the residualization is adjusted appropriately. Ding emphasizes that while the point estimates remain identical, the naive residual-on-residual approach may mischaracterize the variance, especially when the regressors are stochastic. This highlights how intertwined design assumptions and inferential correctness can be.\n\n## Notation\n\nConsider a standard linear model:\n\n  $$Y = \\alpha + D\\tau + X\\beta + \\varepsilon, $$\n\nwhere:\n\n- $Y$ is the outcome variable (e.g., earnings),\n- $D$ is the treatment variable (e.g., whether a person attended a job training program),\n- $X$ is a vector of control variables (e.g., age, education, experience),\n- $\\tau$ is the treatment effect we want to estimate,\n- $\\beta$ represents the coefficients on the controls,\n- $\\varepsilon$ is the error term.\n\n## A Closer Look\n\n### The FWL Theorem\n\nThe OLS estimate of $\\tau$ in the full regression includes both $D$ and $X$. However, FWL tells us that we can obtain the same estimate of $\\tau$ by following these steps:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Regress $Y$ on $X$ and collect the residuals $\\tilde{Y}$.\n2. Regress $D$ on $X$ and collect the residuals $\\tilde{D}$.\n3. Regress $\\tilde{Y}$ on $\\tilde{D}$ (without an intercept). The coefficient on $\\tilde{D}$ is exactly $\\tau$.\n:::\n\n### Intuition\n\nThe intuition behind FWL is simple yet profound. When we regress $Y$ on $X$, we strip out the variation in $Y$ that is explained by $X$, leaving only the part orthogonal to $X$. Similarly, regressing $D$ on $X$ removes the influence of $X$ on $D$, isolating the component of $D$ that is independent of $X$. Since X has been accounted for in both cases, the regression of $\\tilde{Y}$ on $\\tilde{D}$ retrieves the direct relationship between $D$ and $Y$, net of $X$.\n\nMathematically, the key result of FWL is:\n\n  $$\\hat{\\tau} = (D' M_X D)^{-1} D' M_X Y,$$\n\nwhere $M_X = I - X(X'X)^{-1}X'$ is the projection matrix that residualizes variables with respect to $X$. This shows that the estimate of \\tau remains unchanged whether we use the full regression or the residualized regression.\n\nSo what’s really happening here? The residuals $\\tilde{D}$ represent the part of the treatment variable that cannot be explained by the control variables. Similarly, $\\tilde{\\mathbf{y}}$ ​ represents the part of the outcome that cannot be explained by the controls.\n\nThe FWL procedure essentially isolates the relationship between treatment and outcome after removing the influence of all the control variables. It’s as if we’re looking at the relationship in a “purified” form, with all the confounding effects of the controls stripped away.\n\nThis provides a powerful framework for causal inference. Under the assumption that we’ve included all relevant confounders in our control set, the coefficient $\\hat{\\tau}$ from the residualized regression can be interpreted as the causal effect of the treatment on the outcome.\n\nThink of it this way: we’re first “adjusting” both our treatment and outcome variables by removing the predictable parts based on our controls. Then we’re examining how the “adjusted” treatment relates to the “adjusted” outcome. This residual-on-residual regression gives us our causal estimate.\n\n### Geometric Interpretation\n\nGeometrically, we can think of the FWL theorem in terms of projections in vector space. The residuals $\\tilde{D}$ and $\\tilde{\\mathbf{y}}$​ are what remain after projecting $D$ and $Y$ onto the orthogonal complement of the space spanned by $Z$.\n\nIn other words, we’re looking at the components of $D$ and $Y$ that are orthogonal to (i.e., cannot be explained by) the control variables. The relationship between these orthogonal components gives us our treatment effect estimate.\n\n### Variance and Standard Errors\n\nThe precision of our treatment effect estimate depends on how much variation in the treatment remains after accounting for the controls. If the treatment is highly collinear with the controls (meaning $\\tilde{D}$$ has little variation), our estimate will be imprecise.\n\nThis helps us understand the “curse of dimensionality” in causal inference. As we add more control variables, we might reduce omitted variable bias, but we also risk reducing the effective variation in our treatment, leading to less precise estimates.\n\n### Practical Implications\n\n- **Conceptual clarity**: FWL emphasizes that controlling for $X$ means adjusting both $Y$ and $D$ before examining their relationship.\n\n- **Computational benefits**: In high-dimensional settings, it is often more efficient to work with residualized variables rather than estimating the full model.\n\n- **Instrumental variables and two-stage regression**: The residualization step is analogous to first-stage regressions in instrumental variable estimation.\n\n- **Two separate data sources**: In some cases, possibly due to data privacy concerns, the three variables $Y$, $X$ and $D$ might live in two separate datasets - one with $Y$ and $X$, and the other one with $D$ and $X$. The traditional approach is then not avaialable.\n\n## An Example\n\nLet’s go through an example in `R` and `python`. We will generate synthetic data where the treatment effect is nonzero and show that both the full regression and the residualized approach give the same estimate.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(42)\nn <- 1000\nX <- matrix(rnorm(n * 3), ncol = 3)  # Three control variables\nD <- 0.5 * X[,1] + 0.3 * X[,2] + rnorm(n)  # Treatment depends on controls\nY <- 2 * D + 1.5 * X[,1] - 0.5 * X[,2] + 0.3 * X[,3] + rnorm(n)  # Outcome model\n\n# Full Regression\nfull_model <- lm(Y ~ D + X)\nsummary(full_model)coefficients[\"D\",]  \n\n### Residualized Regression \nY_resid <- residuals(lm(Y ~ X)) \nD_resid <- residuals(lm(D ~ X)) \nresid_model <- lm(Y_resid ~ D_resid - 1)  # No intercept \nsummary(resid_model)coefficients[\"D_resid\",]\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data\nn = 1000\nX = np.random.normal(size=(n, 3))  # Three control variables\nD = 0.5 * X[:, 0] + 0.3 * X[:, 1] + np.random.normal(size=n)  # Treatment depends on controls\nY = 2 * D + 1.5 * X[:, 0] - 0.5 * X[:, 1] + 0.3 * X[:, 2] + np.random.normal(size=n)  # Outcome model\n\n# Full Regression\nfull_model = LinearRegression()\nfull_model.fit(np.column_stack((D, X)), Y)\nfull_coef = full_model.coef_[0]  # Coefficient for D\nprint(f\"Full Regression Coefficient for D: {full_coef}\")\n\n# Residualized Regression\n# Step 1: Residualize Y with respect to X\nY_resid_model = LinearRegression()\nY_resid_model.fit(X, Y)\nY_resid = Y - Y_resid_model.predict(X)\n\n# Step 2: Residualize D with respect to X\nD_resid_model = LinearRegression()\nD_resid_model.fit(X, D)\nD_resid = D - D_resid_model.predict(X)\n\n# Step 3: Regress residualized Y on residualized D\nresid_model = LinearRegression(fit_intercept=False)\nresid_model.fit(D_resid.reshape(-1, 1), Y_resid)\nresid_coef = resid_model.coef_[0]\nprint(f\"Residualized Regression Coefficient for D: {resid_coef}\")\n```\n\n::::\n\nThe coefficient on $D$ in the full model and the coefficient on $D_{resid}$ in the residualized model will be identical. This demonstrates that controlling for $X$ can be done implicitly by working with residuals.\n\n## Bottom Line\n\n- The FWL theorem shows that controlling for variables in a regression can be done by residualizing first.\n\n- This approach helps conceptually separate the treatment effect from confounding influences.\n\n- Residualization is particularly useful in high-dimensional settings and instrumental variables estimation.\n\n- Whether you use the full regression or the residualized approach, you get the same treatment effect estimate.\n\n## References\n\nDing, P. (2021). The Frisch–Waugh–Lovell theorem for standard errors. Statistics & Probability Letters, 168, 108945.","srcMarkdownNoYaml":"\n\n## Background\n\nThe Frisch-Waugh-Lovell (FWL) theorem offers an elegant alternative to standard multiple regression when estimating causal effects. Instead of running a full regression with treatment and control variables, FWL demonstrates that we can obtain identical treatment coefficient estimates by first \"residualizing\" both the outcome and treatment variables with respect to controls, then regressing these residuals against each other. This approach provides both computational advantages and conceptual clarity.\n\nThis theorem effectively decomposes multiple regression into simpler components, helping researchers understand what happens \"under the hood\" of regression models. By residualizing variables—removing the components explained by control variables—we can isolate and estimate treatment effects more intuitively, essentially peeling away confounding layers to reveal the core relationship of interest.\n\nFWL proves particularly valuable in high-dimensional settings and causal inference applications, offering researchers a powerful framework for understanding causal relationships. Whether working with observational studies, quasi-experiments, or complex datasets, this approach allows analysts to conceptualize how controlling for confounders isolates the treatment-outcome relationship they truly want to measure.\n\nA related and often underappreciated issue is how standard errors behave under the FWL theorem. A recent paper by Peng Ding (2021) shows that using FWL residualization in regression with random regressors can lead to incorrect standard errors unless the residualization is adjusted appropriately. Ding emphasizes that while the point estimates remain identical, the naive residual-on-residual approach may mischaracterize the variance, especially when the regressors are stochastic. This highlights how intertwined design assumptions and inferential correctness can be.\n\n## Notation\n\nConsider a standard linear model:\n\n  $$Y = \\alpha + D\\tau + X\\beta + \\varepsilon, $$\n\nwhere:\n\n- $Y$ is the outcome variable (e.g., earnings),\n- $D$ is the treatment variable (e.g., whether a person attended a job training program),\n- $X$ is a vector of control variables (e.g., age, education, experience),\n- $\\tau$ is the treatment effect we want to estimate,\n- $\\beta$ represents the coefficients on the controls,\n- $\\varepsilon$ is the error term.\n\n## A Closer Look\n\n### The FWL Theorem\n\nThe OLS estimate of $\\tau$ in the full regression includes both $D$ and $X$. However, FWL tells us that we can obtain the same estimate of $\\tau$ by following these steps:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. Regress $Y$ on $X$ and collect the residuals $\\tilde{Y}$.\n2. Regress $D$ on $X$ and collect the residuals $\\tilde{D}$.\n3. Regress $\\tilde{Y}$ on $\\tilde{D}$ (without an intercept). The coefficient on $\\tilde{D}$ is exactly $\\tau$.\n:::\n\n### Intuition\n\nThe intuition behind FWL is simple yet profound. When we regress $Y$ on $X$, we strip out the variation in $Y$ that is explained by $X$, leaving only the part orthogonal to $X$. Similarly, regressing $D$ on $X$ removes the influence of $X$ on $D$, isolating the component of $D$ that is independent of $X$. Since X has been accounted for in both cases, the regression of $\\tilde{Y}$ on $\\tilde{D}$ retrieves the direct relationship between $D$ and $Y$, net of $X$.\n\nMathematically, the key result of FWL is:\n\n  $$\\hat{\\tau} = (D' M_X D)^{-1} D' M_X Y,$$\n\nwhere $M_X = I - X(X'X)^{-1}X'$ is the projection matrix that residualizes variables with respect to $X$. This shows that the estimate of \\tau remains unchanged whether we use the full regression or the residualized regression.\n\nSo what’s really happening here? The residuals $\\tilde{D}$ represent the part of the treatment variable that cannot be explained by the control variables. Similarly, $\\tilde{\\mathbf{y}}$ ​ represents the part of the outcome that cannot be explained by the controls.\n\nThe FWL procedure essentially isolates the relationship between treatment and outcome after removing the influence of all the control variables. It’s as if we’re looking at the relationship in a “purified” form, with all the confounding effects of the controls stripped away.\n\nThis provides a powerful framework for causal inference. Under the assumption that we’ve included all relevant confounders in our control set, the coefficient $\\hat{\\tau}$ from the residualized regression can be interpreted as the causal effect of the treatment on the outcome.\n\nThink of it this way: we’re first “adjusting” both our treatment and outcome variables by removing the predictable parts based on our controls. Then we’re examining how the “adjusted” treatment relates to the “adjusted” outcome. This residual-on-residual regression gives us our causal estimate.\n\n### Geometric Interpretation\n\nGeometrically, we can think of the FWL theorem in terms of projections in vector space. The residuals $\\tilde{D}$ and $\\tilde{\\mathbf{y}}$​ are what remain after projecting $D$ and $Y$ onto the orthogonal complement of the space spanned by $Z$.\n\nIn other words, we’re looking at the components of $D$ and $Y$ that are orthogonal to (i.e., cannot be explained by) the control variables. The relationship between these orthogonal components gives us our treatment effect estimate.\n\n### Variance and Standard Errors\n\nThe precision of our treatment effect estimate depends on how much variation in the treatment remains after accounting for the controls. If the treatment is highly collinear with the controls (meaning $\\tilde{D}$$ has little variation), our estimate will be imprecise.\n\nThis helps us understand the “curse of dimensionality” in causal inference. As we add more control variables, we might reduce omitted variable bias, but we also risk reducing the effective variation in our treatment, leading to less precise estimates.\n\n### Practical Implications\n\n- **Conceptual clarity**: FWL emphasizes that controlling for $X$ means adjusting both $Y$ and $D$ before examining their relationship.\n\n- **Computational benefits**: In high-dimensional settings, it is often more efficient to work with residualized variables rather than estimating the full model.\n\n- **Instrumental variables and two-stage regression**: The residualization step is analogous to first-stage regressions in instrumental variable estimation.\n\n- **Two separate data sources**: In some cases, possibly due to data privacy concerns, the three variables $Y$, $X$ and $D$ might live in two separate datasets - one with $Y$ and $X$, and the other one with $D$ and $X$. The traditional approach is then not avaialable.\n\n## An Example\n\nLet’s go through an example in `R` and `python`. We will generate synthetic data where the treatment effect is nonzero and show that both the full regression and the residualized approach give the same estimate.\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nrm(list=ls())\nset.seed(42)\nn <- 1000\nX <- matrix(rnorm(n * 3), ncol = 3)  # Three control variables\nD <- 0.5 * X[,1] + 0.3 * X[,2] + rnorm(n)  # Treatment depends on controls\nY <- 2 * D + 1.5 * X[,1] - 0.5 * X[,2] + 0.3 * X[,3] + rnorm(n)  # Outcome model\n\n# Full Regression\nfull_model <- lm(Y ~ D + X)\nsummary(full_model)coefficients[\"D\",]  \n\n### Residualized Regression \nY_resid <- residuals(lm(Y ~ X)) \nD_resid <- residuals(lm(D ~ X)) \nresid_model <- lm(Y_resid ~ D_resid - 1)  # No intercept \nsummary(resid_model)coefficients[\"D_resid\",]\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data\nn = 1000\nX = np.random.normal(size=(n, 3))  # Three control variables\nD = 0.5 * X[:, 0] + 0.3 * X[:, 1] + np.random.normal(size=n)  # Treatment depends on controls\nY = 2 * D + 1.5 * X[:, 0] - 0.5 * X[:, 1] + 0.3 * X[:, 2] + np.random.normal(size=n)  # Outcome model\n\n# Full Regression\nfull_model = LinearRegression()\nfull_model.fit(np.column_stack((D, X)), Y)\nfull_coef = full_model.coef_[0]  # Coefficient for D\nprint(f\"Full Regression Coefficient for D: {full_coef}\")\n\n# Residualized Regression\n# Step 1: Residualize Y with respect to X\nY_resid_model = LinearRegression()\nY_resid_model.fit(X, Y)\nY_resid = Y - Y_resid_model.predict(X)\n\n# Step 2: Residualize D with respect to X\nD_resid_model = LinearRegression()\nD_resid_model.fit(X, D)\nD_resid = D - D_resid_model.predict(X)\n\n# Step 3: Regress residualized Y on residualized D\nresid_model = LinearRegression(fit_intercept=False)\nresid_model.fit(D_resid.reshape(-1, 1), Y_resid)\nresid_coef = resid_model.coef_[0]\nprint(f\"Residualized Regression Coefficient for D: {resid_coef}\")\n```\n\n::::\n\nThe coefficient on $D$ in the full model and the coefficient on $D_{resid}$ in the residualized model will be identical. This demonstrates that controlling for $X$ can be done implicitly by working with residuals.\n\n## Bottom Line\n\n- The FWL theorem shows that controlling for variables in a regression can be done by residualizing first.\n\n- This approach helps conceptually separate the treatment effect from confounding influences.\n\n- Residualization is particularly useful in high-dimensional settings and instrumental variables estimation.\n\n- Whether you use the full regression or the residualized approach, you get the same treatment effect estimate.\n\n## References\n\nDing, P. (2021). The Frisch–Waugh–Lovell theorem for standard errors. Statistics & Probability Letters, 168, 108945."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"ci-residualized-reg.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Causal Inference with Residualized Regressions","date":"2025-04-24","categories":["causal inference","linear model"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}