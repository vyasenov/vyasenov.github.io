{"title":"OLS with Fixed versus Random Regressors","markdown":{"yaml":{"title":"OLS with Fixed versus Random Regressors","date":"2025-04-24","categories":["statistical inference","linear model"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nLinear regression stands as one of the fundamental workhorses of statistical modeling and is ubiquitous in applied work across fields like econometrics, biostatistics, and machine learning. Beneath its seemingly simple exterior lies a nuanced landscape of assumptions and theoretical considerations. One such distinction that often gets overlooked—even by seasoned practitioners—is the difference between fixed and random regressors. This distinction, while subtle, has important implications for how we interpret our models, conduct inference, and understand uncertainty.\n\nThe goal of this article is to clarify the differences between fixed and random regressors in linear regression. We'll explore why this distinction matters and how it affects everything from your confidence intervals to your prediction errors. My goal is to help you develop both the technical understanding and the intuition needed to make informed modeling decisions in your day-to-day work.\n\n## Notation\n\nTo set the stage, consider the standard linear regression model:\n\n$$Y = X\\beta + \\varepsilon,$$\n\nwhere:\n\n- $Y$ is an $ n \\times 1 $ vector of outcomes,\n- $X$ is an $ n \\times p $ matrix of regressors (also called covariates or features),\n- $\\beta$ is a $p \\times 1$ vector of coefficients to be estimated,\n- $\\varepsilon$ is an $ n \\times 1$ vector of errors.\n\nThe least squares estimator of $\\beta$ is given by:\n\n$$ \\hat{\\beta} = (X'X)^{-1}X'Y. $$\n\nThe key question is: What do we assume about $X$? If $ X $ is considered fixed, we condition on it when deriving properties of $\\hat{\\beta}$. If $X$ is random, we take expectations over its distribution. This affects how we think about the variance of our estimator, the validity of standard errors, and the asymptotic behavior of $\\hat{\\beta}$.\n\n## A Closer Look\n\n### Fixed Regressors\n\nIn the fixed regressor framework, we assume that $X$ is determined beforehand—perhaps through experimental design or by conditioning on observed values. The randomness in our model comes solely from the error term $\\varepsilon$.\n\nThe key assumptions typically made under this approach are:\n\n- **Linearity**: The model is correctly specified as $Y = X\\beta + \\varepsilon$.\n- **Exogeneity**: The errors satisfy $E[\\varepsilon \\mid X] = 0 $, ensuring that there is no systematic relationship between $X$ and $\\varepsilon$.\n- **Homoskedasticity**: The variance of errors is constant, i.e., $\\text{Var}(\\varepsilon \\mid X) = \\sigma^2 I_n$.\n- **No Perfect Multicollinearity**: $X'X$ is full rank, ensuring that the inverse $(X'X)^{-1}$ exists.\n\nIn the classical Gauss-Markov framework, the regressors $X$ are treated as fixed. This means that we analyze the behavior of $\\hat{\\beta}$ conditional on the observed $X$. \n\nUnder these conditions, the ordinary least squares (OLS) estimator is unbiased and has the classical variance formula:\n\n$$\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X'X)^{-1}.$$\n\nAn important feature here is that all expectations and variances are conditional on $X$. This conditioning makes sense because we're treating $X$ as fixed and known.\n\n### Random Regressors\n\nIn most real-world scenarios, our $X$ variables aren't actually fixed. We often sample observations from a population, making both $Y$ and $X$ random. The random regressor framework acknowledges this reality. Under this framework, we add assumptions about the distribution of $X$.\n\nWhen $X$ is treated as random, it is assumed to be drawn from some probability distribution. This changes how we analyze $\\hat{\\beta}$, since expectations are now taken over both $\\varepsilon$ and $X$. The key assumptions in this setting are:\n\n- **Joint Distribution**: $(X, Y)$ follows some joint distribution, meaning $X$ is not fixed but rather drawn from a population.\n- **Exogeneity in Expectation**: In both frameworks, we typically assume $E[\\varepsilon \\mid X] = 0$, which rules out omitted variable bias. However, under the random regressor perspective, we also account for the fact that $X$ itself is drawn from a distribution, and expectations (e.g., for variance) are taken over both \n$X$ and $\\varepsilon$.\n- **Law of Large Numbers**: As $n \\to \\infty$, the sample quantities $\\frac{1}{n} X'X$ converge to their population analogs.\n\nA major consequence of assuming $X$ is random is that the variance of $\\hat{\\beta}$ takes a different form:\n\n$$\\text{Var}(\\hat{\\beta}) = E[(X'X)^{-1} X' \\varepsilon \\varepsilon' X (X'X)^{-1}].$$\n\nThis expression accounts for uncertainty in both $ X $ and $ \\varepsilon $, and under large samples, it converges to the population variance.\n\n### Practical Implications\n\nSo what's the big deal? Here are some practical implications:\n\n1. **Inference**: In the fixed regressor framework, inference is conditional on the observed values of $X$—you’re estimating the best linear approximation given this specific sample design. In the random regressor case, inference targets a population-level relationship between $X$ and $Y$.\n2. **Prediction** Error: With fixed regressors, prediction error only accounts for the randomness in $\\varepsilon$. With random regressors, you must also account for the randomness in future $\\mathbf{X} $ values.\n3. **Robustness**: The random regressor framework tends to provide more realistic assessments of model uncertainty, especially when extrapolating.\n\nLet me put this in more intuitive terms: if you're designing an experiment where you precisely control the levels of your predictors, the fixed regressor framework makes sense. If you're analyzing observational data where both predictors and responses are sampled from a population, the random regressor framework is more appropriate.\n\n## Bottom Line\n\n- The distinction between fixed and random regressors affects estimation and inference.\n\n- Fixed regressors condition on $X$, while random regressors integrate $X$ over its distribution.\n\n- OLS properties such as bias and variance differ under these assumptions.\n\n- In practice, whether $X$ is fixed or random depends on the study design and intended inference.\n\n## References\n\nGreene, W. H. (2012). Econometric Analysis. Pearson. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springe","srcMarkdownNoYaml":"\n\n## Background\n\nLinear regression stands as one of the fundamental workhorses of statistical modeling and is ubiquitous in applied work across fields like econometrics, biostatistics, and machine learning. Beneath its seemingly simple exterior lies a nuanced landscape of assumptions and theoretical considerations. One such distinction that often gets overlooked—even by seasoned practitioners—is the difference between fixed and random regressors. This distinction, while subtle, has important implications for how we interpret our models, conduct inference, and understand uncertainty.\n\nThe goal of this article is to clarify the differences between fixed and random regressors in linear regression. We'll explore why this distinction matters and how it affects everything from your confidence intervals to your prediction errors. My goal is to help you develop both the technical understanding and the intuition needed to make informed modeling decisions in your day-to-day work.\n\n## Notation\n\nTo set the stage, consider the standard linear regression model:\n\n$$Y = X\\beta + \\varepsilon,$$\n\nwhere:\n\n- $Y$ is an $ n \\times 1 $ vector of outcomes,\n- $X$ is an $ n \\times p $ matrix of regressors (also called covariates or features),\n- $\\beta$ is a $p \\times 1$ vector of coefficients to be estimated,\n- $\\varepsilon$ is an $ n \\times 1$ vector of errors.\n\nThe least squares estimator of $\\beta$ is given by:\n\n$$ \\hat{\\beta} = (X'X)^{-1}X'Y. $$\n\nThe key question is: What do we assume about $X$? If $ X $ is considered fixed, we condition on it when deriving properties of $\\hat{\\beta}$. If $X$ is random, we take expectations over its distribution. This affects how we think about the variance of our estimator, the validity of standard errors, and the asymptotic behavior of $\\hat{\\beta}$.\n\n## A Closer Look\n\n### Fixed Regressors\n\nIn the fixed regressor framework, we assume that $X$ is determined beforehand—perhaps through experimental design or by conditioning on observed values. The randomness in our model comes solely from the error term $\\varepsilon$.\n\nThe key assumptions typically made under this approach are:\n\n- **Linearity**: The model is correctly specified as $Y = X\\beta + \\varepsilon$.\n- **Exogeneity**: The errors satisfy $E[\\varepsilon \\mid X] = 0 $, ensuring that there is no systematic relationship between $X$ and $\\varepsilon$.\n- **Homoskedasticity**: The variance of errors is constant, i.e., $\\text{Var}(\\varepsilon \\mid X) = \\sigma^2 I_n$.\n- **No Perfect Multicollinearity**: $X'X$ is full rank, ensuring that the inverse $(X'X)^{-1}$ exists.\n\nIn the classical Gauss-Markov framework, the regressors $X$ are treated as fixed. This means that we analyze the behavior of $\\hat{\\beta}$ conditional on the observed $X$. \n\nUnder these conditions, the ordinary least squares (OLS) estimator is unbiased and has the classical variance formula:\n\n$$\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X'X)^{-1}.$$\n\nAn important feature here is that all expectations and variances are conditional on $X$. This conditioning makes sense because we're treating $X$ as fixed and known.\n\n### Random Regressors\n\nIn most real-world scenarios, our $X$ variables aren't actually fixed. We often sample observations from a population, making both $Y$ and $X$ random. The random regressor framework acknowledges this reality. Under this framework, we add assumptions about the distribution of $X$.\n\nWhen $X$ is treated as random, it is assumed to be drawn from some probability distribution. This changes how we analyze $\\hat{\\beta}$, since expectations are now taken over both $\\varepsilon$ and $X$. The key assumptions in this setting are:\n\n- **Joint Distribution**: $(X, Y)$ follows some joint distribution, meaning $X$ is not fixed but rather drawn from a population.\n- **Exogeneity in Expectation**: In both frameworks, we typically assume $E[\\varepsilon \\mid X] = 0$, which rules out omitted variable bias. However, under the random regressor perspective, we also account for the fact that $X$ itself is drawn from a distribution, and expectations (e.g., for variance) are taken over both \n$X$ and $\\varepsilon$.\n- **Law of Large Numbers**: As $n \\to \\infty$, the sample quantities $\\frac{1}{n} X'X$ converge to their population analogs.\n\nA major consequence of assuming $X$ is random is that the variance of $\\hat{\\beta}$ takes a different form:\n\n$$\\text{Var}(\\hat{\\beta}) = E[(X'X)^{-1} X' \\varepsilon \\varepsilon' X (X'X)^{-1}].$$\n\nThis expression accounts for uncertainty in both $ X $ and $ \\varepsilon $, and under large samples, it converges to the population variance.\n\n### Practical Implications\n\nSo what's the big deal? Here are some practical implications:\n\n1. **Inference**: In the fixed regressor framework, inference is conditional on the observed values of $X$—you’re estimating the best linear approximation given this specific sample design. In the random regressor case, inference targets a population-level relationship between $X$ and $Y$.\n2. **Prediction** Error: With fixed regressors, prediction error only accounts for the randomness in $\\varepsilon$. With random regressors, you must also account for the randomness in future $\\mathbf{X} $ values.\n3. **Robustness**: The random regressor framework tends to provide more realistic assessments of model uncertainty, especially when extrapolating.\n\nLet me put this in more intuitive terms: if you're designing an experiment where you precisely control the levels of your predictors, the fixed regressor framework makes sense. If you're analyzing observational data where both predictors and responses are sampled from a population, the random regressor framework is more appropriate.\n\n## Bottom Line\n\n- The distinction between fixed and random regressors affects estimation and inference.\n\n- Fixed regressors condition on $X$, while random regressors integrate $X$ over its distribution.\n\n- OLS properties such as bias and variance differ under these assumptions.\n\n- In practice, whether $X$ is fixed or random depends on the study design and intended inference.\n\n## References\n\nGreene, W. H. (2012). Econometric Analysis. Pearson. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springe"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"ols-fixed-random-x.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"OLS with Fixed versus Random Regressors","date":"2025-04-24","categories":["statistical inference","linear model"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}