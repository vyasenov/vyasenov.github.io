{"title":"Sensitivity Analysis in Causal Inference","markdown":{"yaml":{"title":"Sensitivity Analysis in Causal Inference","date":"2025-00-00","categories":["causal inference","sensitivity analysis"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nOne of the central challenges in causal inference is the problem of **unobserved confounding**. Even after controlling for observed covariates, we always face the nagging question: what if there’s something lurking in the background that we didn’t observe or can’t measure, and that something is driving our results?\n\nThis is where **sensitivity analysis** comes in. Developed and championed by Paul Rosenbaum and collaborators, sensitivity analysis provides a structured way to assess **how robust your causal conclusions are to potential violations of the unconfoundedness assumption** (also known as ignorability or selection on observables).\n\nIn this article, we’ll walk through the basic setup of sensitivity analysis in the potential outcomes framework, focus on Rosenbaum’s classical approach for observational studies, and discuss its practical implications. While sensitivity analysis doesn’t “solve” the confounding problem, it makes transparent how much unobserved bias would be required to overturn your findings.\n\n## Notation\n\nWe use the **potential outcomes framework**, where for each unit $i = 1, \\dots, n$:\n- $Y_i(1)$ is the potential outcome under treatment,\n- $Y_i(0)$ is the potential outcome under control,\n- $D_i \\in \\{0,1\\}$ is the treatment indicator,\n- $X_i$ is the vector of observed covariates.\n\nThe key parameter of interest is the **average treatment effect on the treated (ATT)**:\n$$\n\\text{ATT} = \\mathbb{E} [ Y(1) - Y(0) \\mid D = 1 ].\n$$\n\nStandard causal identification relies on the **unconfoundedness assumption**:\n$$\n(Y(1), Y(0)) \\perp D \\mid X.\n$$\nIn words: conditional on observed covariates $X$, treatment assignment $D$ is as good as random.\n\nHowever, what if this assumption fails? Suppose there’s an **unobserved confounder** $U$ that affects both $D$ and $Y$? Sensitivity analysis allows us to **quantify how much such an unobserved confounder would need to matter** to explain away the observed treatment effect.\n\n## A Closer Look\n\n### Rosenbaum's Sensitivity Model for Binary Treatments\n\nPaul Rosenbaum (1987, 2002) proposed a model for sensitivity analysis in matched observational studies that formalizes the possibility of hidden bias due to unobserved confounding.\n\nThe key idea is to **model treatment assignment probabilities** with a sensitivity parameter $\\Gamma$. Specifically, for any two matched individuals $i$ and $j$ with identical observed covariates:\n$$\n\\frac{1}{\\Gamma} \\leq \\frac{\\Pr(D_i = 1 \\mid X, U)}{\\Pr(D_j = 1 \\mid X, U)} \\leq \\Gamma.\n$$\n\n- If $\\Gamma = 1$, treatment assignment is unconfounded (purely random within matched pairs).\n\n- If $\\Gamma > 1$, there could be hidden bias: even after matching on $X$, units might still differ in treatment probability due to $U$.\n\nThe parameter $\\Gamma$ thus bounds how much the odds of receiving treatment could differ between matched units because of unobserved variables.\n\nRosenbaum then develops a **test statistic framework** to compute bounds on p-values for the null hypothesis of no treatment effect under different values of $\\Gamma$.\n\nThe procedure asks: *How large must $\\Gamma$ be before the p-value crosses a critical threshold (like 0.05)?* This critical value is called the **sensitivity value**.\n\n---\n\n### Interpreting $\\Gamma$: The Intuition\n\nThink of $\\Gamma$ as quantifying the strength of unmeasured confounding needed to change your conclusion. If your result holds up to a large $\\Gamma$ (e.g., $\\Gamma = 2.5$), it would require quite a strong confounder to nullify the result. But if the result disappears for $\\Gamma = 1.05$, your conclusion is quite fragile—even a mild bias could explain the effect.\n\nIn practice, values of $\\Gamma$ between 1.1 and 3 are common depending on the context, but what counts as \"large\" depends on the subject matter.\n\n---\n\n### Sensitivity Analysis Beyond Matching\n\nWhile Rosenbaum’s original framework was developed for matched designs, extensions of sensitivity analysis apply to:\n\n- **Regression adjustment models**\n- **Inverse probability weighting**\n- **Instrumental variable settings**\n- **Difference-in-differences designs**\n\nThe core idea remains: quantify how much unobserved confounding could alter your results.\n\nIn regression contexts, the **Rosenbaum bounds** approach may not be directly applicable, but similar logic applies via methods like:\n\n- **Oster's delta method** (Oster, 2019),\n- **Altonji, Elder, Taber approach**,\n- **Cornfield conditions** (used historically in epidemiology).\n\n---\n\n### Why Sensitivity Analysis Matters\n\nSensitivity analysis doesn’t claim to remove unobserved bias. Rather, it **exposes the vulnerability of your conclusions**. It turns the problem of unmeasured confounding from a scary unknown into a quantifiable scenario:\n\n> “Your effect holds unless there exists an unmeasured confounder that changes the odds of treatment by a factor of 2 *and* is strongly associated with the outcome.”\n\nThis transparency invites substantive experts to weigh in: is such a confounder plausible? If yes, proceed cautiously. If not, gain confidence in your findings.\n\n## An Example\n\nLet’s see how to implement Rosenbaum’s sensitivity analysis in practice. Below we show how to do this in R using the `rbounds` package, and a simple simulation-based illustration in Python.\n\n::::{.panel-tabset}\n\n### R\n\n```r\n# Install and load the package if needed\n# install.packages(\"rbounds\")\nlibrary(rbounds)\n\n# Simulate matched pair data\nset.seed(123)\nn_pairs <- 50\ntreated <- rnorm(n_pairs, mean = 1)\ncontrol <- rnorm(n_pairs, mean = 0.5)\ndiffs <- treated - control\n\n# Perform Wilcoxon signed-rank test for matched pairs\nwilcox.test(diffs, alternative = \"greater\", exact = FALSE)\n\n# Perform Rosenbaum sensitivity analysis (Hodges-Lehmann test)\n# Gamma = 1 means no hidden bias; increase Gamma to test robustness\npsens(diffs, Gamma = 1.5, alternative = \"greater\")\n\n```\n\n### Python\n\n```python\n# Python does not have built-in Rosenbaum bounds yet,\n# but we can simulate how unmeasured confounding could shift results.\n\nimport numpy as np\nfrom scipy.stats import wilcoxon\n\nnp.random.seed(123)\nn_pairs = 50\ntreated = np.random.normal(1, 1, n_pairs)\ncontrol = np.random.normal(0.5, 1, n_pairs)\ndiffs = treated - control\n\n# Wilcoxon signed-rank test (no hidden bias)\nstat, p_value = wilcoxon(diffs, alternative='greater')\nprint(f\"Wilcoxon p-value (no bias): {p_value:.4f}\")\n\n# Simple sensitivity illustration: perturb the control group\nfor gamma_effect in [0.1, 0.2, 0.3]:\n    biased_control = control + gamma_effect\n    biased_diffs = treated - biased_control\n    stat, p_biased = wilcoxon(biased_diffs, alternative='greater')\n    print(f\"Gamma-effect {gamma_effect:.2f} → p-value: {p_biased:.4f}\")\n```\n\n::::\n\n---\n\n## Bottom Line\n\n- Sensitivity analysis quantifies how much unobserved confounding would be needed to explain away your estimated treatment effects.\n- Paul Rosenbaum’s framework introduces a sensitivity parameter $\\Gamma$ to model hidden bias in matched observational studies.\n- A high sensitivity value suggests your results are robust; a low one suggests fragility.\n- Sensitivity analysis doesn’t remove bias—it makes the assumptions about bias explicit and testable.\n\n## Where to Learn More\n\nPaul Rosenbaum’s book *Observational Studies* (2002, Springer) is the definitive reference on sensitivity analysis in causal inference. For a practical introduction, see his papers in *Biometrika* (1987) and subsequent work on sensitivity values and extensions to various designs. The R packages `rbounds` and `sensitivitymv` provide implementations for matched studies. Extensions of sensitivity analysis to regression, IV, and other settings are well-covered in recent papers by Oster (2019) and Cinelli & Hazlett (2020).\n\n## References\n\n- Rosenbaum, P. R. (1987). Sensitivity analysis for certain permutation inferences in matched observational studies. *Biometrika*, 74(1), 13–26.\n- Rosenbaum, P. R. (2002). *Observational Studies* (2nd ed.). Springer.\n- Oster, E. (2019). Unobservable selection and coefficient stability: Theory and evidence. *Journal of Business & Economic Statistics*, 37(2), 187–204.\n- Altonji, J. G., Elder, T. E., & Taber, C. R. (2005). Selection on observed and unobserved variables: Assessing the effectiveness of Catholic schools. *Journal of Political Economy*, 113(1), 151–184.\n- Cinelli, C., & Hazlett, C. (2020). Making sense of sensitivity: Extending omitted variable bias. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 82(1), 39–67.\n","srcMarkdownNoYaml":"\n\n## Background\n\nOne of the central challenges in causal inference is the problem of **unobserved confounding**. Even after controlling for observed covariates, we always face the nagging question: what if there’s something lurking in the background that we didn’t observe or can’t measure, and that something is driving our results?\n\nThis is where **sensitivity analysis** comes in. Developed and championed by Paul Rosenbaum and collaborators, sensitivity analysis provides a structured way to assess **how robust your causal conclusions are to potential violations of the unconfoundedness assumption** (also known as ignorability or selection on observables).\n\nIn this article, we’ll walk through the basic setup of sensitivity analysis in the potential outcomes framework, focus on Rosenbaum’s classical approach for observational studies, and discuss its practical implications. While sensitivity analysis doesn’t “solve” the confounding problem, it makes transparent how much unobserved bias would be required to overturn your findings.\n\n## Notation\n\nWe use the **potential outcomes framework**, where for each unit $i = 1, \\dots, n$:\n- $Y_i(1)$ is the potential outcome under treatment,\n- $Y_i(0)$ is the potential outcome under control,\n- $D_i \\in \\{0,1\\}$ is the treatment indicator,\n- $X_i$ is the vector of observed covariates.\n\nThe key parameter of interest is the **average treatment effect on the treated (ATT)**:\n$$\n\\text{ATT} = \\mathbb{E} [ Y(1) - Y(0) \\mid D = 1 ].\n$$\n\nStandard causal identification relies on the **unconfoundedness assumption**:\n$$\n(Y(1), Y(0)) \\perp D \\mid X.\n$$\nIn words: conditional on observed covariates $X$, treatment assignment $D$ is as good as random.\n\nHowever, what if this assumption fails? Suppose there’s an **unobserved confounder** $U$ that affects both $D$ and $Y$? Sensitivity analysis allows us to **quantify how much such an unobserved confounder would need to matter** to explain away the observed treatment effect.\n\n## A Closer Look\n\n### Rosenbaum's Sensitivity Model for Binary Treatments\n\nPaul Rosenbaum (1987, 2002) proposed a model for sensitivity analysis in matched observational studies that formalizes the possibility of hidden bias due to unobserved confounding.\n\nThe key idea is to **model treatment assignment probabilities** with a sensitivity parameter $\\Gamma$. Specifically, for any two matched individuals $i$ and $j$ with identical observed covariates:\n$$\n\\frac{1}{\\Gamma} \\leq \\frac{\\Pr(D_i = 1 \\mid X, U)}{\\Pr(D_j = 1 \\mid X, U)} \\leq \\Gamma.\n$$\n\n- If $\\Gamma = 1$, treatment assignment is unconfounded (purely random within matched pairs).\n\n- If $\\Gamma > 1$, there could be hidden bias: even after matching on $X$, units might still differ in treatment probability due to $U$.\n\nThe parameter $\\Gamma$ thus bounds how much the odds of receiving treatment could differ between matched units because of unobserved variables.\n\nRosenbaum then develops a **test statistic framework** to compute bounds on p-values for the null hypothesis of no treatment effect under different values of $\\Gamma$.\n\nThe procedure asks: *How large must $\\Gamma$ be before the p-value crosses a critical threshold (like 0.05)?* This critical value is called the **sensitivity value**.\n\n---\n\n### Interpreting $\\Gamma$: The Intuition\n\nThink of $\\Gamma$ as quantifying the strength of unmeasured confounding needed to change your conclusion. If your result holds up to a large $\\Gamma$ (e.g., $\\Gamma = 2.5$), it would require quite a strong confounder to nullify the result. But if the result disappears for $\\Gamma = 1.05$, your conclusion is quite fragile—even a mild bias could explain the effect.\n\nIn practice, values of $\\Gamma$ between 1.1 and 3 are common depending on the context, but what counts as \"large\" depends on the subject matter.\n\n---\n\n### Sensitivity Analysis Beyond Matching\n\nWhile Rosenbaum’s original framework was developed for matched designs, extensions of sensitivity analysis apply to:\n\n- **Regression adjustment models**\n- **Inverse probability weighting**\n- **Instrumental variable settings**\n- **Difference-in-differences designs**\n\nThe core idea remains: quantify how much unobserved confounding could alter your results.\n\nIn regression contexts, the **Rosenbaum bounds** approach may not be directly applicable, but similar logic applies via methods like:\n\n- **Oster's delta method** (Oster, 2019),\n- **Altonji, Elder, Taber approach**,\n- **Cornfield conditions** (used historically in epidemiology).\n\n---\n\n### Why Sensitivity Analysis Matters\n\nSensitivity analysis doesn’t claim to remove unobserved bias. Rather, it **exposes the vulnerability of your conclusions**. It turns the problem of unmeasured confounding from a scary unknown into a quantifiable scenario:\n\n> “Your effect holds unless there exists an unmeasured confounder that changes the odds of treatment by a factor of 2 *and* is strongly associated with the outcome.”\n\nThis transparency invites substantive experts to weigh in: is such a confounder plausible? If yes, proceed cautiously. If not, gain confidence in your findings.\n\n## An Example\n\nLet’s see how to implement Rosenbaum’s sensitivity analysis in practice. Below we show how to do this in R using the `rbounds` package, and a simple simulation-based illustration in Python.\n\n::::{.panel-tabset}\n\n### R\n\n```r\n# Install and load the package if needed\n# install.packages(\"rbounds\")\nlibrary(rbounds)\n\n# Simulate matched pair data\nset.seed(123)\nn_pairs <- 50\ntreated <- rnorm(n_pairs, mean = 1)\ncontrol <- rnorm(n_pairs, mean = 0.5)\ndiffs <- treated - control\n\n# Perform Wilcoxon signed-rank test for matched pairs\nwilcox.test(diffs, alternative = \"greater\", exact = FALSE)\n\n# Perform Rosenbaum sensitivity analysis (Hodges-Lehmann test)\n# Gamma = 1 means no hidden bias; increase Gamma to test robustness\npsens(diffs, Gamma = 1.5, alternative = \"greater\")\n\n```\n\n### Python\n\n```python\n# Python does not have built-in Rosenbaum bounds yet,\n# but we can simulate how unmeasured confounding could shift results.\n\nimport numpy as np\nfrom scipy.stats import wilcoxon\n\nnp.random.seed(123)\nn_pairs = 50\ntreated = np.random.normal(1, 1, n_pairs)\ncontrol = np.random.normal(0.5, 1, n_pairs)\ndiffs = treated - control\n\n# Wilcoxon signed-rank test (no hidden bias)\nstat, p_value = wilcoxon(diffs, alternative='greater')\nprint(f\"Wilcoxon p-value (no bias): {p_value:.4f}\")\n\n# Simple sensitivity illustration: perturb the control group\nfor gamma_effect in [0.1, 0.2, 0.3]:\n    biased_control = control + gamma_effect\n    biased_diffs = treated - biased_control\n    stat, p_biased = wilcoxon(biased_diffs, alternative='greater')\n    print(f\"Gamma-effect {gamma_effect:.2f} → p-value: {p_biased:.4f}\")\n```\n\n::::\n\n---\n\n## Bottom Line\n\n- Sensitivity analysis quantifies how much unobserved confounding would be needed to explain away your estimated treatment effects.\n- Paul Rosenbaum’s framework introduces a sensitivity parameter $\\Gamma$ to model hidden bias in matched observational studies.\n- A high sensitivity value suggests your results are robust; a low one suggests fragility.\n- Sensitivity analysis doesn’t remove bias—it makes the assumptions about bias explicit and testable.\n\n## Where to Learn More\n\nPaul Rosenbaum’s book *Observational Studies* (2002, Springer) is the definitive reference on sensitivity analysis in causal inference. For a practical introduction, see his papers in *Biometrika* (1987) and subsequent work on sensitivity values and extensions to various designs. The R packages `rbounds` and `sensitivitymv` provide implementations for matched studies. Extensions of sensitivity analysis to regression, IV, and other settings are well-covered in recent papers by Oster (2019) and Cinelli & Hazlett (2020).\n\n## References\n\n- Rosenbaum, P. R. (1987). Sensitivity analysis for certain permutation inferences in matched observational studies. *Biometrika*, 74(1), 13–26.\n- Rosenbaum, P. R. (2002). *Observational Studies* (2nd ed.). Springer.\n- Oster, E. (2019). Unobservable selection and coefficient stability: Theory and evidence. *Journal of Business & Economic Statistics*, 37(2), 187–204.\n- Altonji, J. G., Elder, T. E., & Taber, C. R. (2005). Selection on observed and unobserved variables: Assessing the effectiveness of Catholic schools. *Journal of Political Economy*, 113(1), 151–184.\n- Cinelli, C., & Hazlett, C. (2020). Making sense of sensitivity: Extending omitted variable bias. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 82(1), 39–67.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"sensitivity-analysis.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Sensitivity Analysis in Causal Inference","date":"2025-00-00","categories":["causal inference","sensitivity analysis"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}