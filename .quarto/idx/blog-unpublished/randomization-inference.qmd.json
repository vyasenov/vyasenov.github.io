{"title":"Randomization Inference: Exact Testing Without Parametric Assumptions","markdown":{"yaml":{"title":"Randomization Inference: Exact Testing Without Parametric Assumptions","date":"2025-00-00","categories":["causal inference","randomized experiments"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nRandomization inference offers a refreshing alternative to traditional parametric inference, providing exact control over Type I error rates without relying on large-sample approximations or strict distributional assumptions. Born out of Fisher's famous tea-tasting experiment, the approach leverages the symmetry and structure induced by randomization itself to test hypotheses. \n\nThis blog post unpacks the theory and intuition behind randomization inference, drawing on the excellent review by Ritzwoller, Romano, and Shaikh (2025). We'll cover the key ideas, notation, and algorithms involved, and also touch on modern applications like two-sample tests, regression, and conformal inference. Throughout, we'll emphasize the practical considerations — when it works, why it works, and where caution is needed.\n\n## Notation\n\nLet $X$ represent the observed data, generated by some unknown probability law $P$. The parameter space $\\Omega$ contains all possible data-generating processes, and $\\Omega_0 \\subset \\Omega$ specifies the null hypothesis.\n\nA group $G$ of transformations (e.g., permutations or sign-flips) acts on the data. Under the **randomization hypothesis**, the distribution of $X$ is invariant under $G$ if $P \\in \\Omega_0$.\n\nFormally, under the null:\n$$\ngX \\overset{d}{=} X, \\quad \\text{for all } g \\in G.\n$$\n\nLet $T(X)$ be a chosen test statistic.\n\n## A Closer Look\n\n### Exact Testing via Randomization\n\nIf the randomization hypothesis holds, we can compute the distribution of $T(X)$ by applying all transformations in $G$ to the data. The p-value is simply the proportion of these transformed test statistics that are as extreme or more extreme than the observed $T(X)$:\n\n$$\n\\hat{p} = \\frac{1}{|G|} \\sum_{g \\in G} I\\{ T(gX) \\geq T(X) \\}.\n$$\n\nBecause the null implies invariance under $G$, this procedure achieves exact finite-sample control of the Type I error rate.\n\n::: {.callout-note title=\"Algorithm: Randomization Test\"}\n1. Choose a test statistic $T(X)$.\n2. Define the group $G$ of transformations.\n3. Compute $T(X)$ on the observed data.\n4. Apply all (or a random sample of) transformations $g \\in G$ to the data and recompute $T(gX)$.\n5. Calculate the p-value as the proportion of transformed statistics as or more extreme than $T(X)$.\n:::\n\n### Approximate Validity and Asymptotics\n\nIn many real-world problems, the randomization hypothesis may not strictly hold, or the group $G$ may only approximate invariance. The surprising good news is that permutation and randomization tests often remain **asymptotically valid** under broad conditions.\n\nThis validity relies on the test statistic being **asymptotically pivotal** — its limiting distribution under the null does not depend on nuisance parameters. Examples include:\n- The studentized difference of means.\n- Test statistics based on ranks (e.g., Wilcoxon-Mann-Whitney).\n\nBy **studentizing** the test statistic (i.e., scaling by an estimate of its standard error), one can often restore validity even when the randomization hypothesis fails.\n\n### When Things Go Wrong: Failures Without Studentization\n\nIf the test statistic is not pivotal, the permutation distribution might not match the true sampling distribution. This mismatch leads to inflated Type I errors and directional mistakes (Type III errors), especially when variances differ between groups.\n\nFor instance, testing mean differences between two groups with unequal variances using the raw difference in means (without studentization) can lead to massive over-rejection rates. Studentizing solves this problem by aligning the permutation distribution with the true sampling distribution.\n\n### Strengths and Limitations\n\nRandomization inference shines when:\n- The randomization scheme is known and under control (e.g., in experiments).\n- The test statistic is carefully chosen to be pivotal.\n- Exact error control is desirable in finite samples.\n\nIt struggles when:\n- Covariates are correlated with treatment assignment but not accounted for.\n- The sample size is too small to approximate the randomization distribution well via subsampling.\n\n---\n\n## An Example\n\n::::{.panel-tabset}\n\n### R\n\n```r\nset.seed(123)\nn <- 20\nx <- rnorm(n, mean = 0)\ntest_stat <- mean(x)\nn_permutations <- 1000\nperms <- replicate(n_permutations, mean(sample(x)))\np_value <- mean(perms >= test_stat)\np_value\n```\n\n### Python\n\n```python\nimport numpy as np\n\nnp.random.seed(123)\nn = 20\nx = np.random.normal(0, 1, n)\ntest_stat = np.mean(x)\nperms = [np.mean(np.random.permutation(x)) for _ in range(1000)]\np_value = np.mean([p >= test_stat for p in perms])\nprint(p_value)\n```\n\n::::\n\n## Bottom Line\n\n- Randomization inference provides exact finite-sample error control when the randomization hypothesis holds.\n\n- Asymptotic validity can often be rescued by choosing asymptotically pivotal (studentized) test statistics.\n\n- Without studentization, permutation tests may fail badly in the presence of unequal variances.\n\n- Randomization tests are flexible and nonparametric, making them attractive for experimental data and beyond.\n\n## Where to Learn More\n\nThe best starting point is the recent review by Ritzwoller, Romano, and Shaikh (2025). For foundational treatments, see Hoeffding (1952) and Lehmann & Romano's text on nonparametric inference. The practical guide by Good (2005) on permutation tests is also highly recommended.\n\n## References\n\n- Ritzwoller, D. M., Romano, J. P., & Shaikh, A. M. (2025). Randomization Inference: Theory and Applications.\n\n- Hoeffding, W. (1952). The large-sample power of permutation tests. *Annals of Mathematical Statistics*, 23(2), 169-192.\n\n- Lehmann, E. L., & Romano, J. P. (2022). *Testing Statistical Hypotheses*. Springer.\n\n- Good, P. (2005). *Permutation, Parametric, and Bootstrap Tests of Hypotheses*. Springer.\n","srcMarkdownNoYaml":"\n\n## Background\n\nRandomization inference offers a refreshing alternative to traditional parametric inference, providing exact control over Type I error rates without relying on large-sample approximations or strict distributional assumptions. Born out of Fisher's famous tea-tasting experiment, the approach leverages the symmetry and structure induced by randomization itself to test hypotheses. \n\nThis blog post unpacks the theory and intuition behind randomization inference, drawing on the excellent review by Ritzwoller, Romano, and Shaikh (2025). We'll cover the key ideas, notation, and algorithms involved, and also touch on modern applications like two-sample tests, regression, and conformal inference. Throughout, we'll emphasize the practical considerations — when it works, why it works, and where caution is needed.\n\n## Notation\n\nLet $X$ represent the observed data, generated by some unknown probability law $P$. The parameter space $\\Omega$ contains all possible data-generating processes, and $\\Omega_0 \\subset \\Omega$ specifies the null hypothesis.\n\nA group $G$ of transformations (e.g., permutations or sign-flips) acts on the data. Under the **randomization hypothesis**, the distribution of $X$ is invariant under $G$ if $P \\in \\Omega_0$.\n\nFormally, under the null:\n$$\ngX \\overset{d}{=} X, \\quad \\text{for all } g \\in G.\n$$\n\nLet $T(X)$ be a chosen test statistic.\n\n## A Closer Look\n\n### Exact Testing via Randomization\n\nIf the randomization hypothesis holds, we can compute the distribution of $T(X)$ by applying all transformations in $G$ to the data. The p-value is simply the proportion of these transformed test statistics that are as extreme or more extreme than the observed $T(X)$:\n\n$$\n\\hat{p} = \\frac{1}{|G|} \\sum_{g \\in G} I\\{ T(gX) \\geq T(X) \\}.\n$$\n\nBecause the null implies invariance under $G$, this procedure achieves exact finite-sample control of the Type I error rate.\n\n::: {.callout-note title=\"Algorithm: Randomization Test\"}\n1. Choose a test statistic $T(X)$.\n2. Define the group $G$ of transformations.\n3. Compute $T(X)$ on the observed data.\n4. Apply all (or a random sample of) transformations $g \\in G$ to the data and recompute $T(gX)$.\n5. Calculate the p-value as the proportion of transformed statistics as or more extreme than $T(X)$.\n:::\n\n### Approximate Validity and Asymptotics\n\nIn many real-world problems, the randomization hypothesis may not strictly hold, or the group $G$ may only approximate invariance. The surprising good news is that permutation and randomization tests often remain **asymptotically valid** under broad conditions.\n\nThis validity relies on the test statistic being **asymptotically pivotal** — its limiting distribution under the null does not depend on nuisance parameters. Examples include:\n- The studentized difference of means.\n- Test statistics based on ranks (e.g., Wilcoxon-Mann-Whitney).\n\nBy **studentizing** the test statistic (i.e., scaling by an estimate of its standard error), one can often restore validity even when the randomization hypothesis fails.\n\n### When Things Go Wrong: Failures Without Studentization\n\nIf the test statistic is not pivotal, the permutation distribution might not match the true sampling distribution. This mismatch leads to inflated Type I errors and directional mistakes (Type III errors), especially when variances differ between groups.\n\nFor instance, testing mean differences between two groups with unequal variances using the raw difference in means (without studentization) can lead to massive over-rejection rates. Studentizing solves this problem by aligning the permutation distribution with the true sampling distribution.\n\n### Strengths and Limitations\n\nRandomization inference shines when:\n- The randomization scheme is known and under control (e.g., in experiments).\n- The test statistic is carefully chosen to be pivotal.\n- Exact error control is desirable in finite samples.\n\nIt struggles when:\n- Covariates are correlated with treatment assignment but not accounted for.\n- The sample size is too small to approximate the randomization distribution well via subsampling.\n\n---\n\n## An Example\n\n::::{.panel-tabset}\n\n### R\n\n```r\nset.seed(123)\nn <- 20\nx <- rnorm(n, mean = 0)\ntest_stat <- mean(x)\nn_permutations <- 1000\nperms <- replicate(n_permutations, mean(sample(x)))\np_value <- mean(perms >= test_stat)\np_value\n```\n\n### Python\n\n```python\nimport numpy as np\n\nnp.random.seed(123)\nn = 20\nx = np.random.normal(0, 1, n)\ntest_stat = np.mean(x)\nperms = [np.mean(np.random.permutation(x)) for _ in range(1000)]\np_value = np.mean([p >= test_stat for p in perms])\nprint(p_value)\n```\n\n::::\n\n## Bottom Line\n\n- Randomization inference provides exact finite-sample error control when the randomization hypothesis holds.\n\n- Asymptotic validity can often be rescued by choosing asymptotically pivotal (studentized) test statistics.\n\n- Without studentization, permutation tests may fail badly in the presence of unequal variances.\n\n- Randomization tests are flexible and nonparametric, making them attractive for experimental data and beyond.\n\n## Where to Learn More\n\nThe best starting point is the recent review by Ritzwoller, Romano, and Shaikh (2025). For foundational treatments, see Hoeffding (1952) and Lehmann & Romano's text on nonparametric inference. The practical guide by Good (2005) on permutation tests is also highly recommended.\n\n## References\n\n- Ritzwoller, D. M., Romano, J. P., & Shaikh, A. M. (2025). Randomization Inference: Theory and Applications.\n\n- Hoeffding, W. (1952). The large-sample power of permutation tests. *Annals of Mathematical Statistics*, 23(2), 169-192.\n\n- Lehmann, E. L., & Romano, J. P. (2022). *Testing Statistical Hypotheses*. Springer.\n\n- Good, P. (2005). *Permutation, Parametric, and Bootstrap Tests of Hypotheses*. Springer.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"randomization-inference.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Randomization Inference: Exact Testing Without Parametric Assumptions","date":"2025-00-00","categories":["causal inference","randomized experiments"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}