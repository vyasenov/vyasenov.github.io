{"title":"Lasso Flavors: Variants, Objective Functions, and When to Use Them","markdown":{"yaml":{"title":"Lasso Flavors: Variants, Objective Functions, and When to Use Them","date":"2025-00-00","categories":["lasso","regularization"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nThe Lasso (Least Absolute Shrinkage and Selection Operator), introduced by Tibshirani in 1996, has become one of the go-to tools for variable selection and shrinkage in regression problems. But the classic Lasso is just the starting point. Over the years, researchers have developed many **variants of Lasso**, each designed to address specific limitations or tailor the method to different kinds of data structures.\n\nThis article provides a tour of the most popular flavors of Lasso — from standard L1-penalized regression to modern adaptations like Adaptive Lasso, Elastic Net, Square-root Lasso, and more. For each version, we’ll lay out the objective function, describe when it’s applicable, and summarize its key characteristics.\n\n## A Closer Look\n\n### 1. Standard Lasso (Tibshirani, 1996)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right)\n$$\n\n- **Key feature:** Encourages sparsity by shrinking some coefficients exactly to zero.\n- **When to use:** Variable selection with many predictors; handles high-dimensional data.\n- **Characteristics:** Shrinkage bias; struggles with highly correlated predictors.\n\n### 2. Adaptive Lasso (Zou, 2006)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^p w_j | \\beta_j | \\right)\n$$\nwhere $w_j = 1 / |\\hat{\\beta}_j^{\\text{init}}|^\\gamma$.\n\n- **Key feature:** Oracle property (support recovery consistency).\n- **When to use:** When initial estimates (like OLS or Ridge) are available.\n- **Characteristics:** Bias reduction via adaptive weights.\n\n### 3. Relaxed Lasso (Meinshausen, 2007)\n\n- **Step 1:** Use Lasso for selection.\n- **Step 2:** Refit OLS on the selected variables or apply partial shrinkage with a relaxation parameter $\\phi$.\n\n- **Key feature:** Separates selection from estimation.\n- **When to use:** Reduce Lasso’s shrinkage bias after variable selection.\n- **Characteristics:** Combines selection strength of Lasso with unbiased estimation of OLS.\n\n### 4. Square-root Lasso / Scaled Lasso (Belloni, Chernozhukov, Wang, 2011)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{\\sqrt{n}} \\| y - X \\beta \\|_2 + \\lambda \\| \\beta \\|_1 \\right)\n$$\n\n- **Key feature:** Scale-invariant — does not require estimating error variance.\n- **When to use:** Unknown or heteroskedastic error variance.\n- **Characteristics:** Easier tuning; robust to variance misspecification.\n\n### 5. Elastic Net (Zou and Hastie, 2005)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\| \\beta \\|_2^2 \\right)\n$$\n\n- **Key feature:** Combines L1 and L2 penalties.\n- **When to use:** Multicollinearity among predictors.\n- **Characteristics:** Handles correlated variables better than standard Lasso.\n\n### 6. Group Lasso (Yuan & Lin, 2006)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{g=1}^G \\| \\beta^{(g)} \\|_2 \\right)\n$$\n\n- **Key feature:** Selects groups of variables together.\n- **When to use:** Categorical variables or grouped data structures.\n- **Characteristics:** Encourages sparsity at the group level, not individual coefficients.\n\n### 7. Fused Lasso (Tibshirani et al., 2005)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\sum_{j=2}^p | \\beta_j - \\beta_{j-1} | \\right)\n$$\n\n- **Key feature:** Penalizes differences between adjacent coefficients.\n- **When to use:** Ordered features like time series or spatial data.\n- **Characteristics:** Promotes both sparsity and smoothness.\n\n### 8. Bayesian Lasso (Park & Casella, 2008)\n\n- **Objective (via prior):**\n$$\n\\beta_j \\sim \\text{Laplace}(0, \\lambda^{-1}).\n$$\n\n- **Key feature:** Fully Bayesian formulation with posterior inference.\n- **When to use:** When uncertainty quantification (credible intervals) is desired.\n- **Characteristics:** Shrinkage through Laplace priors; allows full posterior analysis.\n\n### 9. Graphical Lasso (Friedman et al., 2008)\n\n**Objective Function:**\n$$\n\\hat{\\Theta} = \\arg \\min_{\\Theta \\succ 0} \\left( -\\log \\det \\Theta + \\text{trace}(S \\Theta) + \\lambda \\| \\Theta \\|_1 \\right)\n$$\n\n- **Key feature:** Estimates sparse precision (inverse covariance) matrices.\n- **When to use:** Gaussian graphical models (network structure estimation).\n- **Characteristics:** Encourages sparsity in partial correlations.\n\n### 10. Stability Selection (Meinshausen & Bühlmann, 2010)\n\n- **Not an objective function per se** — combines subsampling with Lasso to control false discovery rate.\n- **Key feature:** Improves selection stability and robustness.\n- **When to use:** When worried about unstable variable selection.\n- **Characteristics:** Reduces false positives; provides error control guarantees.\n\n### Summary Table\n\n| Variant               | Key Feature                                | Motivation                      |\n|-----------------------|---------------------------------------------|----------------------------------|\n| Standard Lasso        | L1 penalty, sparsity                        | Variable selection              |\n| Adaptive Lasso        | Weighted penalties, oracle property        | Bias reduction                  |\n| Elastic Net           | Combines L1 and L2 penalties                | Handles multicollinearity       |\n| Group Lasso           | Group-wise selection                       | Grouped variables               |\n| Fused Lasso           | Penalizes differences between coefficients | Time-series or spatial data     |\n| Square-root Lasso     | Scale-free loss function                    | Unknown error variance          |\n| Bayesian Lasso        | Laplace priors in a Bayesian framework     | Posterior inference, uncertainty|\n| Graphical Lasso       | Penalizes inverse covariance matrix        | Gaussian graphical models       |\n| Relaxed Lasso         | Selection and estimation separated         | Bias reduction after selection  |\n| Stability Selection   | Adds subsampling for robustness            | Controls false discoveries      |\n\n## Bottom Line\n\n- Different Lasso variants address different modeling challenges like scaling, grouping, collinearity, and bias.\n\n- Understanding your data structure and inference goals helps choose the right Lasso flavor.\n\n\n- Most major machine learning libraries (R: `glmnet`, `grpreg`; Python: `scikit-learn`, `statsmodels`) provide support for these variants.\n\n## Where to Learn More\n\nKey papers: Tibshirani (1996), Zou (2006), Meinshausen (2007), Yuan and Lin (2006), Belloni et al. (2011), Park and Casella (2008), Friedman et al. (2008), Meinshausen and Bühlmann (2010).\n\n## References\n\n[TO ADD]","srcMarkdownNoYaml":"\n\n## Background\n\nThe Lasso (Least Absolute Shrinkage and Selection Operator), introduced by Tibshirani in 1996, has become one of the go-to tools for variable selection and shrinkage in regression problems. But the classic Lasso is just the starting point. Over the years, researchers have developed many **variants of Lasso**, each designed to address specific limitations or tailor the method to different kinds of data structures.\n\nThis article provides a tour of the most popular flavors of Lasso — from standard L1-penalized regression to modern adaptations like Adaptive Lasso, Elastic Net, Square-root Lasso, and more. For each version, we’ll lay out the objective function, describe when it’s applicable, and summarize its key characteristics.\n\n## A Closer Look\n\n### 1. Standard Lasso (Tibshirani, 1996)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right)\n$$\n\n- **Key feature:** Encourages sparsity by shrinking some coefficients exactly to zero.\n- **When to use:** Variable selection with many predictors; handles high-dimensional data.\n- **Characteristics:** Shrinkage bias; struggles with highly correlated predictors.\n\n### 2. Adaptive Lasso (Zou, 2006)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^p w_j | \\beta_j | \\right)\n$$\nwhere $w_j = 1 / |\\hat{\\beta}_j^{\\text{init}}|^\\gamma$.\n\n- **Key feature:** Oracle property (support recovery consistency).\n- **When to use:** When initial estimates (like OLS or Ridge) are available.\n- **Characteristics:** Bias reduction via adaptive weights.\n\n### 3. Relaxed Lasso (Meinshausen, 2007)\n\n- **Step 1:** Use Lasso for selection.\n- **Step 2:** Refit OLS on the selected variables or apply partial shrinkage with a relaxation parameter $\\phi$.\n\n- **Key feature:** Separates selection from estimation.\n- **When to use:** Reduce Lasso’s shrinkage bias after variable selection.\n- **Characteristics:** Combines selection strength of Lasso with unbiased estimation of OLS.\n\n### 4. Square-root Lasso / Scaled Lasso (Belloni, Chernozhukov, Wang, 2011)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{\\sqrt{n}} \\| y - X \\beta \\|_2 + \\lambda \\| \\beta \\|_1 \\right)\n$$\n\n- **Key feature:** Scale-invariant — does not require estimating error variance.\n- **When to use:** Unknown or heteroskedastic error variance.\n- **Characteristics:** Easier tuning; robust to variance misspecification.\n\n### 5. Elastic Net (Zou and Hastie, 2005)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\| \\beta \\|_2^2 \\right)\n$$\n\n- **Key feature:** Combines L1 and L2 penalties.\n- **When to use:** Multicollinearity among predictors.\n- **Characteristics:** Handles correlated variables better than standard Lasso.\n\n### 6. Group Lasso (Yuan & Lin, 2006)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{g=1}^G \\| \\beta^{(g)} \\|_2 \\right)\n$$\n\n- **Key feature:** Selects groups of variables together.\n- **When to use:** Categorical variables or grouped data structures.\n- **Characteristics:** Encourages sparsity at the group level, not individual coefficients.\n\n### 7. Fused Lasso (Tibshirani et al., 2005)\n\n**Objective Function:**\n$$\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\sum_{j=2}^p | \\beta_j - \\beta_{j-1} | \\right)\n$$\n\n- **Key feature:** Penalizes differences between adjacent coefficients.\n- **When to use:** Ordered features like time series or spatial data.\n- **Characteristics:** Promotes both sparsity and smoothness.\n\n### 8. Bayesian Lasso (Park & Casella, 2008)\n\n- **Objective (via prior):**\n$$\n\\beta_j \\sim \\text{Laplace}(0, \\lambda^{-1}).\n$$\n\n- **Key feature:** Fully Bayesian formulation with posterior inference.\n- **When to use:** When uncertainty quantification (credible intervals) is desired.\n- **Characteristics:** Shrinkage through Laplace priors; allows full posterior analysis.\n\n### 9. Graphical Lasso (Friedman et al., 2008)\n\n**Objective Function:**\n$$\n\\hat{\\Theta} = \\arg \\min_{\\Theta \\succ 0} \\left( -\\log \\det \\Theta + \\text{trace}(S \\Theta) + \\lambda \\| \\Theta \\|_1 \\right)\n$$\n\n- **Key feature:** Estimates sparse precision (inverse covariance) matrices.\n- **When to use:** Gaussian graphical models (network structure estimation).\n- **Characteristics:** Encourages sparsity in partial correlations.\n\n### 10. Stability Selection (Meinshausen & Bühlmann, 2010)\n\n- **Not an objective function per se** — combines subsampling with Lasso to control false discovery rate.\n- **Key feature:** Improves selection stability and robustness.\n- **When to use:** When worried about unstable variable selection.\n- **Characteristics:** Reduces false positives; provides error control guarantees.\n\n### Summary Table\n\n| Variant               | Key Feature                                | Motivation                      |\n|-----------------------|---------------------------------------------|----------------------------------|\n| Standard Lasso        | L1 penalty, sparsity                        | Variable selection              |\n| Adaptive Lasso        | Weighted penalties, oracle property        | Bias reduction                  |\n| Elastic Net           | Combines L1 and L2 penalties                | Handles multicollinearity       |\n| Group Lasso           | Group-wise selection                       | Grouped variables               |\n| Fused Lasso           | Penalizes differences between coefficients | Time-series or spatial data     |\n| Square-root Lasso     | Scale-free loss function                    | Unknown error variance          |\n| Bayesian Lasso        | Laplace priors in a Bayesian framework     | Posterior inference, uncertainty|\n| Graphical Lasso       | Penalizes inverse covariance matrix        | Gaussian graphical models       |\n| Relaxed Lasso         | Selection and estimation separated         | Bias reduction after selection  |\n| Stability Selection   | Adds subsampling for robustness            | Controls false discoveries      |\n\n## Bottom Line\n\n- Different Lasso variants address different modeling challenges like scaling, grouping, collinearity, and bias.\n\n- Understanding your data structure and inference goals helps choose the right Lasso flavor.\n\n\n- Most major machine learning libraries (R: `glmnet`, `grpreg`; Python: `scikit-learn`, `statsmodels`) provide support for these variants.\n\n## Where to Learn More\n\nKey papers: Tibshirani (1996), Zou (2006), Meinshausen (2007), Yuan and Lin (2006), Belloni et al. (2011), Park and Casella (2008), Friedman et al. (2008), Meinshausen and Bühlmann (2010).\n\n## References\n\n[TO ADD]"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"flavors-lasso.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Lasso Flavors: Variants, Objective Functions, and When to Use Them","date":"2025-00-00","categories":["lasso","regularization"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}