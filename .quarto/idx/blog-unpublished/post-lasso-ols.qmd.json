{"title":"Post-Lasso OLS: Why Refit After Selection?","markdown":{"yaml":{"title":"Post-Lasso OLS: Why Refit After Selection?","date":"2025-00-00","categories":["lasso","high-dimensional regression"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nWhen working in high-dimensional regression, where the number of covariates $p$ may exceed or be comparable to the sample size $n$, Lasso has become the go-to method for variable selection and estimation. Thanks to its \\(\\ell_1\\)-penalty, Lasso can zero out coefficients of irrelevant variables, effectively performing feature selection while estimating the model. However, as elegant and powerful as Lasso is, it has one well-known drawback: **bias due to shrinkage**.\n\nHere's the big idea: what if we use Lasso to select variables, but then go back and run plain old OLS on just those selected variables? This is the core of **Post-Lasso OLS** (also known as OLS post-Lasso). The promise is that this approach can reduce shrinkage bias while still enjoying the selection capabilities of Lasso.\n\nIn this article, we'll unpack the intuition behind Post-Lasso OLS, its theoretical properties, and its practical benefits and risks. We'll also discuss when you should—and shouldn't—use it.\n\n## Notation\n\nConsider a linear regression model where for observations $i = 1, \\dots, n$, the outcome $y_i$ and covariates $x_i \\in \\mathbb{R}^p$ satisfy:\n\n$$\ny_i = x_i^\\top \\beta_0 + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2),\n$$\n\nwith $\\beta_0$ being the true, unknown coefficient vector. The number of nonzero elements in $\\beta_0$, denoted by $s = \\|\\beta_0\\|_0$, is assumed to be small relative to $n$.\n\nDefine the **Lasso estimator** as:\n\n$$\n\\hat{\\beta}^{\\text{Lasso}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right\\}.\n$$\n\nThe **selected model** is:\n\n$$\n\\hat{T} = \\text{support}(\\hat{\\beta}^{\\text{Lasso}}).\n$$\n\nThe **Post-Lasso OLS estimator** then runs OLS on the selected variables $\\hat{T}$:\n\n$$\n\\tilde{\\beta} = \\arg \\min_{\\beta : \\beta_j = 0 \\text{ for } j \\notin \\hat{T}} \\frac{1}{2n} \\| y - X \\beta \\|_2^2.\n$$\n\n## A Closer Look\n\n### Why Refit After Lasso?\n\nLasso achieves sparsity through penalization, but this same penalty introduces bias. In particular, even the coefficients of truly important variables are shrunk toward zero. While this helps with variance reduction, it may hurt estimation quality, especially when we care about unbiased coefficient estimates or accurate predictions.\n\nThe insight behind Post-Lasso is simple: **use Lasso as a variable selector, not an estimator**. Once the selection is done, we drop the penalty and refit OLS on the reduced model. This removes the shrinkage-induced bias on the selected coefficients.\n\n### Properties and Theoretical Guarantees\n\nThe key results from Belloni and Chernozhukov (2013) show that:\n\n1. Post-Lasso OLS performs **at least as well as Lasso** in terms of convergence rates for prediction error.\n2. Under certain conditions, Post-Lasso OLS can **strictly outperform Lasso**, achieving a faster rate of convergence.\n3. If Lasso perfectly selects the true model (which happens under strong assumptions like well-separated signals and certain design conditions), Post-Lasso OLS becomes the **oracle estimator**—as if we knew the true model all along.\n\nMathematically, the prediction error for Post-Lasso satisfies:\n\n$$\n\\| X ( \\tilde{\\beta} - \\beta_0 ) \\|_2^2 / n = O_p \\left( \\frac{s \\log p}{n} \\right),\n$$\n\nmatching Lasso's rate, and potentially improving upon it when model selection is good.\n\n### When Does Post-Lasso Work Well?\n\nThe advantage of Post-Lasso depends critically on how well the first-step selection works. If Lasso misses important variables (false negatives), OLS cannot \"recover\" those missing predictors. However, if Lasso selects all relevant variables (even with some extras), Post-Lasso can effectively de-bias the estimates while tolerating mild over-selection.\n\nThis behavior is particularly attractive in **near-sparse models**, where there are a few large coefficients and many small or zero ones.\n\n### When Should You Be Careful?\n\nDespite its appeal, Post-Lasso is **not a magic bullet**. There are cases where using it might backfire:\n\n- **Severe model selection failure**: If Lasso misses key variables, Post-Lasso inherits this problem.\n- **Highly correlated predictors**: If the design matrix has strong collinearity, selection may be unstable, leading to poor performance.\n- **Small sample sizes**: If $n$ is small relative to the number of selected variables, the post-selection OLS may overfit.\n\nA helpful rule of thumb: Post-Lasso is most reliable when selection is **conservative** (including all relevant predictors, perhaps with some extras) rather than aggressive (cutting out too many).\n\n## An Example\n\nHere’s how to implement Post-Lasso OLS in both R and Python. First, use Lasso to select variables, then refit an OLS model on the selected subset.\n\n:::: {.panel-tabset}\n\n### R \n```r\n# Load required packages\nlibrary(glmnet)\nlibrary(hdm)  # for post-lasso OLS (optional)\n\nset.seed(123)\nn <- 100\np <- 20\nX <- matrix(rnorm(n * p), n, p)\nbeta_true <- c(3, 1.5, 0, 0, 2, rep(0, p - 5))\ny <- X %*% beta_true + rnorm(n)\n\n# Step 1: Fit Lasso to select variables\nlasso_fit <- cv.glmnet(X, y, alpha = 1)  # alpha=1 for Lasso\ncoef_lasso <- coef(lasso_fit, s = \"lambda.min\")\nselected <- which(coef_lasso != 0)[-1]  # remove intercept\n\n# Step 2: Refit OLS on selected variables\nif (length(selected) > 0) {\n  X_selected <- X[, selected, drop = FALSE]\n  ols_fit <- lm(y ~ X_selected)\n  summary(ols_fit)\n} else {\n  print(\"No variables selected by Lasso.\")\n}\n```\n\n### Python\n```python\nimport numpy as np\nfrom sklearn.linear_model import LassoCV, LinearRegression\n\n# Simulate data\nnp.random.seed(123)\nn, p = 100, 20\nX = np.random.randn(n, p)\nbeta_true = np.array([3, 1.5, 0, 0, 2] + [0]*(p - 5))\ny = X @ beta_true + np.random.randn(n)\n\n# Step 1: Lasso for variable selection\nlasso = LassoCV(cv=5).fit(X, y)\nselected = np.where(lasso.coef_ != 0)[0]\n\n# Step 2: Refit OLS on selected variables\nif len(selected) > 0:\n    X_selected = X[:, selected]\n    ols = LinearRegression().fit(X_selected, y)\n    print(\"Selected variables:\", selected)\n    print(\"OLS coefficients:\", ols.coef_)\nelse:\n    print(\"No variables selected by Lasso.\")\n```\n\n::::\n\n---\n\n## Bottom Line\n\n- Post-Lasso OLS reduces the bias introduced by Lasso’s shrinkage while retaining its variable selection benefits.\n\n- It matches or improves upon the prediction error rates of Lasso, especially when selection works well.\n\n- Use Post-Lasso when you're confident that selection includes most or all relevant variables.\n\n- Avoid Post-Lasso if your selection step is unstable, misses important variables, or you're working with tiny sample sizes.\n\n## Where to Learn More\n\nThe seminal paper by Belloni and Chernozhukov (2013), \"Least Squares After Model Selection in High-Dimensional Sparse Models,\" provides a rigorous theoretical treatment of Post-Lasso OLS. Additional discussions on related methods can be found in the broader high-dimensional regression literature, including works on adaptive Lasso, thresholded Lasso, and debiased Lasso. For practical implementation, both the `hdm` package in R and the `sklearn.linear_model` module in Python offer accessible ways to perform Lasso and OLS refitting.\n\n## References\n\n- Belloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse models. *Bernoulli*, 19(2), 521–547.\n\n- Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. *Journal of the Royal Statistical Society: Series B*, 58(1), 267–288.\n\n- Zhao, P., & Yu, B. (2006). On model selection consistency of Lasso. *Journal of Machine Learning Research*, 7, 2541–2563.\n","srcMarkdownNoYaml":"\n\n## Background\n\nWhen working in high-dimensional regression, where the number of covariates $p$ may exceed or be comparable to the sample size $n$, Lasso has become the go-to method for variable selection and estimation. Thanks to its \\(\\ell_1\\)-penalty, Lasso can zero out coefficients of irrelevant variables, effectively performing feature selection while estimating the model. However, as elegant and powerful as Lasso is, it has one well-known drawback: **bias due to shrinkage**.\n\nHere's the big idea: what if we use Lasso to select variables, but then go back and run plain old OLS on just those selected variables? This is the core of **Post-Lasso OLS** (also known as OLS post-Lasso). The promise is that this approach can reduce shrinkage bias while still enjoying the selection capabilities of Lasso.\n\nIn this article, we'll unpack the intuition behind Post-Lasso OLS, its theoretical properties, and its practical benefits and risks. We'll also discuss when you should—and shouldn't—use it.\n\n## Notation\n\nConsider a linear regression model where for observations $i = 1, \\dots, n$, the outcome $y_i$ and covariates $x_i \\in \\mathbb{R}^p$ satisfy:\n\n$$\ny_i = x_i^\\top \\beta_0 + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2),\n$$\n\nwith $\\beta_0$ being the true, unknown coefficient vector. The number of nonzero elements in $\\beta_0$, denoted by $s = \\|\\beta_0\\|_0$, is assumed to be small relative to $n$.\n\nDefine the **Lasso estimator** as:\n\n$$\n\\hat{\\beta}^{\\text{Lasso}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right\\}.\n$$\n\nThe **selected model** is:\n\n$$\n\\hat{T} = \\text{support}(\\hat{\\beta}^{\\text{Lasso}}).\n$$\n\nThe **Post-Lasso OLS estimator** then runs OLS on the selected variables $\\hat{T}$:\n\n$$\n\\tilde{\\beta} = \\arg \\min_{\\beta : \\beta_j = 0 \\text{ for } j \\notin \\hat{T}} \\frac{1}{2n} \\| y - X \\beta \\|_2^2.\n$$\n\n## A Closer Look\n\n### Why Refit After Lasso?\n\nLasso achieves sparsity through penalization, but this same penalty introduces bias. In particular, even the coefficients of truly important variables are shrunk toward zero. While this helps with variance reduction, it may hurt estimation quality, especially when we care about unbiased coefficient estimates or accurate predictions.\n\nThe insight behind Post-Lasso is simple: **use Lasso as a variable selector, not an estimator**. Once the selection is done, we drop the penalty and refit OLS on the reduced model. This removes the shrinkage-induced bias on the selected coefficients.\n\n### Properties and Theoretical Guarantees\n\nThe key results from Belloni and Chernozhukov (2013) show that:\n\n1. Post-Lasso OLS performs **at least as well as Lasso** in terms of convergence rates for prediction error.\n2. Under certain conditions, Post-Lasso OLS can **strictly outperform Lasso**, achieving a faster rate of convergence.\n3. If Lasso perfectly selects the true model (which happens under strong assumptions like well-separated signals and certain design conditions), Post-Lasso OLS becomes the **oracle estimator**—as if we knew the true model all along.\n\nMathematically, the prediction error for Post-Lasso satisfies:\n\n$$\n\\| X ( \\tilde{\\beta} - \\beta_0 ) \\|_2^2 / n = O_p \\left( \\frac{s \\log p}{n} \\right),\n$$\n\nmatching Lasso's rate, and potentially improving upon it when model selection is good.\n\n### When Does Post-Lasso Work Well?\n\nThe advantage of Post-Lasso depends critically on how well the first-step selection works. If Lasso misses important variables (false negatives), OLS cannot \"recover\" those missing predictors. However, if Lasso selects all relevant variables (even with some extras), Post-Lasso can effectively de-bias the estimates while tolerating mild over-selection.\n\nThis behavior is particularly attractive in **near-sparse models**, where there are a few large coefficients and many small or zero ones.\n\n### When Should You Be Careful?\n\nDespite its appeal, Post-Lasso is **not a magic bullet**. There are cases where using it might backfire:\n\n- **Severe model selection failure**: If Lasso misses key variables, Post-Lasso inherits this problem.\n- **Highly correlated predictors**: If the design matrix has strong collinearity, selection may be unstable, leading to poor performance.\n- **Small sample sizes**: If $n$ is small relative to the number of selected variables, the post-selection OLS may overfit.\n\nA helpful rule of thumb: Post-Lasso is most reliable when selection is **conservative** (including all relevant predictors, perhaps with some extras) rather than aggressive (cutting out too many).\n\n## An Example\n\nHere’s how to implement Post-Lasso OLS in both R and Python. First, use Lasso to select variables, then refit an OLS model on the selected subset.\n\n:::: {.panel-tabset}\n\n### R \n```r\n# Load required packages\nlibrary(glmnet)\nlibrary(hdm)  # for post-lasso OLS (optional)\n\nset.seed(123)\nn <- 100\np <- 20\nX <- matrix(rnorm(n * p), n, p)\nbeta_true <- c(3, 1.5, 0, 0, 2, rep(0, p - 5))\ny <- X %*% beta_true + rnorm(n)\n\n# Step 1: Fit Lasso to select variables\nlasso_fit <- cv.glmnet(X, y, alpha = 1)  # alpha=1 for Lasso\ncoef_lasso <- coef(lasso_fit, s = \"lambda.min\")\nselected <- which(coef_lasso != 0)[-1]  # remove intercept\n\n# Step 2: Refit OLS on selected variables\nif (length(selected) > 0) {\n  X_selected <- X[, selected, drop = FALSE]\n  ols_fit <- lm(y ~ X_selected)\n  summary(ols_fit)\n} else {\n  print(\"No variables selected by Lasso.\")\n}\n```\n\n### Python\n```python\nimport numpy as np\nfrom sklearn.linear_model import LassoCV, LinearRegression\n\n# Simulate data\nnp.random.seed(123)\nn, p = 100, 20\nX = np.random.randn(n, p)\nbeta_true = np.array([3, 1.5, 0, 0, 2] + [0]*(p - 5))\ny = X @ beta_true + np.random.randn(n)\n\n# Step 1: Lasso for variable selection\nlasso = LassoCV(cv=5).fit(X, y)\nselected = np.where(lasso.coef_ != 0)[0]\n\n# Step 2: Refit OLS on selected variables\nif len(selected) > 0:\n    X_selected = X[:, selected]\n    ols = LinearRegression().fit(X_selected, y)\n    print(\"Selected variables:\", selected)\n    print(\"OLS coefficients:\", ols.coef_)\nelse:\n    print(\"No variables selected by Lasso.\")\n```\n\n::::\n\n---\n\n## Bottom Line\n\n- Post-Lasso OLS reduces the bias introduced by Lasso’s shrinkage while retaining its variable selection benefits.\n\n- It matches or improves upon the prediction error rates of Lasso, especially when selection works well.\n\n- Use Post-Lasso when you're confident that selection includes most or all relevant variables.\n\n- Avoid Post-Lasso if your selection step is unstable, misses important variables, or you're working with tiny sample sizes.\n\n## Where to Learn More\n\nThe seminal paper by Belloni and Chernozhukov (2013), \"Least Squares After Model Selection in High-Dimensional Sparse Models,\" provides a rigorous theoretical treatment of Post-Lasso OLS. Additional discussions on related methods can be found in the broader high-dimensional regression literature, including works on adaptive Lasso, thresholded Lasso, and debiased Lasso. For practical implementation, both the `hdm` package in R and the `sklearn.linear_model` module in Python offer accessible ways to perform Lasso and OLS refitting.\n\n## References\n\n- Belloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse models. *Bernoulli*, 19(2), 521–547.\n\n- Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. *Journal of the Royal Statistical Society: Series B*, 58(1), 267–288.\n\n- Zhao, P., & Yu, B. (2006). On model selection consistency of Lasso. *Journal of Machine Learning Research*, 7, 2541–2563.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"post-lasso-ols.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Post-Lasso OLS: Why Refit After Selection?","date":"2025-00-00","categories":["lasso","high-dimensional regression"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}