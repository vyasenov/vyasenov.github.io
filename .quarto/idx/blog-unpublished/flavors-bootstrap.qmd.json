{"title":"The Many Flavors of Bootstrap","markdown":{"yaml":{"title":"The Many Flavors of Bootstrap","date":"2025-04-27","categories":["bootstrap","inference"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nThe bootstrap is one of those statistical ideas that feels almost too simple to work—and yet, it works beautifully. At its core, the bootstrap is about asking, \"What if we could repeatedly sample from the data we already have, as if we were drawing fresh samples from the population?\" This clever sleight of hand allows us to estimate variability, construct confidence intervals, and even perform hypothesis tests—all without relying heavily on strong parametric assumptions.\n\nBut here's the thing: there isn’t just *one* bootstrap. Over the years, statisticians have developed many flavors of the bootstrap to address different challenges. Some handle small samples better. Some are designed for dependent data like time series. Others shine when the assumptions of classic bootstrapping crumble (think clustered data or heteroskedasticity).\n\nIn this post, we’ll take a tour through the zoo of bootstrap methods: from the classic nonparametric bootstrap to the jackknife, parametric bootstrap, Bayesian bootstrap, wild bootstrap, moving block bootstrap, and more. We’ll explore where each method shines, where it stumbles, and how to pick the right tool for your problem.\n\nNo need to worry—I won’t just throw formulas at you (though there will be some of those). The focus here is on understanding *why* these methods work, not just how to mechanically apply them.\n\n## Notation\n\nThroughout this article, we’ll assume we have data $\\{Y_1, Y_2, \\dots, Y_n\\}$, where $Y_i$ are independent and identically distributed (i.i.d.) random variables drawn from some unknown distribution $F$. We’re interested in estimating some parameter $\\theta = T(F)$, like the mean, median, regression coefficients, or a more complicated functional.\n\nOur estimator of $\\theta$ from the observed sample is $\\hat{\\theta} = T(\\hat{F}_n)$, where $\\hat{F}_n$ is the empirical distribution function that puts mass $1/n$ on each observed data point.\n\nThe big question is: *How variable is $\\hat{\\theta}$?* And that’s where the bootstrap comes in.\n\n## A Closer Look\n\n### The Jackknife\n\nLet’s start with the jackknife, developed back in the 1950s by Quenouille and popularized by Tukey. The jackknife isn’t technically a bootstrap, but it’s often the gateway drug to resampling methods.\n\nHere’s the idea: drop one observation at a time, recompute your estimate, and use the variability across these \"leave-one-out\" estimates to approximate the variance of $\\hat{\\theta}$.\n\nMathematically, the jackknife replicates are:\n$$\n\\hat{\\theta}_{(i)} = T(\\hat{F}_{n,-i}),\n$$\nwhere $\\hat{F}_{n,-i}$ is the empirical distribution leaving out the $i$-th observation.\n\nThe jackknife variance estimate is:\n$$\n\\hat{V}_{\\text{jack}} = \\frac{n - 1}{n} \\sum_{i=1}^n \\left( \\hat{\\theta}_{(i)} - \\bar{\\theta}_{\\text{jack}} \\right)^2,\n$$\nwhere $\\bar{\\theta}_{\\text{jack}} = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\theta}_{(i)}$.\n\n**When to use it?** The jackknife works well for smooth statistics like the mean or regression coefficients. But it can fail miserably for non-smooth functionals like the median or quantiles.\n\n**Strengths:** Fast, easy to implement, no randomness involved.\n\n**Weaknesses:** Limited to statistics that are smooth in the data. Doesn’t handle complex dependency structures or non-smooth parameters well.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\ny <- rnorm(100)\njackknife_estimates <- sapply(1:length(y), function(i) mean(y[-i]))\njackknife_variance <- (length(y) - 1) / length(y) * var(jackknife_estimates)\nprint(jackknife_variance)\n```\n\n### Python\n```python\nimport numpy as np\nnp.random.seed(42)\ny = np.random.normal(size=100)\njackknife_estimates = np.array([np.mean(np.delete(y, i)) for i in range(len(y))])\njackknife_variance = (len(y) - 1) / len(y) * np.var(jackknife_estimates, ddof=1)\nprint(jackknife_variance)\n```\n::::\n\n---\n\n### Classic Nonparametric Bootstrap\n\nThe classic bootstrap, introduced by Bradley Efron in 1979, takes the idea of resampling and turns it up a notch. Instead of dropping one observation at a time, we repeatedly resample **with replacement** from our data to create many \"new\" datasets, each the same size as the original.\n\nFor each bootstrap sample $b = 1, \\dots, B$:\n1. Sample $n$ observations with replacement.\n2. Compute the statistic $\\hat{\\theta}^*_b = T(\\hat{F}^*_b)$.\n\nThe bootstrap variance estimate is:\n$$\n\\hat{V}_{\\text{boot}} = \\frac{1}{B - 1} \\sum_{b=1}^B \\left( \\hat{\\theta}^*_b - \\bar{\\theta}^* \\right)^2,\n$$\nwhere $\\bar{\\theta}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*_b$.\n\n**When to use it?** Whenever the sample size is moderate or large and you can safely assume i.i.d. observations.\n\n**Strengths:** Flexible, broadly applicable, works well for non-smooth statistics.\n\n**Weaknesses:** Can struggle with small samples or dependent data (like time series). Resampling with replacement assumes independence.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\ny <- rnorm(100)\nB <- 1000\nboot_means <- replicate(B, mean(sample(y, replace = TRUE)))\nboot_variance <- var(boot_means)\nprint(boot_variance)\n```\n\n### Python\n```python\nnp.random.seed(42)\nB = 1000\nboot_means = [np.mean(np.random.choice(y, size=len(y), replace=True)) for _ in range(B)]\nboot_variance = np.var(boot_means, ddof=1)\nprint(boot_variance)\n```\n::::\n\n---\n\n### Parametric Bootstrap\n\nThe parametric bootstrap tweaks the classic idea. Instead of sampling from the empirical distribution $\\hat{F}_n$, you assume a parametric model $F_\\theta$ for the data, fit it to the sample, and then generate new data from the fitted model.\n\nFor example, if you assume $Y_i \\sim N(\\mu, \\sigma^2)$, estimate $\\hat{\\mu}$ and $\\hat{\\sigma}^2$, and then generate bootstrap samples from $N(\\hat{\\mu}, \\hat{\\sigma}^2)$.\n\n**When to use it?** When you trust your parametric model (or at least trust it more than the empirical distribution) and want to leverage that structure.\n\n**Strengths:** More efficient than nonparametric bootstrap if the model is well-specified. Can handle small samples better.\n\n**Weaknesses:** Garbage in, garbage out—if the parametric model is wrong, so are your bootstrap results.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\ny <- rnorm(100)\nmu_hat <- mean(y)\nsigma_hat <- sd(y)\nparam_boot_means <- replicate(B, mean(rnorm(100, mu_hat, sigma_hat)))\nparam_boot_variance <- var(param_boot_means)\nprint(param_boot_variance)\n```\n\n### Python\n```python\nmu_hat = np.mean(y)\nsigma_hat = np.std(y, ddof=1)\nparam_boot_means = [np.mean(np.random.normal(mu_hat, sigma_hat, size=len(y))) for _ in range(B)]\nparam_boot_variance = np.var(param_boot_means, ddof=1)\nprint(param_boot_variance)\n```\n::::\n\n---\n\n### Bayesian Bootstrap\n\nInvented by Rubin in 1981, the Bayesian bootstrap doesn’t resample data points directly. Instead, it puts a **Dirichlet prior** on the weights assigned to each observation.\n\nThe weights $(w_1, \\dots, w_n)$ are drawn from:\n$$\n(w_1, \\dots, w_n) \\sim \\text{Dirichlet}(1, \\dots, 1).\n$$\nThen the statistic is computed as:\n$$\n\\hat{\\theta}^* = T\\left( \\sum_{i=1}^n w_i \\delta_{Y_i} \\right).\n$$\n\n**When to use it?** When you're in a Bayesian mood or want a resampling scheme without discrete resampling (i.e., no repeated observations).\n\n**Strengths:** Smooth, avoids ties from discrete resampling, easy to implement.\n\n**Weaknesses:** Interpretation may feel less intuitive if you're used to classical frequentist bootstrap.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nlibrary(MCMCpack)  # for rdirichlet\nset.seed(42)\ny <- rnorm(100)\nB <- 1000\nbayes_boot_means <- replicate(B, {\n  weights <- as.numeric(rdirichlet(1, rep(1, length(y))))\n  sum(weights * y)\n})\nvar(bayes_boot_means)\n```\n\n### Python\n```python\nfrom scipy.stats import dirichlet\nbayes_boot_means = []\nfor _ in range(B):\n    weights = dirichlet.rvs([1] * len(y))[0]\n    bayes_boot_means.append(np.sum(weights * y))\nnp.var(bayes_boot_means, ddof=1)\n```\n::::\n\n---\n\n### Wild Bootstrap\n\nThe wild bootstrap is a lifesaver when dealing with **heteroskedasticity** or few clusters. Instead of resampling observations, it perturbs the residuals.\n\nSuppose you’re estimating a regression model:\n$$\nY_i = X_i \\beta + \\varepsilon_i.\n$$\nThe wild bootstrap generates:\n$$\nY^*_i = X_i \\hat{\\beta} + v_i \\hat{\\varepsilon}_i,\n$$\nwhere $v_i$ are random variables with mean zero and variance one (e.g., Rademacher random variables taking values $\\pm1$ with probability $0.5$).\n\n**When to use it?** Heteroskedastic models, clustered data with few clusters.\n\n**Strengths:** Handles heteroskedasticity gracefully, robust in small-sample settings.\n\n**Weaknesses:** Mostly designed for regression contexts. Choice of perturbation distribution matters.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100, sd = abs(x))\nmodel <- lm(y ~ x)\nresiduals <- resid(model)\npredicted <- fitted(model)\nB <- 1000\nwild_means <- replicate(B, {\n  v <- sample(c(-1, 1), length(residuals), replace = TRUE)\n  y_star <- predicted + v * residuals\n  coef(lm(y_star ~ x))[2]\n})\nvar(wild_means)\n```\n\n### Python\n```python\nfrom sklearn.linear_model import LinearRegression\nx = np.random.normal(size=100).reshape(-1, 1)\ny = 2 * x.flatten() + np.random.normal(scale=np.abs(x.flatten()))\nmodel = LinearRegression().fit(x, y)\nresiduals = y - model.predict(x)\npredicted = model.predict(x)\nwild_boot_coefs = []\nfor _ in range(B):\n    v = np.random.choice([-1, 1], size=len(residuals))\n    y_star = predicted + v * residuals\n    coef = LinearRegression().fit(x, y_star).coef_[0]\n    wild_boot_coefs.append(coef)\nnp.var(wild_boot_coefs, ddof=1)\n```\n::::\n\n---\n\n### Moving Block Bootstrap\n\nIf your data are **dependent**, like time series, the classic bootstrap fails because it breaks the correlation structure. The moving block bootstrap fixes this by resampling blocks of adjacent observations instead of individual data points.\n\nYou choose a block length $l$ and create overlapping blocks of data:\n$$\n\\{Y_1, \\dots, Y_l\\}, \\{Y_2, \\dots, Y_{l+1}\\}, \\dots, \\{Y_{n-l+1}, \\dots, Y_n\\}.\n$$\nThen resample these blocks with replacement to form a new dataset.\n\n**When to use it?** Time series or spatial data with short-range dependence.\n\n**Strengths:** Maintains local dependence within blocks.\n\n**Weaknesses:** Choice of block size can be tricky; too small loses dependence, too big reduces variability.\n\n::::{.panel-tabset}\n\n### R\n```r\nlibrary(boot)\nset.seed(42)\ny <- arima.sim(model = list(ar = 0.7), n = 100)\nblock_length <- 5\nB <- 1000\nblock_boot_means <- tsboot(y, statistic = function(x) mean(x), R = B, l = block_length, sim = \"fixed\")\nvar(block_boot_means$t)\n```\n\n### Python\n```python\nfrom arch.bootstrap import MovingBlockBootstrap\nnp.random.seed(42)\ny = np.random.normal(size=100)\nblock_length = 5\nbs = MovingBlockBootstrap(block_length, y)\nboot_means = np.array([np.mean(data[0]) for data in bs.bootstrap(B)])\nnp.var(boot_means, ddof=1)\n```\n::::\n\n---\n\n### Subsampling\n\nSubsampling is like bootstrap’s minimalist cousin. Instead of resampling with replacement, it draws subsamples **without replacement** of size $m < n$.\n\nYou then adjust for the fact that your subsamples are smaller. Subsampling doesn’t assume i.i.d. data, making it attractive for dependent or non-identically distributed data.\n\n**When to use it?** Dependent data, heavy-tailed distributions, or when bootstrap consistency fails.\n\n**Strengths:** Fewer assumptions than classic bootstrap.\n\n**Weaknesses:** Choosing the subsample size $m$ is non-trivial. Usually requires theoretical justification.\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\ny <- rnorm(100)\nsubsample_size <- 50\nB <- 1000\nsubsample_means <- replicate(B, mean(sample(y, subsample_size, replace = FALSE)))\nvar(subsample_means)\n```\n\n### Python\n```python\nsubsample_size = 50\nsubsample_means = [np.mean(np.random.choice(y, size=subsample_size, replace=False)) for _ in range(B)]\nnp.var(subsample_means, ddof=1)\n```\n::::\n\n---\n\n## Bottom Line\n\n- The bootstrap is not a single method—it’s a whole family of techniques, each with its own sweet spot.\n\n- The jackknife is fast and simple but struggles with non-smooth statistics.\n\n- The classic bootstrap works great for i.i.d. data and smooth or non-smooth statistics, but fails with dependence or small samples.\n\n- Specialized bootstraps (wild, block, Bayesian, subsampling) handle heteroskedasticity, clustering, dependence, and other real-world challenges that trip up the classic approach.\n\n## Where to Learn More\n\nFor a thorough dive into bootstrap methods, I recommend the textbook *An Introduction to the Bootstrap* by Efron and Tibshirani. For time series applications, Lahiri’s *Resampling Methods for Dependent Data* is a gem. If you’re interested in the asymptotic theory behind these methods, consult Davison and Hinkley’s *Bootstrap Methods and Their Application*. There are also excellent lecture notes floating around online from advanced econometrics courses that cover these topics with a modern perspective.\n\n---\n\n## References\n\nEfron, B. (1979). Bootstrap methods: Another look at the jackknife. *Annals of Statistics*, 7(1), 1–26.\n\nRubin, D. B. (1981). The Bayesian bootstrap. *Annals of Statistics*, 9(1), 130–134.\n\nLahiri, S. N. (2003). *Resampling Methods for Dependent Data*. Springer.\n\nDavison, A. C., & Hinkley, D. V. (1997). *Bootstrap Methods and Their Application*. Cambridge University Press.\n","srcMarkdownNoYaml":"\n\n## Background\n\nThe bootstrap is one of those statistical ideas that feels almost too simple to work—and yet, it works beautifully. At its core, the bootstrap is about asking, \"What if we could repeatedly sample from the data we already have, as if we were drawing fresh samples from the population?\" This clever sleight of hand allows us to estimate variability, construct confidence intervals, and even perform hypothesis tests—all without relying heavily on strong parametric assumptions.\n\nBut here's the thing: there isn’t just *one* bootstrap. Over the years, statisticians have developed many flavors of the bootstrap to address different challenges. Some handle small samples better. Some are designed for dependent data like time series. Others shine when the assumptions of classic bootstrapping crumble (think clustered data or heteroskedasticity).\n\nIn this post, we’ll take a tour through the zoo of bootstrap methods: from the classic nonparametric bootstrap to the jackknife, parametric bootstrap, Bayesian bootstrap, wild bootstrap, moving block bootstrap, and more. We’ll explore where each method shines, where it stumbles, and how to pick the right tool for your problem.\n\nNo need to worry—I won’t just throw formulas at you (though there will be some of those). The focus here is on understanding *why* these methods work, not just how to mechanically apply them.\n\n## Notation\n\nThroughout this article, we’ll assume we have data $\\{Y_1, Y_2, \\dots, Y_n\\}$, where $Y_i$ are independent and identically distributed (i.i.d.) random variables drawn from some unknown distribution $F$. We’re interested in estimating some parameter $\\theta = T(F)$, like the mean, median, regression coefficients, or a more complicated functional.\n\nOur estimator of $\\theta$ from the observed sample is $\\hat{\\theta} = T(\\hat{F}_n)$, where $\\hat{F}_n$ is the empirical distribution function that puts mass $1/n$ on each observed data point.\n\nThe big question is: *How variable is $\\hat{\\theta}$?* And that’s where the bootstrap comes in.\n\n## A Closer Look\n\n### The Jackknife\n\nLet’s start with the jackknife, developed back in the 1950s by Quenouille and popularized by Tukey. The jackknife isn’t technically a bootstrap, but it’s often the gateway drug to resampling methods.\n\nHere’s the idea: drop one observation at a time, recompute your estimate, and use the variability across these \"leave-one-out\" estimates to approximate the variance of $\\hat{\\theta}$.\n\nMathematically, the jackknife replicates are:\n$$\n\\hat{\\theta}_{(i)} = T(\\hat{F}_{n,-i}),\n$$\nwhere $\\hat{F}_{n,-i}$ is the empirical distribution leaving out the $i$-th observation.\n\nThe jackknife variance estimate is:\n$$\n\\hat{V}_{\\text{jack}} = \\frac{n - 1}{n} \\sum_{i=1}^n \\left( \\hat{\\theta}_{(i)} - \\bar{\\theta}_{\\text{jack}} \\right)^2,\n$$\nwhere $\\bar{\\theta}_{\\text{jack}} = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\theta}_{(i)}$.\n\n**When to use it?** The jackknife works well for smooth statistics like the mean or regression coefficients. But it can fail miserably for non-smooth functionals like the median or quantiles.\n\n**Strengths:** Fast, easy to implement, no randomness involved.\n\n**Weaknesses:** Limited to statistics that are smooth in the data. Doesn’t handle complex dependency structures or non-smooth parameters well.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\ny <- rnorm(100)\njackknife_estimates <- sapply(1:length(y), function(i) mean(y[-i]))\njackknife_variance <- (length(y) - 1) / length(y) * var(jackknife_estimates)\nprint(jackknife_variance)\n```\n\n### Python\n```python\nimport numpy as np\nnp.random.seed(42)\ny = np.random.normal(size=100)\njackknife_estimates = np.array([np.mean(np.delete(y, i)) for i in range(len(y))])\njackknife_variance = (len(y) - 1) / len(y) * np.var(jackknife_estimates, ddof=1)\nprint(jackknife_variance)\n```\n::::\n\n---\n\n### Classic Nonparametric Bootstrap\n\nThe classic bootstrap, introduced by Bradley Efron in 1979, takes the idea of resampling and turns it up a notch. Instead of dropping one observation at a time, we repeatedly resample **with replacement** from our data to create many \"new\" datasets, each the same size as the original.\n\nFor each bootstrap sample $b = 1, \\dots, B$:\n1. Sample $n$ observations with replacement.\n2. Compute the statistic $\\hat{\\theta}^*_b = T(\\hat{F}^*_b)$.\n\nThe bootstrap variance estimate is:\n$$\n\\hat{V}_{\\text{boot}} = \\frac{1}{B - 1} \\sum_{b=1}^B \\left( \\hat{\\theta}^*_b - \\bar{\\theta}^* \\right)^2,\n$$\nwhere $\\bar{\\theta}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*_b$.\n\n**When to use it?** Whenever the sample size is moderate or large and you can safely assume i.i.d. observations.\n\n**Strengths:** Flexible, broadly applicable, works well for non-smooth statistics.\n\n**Weaknesses:** Can struggle with small samples or dependent data (like time series). Resampling with replacement assumes independence.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\ny <- rnorm(100)\nB <- 1000\nboot_means <- replicate(B, mean(sample(y, replace = TRUE)))\nboot_variance <- var(boot_means)\nprint(boot_variance)\n```\n\n### Python\n```python\nnp.random.seed(42)\nB = 1000\nboot_means = [np.mean(np.random.choice(y, size=len(y), replace=True)) for _ in range(B)]\nboot_variance = np.var(boot_means, ddof=1)\nprint(boot_variance)\n```\n::::\n\n---\n\n### Parametric Bootstrap\n\nThe parametric bootstrap tweaks the classic idea. Instead of sampling from the empirical distribution $\\hat{F}_n$, you assume a parametric model $F_\\theta$ for the data, fit it to the sample, and then generate new data from the fitted model.\n\nFor example, if you assume $Y_i \\sim N(\\mu, \\sigma^2)$, estimate $\\hat{\\mu}$ and $\\hat{\\sigma}^2$, and then generate bootstrap samples from $N(\\hat{\\mu}, \\hat{\\sigma}^2)$.\n\n**When to use it?** When you trust your parametric model (or at least trust it more than the empirical distribution) and want to leverage that structure.\n\n**Strengths:** More efficient than nonparametric bootstrap if the model is well-specified. Can handle small samples better.\n\n**Weaknesses:** Garbage in, garbage out—if the parametric model is wrong, so are your bootstrap results.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\ny <- rnorm(100)\nmu_hat <- mean(y)\nsigma_hat <- sd(y)\nparam_boot_means <- replicate(B, mean(rnorm(100, mu_hat, sigma_hat)))\nparam_boot_variance <- var(param_boot_means)\nprint(param_boot_variance)\n```\n\n### Python\n```python\nmu_hat = np.mean(y)\nsigma_hat = np.std(y, ddof=1)\nparam_boot_means = [np.mean(np.random.normal(mu_hat, sigma_hat, size=len(y))) for _ in range(B)]\nparam_boot_variance = np.var(param_boot_means, ddof=1)\nprint(param_boot_variance)\n```\n::::\n\n---\n\n### Bayesian Bootstrap\n\nInvented by Rubin in 1981, the Bayesian bootstrap doesn’t resample data points directly. Instead, it puts a **Dirichlet prior** on the weights assigned to each observation.\n\nThe weights $(w_1, \\dots, w_n)$ are drawn from:\n$$\n(w_1, \\dots, w_n) \\sim \\text{Dirichlet}(1, \\dots, 1).\n$$\nThen the statistic is computed as:\n$$\n\\hat{\\theta}^* = T\\left( \\sum_{i=1}^n w_i \\delta_{Y_i} \\right).\n$$\n\n**When to use it?** When you're in a Bayesian mood or want a resampling scheme without discrete resampling (i.e., no repeated observations).\n\n**Strengths:** Smooth, avoids ties from discrete resampling, easy to implement.\n\n**Weaknesses:** Interpretation may feel less intuitive if you're used to classical frequentist bootstrap.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nlibrary(MCMCpack)  # for rdirichlet\nset.seed(42)\ny <- rnorm(100)\nB <- 1000\nbayes_boot_means <- replicate(B, {\n  weights <- as.numeric(rdirichlet(1, rep(1, length(y))))\n  sum(weights * y)\n})\nvar(bayes_boot_means)\n```\n\n### Python\n```python\nfrom scipy.stats import dirichlet\nbayes_boot_means = []\nfor _ in range(B):\n    weights = dirichlet.rvs([1] * len(y))[0]\n    bayes_boot_means.append(np.sum(weights * y))\nnp.var(bayes_boot_means, ddof=1)\n```\n::::\n\n---\n\n### Wild Bootstrap\n\nThe wild bootstrap is a lifesaver when dealing with **heteroskedasticity** or few clusters. Instead of resampling observations, it perturbs the residuals.\n\nSuppose you’re estimating a regression model:\n$$\nY_i = X_i \\beta + \\varepsilon_i.\n$$\nThe wild bootstrap generates:\n$$\nY^*_i = X_i \\hat{\\beta} + v_i \\hat{\\varepsilon}_i,\n$$\nwhere $v_i$ are random variables with mean zero and variance one (e.g., Rademacher random variables taking values $\\pm1$ with probability $0.5$).\n\n**When to use it?** Heteroskedastic models, clustered data with few clusters.\n\n**Strengths:** Handles heteroskedasticity gracefully, robust in small-sample settings.\n\n**Weaknesses:** Mostly designed for regression contexts. Choice of perturbation distribution matters.\n\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100, sd = abs(x))\nmodel <- lm(y ~ x)\nresiduals <- resid(model)\npredicted <- fitted(model)\nB <- 1000\nwild_means <- replicate(B, {\n  v <- sample(c(-1, 1), length(residuals), replace = TRUE)\n  y_star <- predicted + v * residuals\n  coef(lm(y_star ~ x))[2]\n})\nvar(wild_means)\n```\n\n### Python\n```python\nfrom sklearn.linear_model import LinearRegression\nx = np.random.normal(size=100).reshape(-1, 1)\ny = 2 * x.flatten() + np.random.normal(scale=np.abs(x.flatten()))\nmodel = LinearRegression().fit(x, y)\nresiduals = y - model.predict(x)\npredicted = model.predict(x)\nwild_boot_coefs = []\nfor _ in range(B):\n    v = np.random.choice([-1, 1], size=len(residuals))\n    y_star = predicted + v * residuals\n    coef = LinearRegression().fit(x, y_star).coef_[0]\n    wild_boot_coefs.append(coef)\nnp.var(wild_boot_coefs, ddof=1)\n```\n::::\n\n---\n\n### Moving Block Bootstrap\n\nIf your data are **dependent**, like time series, the classic bootstrap fails because it breaks the correlation structure. The moving block bootstrap fixes this by resampling blocks of adjacent observations instead of individual data points.\n\nYou choose a block length $l$ and create overlapping blocks of data:\n$$\n\\{Y_1, \\dots, Y_l\\}, \\{Y_2, \\dots, Y_{l+1}\\}, \\dots, \\{Y_{n-l+1}, \\dots, Y_n\\}.\n$$\nThen resample these blocks with replacement to form a new dataset.\n\n**When to use it?** Time series or spatial data with short-range dependence.\n\n**Strengths:** Maintains local dependence within blocks.\n\n**Weaknesses:** Choice of block size can be tricky; too small loses dependence, too big reduces variability.\n\n::::{.panel-tabset}\n\n### R\n```r\nlibrary(boot)\nset.seed(42)\ny <- arima.sim(model = list(ar = 0.7), n = 100)\nblock_length <- 5\nB <- 1000\nblock_boot_means <- tsboot(y, statistic = function(x) mean(x), R = B, l = block_length, sim = \"fixed\")\nvar(block_boot_means$t)\n```\n\n### Python\n```python\nfrom arch.bootstrap import MovingBlockBootstrap\nnp.random.seed(42)\ny = np.random.normal(size=100)\nblock_length = 5\nbs = MovingBlockBootstrap(block_length, y)\nboot_means = np.array([np.mean(data[0]) for data in bs.bootstrap(B)])\nnp.var(boot_means, ddof=1)\n```\n::::\n\n---\n\n### Subsampling\n\nSubsampling is like bootstrap’s minimalist cousin. Instead of resampling with replacement, it draws subsamples **without replacement** of size $m < n$.\n\nYou then adjust for the fact that your subsamples are smaller. Subsampling doesn’t assume i.i.d. data, making it attractive for dependent or non-identically distributed data.\n\n**When to use it?** Dependent data, heavy-tailed distributions, or when bootstrap consistency fails.\n\n**Strengths:** Fewer assumptions than classic bootstrap.\n\n**Weaknesses:** Choosing the subsample size $m$ is non-trivial. Usually requires theoretical justification.\n\n::::{.panel-tabset}\n\n### R\n```r\nset.seed(42)\ny <- rnorm(100)\nsubsample_size <- 50\nB <- 1000\nsubsample_means <- replicate(B, mean(sample(y, subsample_size, replace = FALSE)))\nvar(subsample_means)\n```\n\n### Python\n```python\nsubsample_size = 50\nsubsample_means = [np.mean(np.random.choice(y, size=subsample_size, replace=False)) for _ in range(B)]\nnp.var(subsample_means, ddof=1)\n```\n::::\n\n---\n\n## Bottom Line\n\n- The bootstrap is not a single method—it’s a whole family of techniques, each with its own sweet spot.\n\n- The jackknife is fast and simple but struggles with non-smooth statistics.\n\n- The classic bootstrap works great for i.i.d. data and smooth or non-smooth statistics, but fails with dependence or small samples.\n\n- Specialized bootstraps (wild, block, Bayesian, subsampling) handle heteroskedasticity, clustering, dependence, and other real-world challenges that trip up the classic approach.\n\n## Where to Learn More\n\nFor a thorough dive into bootstrap methods, I recommend the textbook *An Introduction to the Bootstrap* by Efron and Tibshirani. For time series applications, Lahiri’s *Resampling Methods for Dependent Data* is a gem. If you’re interested in the asymptotic theory behind these methods, consult Davison and Hinkley’s *Bootstrap Methods and Their Application*. There are also excellent lecture notes floating around online from advanced econometrics courses that cover these topics with a modern perspective.\n\n---\n\n## References\n\nEfron, B. (1979). Bootstrap methods: Another look at the jackknife. *Annals of Statistics*, 7(1), 1–26.\n\nRubin, D. B. (1981). The Bayesian bootstrap. *Annals of Statistics*, 9(1), 130–134.\n\nLahiri, S. N. (2003). *Resampling Methods for Dependent Data*. Springer.\n\nDavison, A. C., & Hinkley, D. V. (1997). *Bootstrap Methods and Their Application*. Cambridge University Press.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"flavors-bootstrap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"The Many Flavors of Bootstrap","date":"2025-04-27","categories":["bootstrap","inference"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}