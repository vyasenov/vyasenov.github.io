{"title":"Principal Components Analysis (PCA) and Its Variants","markdown":{"yaml":{"title":"Principal Components Analysis (PCA) and Its Variants","date":"2025-00-00","categories":["PCA"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nPrincipal Components Analysis (PCA) is one of the most widely used techniques for **dimensionality reduction** and **unsupervised learning**. Originally introduced by Karl Pearson in 1901 and later formalized by Harold Hotelling in 1933, PCA aims to find new, uncorrelated variables (the *principal components*) that successively maximize the variance in the data. It provides a systematic way to summarize high-dimensional datasets with fewer dimensions, making it easier to visualize, compress, or preprocess data for further modeling.\n\nBeyond standard PCA, many **variants and extensions** have been developed to handle specific challenges like sparsity, robustness to outliers, missing data, and non-linear structures. These PCA flavors extend the core idea to different contexts, making PCA a versatile workhorse in both theory and practice.\n\nIn this article, we’ll explain the classical PCA approach thoroughly and then introduce several important PCA variants, providing their mathematical formulations and discussing when and why they are useful.\n\n## Notation\n\nLet $X \\in \\mathbb{R}^{n \\times p}$ be a data matrix with:\n- $n$ observations (rows),\n- $p$ variables (columns).\n\nAssume that each column of $X$ has been **centered** (mean zero):\n$$\n\\frac{1}{n} \\sum_{i=1}^n X_{ij} = 0 \\quad \\text{for all} \\ j = 1, \\dots, p.\n$$\n\nThe **empirical covariance matrix** of $X$ is:\n$$\n\\Sigma = \\frac{1}{n} X^T X.\n$$\n\nOur goal is to find new orthogonal directions (principal components) $u_1, \\dots, u_p \\in \\mathbb{R}^p$, such that projecting the data onto these directions captures the maximum variance.\n\n## A Closer Look\n\n### Classical PCA: Variance Maximization and Eigen Decomposition\n\nThe principal components are obtained by solving the following optimization problem:\n$$\n\\max_{u \\in \\mathbb{R}^p} u^T \\Sigma u \\quad \\text{subject to} \\quad \\| u \\|_2 = 1.\n$$\n\nThis is a **Rayleigh quotient maximization problem**, whose solution is the eigenvector of $\\Sigma$ corresponding to the largest eigenvalue. The eigenvalues $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0$ represent the variances explained by each principal component.\n\n#### Sequential Extraction\n\nThe $k$-th principal component direction $u_k$ is obtained by solving:\n$$\n\\max_{u} u^T \\Sigma u, \\quad \\text{subject to} \\quad \\| u \\|_2 = 1, \\quad u^T u_j = 0 \\quad \\text{for} \\ j = 1, \\dots, k-1.\n$$\n\nThe principal components themselves (the transformed data) are:\n$$\nZ = X U,\n$$\nwhere $U = [u_1, u_2, \\dots, u_p]$ is the matrix of eigenvectors.\n\n#### Singular Value Decomposition (SVD) Formulation\n\nPCA can also be performed via **Singular Value Decomposition (SVD)** of the centered data matrix $X$:\n$$\nX = U D V^T,\n$$\nwhere:\n- $U \\in \\mathbb{R}^{n \\times p}$ contains the left singular vectors,\n- $D \\in \\mathbb{R}^{p \\times p}$ is diagonal with singular values,\n- $V \\in \\mathbb{R}^{p \\times p}$ contains the right singular vectors (principal component directions).\n\nThe columns of $V$ are the eigenvectors of $\\Sigma$, and the squared singular values $D^2 / n$ are the eigenvalues.\n\n#### Variance Explained\n\nThe **proportion of variance explained** by the first $k$ components is:\n$$\n\\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^p \\lambda_j}.\n$$\n\nPCA allows data compression by projecting onto the first $k$ components while retaining most of the variance.\n\nWhile classical PCA provides a powerful linear dimensionality reduction tool, it has several limitations: it does not handle sparsity well, it is sensitive to outliers, and it only captures linear relationships. Over the years, many extensions of PCA have been developed to address these challenges. Below we discuss several of the most popular PCA variants.\n\n### Sparse PCA (Zou, Hastie, Tibshirani, 2006)\n\n**Key Idea:**  \nEncourage sparsity in the principal component loading vectors to improve interpretability.\n\n**Objective Function (simplified):**\n$$\n\\max_{u} \\quad u^T \\Sigma u - \\lambda \\| u \\|_1 \\quad \\text{subject to} \\quad \\| u \\|_2 = 1.\n$$\n\n**When to Use:**  \n\n- When you expect only a subset of variables to be important in each component.\n- Useful in high-dimensional settings like genomics, image processing, and text analysis.\n\n**Characteristics:**  \n\n- Enhances interpretability by producing sparse loadings.\n- Retains much of the variance while simplifying the component structure.\n\n---\n\n### Kernel PCA (Schölkopf et al., 1998)\n\n**Key Idea:**  \n\nMap the data into a higher-dimensional feature space using a nonlinear kernel and then apply PCA in that space.\n\n**Objective Function:**  \n\nStandard PCA applied to the kernel matrix:\n$$\nK_{ij} = k(x_i, x_j),\n$$\nwhere $k(\\cdot, \\cdot)$ is a positive-definite kernel function (e.g., RBF, polynomial).\n\n**When to Use:**  \n\n- When the data exhibit nonlinear structures that linear PCA cannot capture.\n- Popular in pattern recognition, computer vision, and bioinformatics.\n\n**Characteristics:**  \n\n- Captures nonlinear relationships between variables.\n- Choice of kernel critically affects performance.\n\n---\n\n### Robust PCA (Candes et al., 2011)\n\n**Key Idea:**  \nDecompose the data matrix $X$ into a low-rank component $L$ and a sparse outlier component $S$:\n$$\n\\min_{L, S} \\| L \\|_* + \\lambda \\| S \\|_1 \\quad \\text{subject to} \\quad X = L + S.\n$$\nwhere $\\| L \\|_*$ is the nuclear norm (sum of singular values).\n\n**When to Use:**  \n\n- When the data contain gross outliers or corruptions.\n- Common in computer vision (background subtraction), video surveillance, and recommender systems.\n\n**Characteristics:**  \n\n- More robust to outliers than classical PCA.\n- Separates structured low-rank signals from sparse noise.\n\n---\n\n### Probabilistic PCA (Tipping and Bishop, 1999)\n\n**Key Idea:**  \nReformulate PCA as a latent variable model with Gaussian noise:\n$$\nx_i = W z_i + \\mu + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2 I),\n$$\nwhere $z_i$ are latent factors.\n\n**When to Use:**  \n\n- When modeling uncertainty and likelihood is important.\n- Allows probabilistic interpretation and missing data handling.\n\n**Characteristics:**  \n\n- Maximum likelihood estimation provides the same solution as classical PCA in the limit.\n- Naturally extends to mixture models and Bayesian frameworks.\n\n---\n\n### Truncated SVD (a Computational Variant)\n\n**Key Idea:**  \nUse the first $k$ singular vectors from the SVD of $X$ without computing the full decomposition:\n$$\nX \\approx U_k D_k V_k^T.\n$$\n\n**When to Use:**  \n\n- Very large-scale data (e.g., text mining, collaborative filtering).\n- When speed and memory efficiency are critical.\n\n**Characteristics:**  \n\n- Often implemented using randomized algorithms.\n- Computationally efficient for sparse or massive datasets.\n\n---\n\n### Nonnegative Matrix Factorization (NMF) (Lee and Seung, 1999)\n\n**Key Idea:**  \nDecompose the data into nonnegative factors:\n$$\nX \\approx WH, \\quad W, H \\geq 0.\n$$\n\n**When to Use:**  \n\n- Nonnegative data (e.g., image pixels, word counts).\n- When parts-based or additive decompositions are meaningful.\n\n**Characteristics:**  \n\n- Unlike PCA, does not produce orthogonal components.\n- Often yields interpretable, parts-based representations.\n\n---\n\n### Independent Component Analysis (ICA)\n\n**Key Idea:**  \nFind components that are statistically independent, not just uncorrelated:\n$$\nX = A S,\n$$\nwhere $S$ contains independent components.\n\n**When to Use:**  \n\n- When underlying sources are assumed to be independent (e.g., EEG signal separation).\n- Suitable for blind source separation problems.\n\n**Characteristics:**  \n\n- Goes beyond PCA by removing higher-order dependencies.\n- Sensitive to scaling and noise.\n\n---\n\nThese PCA variants allow the core idea of variance decomposition to be adapted to a wide variety of practical problems, whether by enforcing sparsity, allowing for nonlinearity, handling outliers, or modeling uncertainty.\n\n## Bottom Line\n\n- Classical PCA reduces dimensionality by projecting data onto orthogonal directions that maximize variance.\n\n- Many PCA variants exist to handle real-world challenges like sparsity (Sparse PCA), outliers (Robust PCA), nonlinearity (Kernel PCA), and uncertainty (Probabilistic PCA).\n\n- Choosing the right PCA flavor depends on the structure of your data and the goals of your analysis — interpretability, robustness, scalability, or flexibility.\n\n- Several of these extensions (e.g., Kernel PCA, NMF, ICA) relax key assumptions of traditional PCA, making them better suited for specialized applications like image analysis, genomics, and signal processing.\n\n- Understanding the mathematical foundation behind these methods helps avoid misapplication and improves the quality of insights from dimensionality reduction.\n\n## Where to Learn More\n\nFor a comprehensive introduction to PCA, see *Principal Component Analysis* by Jolliffe (2002), which remains a classic reference. The review paper by Shlens (2014), *A Tutorial on Principal Component Analysis*, offers an accessible and intuitive explanation of the method and its geometric interpretation. For deeper dives into specific variants, refer to Zou, Hastie, and Tibshirani (2006)\n\n## References\n\n- Bishop, C. M. (1999). “Bayesian PCA.” *Advances in Neural Information Processing Systems*, 11.\n\n- Pearson, K. (1901). “On Lines and Planes of Closest Fit to Systems of Points in Space.” *Philosophical Magazine*, 2(11), 559–572.\n\n- Hotelling, H. (1933). “Analysis of a Complex of Statistical Variables into Principal Components.” *Journal of Educational Psychology*, 24(6), 417–441.\n\n- Jolliffe, I. T. (2002). *Principal Component Analysis*. Springer Series in Statistics.\n\n- Zou, H., Hastie, T., & Tibshirani, R. (2006). “Sparse Principal Component Analysis.” *Journal of Computational and Graphical Statistics*, 15(2), 265–286.\n\n","srcMarkdownNoYaml":"\n\n## Background\n\nPrincipal Components Analysis (PCA) is one of the most widely used techniques for **dimensionality reduction** and **unsupervised learning**. Originally introduced by Karl Pearson in 1901 and later formalized by Harold Hotelling in 1933, PCA aims to find new, uncorrelated variables (the *principal components*) that successively maximize the variance in the data. It provides a systematic way to summarize high-dimensional datasets with fewer dimensions, making it easier to visualize, compress, or preprocess data for further modeling.\n\nBeyond standard PCA, many **variants and extensions** have been developed to handle specific challenges like sparsity, robustness to outliers, missing data, and non-linear structures. These PCA flavors extend the core idea to different contexts, making PCA a versatile workhorse in both theory and practice.\n\nIn this article, we’ll explain the classical PCA approach thoroughly and then introduce several important PCA variants, providing their mathematical formulations and discussing when and why they are useful.\n\n## Notation\n\nLet $X \\in \\mathbb{R}^{n \\times p}$ be a data matrix with:\n- $n$ observations (rows),\n- $p$ variables (columns).\n\nAssume that each column of $X$ has been **centered** (mean zero):\n$$\n\\frac{1}{n} \\sum_{i=1}^n X_{ij} = 0 \\quad \\text{for all} \\ j = 1, \\dots, p.\n$$\n\nThe **empirical covariance matrix** of $X$ is:\n$$\n\\Sigma = \\frac{1}{n} X^T X.\n$$\n\nOur goal is to find new orthogonal directions (principal components) $u_1, \\dots, u_p \\in \\mathbb{R}^p$, such that projecting the data onto these directions captures the maximum variance.\n\n## A Closer Look\n\n### Classical PCA: Variance Maximization and Eigen Decomposition\n\nThe principal components are obtained by solving the following optimization problem:\n$$\n\\max_{u \\in \\mathbb{R}^p} u^T \\Sigma u \\quad \\text{subject to} \\quad \\| u \\|_2 = 1.\n$$\n\nThis is a **Rayleigh quotient maximization problem**, whose solution is the eigenvector of $\\Sigma$ corresponding to the largest eigenvalue. The eigenvalues $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0$ represent the variances explained by each principal component.\n\n#### Sequential Extraction\n\nThe $k$-th principal component direction $u_k$ is obtained by solving:\n$$\n\\max_{u} u^T \\Sigma u, \\quad \\text{subject to} \\quad \\| u \\|_2 = 1, \\quad u^T u_j = 0 \\quad \\text{for} \\ j = 1, \\dots, k-1.\n$$\n\nThe principal components themselves (the transformed data) are:\n$$\nZ = X U,\n$$\nwhere $U = [u_1, u_2, \\dots, u_p]$ is the matrix of eigenvectors.\n\n#### Singular Value Decomposition (SVD) Formulation\n\nPCA can also be performed via **Singular Value Decomposition (SVD)** of the centered data matrix $X$:\n$$\nX = U D V^T,\n$$\nwhere:\n- $U \\in \\mathbb{R}^{n \\times p}$ contains the left singular vectors,\n- $D \\in \\mathbb{R}^{p \\times p}$ is diagonal with singular values,\n- $V \\in \\mathbb{R}^{p \\times p}$ contains the right singular vectors (principal component directions).\n\nThe columns of $V$ are the eigenvectors of $\\Sigma$, and the squared singular values $D^2 / n$ are the eigenvalues.\n\n#### Variance Explained\n\nThe **proportion of variance explained** by the first $k$ components is:\n$$\n\\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^p \\lambda_j}.\n$$\n\nPCA allows data compression by projecting onto the first $k$ components while retaining most of the variance.\n\nWhile classical PCA provides a powerful linear dimensionality reduction tool, it has several limitations: it does not handle sparsity well, it is sensitive to outliers, and it only captures linear relationships. Over the years, many extensions of PCA have been developed to address these challenges. Below we discuss several of the most popular PCA variants.\n\n### Sparse PCA (Zou, Hastie, Tibshirani, 2006)\n\n**Key Idea:**  \nEncourage sparsity in the principal component loading vectors to improve interpretability.\n\n**Objective Function (simplified):**\n$$\n\\max_{u} \\quad u^T \\Sigma u - \\lambda \\| u \\|_1 \\quad \\text{subject to} \\quad \\| u \\|_2 = 1.\n$$\n\n**When to Use:**  \n\n- When you expect only a subset of variables to be important in each component.\n- Useful in high-dimensional settings like genomics, image processing, and text analysis.\n\n**Characteristics:**  \n\n- Enhances interpretability by producing sparse loadings.\n- Retains much of the variance while simplifying the component structure.\n\n---\n\n### Kernel PCA (Schölkopf et al., 1998)\n\n**Key Idea:**  \n\nMap the data into a higher-dimensional feature space using a nonlinear kernel and then apply PCA in that space.\n\n**Objective Function:**  \n\nStandard PCA applied to the kernel matrix:\n$$\nK_{ij} = k(x_i, x_j),\n$$\nwhere $k(\\cdot, \\cdot)$ is a positive-definite kernel function (e.g., RBF, polynomial).\n\n**When to Use:**  \n\n- When the data exhibit nonlinear structures that linear PCA cannot capture.\n- Popular in pattern recognition, computer vision, and bioinformatics.\n\n**Characteristics:**  \n\n- Captures nonlinear relationships between variables.\n- Choice of kernel critically affects performance.\n\n---\n\n### Robust PCA (Candes et al., 2011)\n\n**Key Idea:**  \nDecompose the data matrix $X$ into a low-rank component $L$ and a sparse outlier component $S$:\n$$\n\\min_{L, S} \\| L \\|_* + \\lambda \\| S \\|_1 \\quad \\text{subject to} \\quad X = L + S.\n$$\nwhere $\\| L \\|_*$ is the nuclear norm (sum of singular values).\n\n**When to Use:**  \n\n- When the data contain gross outliers or corruptions.\n- Common in computer vision (background subtraction), video surveillance, and recommender systems.\n\n**Characteristics:**  \n\n- More robust to outliers than classical PCA.\n- Separates structured low-rank signals from sparse noise.\n\n---\n\n### Probabilistic PCA (Tipping and Bishop, 1999)\n\n**Key Idea:**  \nReformulate PCA as a latent variable model with Gaussian noise:\n$$\nx_i = W z_i + \\mu + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2 I),\n$$\nwhere $z_i$ are latent factors.\n\n**When to Use:**  \n\n- When modeling uncertainty and likelihood is important.\n- Allows probabilistic interpretation and missing data handling.\n\n**Characteristics:**  \n\n- Maximum likelihood estimation provides the same solution as classical PCA in the limit.\n- Naturally extends to mixture models and Bayesian frameworks.\n\n---\n\n### Truncated SVD (a Computational Variant)\n\n**Key Idea:**  \nUse the first $k$ singular vectors from the SVD of $X$ without computing the full decomposition:\n$$\nX \\approx U_k D_k V_k^T.\n$$\n\n**When to Use:**  \n\n- Very large-scale data (e.g., text mining, collaborative filtering).\n- When speed and memory efficiency are critical.\n\n**Characteristics:**  \n\n- Often implemented using randomized algorithms.\n- Computationally efficient for sparse or massive datasets.\n\n---\n\n### Nonnegative Matrix Factorization (NMF) (Lee and Seung, 1999)\n\n**Key Idea:**  \nDecompose the data into nonnegative factors:\n$$\nX \\approx WH, \\quad W, H \\geq 0.\n$$\n\n**When to Use:**  \n\n- Nonnegative data (e.g., image pixels, word counts).\n- When parts-based or additive decompositions are meaningful.\n\n**Characteristics:**  \n\n- Unlike PCA, does not produce orthogonal components.\n- Often yields interpretable, parts-based representations.\n\n---\n\n### Independent Component Analysis (ICA)\n\n**Key Idea:**  \nFind components that are statistically independent, not just uncorrelated:\n$$\nX = A S,\n$$\nwhere $S$ contains independent components.\n\n**When to Use:**  \n\n- When underlying sources are assumed to be independent (e.g., EEG signal separation).\n- Suitable for blind source separation problems.\n\n**Characteristics:**  \n\n- Goes beyond PCA by removing higher-order dependencies.\n- Sensitive to scaling and noise.\n\n---\n\nThese PCA variants allow the core idea of variance decomposition to be adapted to a wide variety of practical problems, whether by enforcing sparsity, allowing for nonlinearity, handling outliers, or modeling uncertainty.\n\n## Bottom Line\n\n- Classical PCA reduces dimensionality by projecting data onto orthogonal directions that maximize variance.\n\n- Many PCA variants exist to handle real-world challenges like sparsity (Sparse PCA), outliers (Robust PCA), nonlinearity (Kernel PCA), and uncertainty (Probabilistic PCA).\n\n- Choosing the right PCA flavor depends on the structure of your data and the goals of your analysis — interpretability, robustness, scalability, or flexibility.\n\n- Several of these extensions (e.g., Kernel PCA, NMF, ICA) relax key assumptions of traditional PCA, making them better suited for specialized applications like image analysis, genomics, and signal processing.\n\n- Understanding the mathematical foundation behind these methods helps avoid misapplication and improves the quality of insights from dimensionality reduction.\n\n## Where to Learn More\n\nFor a comprehensive introduction to PCA, see *Principal Component Analysis* by Jolliffe (2002), which remains a classic reference. The review paper by Shlens (2014), *A Tutorial on Principal Component Analysis*, offers an accessible and intuitive explanation of the method and its geometric interpretation. For deeper dives into specific variants, refer to Zou, Hastie, and Tibshirani (2006)\n\n## References\n\n- Bishop, C. M. (1999). “Bayesian PCA.” *Advances in Neural Information Processing Systems*, 11.\n\n- Pearson, K. (1901). “On Lines and Planes of Closest Fit to Systems of Points in Space.” *Philosophical Magazine*, 2(11), 559–572.\n\n- Hotelling, H. (1933). “Analysis of a Complex of Statistical Variables into Principal Components.” *Journal of Educational Psychology*, 24(6), 417–441.\n\n- Jolliffe, I. T. (2002). *Principal Component Analysis*. Springer Series in Statistics.\n\n- Zou, H., Hastie, T., & Tibshirani, R. (2006). “Sparse Principal Component Analysis.” *Journal of Computational and Graphical Statistics*, 15(2), 265–286.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"flavors-pca.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Principal Components Analysis (PCA) and Its Variants","date":"2025-00-00","categories":["PCA"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}