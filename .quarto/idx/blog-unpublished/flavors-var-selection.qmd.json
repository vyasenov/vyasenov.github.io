{"title":"The Many Flavors of Variable Selection","markdown":{"yaml":{"title":"The Many Flavors of Variable Selection","date":"2025-04-27","categories":["variable selection","model selection"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nIf you’ve ever worked with high-dimensional data, you’ve probably run into this problem: there are just too many variables, and not all of them matter. Some are noise, some are collinear, and a few are the real signal you care about. So how do we separate the wheat from the chaff?\n\nWelcome to the world of variable selection.\n\nOver the years, statisticians and machine learning researchers have cooked up a rich menu of methods for variable selection—each with its own philosophy, strengths, and blind spots. Some use penalty terms to shrink coefficients (like Lasso and Ridge). Others cleverly exploit data geometry (like Principal Components Analysis). Some work their magic through randomized constructions (like Knockoffs). And a few rely on stepwise or greedy search strategies (like Forward Selection or Least Angle Regression).\n\nIn this post, we’re going to take a guided tour through these different approaches—what they do, when to use them, and why they work. We’ll also highlight their limitations, because no method is perfect. The goal is not to crown a winner but to help you recognize which tool fits your problem.\n\nThis won’t be a recipe book with code (though that might come in a future post). Instead, we’ll focus on the ideas and intuition behind these methods. Think of this as your field guide to variable selection.\n\n## Notation\n\nSuppose we observe data $(Y, X)$, where $Y \\in \\mathbb{R}^n$ is the outcome vector and $X \\in \\mathbb{R}^{n \\times p}$ is the matrix of predictors (covariates, features, regressors—pick your favorite term). We’re interested in estimating a relationship like:\n$$\nY = X \\beta + \\varepsilon,\n$$\nwhere $\\beta \\in \\mathbb{R}^p$ is the vector of coefficients and $\\varepsilon$ is the error term.\n\nIn high-dimensional settings, $p$ may be large—possibly even larger than $n$. The core task of variable selection is to identify which components of $\\beta$ are nonzero (or, more generally, which features matter for predicting $Y$).\n\n## A Closer Look\n\n### Lasso (aka ℓ₁ Regularization)\n\nLasso introduced the big idea of *sparsity*. It penalizes the sum of the absolute values of the coefficients:\n$$\n\\hat{\\beta}^{\\text{lasso}} = \\arg\\min_{\\beta} \\left\\{ \\| Y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right\\}.\n$$\nThe magic of the ℓ₁ penalty is that it can shrink some coefficients exactly to zero, performing variable selection as part of the estimation.\n\n**When to use it?** When you believe that only a subset of predictors are relevant and want an interpretable model.\n\n**Strengths:** Sparse solutions, automatic variable selection, computationally efficient.\n\n**Weaknesses:** Can struggle with groups of correlated predictors (tends to pick one arbitrarily), biased estimates due to shrinkage.\n\n::::{.panel-tabset}\n\n#### R\n```r\nlibrary(glmnet)\ndata(iris)\nX <- as.matrix(iris[, c(\"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")])\nY <- iris$Sepal.Length\nfit <- cv.glmnet(X, Y, alpha = 1)\ncoef(fit, s = \"lambda.min\")\n```\n\n#### Python\n```python\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n\niris = load_iris(as_frame=True).frame\nX = iris[['sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\ny = iris['sepal length (cm)']\nlasso = LassoCV(cv=5).fit(X, y)\nlasso.coef_\n```\n::::\n\n---\n\n### Ridge Regression (aka ℓ₂ Regularization)\n\nLet’s start with Ridge regression, one of the oldest forms of regularization. Ridge doesn’t exactly *select* variables—it shrinks them. The idea is to add a penalty on the size of the coefficients:\n$$\n\\hat{\\beta}^{\\text{ridge}} = \\arg\\min_{\\beta} \\left\\{ \\| Y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2 \\right\\}.\n$$\nThis discourages large coefficients but never forces them exactly to zero.\n\n**When to use it?** When multicollinearity is a problem or when you prefer stability over sparsity. Ridge is especially good when many small effects contribute to the outcome.\n\n**Strengths:** Stabilizes estimates, handles multicollinearity gracefully, works even when $p > n$.\n\n**Weaknesses:** Does not produce sparse solutions; all coefficients remain in the model.\n\n::::{.panel-tabset}\n\n#### R\n```r\nfit_ridge <- cv.glmnet(X, Y, alpha = 0)\ncoef(fit_ridge, s = \"lambda.min\")\n```\n\n#### Python\n```python\nfrom sklearn.linear_model import RidgeCV\nridge = RidgeCV(alphas=np.logspace(-6, 6, 13), cv=5).fit(X, y)\nridge.coef_\n```\n::::\n\n---\n\n### Elastic Net\n\nElastic Net combines the penalties of Ridge and Lasso:\n$$\n\\hat{\\beta}^{\\text{EN}} = \\arg\\min_{\\beta} \\left\\{ \\| Y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\| \\beta \\|_2^2 \\right\\}.\n$$\nIt keeps the sparsity of Lasso but borrows Ridge’s ability to handle correlated predictors.\n\n**When to use it?** When predictors are correlated, and you want both sparsity and stability.\n\n**Strengths:** Handles groups of correlated variables better than Lasso alone.\n\n**Weaknesses:** Adds an extra tuning parameter to balance the ℓ₁ and ℓ₂ penalties.\n\n:::: {.panel-tabset}\n\n### R\n```r\nfit_enet <- cv.glmnet(X, Y, alpha = 0.5)\ncoef(fit_enet, s = \"lambda.min\")\n```\n\n### Python\n```python\nfrom sklearn.linear_model import ElasticNetCV\nenet = ElasticNetCV(cv=5).fit(X, y)\nenet.coef_\n```\n::::\n\n---\n\n### Principal Components Regression (PCR)\n\nPrincipal Components Analysis (PCA) finds linear combinations of the original variables that explain the most variance. In Principal Components Regression, we regress $Y$ on the top $k$ principal components of $X$ instead of on the original variables.\n\n**When to use it?** When predictors are highly correlated or when dimensionality reduction is needed before regression.\n\n**Strengths:** Reduces dimensionality, handles multicollinearity.\n\n**Weaknesses:** Components may be hard to interpret; variable selection is indirect since it selects combinations of variables, not individual variables.\n\n\n:::: {.panel-tabset}\n\n#### R\n```r\nlibrary(pls)\npcr_model <- pcr(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris, scale = TRUE, validation = \"CV\")\nsummary(pcr_model)\n```\n\n#### Python\n```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nreg = LinearRegression().fit(X_pca, y)\nreg.coef_\n```\n::::\n\n---\n\n### Knockoffs\n\nKnockoffs, introduced by Barber and Candès (2015), is a clever framework for variable selection with **false discovery rate (FDR) control**. The method constructs “knockoff copies” of each feature—artificial variables that mimic the correlation structure of the real ones but are known to be null. Then it tests whether the real variables outperform their knockoffs.\n\n**When to use it?** When you care about valid statistical guarantees like FDR control.\n\n**Strengths:** Controls FDR rigorously; applicable even in high-dimensional settings.\n\n**Weaknesses:** Requires construction of knockoff variables, which can be challenging for non-Gaussian designs.\n\n:::: {.panel-tabset}\n\n#### R\n```r\n# Knockoff example requires knockoff package and Gaussian design\n# Skipping implementation here due to complexity\n```\n\n#### Python\n```python\n# Requires knockpy package and Gaussian assumption\n# Skipping detailed implementation here\n\n```\n::::\n\n---\n\n### SCAD (Smoothly Clipped Absolute Deviation)\n\nSCAD is a non-convex penalty designed to overcome the bias problem of Lasso. The SCAD penalty behaves like Lasso for small coefficients but applies less shrinkage to larger ones, reducing bias:\n$$\nP'_\\lambda(\\beta) = \\lambda \\left[ I(|\\beta| \\leq \\lambda) + \\frac{(a \\lambda - |\\beta|)_+}{(a - 1)\\lambda} I(|\\beta| > \\lambda) \\right],\n$$\nwhere $a > 2$ is typically set to 3.7.\n\n**When to use it?** When you want sparsity without the strong bias of Lasso.\n\n**Strengths:** Less biased than Lasso, still encourages sparsity.\n\n**Weaknesses:** Non-convex optimization problem; computationally more demanding.\n\n::: {.panel-tabset}\n\n\n#### R\n```r\nlibrary(ncvreg)\nscad_fit <- ncvreg(X, Y, penalty = \"SCAD\")\ncoef(scad_fit, lambda = scad_fit$lambda.min)\n```\n\n#### Python\n```python\n# SCAD is not widely available in sklearn, typically uses specialized packages like pyglmnet or custom implementation\n```\n::::\n\n---\n\n### Least Angle Regression (LAR)\n\nLAR is a stepwise procedure that adds variables to the model one at a time, moving in the direction of the most correlated predictor. It’s closely related to the path-following algorithm for Lasso.\n\n**When to use it?** When you want a fast, interpretable selection process similar to forward selection.\n\n**Strengths:** Computationally efficient, provides the full regularization path.\n\n**Weaknesses:** Like Lasso, can behave poorly with correlated predictors.\n\n:::: {.panel-tabset}\n\n#### R\n```r\nlibrary(lars)\nlar_model <- lars(X, Y, type = \"lar\")\nprint(lar_model)\n```\n\n#### Python\n```python\nfrom sklearn.linear_model import Lars\nlar = Lars().fit(X, y)\nlar.coef_\n```\n::::\n\n---\n\n### FOCI (Feature Ordering by Conditional Independence)\n\nFOCI is a recent, information-theoretic method that orders features by how much conditional mutual information they contribute to the outcome. It’s model-free and does not assume a particular parametric form.\n\n**When to use it?** When you suspect nonlinear relationships or want model-agnostic feature screening.\n\n**Strengths:** Handles nonlinearities, no need for parametric models.\n\n**Weaknesses:** More computationally intensive; newer and less widely used in practice.\n\n---\n\n### Stepwise Selection (Forward, Backward, Both)\n\nThe classic workhorse of variable selection, stepwise procedures iteratively add or remove variables based on some criterion like AIC, BIC, or p-values.\n\n**When to use it?** For smaller problems where computational cost is low and interpretability is key.\n\n**Strengths:** Simple, interpretable, available in every stats package.\n\n**Weaknesses:** Can be unstable, prone to overfitting, ignores model uncertainty.\n\n:::: {.panel-tabset}\n\n#### R\n```r\nlibrary(MASS)\nfull_model <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris)\nstep_model <- stepAIC(full_model, direction = \"both\")\nsummary(step_model)\n```\n\n#### Python\n```python\n# Stepwise selection is not built into sklearn; can be implemented manually or with statsmodels\n# Skipping for brevity\n```\n::::\n\n---\n\n## Bottom Line\n\n- Lasso, Ridge, and Elastic Net are the go-to penalized regression methods, with Lasso giving sparsity, Ridge providing stability, and Elastic Net blending the two.\n- Knockoffs offer strong statistical guarantees like FDR control but require careful implementation.\n- Non-convex penalties like SCAD address Lasso’s bias issue but at a computational cost.\n- PCA-based methods reduce dimensionality but don't directly select variables.\n- Modern approaches like FOCI expand the toolkit to nonlinear and information-theoretic settings.\n\n## Where to Learn More\n\nFor a great introduction to penalized regression methods, *The Elements of Statistical Learning* by Hastie, Tibshirani, and Friedman is a classic. For a deeper dive into FDR control and Knockoffs, see Barber and Candès (2015). The SCAD penalty was introduced by Fan and Li (2001), and the literature on FOCI is still developing, but the original papers provide a solid starting point. If you’re curious about the algorithmic side, Trevor Hastie’s lectures on LAR and variable selection are highly recommended.\n\n## References\n\nBarber, R. F., & Candès, E. J. (2015). Controlling the false discovery rate via knockoffs. *Annals of Statistics*, 43(5), 2055–2085.\n\nFan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. *Journal of the American Statistical Association*, 96(456), 1348–1360.\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.","srcMarkdownNoYaml":"\n\n## Background\n\nIf you’ve ever worked with high-dimensional data, you’ve probably run into this problem: there are just too many variables, and not all of them matter. Some are noise, some are collinear, and a few are the real signal you care about. So how do we separate the wheat from the chaff?\n\nWelcome to the world of variable selection.\n\nOver the years, statisticians and machine learning researchers have cooked up a rich menu of methods for variable selection—each with its own philosophy, strengths, and blind spots. Some use penalty terms to shrink coefficients (like Lasso and Ridge). Others cleverly exploit data geometry (like Principal Components Analysis). Some work their magic through randomized constructions (like Knockoffs). And a few rely on stepwise or greedy search strategies (like Forward Selection or Least Angle Regression).\n\nIn this post, we’re going to take a guided tour through these different approaches—what they do, when to use them, and why they work. We’ll also highlight their limitations, because no method is perfect. The goal is not to crown a winner but to help you recognize which tool fits your problem.\n\nThis won’t be a recipe book with code (though that might come in a future post). Instead, we’ll focus on the ideas and intuition behind these methods. Think of this as your field guide to variable selection.\n\n## Notation\n\nSuppose we observe data $(Y, X)$, where $Y \\in \\mathbb{R}^n$ is the outcome vector and $X \\in \\mathbb{R}^{n \\times p}$ is the matrix of predictors (covariates, features, regressors—pick your favorite term). We’re interested in estimating a relationship like:\n$$\nY = X \\beta + \\varepsilon,\n$$\nwhere $\\beta \\in \\mathbb{R}^p$ is the vector of coefficients and $\\varepsilon$ is the error term.\n\nIn high-dimensional settings, $p$ may be large—possibly even larger than $n$. The core task of variable selection is to identify which components of $\\beta$ are nonzero (or, more generally, which features matter for predicting $Y$).\n\n## A Closer Look\n\n### Lasso (aka ℓ₁ Regularization)\n\nLasso introduced the big idea of *sparsity*. It penalizes the sum of the absolute values of the coefficients:\n$$\n\\hat{\\beta}^{\\text{lasso}} = \\arg\\min_{\\beta} \\left\\{ \\| Y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right\\}.\n$$\nThe magic of the ℓ₁ penalty is that it can shrink some coefficients exactly to zero, performing variable selection as part of the estimation.\n\n**When to use it?** When you believe that only a subset of predictors are relevant and want an interpretable model.\n\n**Strengths:** Sparse solutions, automatic variable selection, computationally efficient.\n\n**Weaknesses:** Can struggle with groups of correlated predictors (tends to pick one arbitrarily), biased estimates due to shrinkage.\n\n::::{.panel-tabset}\n\n#### R\n```r\nlibrary(glmnet)\ndata(iris)\nX <- as.matrix(iris[, c(\"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")])\nY <- iris$Sepal.Length\nfit <- cv.glmnet(X, Y, alpha = 1)\ncoef(fit, s = \"lambda.min\")\n```\n\n#### Python\n```python\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n\niris = load_iris(as_frame=True).frame\nX = iris[['sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\ny = iris['sepal length (cm)']\nlasso = LassoCV(cv=5).fit(X, y)\nlasso.coef_\n```\n::::\n\n---\n\n### Ridge Regression (aka ℓ₂ Regularization)\n\nLet’s start with Ridge regression, one of the oldest forms of regularization. Ridge doesn’t exactly *select* variables—it shrinks them. The idea is to add a penalty on the size of the coefficients:\n$$\n\\hat{\\beta}^{\\text{ridge}} = \\arg\\min_{\\beta} \\left\\{ \\| Y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2 \\right\\}.\n$$\nThis discourages large coefficients but never forces them exactly to zero.\n\n**When to use it?** When multicollinearity is a problem or when you prefer stability over sparsity. Ridge is especially good when many small effects contribute to the outcome.\n\n**Strengths:** Stabilizes estimates, handles multicollinearity gracefully, works even when $p > n$.\n\n**Weaknesses:** Does not produce sparse solutions; all coefficients remain in the model.\n\n::::{.panel-tabset}\n\n#### R\n```r\nfit_ridge <- cv.glmnet(X, Y, alpha = 0)\ncoef(fit_ridge, s = \"lambda.min\")\n```\n\n#### Python\n```python\nfrom sklearn.linear_model import RidgeCV\nridge = RidgeCV(alphas=np.logspace(-6, 6, 13), cv=5).fit(X, y)\nridge.coef_\n```\n::::\n\n---\n\n### Elastic Net\n\nElastic Net combines the penalties of Ridge and Lasso:\n$$\n\\hat{\\beta}^{\\text{EN}} = \\arg\\min_{\\beta} \\left\\{ \\| Y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\| \\beta \\|_2^2 \\right\\}.\n$$\nIt keeps the sparsity of Lasso but borrows Ridge’s ability to handle correlated predictors.\n\n**When to use it?** When predictors are correlated, and you want both sparsity and stability.\n\n**Strengths:** Handles groups of correlated variables better than Lasso alone.\n\n**Weaknesses:** Adds an extra tuning parameter to balance the ℓ₁ and ℓ₂ penalties.\n\n:::: {.panel-tabset}\n\n### R\n```r\nfit_enet <- cv.glmnet(X, Y, alpha = 0.5)\ncoef(fit_enet, s = \"lambda.min\")\n```\n\n### Python\n```python\nfrom sklearn.linear_model import ElasticNetCV\nenet = ElasticNetCV(cv=5).fit(X, y)\nenet.coef_\n```\n::::\n\n---\n\n### Principal Components Regression (PCR)\n\nPrincipal Components Analysis (PCA) finds linear combinations of the original variables that explain the most variance. In Principal Components Regression, we regress $Y$ on the top $k$ principal components of $X$ instead of on the original variables.\n\n**When to use it?** When predictors are highly correlated or when dimensionality reduction is needed before regression.\n\n**Strengths:** Reduces dimensionality, handles multicollinearity.\n\n**Weaknesses:** Components may be hard to interpret; variable selection is indirect since it selects combinations of variables, not individual variables.\n\n\n:::: {.panel-tabset}\n\n#### R\n```r\nlibrary(pls)\npcr_model <- pcr(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris, scale = TRUE, validation = \"CV\")\nsummary(pcr_model)\n```\n\n#### Python\n```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nreg = LinearRegression().fit(X_pca, y)\nreg.coef_\n```\n::::\n\n---\n\n### Knockoffs\n\nKnockoffs, introduced by Barber and Candès (2015), is a clever framework for variable selection with **false discovery rate (FDR) control**. The method constructs “knockoff copies” of each feature—artificial variables that mimic the correlation structure of the real ones but are known to be null. Then it tests whether the real variables outperform their knockoffs.\n\n**When to use it?** When you care about valid statistical guarantees like FDR control.\n\n**Strengths:** Controls FDR rigorously; applicable even in high-dimensional settings.\n\n**Weaknesses:** Requires construction of knockoff variables, which can be challenging for non-Gaussian designs.\n\n:::: {.panel-tabset}\n\n#### R\n```r\n# Knockoff example requires knockoff package and Gaussian design\n# Skipping implementation here due to complexity\n```\n\n#### Python\n```python\n# Requires knockpy package and Gaussian assumption\n# Skipping detailed implementation here\n\n```\n::::\n\n---\n\n### SCAD (Smoothly Clipped Absolute Deviation)\n\nSCAD is a non-convex penalty designed to overcome the bias problem of Lasso. The SCAD penalty behaves like Lasso for small coefficients but applies less shrinkage to larger ones, reducing bias:\n$$\nP'_\\lambda(\\beta) = \\lambda \\left[ I(|\\beta| \\leq \\lambda) + \\frac{(a \\lambda - |\\beta|)_+}{(a - 1)\\lambda} I(|\\beta| > \\lambda) \\right],\n$$\nwhere $a > 2$ is typically set to 3.7.\n\n**When to use it?** When you want sparsity without the strong bias of Lasso.\n\n**Strengths:** Less biased than Lasso, still encourages sparsity.\n\n**Weaknesses:** Non-convex optimization problem; computationally more demanding.\n\n::: {.panel-tabset}\n\n\n#### R\n```r\nlibrary(ncvreg)\nscad_fit <- ncvreg(X, Y, penalty = \"SCAD\")\ncoef(scad_fit, lambda = scad_fit$lambda.min)\n```\n\n#### Python\n```python\n# SCAD is not widely available in sklearn, typically uses specialized packages like pyglmnet or custom implementation\n```\n::::\n\n---\n\n### Least Angle Regression (LAR)\n\nLAR is a stepwise procedure that adds variables to the model one at a time, moving in the direction of the most correlated predictor. It’s closely related to the path-following algorithm for Lasso.\n\n**When to use it?** When you want a fast, interpretable selection process similar to forward selection.\n\n**Strengths:** Computationally efficient, provides the full regularization path.\n\n**Weaknesses:** Like Lasso, can behave poorly with correlated predictors.\n\n:::: {.panel-tabset}\n\n#### R\n```r\nlibrary(lars)\nlar_model <- lars(X, Y, type = \"lar\")\nprint(lar_model)\n```\n\n#### Python\n```python\nfrom sklearn.linear_model import Lars\nlar = Lars().fit(X, y)\nlar.coef_\n```\n::::\n\n---\n\n### FOCI (Feature Ordering by Conditional Independence)\n\nFOCI is a recent, information-theoretic method that orders features by how much conditional mutual information they contribute to the outcome. It’s model-free and does not assume a particular parametric form.\n\n**When to use it?** When you suspect nonlinear relationships or want model-agnostic feature screening.\n\n**Strengths:** Handles nonlinearities, no need for parametric models.\n\n**Weaknesses:** More computationally intensive; newer and less widely used in practice.\n\n---\n\n### Stepwise Selection (Forward, Backward, Both)\n\nThe classic workhorse of variable selection, stepwise procedures iteratively add or remove variables based on some criterion like AIC, BIC, or p-values.\n\n**When to use it?** For smaller problems where computational cost is low and interpretability is key.\n\n**Strengths:** Simple, interpretable, available in every stats package.\n\n**Weaknesses:** Can be unstable, prone to overfitting, ignores model uncertainty.\n\n:::: {.panel-tabset}\n\n#### R\n```r\nlibrary(MASS)\nfull_model <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris)\nstep_model <- stepAIC(full_model, direction = \"both\")\nsummary(step_model)\n```\n\n#### Python\n```python\n# Stepwise selection is not built into sklearn; can be implemented manually or with statsmodels\n# Skipping for brevity\n```\n::::\n\n---\n\n## Bottom Line\n\n- Lasso, Ridge, and Elastic Net are the go-to penalized regression methods, with Lasso giving sparsity, Ridge providing stability, and Elastic Net blending the two.\n- Knockoffs offer strong statistical guarantees like FDR control but require careful implementation.\n- Non-convex penalties like SCAD address Lasso’s bias issue but at a computational cost.\n- PCA-based methods reduce dimensionality but don't directly select variables.\n- Modern approaches like FOCI expand the toolkit to nonlinear and information-theoretic settings.\n\n## Where to Learn More\n\nFor a great introduction to penalized regression methods, *The Elements of Statistical Learning* by Hastie, Tibshirani, and Friedman is a classic. For a deeper dive into FDR control and Knockoffs, see Barber and Candès (2015). The SCAD penalty was introduced by Fan and Li (2001), and the literature on FOCI is still developing, but the original papers provide a solid starting point. If you’re curious about the algorithmic side, Trevor Hastie’s lectures on LAR and variable selection are highly recommended.\n\n## References\n\nBarber, R. F., & Candès, E. J. (2015). Controlling the false discovery rate via knockoffs. *Annals of Statistics*, 43(5), 2055–2085.\n\nFan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. *Journal of the American Statistical Association*, 96(456), 1348–1360.\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"flavors-var-selection.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"The Many Flavors of Variable Selection","date":"2025-04-27","categories":["variable selection","model selection"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}