{"title":"Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary","markdown":{"yaml":{"title":"Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary","date":"2025-00-00","categories":["causal inference","propensity score"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nPropensity scores are one of the most celebrated ideas in modern causal inference. They elegantly reduce a high-dimensional covariate adjustment problem into a one-dimensional balancing act. But here’s the catch: the canonical setup assumes a **binary treatment**. Treated or not. Drug or placebo. Exposed or unexposed. That’s great, but real-world interventions are rarely so black-and-white.\n\nWhat if your treatment is a dosage? Or a level of exposure? Or some index that varies continuously, like air pollution levels, advertising intensity, or participation hours in a program? That’s when we enter the more nuanced world of **continuous treatment** causal inference, and our trusty binary propensity score needs a serious upgrade.\n\nIn this article, we'll journey from the classic binary case through discrete levels of treatment, and ultimately arrive at the continuous treatment setting. Along the way, we’ll unpack the intuition and math behind **generalized propensity scores**, **density estimation**, and what it really means to estimate a dose-response function. We'll arm you with the tools to think clearly and rigorously about treatment effects when there are more than just two treatment arms—or infinitely many.\n\n## Notation\n\nLet’s set up some notation that will help us keep our thoughts clean as we move through different levels of treatment granularity.\n\nLet:\n- $Y \\in \\mathbb{R}$: observed outcome.\n- $T \\in \\mathbb{R}$: treatment variable, which can be binary, discrete, or continuous.\n- $X \\in \\mathbb{R}^p$: vector of observed pre-treatment covariates.\n- $Y(t)$: potential outcome if the individual were assigned treatment level $t$.\n\nOur goal is to estimate a treatment effect function, like:\n- For binary: $\\mathbb{E}[Y(1) - Y(0)]$,\n- For continuous: $\\mathbb{E}[Y(t)]$ for all $t \\in \\mathbb{R}$, also known as the **dose-response function**.\n\nThe key object of interest generalizes accordingly:\n- In the binary case: the **propensity score** is $e(X) = \\Pr(T=1 \\mid X)$.\n- In the continuous case: the **generalized propensity score (GPS)** is the conditional density $r(t, X) = f_{T|X}(t \\mid X)$.\n\n## A Closer Look\n\n### Binary Treatment: The Classic Setup\n\nIn the binary world, Rosenbaum and Rubin (1983) showed that adjusting for the propensity score $e(X)$ is sufficient to remove confounding, under the assumption of **strong ignorability**:\n$$\nY(1), Y(0) \\perp T \\mid X.\n$$\n\nThey also proved that $Y(1), Y(0) \\perp T \\mid e(X)$, which means we can reduce the dimensionality of covariate adjustment from $p$ to 1. Score!\n\nPropensity scores can be estimated via logistic regression or any predictive ML method, and treatment effects can be estimated via matching, inverse probability weighting (IPW), or regression adjustment using $e(X)$.\n\n### Discrete Treatment: More Than Two Levels\n\nWhat if treatment takes on three or more levels? Say, a low, medium, and high dose of a medication?\n\nIn that case, we generalize the propensity score into a **vector**: one probability for each treatment level, conditional on covariates:\n$$\ne_j(X) = \\Pr(T = j \\mid X), \\quad j = 1, \\dots, K.\n$$\n\nThe adjustment strategy is similar: match or weight across strata of these multinomial probabilities to balance covariates. Some methods treat this as a multi-class classification problem. But what if treatment isn’t just levels 1, 2, or 3, but a smooth continuum?\n\n### Continuous Treatment: The Generalized Propensity Score\n\nNow we hit the interesting case. Suppose $T \\in \\mathbb{R}$ and can take on many (even infinite) values. Instead of estimating a probability, we estimate a **density**. The generalized propensity score is defined as:\n\n$$\nr(t, X) = f_{T|X}(t \\mid X),\n$$\n\nthe **conditional density** of the treatment given covariates. This is a continuous analogue of the classic propensity score.\n\nJust like before, we assume **weak unconfoundedness** (Hirano and Imbens, 2004):\n\n$$\nY(t) \\perp T \\mid X \\quad \\text{for all } t.\n$$\n\nAnd just like in the binary case, conditioning on the GPS balances covariates, but now at each level of $t$. Hirano and Imbens showed that adjusting for $r(T, X)$ is sufficient for identification of the dose-response function $\\mu(t) = \\mathbb{E}[Y(t)]$.\n\n### Estimating the Dose-Response Function\n\nThe general workflow goes like this:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. **Estimate the GPS:** Fit a model for the conditional density $f_{T|X}(t \\mid X)$. This could be a normal model, or a more flexible density estimator.\n2. **Model the outcome given treatment and GPS:** Fit a model for $\\mathbb{E}[Y \\mid T=t, R=r]$, where $R = r(t, X)$ is the estimated GPS.\n3. **Average over the population:** For a fixed value $t$, compute:\n   $$\\hat{\\mu}(t) = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}(t, \\hat{r}(t, X_i)),$$\nwhere $\\hat{m}$ is the estimated conditional expectation of $Y$ given $T$ and $R$.\n:::\n\nThis approach estimates the full dose-response curve $t \\mapsto \\mu(t)$, giving you a complete picture of how outcomes evolve with different levels of treatment.\n\n### A Note on Density Estimation\n\nDensity estimation is the crux of this whole approach. You’re estimating $f_{T|X}(t \\mid X)$, which can be tricky. If you assume normality:\n\n$$T \\mid X \\sim \\mathcal{N}(\\mu(X), \\sigma^2(X)),$$\n\nthen you can fit a regression model for $T$ and use the residuals to compute the density. For more flexibility, kernel density estimation or machine learning methods like normalizing flows can be used to approximate the conditional distribution of $T \\mid X$.\n\nKeep in mind: poor density estimation leads to poor GPS, which leads to biased treatment effect estimates. Garbage in, garbage out.\n\nWhile density estimation is the cornerstone of the GPS approach, it becomes increasingly challenging as the number of covariates grows—a phenomenon often called the \"curse of dimensionality.\" In high-dimensional spaces issues include data sparistiy making local density estimation unreliable, parametric assumptions also become increasingly restircive.  To address these challenges, consider dimensionality reduction techniques before density estimation, semi-parametric approaches that make assumptions on the functional form but allow flexibility in other aspects, or Bayesian nonparametric methods that adapt to the complexity of the data.\n\n## Covariate Balancing Generalized Propensity Scores (CBGPS)\n\nCBGPS, introduced by Fong, Hazlett, and Imai (2018), directly optimizes covariate balance rather than focusing on accurately modeling the treatment mechanism. It estimates weights $w_i$ by solving a set of moment conditions that ensure covariates are uncorrelated with treatment after weighting:\n$$ \\sum_{i=1}^n w_i(T_i - \\bar{T})X_i = 0 $$\n\nThis approach avoids sequential modeling and estimation, potentially reducing bias from model misspecification. It's particularly valuable when the treatment mechanism is complex or difficult to model accurately.\n\n### Diagnostics\n\nAssessing GPS quality requires thorough diagnostics focused on three key areas: covariate balance checks, model diagnostics, and sensitivity analysis. For covariate balance, practitioners should evaluate whether correlations between covariates and treatment approach zero after GPS adjustment, check balance within treatment strata, and create visual plots of standardized differences. GPS model quality can be verified through residual analysis against covariates and treatments, Q-Q plots to assess distributional assumptions, and cross-validation to evaluate predictive performance. Sensitivity testing should include trimming extreme GPS values, comparing results across different model specifications, and conducting placebo tests on outcomes that should be unaffected by treatment. While perfect balance may be unattainable, these diagnostics build confidence in causal estimates by revealing substantial improvements over unadjusted comparisons and identifying potential estimation issues.\n\n## An Example\n\nLet’s try estimating a dose-response function using simulated data.\n\n::::{.panel-tabset}\n\n### R\n\n```r\nlibrary(MASS)\nlibrary(np)\n\n# Simulate data\nset.seed(1988)\nn <- 500\nX <- matrix(rnorm(n * 3), n, 3)\nT <- 0.5 * X[,1] - 0.3 * X[,2] + rnorm(n)\nY <- 2 + 3 * T - T^2 + 0.5 * X[,1] + rnorm(n)\n\n# Estimate GPS via kernel regression\ngps_model <- npcdensbw(xdat = data.frame(X), ydat = T)\ngps <- npcdens(bws = gps_model, xdat = data.frame(X), ydat = T)$condens\n\n# Estimate outcome model\ndf <- data.frame(Y = Y, T = T, GPS = gps)\nfit <- lm(Y ~ poly(T, 2) + GPS + T*GPS, data = df)\n\n# Estimate dose-response\nt_vals <- seq(min(T), max(T), length.out = 100)\ngps_vals <- predict(np::npudens(~ T + X1 + X2 + X3, data = data.frame(T = t_vals, X1 = X[,1], X2 = X[,2], X3 = X[,3])))\npreds <- predict(fit, newdata = data.frame(T = t_vals, GPS = gps_vals))\nplot(t_vals, preds, type = 'l', lwd = 2)\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KernelDensity\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Simulate data\nnp.random.seed(1988)\nn = 500\nX = np.random.randn(n, 3)\nT = 0.5 * X[:, 0] - 0.3 * X[:, 1] + np.random.randn(n)\nY = 2 + 3 * T - T**2 + 0.5 * X[:, 0] + np.random.randn(n)\n\n# Estimate GPS: assume normality\nfrom sklearn.linear_model import LinearRegression\ngps_model = LinearRegression().fit(X, T)\nmu = gps_model.predict(X)\nresid = T - mu\nsigma = np.std(resid)\ngps = norm.pdf(T, loc=mu, scale=sigma)\n\n# Fit outcome model\ndf = pd.DataFrame({'Y': Y, 'T': T, 'GPS': gps})\ndf['T2'] = T**2\ndf['T_GPS'] = T * gps\nX_outcome = df[['T', 'T2', 'GPS', 'T_GPS']]\nfit = LinearRegression().fit(X_outcome, df['Y'])\n\n# Estimate dose-response\nt_vals = np.linspace(T.min(), T.max(), 100)\nmu_vals = gps_model.predict(X)\ndose_response = []\nfor t in t_vals:\n    gps_t = norm.pdf(t, loc=mu_vals, scale=sigma)\n    X_pred = np.column_stack((np.repeat(t, n), np.repeat(t**2, n), gps_t, t * gps_t))\n    preds = fit.predict(X_pred)\n    dose_response.append(np.mean(preds))\n\nplt.plot(t_vals, dose_response)\nplt.title(\"Estimated Dose-Response Function\")\nplt.xlabel(\"Treatment (T)\")\nplt.ylabel(\"Expected Outcome\")\nplt.show()\n\n```\n\n::::\n\n## Bottom Line\n\n- Propensity score methods can be extended beyond binary treatments to continuous treatments using generalized propensity scores.\n\n- The GPS is the conditional density of treatment given covariates: $f_{T∣X}(t\\mid X)$.\n\n- You estimate the dose-response function by modeling outcomes as a function of both treatment and GPS, then averaging.\n\n- Estimating the GPS well is crucial—garbage density estimates lead to poor causal conclusions.\n\n## Where to Learn More\n\nIf you're interested in mastering this topic, I recommend starting with Hirano and Imbens' 2004 Econometrica paper, which formally introduces the generalized propensity score framework. From there, take a look at more recent work in causal machine learning and semiparametric methods (like those in the DoubleML or EconML libraries), which handle continuous treatments and flexible outcome models. Also, keep an eye on newer approaches using generative models and normalizing flows for high-quality density estimation—an exciting frontier for continuous causal inference.\n\n## References\n\nBrown, D. W., Greene, T. J., Swartz, M. D., Wilkinson, A. V., & DeSantis, S. M. (2021). Propensity score stratification methods for continuous treatments. Statistics in medicine, 40(5), 1189-1203.\n\nHirano, K., & Imbens, G. W. (2004). The propensity score with continuous treatments. Econometrica, 73(2), 731–748.\n\nImai, K., & van Dyk, D. A. (2004). Causal inference with general treatment regimes: Generalizing the propensity score. Journal of the American Statistical Association.\n\nFong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score for a continuous treatment: Application to the efficacy of political advertisements. The Annals of Applied Statistics, 12(1), 156-177.\n\nKluve, J., Schneider, H., Uhlendorff, A., & Zhao, Z. (2012). Evaluating continuous training programmes by using the generalized propensity score. Journal of the Royal Statistical Society Series A: Statistics in Society, 175(2), 587-617.\n\nMcCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). A tutorial on propensity score estimation for multiple treatments using generalized boosted models. Statistics in medicine, 32(19), 3388-3414.\n\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika.","srcMarkdownNoYaml":"\n\n## Background\n\nPropensity scores are one of the most celebrated ideas in modern causal inference. They elegantly reduce a high-dimensional covariate adjustment problem into a one-dimensional balancing act. But here’s the catch: the canonical setup assumes a **binary treatment**. Treated or not. Drug or placebo. Exposed or unexposed. That’s great, but real-world interventions are rarely so black-and-white.\n\nWhat if your treatment is a dosage? Or a level of exposure? Or some index that varies continuously, like air pollution levels, advertising intensity, or participation hours in a program? That’s when we enter the more nuanced world of **continuous treatment** causal inference, and our trusty binary propensity score needs a serious upgrade.\n\nIn this article, we'll journey from the classic binary case through discrete levels of treatment, and ultimately arrive at the continuous treatment setting. Along the way, we’ll unpack the intuition and math behind **generalized propensity scores**, **density estimation**, and what it really means to estimate a dose-response function. We'll arm you with the tools to think clearly and rigorously about treatment effects when there are more than just two treatment arms—or infinitely many.\n\n## Notation\n\nLet’s set up some notation that will help us keep our thoughts clean as we move through different levels of treatment granularity.\n\nLet:\n- $Y \\in \\mathbb{R}$: observed outcome.\n- $T \\in \\mathbb{R}$: treatment variable, which can be binary, discrete, or continuous.\n- $X \\in \\mathbb{R}^p$: vector of observed pre-treatment covariates.\n- $Y(t)$: potential outcome if the individual were assigned treatment level $t$.\n\nOur goal is to estimate a treatment effect function, like:\n- For binary: $\\mathbb{E}[Y(1) - Y(0)]$,\n- For continuous: $\\mathbb{E}[Y(t)]$ for all $t \\in \\mathbb{R}$, also known as the **dose-response function**.\n\nThe key object of interest generalizes accordingly:\n- In the binary case: the **propensity score** is $e(X) = \\Pr(T=1 \\mid X)$.\n- In the continuous case: the **generalized propensity score (GPS)** is the conditional density $r(t, X) = f_{T|X}(t \\mid X)$.\n\n## A Closer Look\n\n### Binary Treatment: The Classic Setup\n\nIn the binary world, Rosenbaum and Rubin (1983) showed that adjusting for the propensity score $e(X)$ is sufficient to remove confounding, under the assumption of **strong ignorability**:\n$$\nY(1), Y(0) \\perp T \\mid X.\n$$\n\nThey also proved that $Y(1), Y(0) \\perp T \\mid e(X)$, which means we can reduce the dimensionality of covariate adjustment from $p$ to 1. Score!\n\nPropensity scores can be estimated via logistic regression or any predictive ML method, and treatment effects can be estimated via matching, inverse probability weighting (IPW), or regression adjustment using $e(X)$.\n\n### Discrete Treatment: More Than Two Levels\n\nWhat if treatment takes on three or more levels? Say, a low, medium, and high dose of a medication?\n\nIn that case, we generalize the propensity score into a **vector**: one probability for each treatment level, conditional on covariates:\n$$\ne_j(X) = \\Pr(T = j \\mid X), \\quad j = 1, \\dots, K.\n$$\n\nThe adjustment strategy is similar: match or weight across strata of these multinomial probabilities to balance covariates. Some methods treat this as a multi-class classification problem. But what if treatment isn’t just levels 1, 2, or 3, but a smooth continuum?\n\n### Continuous Treatment: The Generalized Propensity Score\n\nNow we hit the interesting case. Suppose $T \\in \\mathbb{R}$ and can take on many (even infinite) values. Instead of estimating a probability, we estimate a **density**. The generalized propensity score is defined as:\n\n$$\nr(t, X) = f_{T|X}(t \\mid X),\n$$\n\nthe **conditional density** of the treatment given covariates. This is a continuous analogue of the classic propensity score.\n\nJust like before, we assume **weak unconfoundedness** (Hirano and Imbens, 2004):\n\n$$\nY(t) \\perp T \\mid X \\quad \\text{for all } t.\n$$\n\nAnd just like in the binary case, conditioning on the GPS balances covariates, but now at each level of $t$. Hirano and Imbens showed that adjusting for $r(T, X)$ is sufficient for identification of the dose-response function $\\mu(t) = \\mathbb{E}[Y(t)]$.\n\n### Estimating the Dose-Response Function\n\nThe general workflow goes like this:\n\n::: {.callout-note title=\"Algorithm:\"}\n1. **Estimate the GPS:** Fit a model for the conditional density $f_{T|X}(t \\mid X)$. This could be a normal model, or a more flexible density estimator.\n2. **Model the outcome given treatment and GPS:** Fit a model for $\\mathbb{E}[Y \\mid T=t, R=r]$, where $R = r(t, X)$ is the estimated GPS.\n3. **Average over the population:** For a fixed value $t$, compute:\n   $$\\hat{\\mu}(t) = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}(t, \\hat{r}(t, X_i)),$$\nwhere $\\hat{m}$ is the estimated conditional expectation of $Y$ given $T$ and $R$.\n:::\n\nThis approach estimates the full dose-response curve $t \\mapsto \\mu(t)$, giving you a complete picture of how outcomes evolve with different levels of treatment.\n\n### A Note on Density Estimation\n\nDensity estimation is the crux of this whole approach. You’re estimating $f_{T|X}(t \\mid X)$, which can be tricky. If you assume normality:\n\n$$T \\mid X \\sim \\mathcal{N}(\\mu(X), \\sigma^2(X)),$$\n\nthen you can fit a regression model for $T$ and use the residuals to compute the density. For more flexibility, kernel density estimation or machine learning methods like normalizing flows can be used to approximate the conditional distribution of $T \\mid X$.\n\nKeep in mind: poor density estimation leads to poor GPS, which leads to biased treatment effect estimates. Garbage in, garbage out.\n\nWhile density estimation is the cornerstone of the GPS approach, it becomes increasingly challenging as the number of covariates grows—a phenomenon often called the \"curse of dimensionality.\" In high-dimensional spaces issues include data sparistiy making local density estimation unreliable, parametric assumptions also become increasingly restircive.  To address these challenges, consider dimensionality reduction techniques before density estimation, semi-parametric approaches that make assumptions on the functional form but allow flexibility in other aspects, or Bayesian nonparametric methods that adapt to the complexity of the data.\n\n## Covariate Balancing Generalized Propensity Scores (CBGPS)\n\nCBGPS, introduced by Fong, Hazlett, and Imai (2018), directly optimizes covariate balance rather than focusing on accurately modeling the treatment mechanism. It estimates weights $w_i$ by solving a set of moment conditions that ensure covariates are uncorrelated with treatment after weighting:\n$$ \\sum_{i=1}^n w_i(T_i - \\bar{T})X_i = 0 $$\n\nThis approach avoids sequential modeling and estimation, potentially reducing bias from model misspecification. It's particularly valuable when the treatment mechanism is complex or difficult to model accurately.\n\n### Diagnostics\n\nAssessing GPS quality requires thorough diagnostics focused on three key areas: covariate balance checks, model diagnostics, and sensitivity analysis. For covariate balance, practitioners should evaluate whether correlations between covariates and treatment approach zero after GPS adjustment, check balance within treatment strata, and create visual plots of standardized differences. GPS model quality can be verified through residual analysis against covariates and treatments, Q-Q plots to assess distributional assumptions, and cross-validation to evaluate predictive performance. Sensitivity testing should include trimming extreme GPS values, comparing results across different model specifications, and conducting placebo tests on outcomes that should be unaffected by treatment. While perfect balance may be unattainable, these diagnostics build confidence in causal estimates by revealing substantial improvements over unadjusted comparisons and identifying potential estimation issues.\n\n## An Example\n\nLet’s try estimating a dose-response function using simulated data.\n\n::::{.panel-tabset}\n\n### R\n\n```r\nlibrary(MASS)\nlibrary(np)\n\n# Simulate data\nset.seed(1988)\nn <- 500\nX <- matrix(rnorm(n * 3), n, 3)\nT <- 0.5 * X[,1] - 0.3 * X[,2] + rnorm(n)\nY <- 2 + 3 * T - T^2 + 0.5 * X[,1] + rnorm(n)\n\n# Estimate GPS via kernel regression\ngps_model <- npcdensbw(xdat = data.frame(X), ydat = T)\ngps <- npcdens(bws = gps_model, xdat = data.frame(X), ydat = T)$condens\n\n# Estimate outcome model\ndf <- data.frame(Y = Y, T = T, GPS = gps)\nfit <- lm(Y ~ poly(T, 2) + GPS + T*GPS, data = df)\n\n# Estimate dose-response\nt_vals <- seq(min(T), max(T), length.out = 100)\ngps_vals <- predict(np::npudens(~ T + X1 + X2 + X3, data = data.frame(T = t_vals, X1 = X[,1], X2 = X[,2], X3 = X[,3])))\npreds <- predict(fit, newdata = data.frame(T = t_vals, GPS = gps_vals))\nplot(t_vals, preds, type = 'l', lwd = 2)\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KernelDensity\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Simulate data\nnp.random.seed(1988)\nn = 500\nX = np.random.randn(n, 3)\nT = 0.5 * X[:, 0] - 0.3 * X[:, 1] + np.random.randn(n)\nY = 2 + 3 * T - T**2 + 0.5 * X[:, 0] + np.random.randn(n)\n\n# Estimate GPS: assume normality\nfrom sklearn.linear_model import LinearRegression\ngps_model = LinearRegression().fit(X, T)\nmu = gps_model.predict(X)\nresid = T - mu\nsigma = np.std(resid)\ngps = norm.pdf(T, loc=mu, scale=sigma)\n\n# Fit outcome model\ndf = pd.DataFrame({'Y': Y, 'T': T, 'GPS': gps})\ndf['T2'] = T**2\ndf['T_GPS'] = T * gps\nX_outcome = df[['T', 'T2', 'GPS', 'T_GPS']]\nfit = LinearRegression().fit(X_outcome, df['Y'])\n\n# Estimate dose-response\nt_vals = np.linspace(T.min(), T.max(), 100)\nmu_vals = gps_model.predict(X)\ndose_response = []\nfor t in t_vals:\n    gps_t = norm.pdf(t, loc=mu_vals, scale=sigma)\n    X_pred = np.column_stack((np.repeat(t, n), np.repeat(t**2, n), gps_t, t * gps_t))\n    preds = fit.predict(X_pred)\n    dose_response.append(np.mean(preds))\n\nplt.plot(t_vals, dose_response)\nplt.title(\"Estimated Dose-Response Function\")\nplt.xlabel(\"Treatment (T)\")\nplt.ylabel(\"Expected Outcome\")\nplt.show()\n\n```\n\n::::\n\n## Bottom Line\n\n- Propensity score methods can be extended beyond binary treatments to continuous treatments using generalized propensity scores.\n\n- The GPS is the conditional density of treatment given covariates: $f_{T∣X}(t\\mid X)$.\n\n- You estimate the dose-response function by modeling outcomes as a function of both treatment and GPS, then averaging.\n\n- Estimating the GPS well is crucial—garbage density estimates lead to poor causal conclusions.\n\n## Where to Learn More\n\nIf you're interested in mastering this topic, I recommend starting with Hirano and Imbens' 2004 Econometrica paper, which formally introduces the generalized propensity score framework. From there, take a look at more recent work in causal machine learning and semiparametric methods (like those in the DoubleML or EconML libraries), which handle continuous treatments and flexible outcome models. Also, keep an eye on newer approaches using generative models and normalizing flows for high-quality density estimation—an exciting frontier for continuous causal inference.\n\n## References\n\nBrown, D. W., Greene, T. J., Swartz, M. D., Wilkinson, A. V., & DeSantis, S. M. (2021). Propensity score stratification methods for continuous treatments. Statistics in medicine, 40(5), 1189-1203.\n\nHirano, K., & Imbens, G. W. (2004). The propensity score with continuous treatments. Econometrica, 73(2), 731–748.\n\nImai, K., & van Dyk, D. A. (2004). Causal inference with general treatment regimes: Generalizing the propensity score. Journal of the American Statistical Association.\n\nFong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score for a continuous treatment: Application to the efficacy of political advertisements. The Annals of Applied Statistics, 12(1), 156-177.\n\nKluve, J., Schneider, H., Uhlendorff, A., & Zhao, Z. (2012). Evaluating continuous training programmes by using the generalized propensity score. Journal of the Royal Statistical Society Series A: Statistics in Society, 175(2), 587-617.\n\nMcCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). A tutorial on propensity score estimation for multiple treatments using generalized boosted models. Statistics in medicine, 32(19), 3388-3414.\n\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"prop-score-cont-var.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary","date":"2025-00-00","categories":["causal inference","propensity score"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}