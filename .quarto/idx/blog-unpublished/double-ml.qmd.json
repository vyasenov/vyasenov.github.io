{"title":"Double/Debiased ML for Causal Inference","markdown":{"yaml":{"title":"Double/Debiased ML for Causal Inference","date":"2025-00-00","categories":["causal inference","machine learning"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nCausal inference meets machine learning — but with a twist. Many data scientists are eager to bring the power of flexible machine learning tools into their causal analyses. But here’s the catch: naïvely plugging ML estimates into your causal models can lead to biased results and invalid confidence intervals. Enter **Double/Debiased Machine Learning (DML)**, a framework designed to handle these problems systematically.\n\nDML, pioneered by Chernozhukov, Hansen, and colleagues, combines classical econometric insights with modern machine learning to estimate causal parameters like average treatment effects (ATE) in the presence of high-dimensional or complex nuisance components. The magic lies in two key ideas: **Neyman orthogonality** and **cross-fitting**.\n\nThis article unpacks how DML works, what problems it solves, and how to implement it using modern tools in `R` and `Python`. Forget about functional form assumptions — DML lets your nuisance functions be estimated by any ML method you like (random forests, boosting, neural nets, etc.) while still guaranteeing valid inference on your treatment effects.\n\n## Notation\n\nConsider an independent and identically distributed (i.i.d.) sample $\\{ W_i \\}_{i=1}^n$, where each $W_i = (Y_i, D_i, X_i)$:\n- $Y_i$: outcome variable,\n- $D_i$: treatment variable (binary or continuous),\n- $X_i$: covariates or controls.\n\nThe parameter of interest is a low-dimensional causal object $\\theta_0$ (such as the ATE), identified through a moment condition:\n\n$$\n\\mathbb{E}[m(W_i, \\theta_0, \\eta_0)] = 0,\n$$\n\nwhere:\n- $m(\\cdot)$ is a score (moment) function,\n- $\\eta_0$ is a high-dimensional or complex nuisance parameter (e.g., propensity score, outcome regression).\n\nDirectly plugging in estimated $\\hat{\\eta}$ can cause bias if $\\hat{\\eta}$ is not perfectly estimated, especially with ML models prone to regularization or overfitting.\n\n## A Closer Look\n\n### The Problem: Nuisance Parameters Everywhere\n\nIn causal inference, we often need to estimate nuisance functions like:\n- The **propensity score** $r(X) = \\mathbb{E}[D | X]$,\n- The **outcome regression** $\\mathbb{E}[Y | D, X]$.\n\nWhen we use flexible ML methods to estimate these, we risk **regularization bias** (from penalties or smoothing) and **overfitting bias** (if the same data is used both to estimate the nuisance functions and the causal parameter).\n\nDML solves this problem using two ideas.\n\n### Neyman Orthogonality: Guarding Against Small Mistakes\n\nThe first ingredient is **Neyman orthogonality**. This property ensures that the score function $m(\\cdot)$ is \"locally insensitive\" to small errors in the nuisance parameters. Formally, a score function $\\psi(W_i; \\theta, \\eta)$ is Neyman orthogonal if:\n\n$$\n\\frac{\\partial}{\\partial \\lambda} \\mathbb{E}[\\psi(W_i; \\theta_0, \\eta_0 + \\lambda (\\eta - \\eta_0))] \\Big|_{\\lambda=0} = 0.\n$$\n\nIn other words, small perturbations in $\\eta$ don't affect the identifying moment condition at first order.\n\nFor the ATE, the **doubly robust score** satisfies Neyman orthogonality:\n\n$$\n\\psi(W_i; \\theta, \\eta) = \\alpha(D_i, X_i)(Y_i - \\ell(D_i, X_i)) + \\ell(1, X_i) - \\ell(0, X_i) - \\theta,\n$$\n\nwhere:\n- $\\alpha(D, X) = \\frac{D}{r(X)} - \\frac{1 - D}{1 - r(X)}$,\n- $\\ell(D, X) = \\mathbb{E}[Y | D, X]$.\n\nThis score remains stable even when $r(X)$ or $\\ell(D, X)$ are estimated imperfectly.\n\n### Cross-Fitting: Outsmarting Overfitting Bias\n\nThe second ingredient is **cross-fitting**, a clever form of sample splitting.\n\n1. Split the data into $K$ folds.\n2. Estimate nuisance functions $\\hat{\\eta}^{(-k)}$ on the data excluding fold $k$.\n3. Plug these estimates into the score function for observations in fold $k$.\n4. Cycle through all folds so that every observation is used for inference, but never for its own nuisance estimation.\n\nCross-fitting reduces dependence between nuisance estimation and target parameter estimation — making overfitting bias negligible.\n\n### Estimation Algorithm for Average Treatment Effects\n\nThe DML procedure for estimating the ATE proceeds as follows:\n\n::: {.callout-note title=\"Algorithm: DML for ATE Estimation\"}\n1. Randomly partition the data into $K$ folds.\n2. For each fold $k$, estimate the propensity score $\\hat{r}^{(-k)}(X)$ and outcome regressions $\\hat{\\ell}^{(-k)}(D, X)$ using the data excluding fold $k$.\n3. For each observation in fold $k$, compute the orthogonal score $\\psi(W_i; \\theta, \\hat{\\eta}^{(-k)})$.\n4. Solve for $\\theta$ such that the sample average of $\\psi$ equals zero.\n5. Estimate variance and construct confidence intervals using standard errors based on the orthogonal scores.\n:::\n\n## An Example\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nlibrary(DoubleML)\nlibrary(mlr3)\nlibrary(mlr3learners)\n\nset.seed(42)\nn <- 1000\nx <- matrix(rnorm(n * 5), n, 5)\nd <- rbinom(n, 1, plogis(x[,1]))\ny <- 0.5 * d + x[,1] + rnorm(n)\n\ndata <- data.frame(y = y, d = d, x)\nml_g <- lrn(\"regr.rpart\")\nml_m <- lrn(\"classif.rpart\", predict_type = \"prob\")\n\ndml_data <- DoubleMLData$new(data, y_col = \"y\", d_cols = \"d\", x_cols = paste0(\"x\", 1:5))\ndml_plr <- DoubleMLPLR$new(dml_data, ml_g, ml_m)\ndml_plr$fit()\ndml_plr$summary()\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom doubleml import DoubleMLData, DoubleMLPLR\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nnp.random.seed(42)\nn = 1000\nX = np.random.randn(n, 5)\nD = np.random.binomial(1, p=1/(1 + np.exp(-X[:, 0])))\nY = 0.5 * D + X[:, 0] + np.random.randn(n)\n\ndata = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(5)])\ndata['d'] = D\ndata['y'] = Y\n\ndml_data = DoubleMLData.from_arrays(X=data[[f'x{i+1}' for i in range(5)]].values,\n                                    y=data['y'].values,\n                                    d=data['d'].values)\n\nml_g = RandomForestRegressor()\nml_m = RandomForestClassifier()\n\ndml_plr = DoubleMLPLR(dml_data, ml_g, ml_m)\ndml_plr.fit()\nprint(dml_plr.summary)\n```\n\n::::\n\n## Bottom Line\n\n- DML enables valid causal inference even when nuisance functions are estimated with machine learning.\n\n- Neyman orthogonality reduces sensitivity to small errors in nuisance estimation.\n\n- Cross-fitting prevents overfitting bias and stabilizes inference.\n\n- DML offers a plug-and-play approach compatible with any ML method for nuisance functions.\n\n## Where to Learn More\n\nThe excellent review by Ahrens, Chernozhukov, Hansen, and others (2025) provides a clear and thorough introduction to DML. For implementation, the `DoubleML` package in R and Python offers flexible tools with built-in orthogonal scores and cross-fitting. Chernozhukov et al. (2018) remains the foundational paper in this area.\n\n## References\n\n- Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. *Econometrics Journal*, 21(1), C1–C68.\n- Ahrens, A., Chernozhukov, V., Hansen, C., et al. (2025). An Introduction to Double/Debiased Machine Learning.\n","srcMarkdownNoYaml":"\n\n## Background\n\nCausal inference meets machine learning — but with a twist. Many data scientists are eager to bring the power of flexible machine learning tools into their causal analyses. But here’s the catch: naïvely plugging ML estimates into your causal models can lead to biased results and invalid confidence intervals. Enter **Double/Debiased Machine Learning (DML)**, a framework designed to handle these problems systematically.\n\nDML, pioneered by Chernozhukov, Hansen, and colleagues, combines classical econometric insights with modern machine learning to estimate causal parameters like average treatment effects (ATE) in the presence of high-dimensional or complex nuisance components. The magic lies in two key ideas: **Neyman orthogonality** and **cross-fitting**.\n\nThis article unpacks how DML works, what problems it solves, and how to implement it using modern tools in `R` and `Python`. Forget about functional form assumptions — DML lets your nuisance functions be estimated by any ML method you like (random forests, boosting, neural nets, etc.) while still guaranteeing valid inference on your treatment effects.\n\n## Notation\n\nConsider an independent and identically distributed (i.i.d.) sample $\\{ W_i \\}_{i=1}^n$, where each $W_i = (Y_i, D_i, X_i)$:\n- $Y_i$: outcome variable,\n- $D_i$: treatment variable (binary or continuous),\n- $X_i$: covariates or controls.\n\nThe parameter of interest is a low-dimensional causal object $\\theta_0$ (such as the ATE), identified through a moment condition:\n\n$$\n\\mathbb{E}[m(W_i, \\theta_0, \\eta_0)] = 0,\n$$\n\nwhere:\n- $m(\\cdot)$ is a score (moment) function,\n- $\\eta_0$ is a high-dimensional or complex nuisance parameter (e.g., propensity score, outcome regression).\n\nDirectly plugging in estimated $\\hat{\\eta}$ can cause bias if $\\hat{\\eta}$ is not perfectly estimated, especially with ML models prone to regularization or overfitting.\n\n## A Closer Look\n\n### The Problem: Nuisance Parameters Everywhere\n\nIn causal inference, we often need to estimate nuisance functions like:\n- The **propensity score** $r(X) = \\mathbb{E}[D | X]$,\n- The **outcome regression** $\\mathbb{E}[Y | D, X]$.\n\nWhen we use flexible ML methods to estimate these, we risk **regularization bias** (from penalties or smoothing) and **overfitting bias** (if the same data is used both to estimate the nuisance functions and the causal parameter).\n\nDML solves this problem using two ideas.\n\n### Neyman Orthogonality: Guarding Against Small Mistakes\n\nThe first ingredient is **Neyman orthogonality**. This property ensures that the score function $m(\\cdot)$ is \"locally insensitive\" to small errors in the nuisance parameters. Formally, a score function $\\psi(W_i; \\theta, \\eta)$ is Neyman orthogonal if:\n\n$$\n\\frac{\\partial}{\\partial \\lambda} \\mathbb{E}[\\psi(W_i; \\theta_0, \\eta_0 + \\lambda (\\eta - \\eta_0))] \\Big|_{\\lambda=0} = 0.\n$$\n\nIn other words, small perturbations in $\\eta$ don't affect the identifying moment condition at first order.\n\nFor the ATE, the **doubly robust score** satisfies Neyman orthogonality:\n\n$$\n\\psi(W_i; \\theta, \\eta) = \\alpha(D_i, X_i)(Y_i - \\ell(D_i, X_i)) + \\ell(1, X_i) - \\ell(0, X_i) - \\theta,\n$$\n\nwhere:\n- $\\alpha(D, X) = \\frac{D}{r(X)} - \\frac{1 - D}{1 - r(X)}$,\n- $\\ell(D, X) = \\mathbb{E}[Y | D, X]$.\n\nThis score remains stable even when $r(X)$ or $\\ell(D, X)$ are estimated imperfectly.\n\n### Cross-Fitting: Outsmarting Overfitting Bias\n\nThe second ingredient is **cross-fitting**, a clever form of sample splitting.\n\n1. Split the data into $K$ folds.\n2. Estimate nuisance functions $\\hat{\\eta}^{(-k)}$ on the data excluding fold $k$.\n3. Plug these estimates into the score function for observations in fold $k$.\n4. Cycle through all folds so that every observation is used for inference, but never for its own nuisance estimation.\n\nCross-fitting reduces dependence between nuisance estimation and target parameter estimation — making overfitting bias negligible.\n\n### Estimation Algorithm for Average Treatment Effects\n\nThe DML procedure for estimating the ATE proceeds as follows:\n\n::: {.callout-note title=\"Algorithm: DML for ATE Estimation\"}\n1. Randomly partition the data into $K$ folds.\n2. For each fold $k$, estimate the propensity score $\\hat{r}^{(-k)}(X)$ and outcome regressions $\\hat{\\ell}^{(-k)}(D, X)$ using the data excluding fold $k$.\n3. For each observation in fold $k$, compute the orthogonal score $\\psi(W_i; \\theta, \\hat{\\eta}^{(-k)})$.\n4. Solve for $\\theta$ such that the sample average of $\\psi$ equals zero.\n5. Estimate variance and construct confidence intervals using standard errors based on the orthogonal scores.\n:::\n\n## An Example\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nlibrary(DoubleML)\nlibrary(mlr3)\nlibrary(mlr3learners)\n\nset.seed(42)\nn <- 1000\nx <- matrix(rnorm(n * 5), n, 5)\nd <- rbinom(n, 1, plogis(x[,1]))\ny <- 0.5 * d + x[,1] + rnorm(n)\n\ndata <- data.frame(y = y, d = d, x)\nml_g <- lrn(\"regr.rpart\")\nml_m <- lrn(\"classif.rpart\", predict_type = \"prob\")\n\ndml_data <- DoubleMLData$new(data, y_col = \"y\", d_cols = \"d\", x_cols = paste0(\"x\", 1:5))\ndml_plr <- DoubleMLPLR$new(dml_data, ml_g, ml_m)\ndml_plr$fit()\ndml_plr$summary()\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom doubleml import DoubleMLData, DoubleMLPLR\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nnp.random.seed(42)\nn = 1000\nX = np.random.randn(n, 5)\nD = np.random.binomial(1, p=1/(1 + np.exp(-X[:, 0])))\nY = 0.5 * D + X[:, 0] + np.random.randn(n)\n\ndata = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(5)])\ndata['d'] = D\ndata['y'] = Y\n\ndml_data = DoubleMLData.from_arrays(X=data[[f'x{i+1}' for i in range(5)]].values,\n                                    y=data['y'].values,\n                                    d=data['d'].values)\n\nml_g = RandomForestRegressor()\nml_m = RandomForestClassifier()\n\ndml_plr = DoubleMLPLR(dml_data, ml_g, ml_m)\ndml_plr.fit()\nprint(dml_plr.summary)\n```\n\n::::\n\n## Bottom Line\n\n- DML enables valid causal inference even when nuisance functions are estimated with machine learning.\n\n- Neyman orthogonality reduces sensitivity to small errors in nuisance estimation.\n\n- Cross-fitting prevents overfitting bias and stabilizes inference.\n\n- DML offers a plug-and-play approach compatible with any ML method for nuisance functions.\n\n## Where to Learn More\n\nThe excellent review by Ahrens, Chernozhukov, Hansen, and others (2025) provides a clear and thorough introduction to DML. For implementation, the `DoubleML` package in R and Python offers flexible tools with built-in orthogonal scores and cross-fitting. Chernozhukov et al. (2018) remains the foundational paper in this area.\n\n## References\n\n- Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. *Econometrics Journal*, 21(1), C1–C68.\n- Ahrens, A., Chernozhukov, V., Hansen, C., et al. (2025). An Introduction to Double/Debiased Machine Learning.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"double-ml.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Double/Debiased ML for Causal Inference","date":"2025-00-00","categories":["causal inference","machine learning"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}