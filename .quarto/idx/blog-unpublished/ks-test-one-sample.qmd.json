{"title":"The Kolmogorov–Smirnov Test as a Goodness-of-fit","markdown":{"yaml":{"title":"The Kolmogorov–Smirnov Test as a Goodness-of-fit","date":"2025-00-00","categories":["statistical inference","hypothesis testing"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nThe Kolmogorov–Smirnov (KS) test is a staple in the statistical toolbox for checking how well data fit a hypothesized distribution. It’s nonparametric, straightforward to compute, and widely implemented in `R`, `Python`, and just about every statistical software you can think of. But—and this is a big but—using the KS test naively can lead to some serious misinterpretations, especially when parameters are estimated from the data.\n\nThis article is based on the 2024 paper by Zeimbekakis, Schifano, and Yan, which takes a hard look at the common misuses of the one-sample KS test. We’ll walk through what the KS test is supposed to do, when it goes wrong, and how to think more clearly about assessing goodness-of-fit.\n\n## Notation\n\nLet $X_1, \\dots, X_n$ be i.i.d. random variables with unknown distribution function $F$. We want to test whether $F = F_0$, for some known distribution function $F_0$.\n\nThe empirical distribution function (EDF) is:\n$$F_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i \\leq x)$$\n\nThe KS statistic is:\n$$D_n = \\sup_{x \\in \\mathbb{R}} |F_n(x) - F_0(x)|$$\n\nThe null distribution of $D_n$ is known under the assumption that $F_0$ is fully specified, i.e., no parameters have been estimated from the data.\n\n## A Closer Look\n\n### What the KS Test Measures\n\nThe KS test is sensitive to discrepancies in the cumulative distribution function. Intuitively, it’s measuring the largest vertical distance between the EDF and the hypothesized CDF $F_0$. This gives you a global measure of discrepancy, not a local one—so it’s less powerful for detecting issues like tail misspecification or multimodality.\n\nThe KS test gives equal weight to all observations when computing the maximum deviation, making it less effective at detecting differences in distribution tails compared to other tests like Anderson-Darling. This is important because in many applications, tail behavior is critically important, such as in risk modeling or extreme value analysis.\n\nWith small samples, the test has limited power to detect distributional differences, while with very large samples, it may detect statistically significant but practically trivial deviations from the hypothesized distribution.\n\n### When Things Go Wrong: Parameter Estimation\n\nHere’s the catch: the null distribution of the KS statistic assumes $F_0$ is fully known. But in practice, people often use the test to evaluate model fit *after* estimating parameters—e.g., fitting a normal distribution by MLE and then checking fit with KS.\n\nThat invalidates the test.\n\nWhy? Because the theoretical distribution of $D_n$ changes when parameters are estimated. The true distribution of the test statistic becomes conditional on the data, and the critical values are no longer accurate. This leads to an inflated Type I error rate: you're more likely to incorrectly reject the null.\n\n### Better Alternatives\n\nWhen parameters are estimated, we need modified procedures:\n\n- **Lilliefors test**: An adaptation of the KS test that adjusts the null distribution when testing for normality with estimated parameters.\n- **Parametric bootstrap**: Simulate the null distribution of the test statistic by repeatedly fitting the model and computing $D_n$ on simulated data.\n- **Other GOF tests**: Anderson-Darling and Cramér-von Mises tests have versions that handle estimated parameters more gracefully.\n\n## An Example\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nset.seed(42)\nx <- rnorm(100, mean = 5, sd = 2)\nks.test(x, \"pnorm\", mean = mean(x), sd = sd(x)) # misuse!\n\n# Better: use Lilliefors test\nlibrary(nortest)\nlillie.test(x)\n```\n\n### Python\n\n```python\nimport numpy as np\nfrom scipy.stats import kstest, norm\nfrom statsmodels.stats.diagnostic import lilliefors\n\nnp.random.seed(42)\nx = np.random.normal(loc=5, scale=2, size=100)\n\n# Misuse: parameters estimated from x\nkstest(x, 'norm', args=(np.mean(x), np.std(x, ddof=1)))\n\n# Better: use Lilliefors test\nstat, pval = lilliefors(x)\nprint(f\"Lilliefors test: statistic={stat}, p-value={pval}\")\n```\n\n::::\n\n## Bottom Line\n\n- The KS test assumes no parameters are estimated—violating this leads to invalid inference.\n\n- Estimating parameters from the same data used in the test inflates Type I error.\n\n- Use alternatives like the Lilliefors test or bootstrap methods when parameters are estimated.\n\n- The KS test is less sensitive in the tails—be cautious with it as a global fit measure.\n\n## Where to Learn More\n\nIf you're regularly checking distributional assumptions, it's worth diving into goodness-of-fit tests in more depth. The *American Statistician* paper by Zeimbekakis et al. (2024) is a great read. Also consider books like *Goodness-of-Fit Techniques* by D’Agostino and Stephens, or *All of Statistics* by Larry Wasserman for more intuitive overviews.\n\n## References\n\nZeimbekakis, A., Schifano, E. D., & Yan, J. (2024). On Misuses of the Kolmogorov–Smirnov Test for One-Sample Goodness-of-Fit. *The American Statistician*, 78(4), 481-487.\n\n","srcMarkdownNoYaml":"\n\n## Background\n\nThe Kolmogorov–Smirnov (KS) test is a staple in the statistical toolbox for checking how well data fit a hypothesized distribution. It’s nonparametric, straightforward to compute, and widely implemented in `R`, `Python`, and just about every statistical software you can think of. But—and this is a big but—using the KS test naively can lead to some serious misinterpretations, especially when parameters are estimated from the data.\n\nThis article is based on the 2024 paper by Zeimbekakis, Schifano, and Yan, which takes a hard look at the common misuses of the one-sample KS test. We’ll walk through what the KS test is supposed to do, when it goes wrong, and how to think more clearly about assessing goodness-of-fit.\n\n## Notation\n\nLet $X_1, \\dots, X_n$ be i.i.d. random variables with unknown distribution function $F$. We want to test whether $F = F_0$, for some known distribution function $F_0$.\n\nThe empirical distribution function (EDF) is:\n$$F_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i \\leq x)$$\n\nThe KS statistic is:\n$$D_n = \\sup_{x \\in \\mathbb{R}} |F_n(x) - F_0(x)|$$\n\nThe null distribution of $D_n$ is known under the assumption that $F_0$ is fully specified, i.e., no parameters have been estimated from the data.\n\n## A Closer Look\n\n### What the KS Test Measures\n\nThe KS test is sensitive to discrepancies in the cumulative distribution function. Intuitively, it’s measuring the largest vertical distance between the EDF and the hypothesized CDF $F_0$. This gives you a global measure of discrepancy, not a local one—so it’s less powerful for detecting issues like tail misspecification or multimodality.\n\nThe KS test gives equal weight to all observations when computing the maximum deviation, making it less effective at detecting differences in distribution tails compared to other tests like Anderson-Darling. This is important because in many applications, tail behavior is critically important, such as in risk modeling or extreme value analysis.\n\nWith small samples, the test has limited power to detect distributional differences, while with very large samples, it may detect statistically significant but practically trivial deviations from the hypothesized distribution.\n\n### When Things Go Wrong: Parameter Estimation\n\nHere’s the catch: the null distribution of the KS statistic assumes $F_0$ is fully known. But in practice, people often use the test to evaluate model fit *after* estimating parameters—e.g., fitting a normal distribution by MLE and then checking fit with KS.\n\nThat invalidates the test.\n\nWhy? Because the theoretical distribution of $D_n$ changes when parameters are estimated. The true distribution of the test statistic becomes conditional on the data, and the critical values are no longer accurate. This leads to an inflated Type I error rate: you're more likely to incorrectly reject the null.\n\n### Better Alternatives\n\nWhen parameters are estimated, we need modified procedures:\n\n- **Lilliefors test**: An adaptation of the KS test that adjusts the null distribution when testing for normality with estimated parameters.\n- **Parametric bootstrap**: Simulate the null distribution of the test statistic by repeatedly fitting the model and computing $D_n$ on simulated data.\n- **Other GOF tests**: Anderson-Darling and Cramér-von Mises tests have versions that handle estimated parameters more gracefully.\n\n## An Example\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nset.seed(42)\nx <- rnorm(100, mean = 5, sd = 2)\nks.test(x, \"pnorm\", mean = mean(x), sd = sd(x)) # misuse!\n\n# Better: use Lilliefors test\nlibrary(nortest)\nlillie.test(x)\n```\n\n### Python\n\n```python\nimport numpy as np\nfrom scipy.stats import kstest, norm\nfrom statsmodels.stats.diagnostic import lilliefors\n\nnp.random.seed(42)\nx = np.random.normal(loc=5, scale=2, size=100)\n\n# Misuse: parameters estimated from x\nkstest(x, 'norm', args=(np.mean(x), np.std(x, ddof=1)))\n\n# Better: use Lilliefors test\nstat, pval = lilliefors(x)\nprint(f\"Lilliefors test: statistic={stat}, p-value={pval}\")\n```\n\n::::\n\n## Bottom Line\n\n- The KS test assumes no parameters are estimated—violating this leads to invalid inference.\n\n- Estimating parameters from the same data used in the test inflates Type I error.\n\n- Use alternatives like the Lilliefors test or bootstrap methods when parameters are estimated.\n\n- The KS test is less sensitive in the tails—be cautious with it as a global fit measure.\n\n## Where to Learn More\n\nIf you're regularly checking distributional assumptions, it's worth diving into goodness-of-fit tests in more depth. The *American Statistician* paper by Zeimbekakis et al. (2024) is a great read. Also consider books like *Goodness-of-Fit Techniques* by D’Agostino and Stephens, or *All of Statistics* by Larry Wasserman for more intuitive overviews.\n\n## References\n\nZeimbekakis, A., Schifano, E. D., & Yan, J. (2024). On Misuses of the Kolmogorov–Smirnov Test for One-Sample Goodness-of-Fit. *The American Statistician*, 78(4), 481-487.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"ks-test-one-sample.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"The Kolmogorov–Smirnov Test as a Goodness-of-fit","date":"2025-00-00","categories":["statistical inference","hypothesis testing"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}