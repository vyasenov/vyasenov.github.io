{"title":"Causal Inference with Log Transformations","markdown":{"yaml":{"title":"Causal Inference with Log Transformations","date":"2025-00-00","categories":["causal-inference","log-transformations"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nLogarithmic transformations are among the most common tools in applied regression modeling, often motivated by their ability to linearize exponential relationships or to stabilize variance. They are also appealing in causal inference because they allow for treatment effects to be interpreted as **percentage changes**, a language that feels intuitive in policy discussions. But what happens when your outcome variable can be zero — or when you use alternative transformations like $\\log(1 + Y)$ or $\\arcsinh(Y)$ to handle this?\n\nThis article walks through the core issues with log transformations in causal inference, focusing on the recent findings of Chen and Roth (2024) and insights from Huntington-Klein (2023). Along the way, we'll clarify what you can and cannot interpret as percentage effects, discuss the critical issue of scale dependence, and offer practical alternatives.\n\n## Notation\n\nAssume a binary treatment $D \\in \\{0, 1\\}$ and an outcome $Y \\geq 0$, with potential outcomes $Y(1)$ and $Y(0)$. The canonical average treatment effect (ATE) is:\n\n$$\nATE = \\mathbb{E}[Y(1) - Y(0)].\n$$\n\nWhen $Y$ is strictly positive, many researchers instead report the **ATE in logs**:\n\n$$\n\\mathbb{E}[\\log(Y(1)) - \\log(Y(0))].\n$$\n\nIf $Y$ can equal zero, log is undefined, so analysts often use **log-like transformations** like $\\log(1 + Y)$ or $\\arcsinh(Y)$, both of which behave like $\\log(Y)$ for large $Y$.\n\n## A Closer Look\n\n### The Problem with Logs and Zeros\n\nThe main challenge arises when some individuals have $Y(0) = 0$ but $Y(1) > 0$, or vice versa. In these cases, a **percentage change** isn’t well-defined at the individual level: you can’t compute a percentage increase from zero.\n\nChen and Roth (2024) demonstrate that for log-like transformations $m(Y)$, the estimated ATE:\n\n$$\n\\mathbb{E}[m(Y(1)) - m(Y(0))]\n$$\n\nis **not unit-invariant**. In fact, the magnitude of this estimated effect can be arbitrarily manipulated simply by rescaling the units of $Y$ (e.g., from dollars to cents). This violates the intuition that a percentage change should be independent of whether we’re measuring income in dollars or pennies.\n\nThe core insight is that log-like transformations place implicit and arbitrary weight on the **extensive margin** — that is, on individuals moving from zero to nonzero outcomes — and that this weight is sensitive to the scale of $Y$.\n\n---\n\n### A Trilemma: Pick Two\n\nChen and Roth formalize a \"trilemma\": when outcomes can be zero, no treatment effect parameter can satisfy all three of the following:\n\n1. Be an average of individual-level treatment effects.\n2. Be invariant to rescaling of units.\n3. Be point-identified from observed data.\n\nThis means researchers must choose which property to sacrifice. If you want unit-invariance (percentage effects), you can’t use log-like ATEs. If you want point identification and interpretability, you may need to abandon simple averaging.\n\n---\n\n### Alternative Approaches\n\n#### Percentage Effects in Levels (Poisson Regression)\n\nOne solution is to return to **level-based** treatment effects, such as:\n\n$$\n\\theta_{\\text{level-percentage}} = \\frac{\\mathbb{E}[Y(1) - Y(0)]}{\\mathbb{E}[Y(0)]}.\n$$\n\nThis can be estimated via **Poisson regression** or by directly scaling the level-based ATE.\n\n#### Explicit Weighting of Margins\n\nIf both the extensive (zero to nonzero) and intensive (positive changes) margins are of interest, researchers can specify how much they value a zero-to-one change relative to continuous changes. This requires subjective calibration but makes assumptions transparent.\n\n#### Separate Estimates for Intensive and Extensive Margins\n\nAlternatively, estimate separate treatment effects for:\n\n- The probability of having $Y > 0$ (extensive margin).\n- The mean of $\\log(Y)$ conditional on $Y > 0$ (intensive margin).\n\nThis aligns with methods like **Lee bounds** or other partial identification strategies.\n\n---\n\n### The Huntington-Klein Adjustment: Linear Rescaling\n\nHuntington-Klein (2023) offers a complementary insight: even when using standard logs, **interpretation often relies on sloppy approximations** (e.g., interpreting $p$ units in $\\log(X)$ as $p \\times 100\\%$ change in $X$).\n\nHe proposes using **logarithms with alternative bases** (log\\(_{1+p}\\)) where a one-unit change corresponds exactly to a $p$-percent increase. This method avoids approximation error and can be written directly into regression tables for clarity.\n\nWhile this doesn't fix the zero problem per se, it improves interpretability when outcomes are positive.\n\n## An Example\n\n::::{.panel-tabset}\n\n### R\n\n```r\nset.seed(123)\nn <- 1000\nd <- rbinom(n, 1, 0.5)\ny0 <- rgamma(n, shape = 2, rate = 1)\ny1 <- y0 + d * rgamma(n, shape = 1, rate = 0.5)\ny <- ifelse(d == 1, y1, y0)\nlog1p_y <- log1p(y)\nmodel <- lm(log1p_y ~ d)\nsummary(model)\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(123)\nn = 1000\nd = np.random.binomial(1, 0.5, n)\ny0 = np.random.gamma(2, 1, n)\ny1 = y0 + d * np.random.gamma(1, 2, n)\ny = np.where(d == 1, y1, y0)\nlog1p_y = np.log1p(y)\nX = sm.add_constant(d)\nmodel = sm.OLS(log1p_y, X).fit()\nprint(model.summary())\n```\n\n::::\n\n## Bottom Line\n\n- Log transformations with zeros pose serious interpretational challenges for causal inference.\n\n- Log-like ATEs are arbitrarily sensitive to the scaling of the outcome variable.\n\n- Alternative strategies like Poisson regression, explicit weighting of margins, or separate modeling of intensive and extensive effects provide better-grounded estimates.\n\n- Linear rescaling (choosing alternative log bases) improves interpretation accuracy but does not solve the zero problem alone.\n\n## Where to Learn More\n\nThe must-read paper on this topic is Chen and Roth (2024), which provides both the formal theory and empirical illustrations of these issues. Huntington-Klein (2023) adds valuable advice on how to interpret log coefficients cleanly. Related discussions appear in work on Poisson models, two-part models, and Lee bounds for partial identification.\n\n## References\n\n- Chen, J., & Roth, J. (2024). Logs with Zeros? Some Problems and Solutions. *Quarterly Journal of Economics*, 139(2), 891–936.\n\n- Huntington-Klein, N. (2023). Linear Rescaling to Accurately Interpret Logarithms. *Journal of Economic Methods*, 12(1), 139–147.\n","srcMarkdownNoYaml":"\n\n## Background\n\nLogarithmic transformations are among the most common tools in applied regression modeling, often motivated by their ability to linearize exponential relationships or to stabilize variance. They are also appealing in causal inference because they allow for treatment effects to be interpreted as **percentage changes**, a language that feels intuitive in policy discussions. But what happens when your outcome variable can be zero — or when you use alternative transformations like $\\log(1 + Y)$ or $\\arcsinh(Y)$ to handle this?\n\nThis article walks through the core issues with log transformations in causal inference, focusing on the recent findings of Chen and Roth (2024) and insights from Huntington-Klein (2023). Along the way, we'll clarify what you can and cannot interpret as percentage effects, discuss the critical issue of scale dependence, and offer practical alternatives.\n\n## Notation\n\nAssume a binary treatment $D \\in \\{0, 1\\}$ and an outcome $Y \\geq 0$, with potential outcomes $Y(1)$ and $Y(0)$. The canonical average treatment effect (ATE) is:\n\n$$\nATE = \\mathbb{E}[Y(1) - Y(0)].\n$$\n\nWhen $Y$ is strictly positive, many researchers instead report the **ATE in logs**:\n\n$$\n\\mathbb{E}[\\log(Y(1)) - \\log(Y(0))].\n$$\n\nIf $Y$ can equal zero, log is undefined, so analysts often use **log-like transformations** like $\\log(1 + Y)$ or $\\arcsinh(Y)$, both of which behave like $\\log(Y)$ for large $Y$.\n\n## A Closer Look\n\n### The Problem with Logs and Zeros\n\nThe main challenge arises when some individuals have $Y(0) = 0$ but $Y(1) > 0$, or vice versa. In these cases, a **percentage change** isn’t well-defined at the individual level: you can’t compute a percentage increase from zero.\n\nChen and Roth (2024) demonstrate that for log-like transformations $m(Y)$, the estimated ATE:\n\n$$\n\\mathbb{E}[m(Y(1)) - m(Y(0))]\n$$\n\nis **not unit-invariant**. In fact, the magnitude of this estimated effect can be arbitrarily manipulated simply by rescaling the units of $Y$ (e.g., from dollars to cents). This violates the intuition that a percentage change should be independent of whether we’re measuring income in dollars or pennies.\n\nThe core insight is that log-like transformations place implicit and arbitrary weight on the **extensive margin** — that is, on individuals moving from zero to nonzero outcomes — and that this weight is sensitive to the scale of $Y$.\n\n---\n\n### A Trilemma: Pick Two\n\nChen and Roth formalize a \"trilemma\": when outcomes can be zero, no treatment effect parameter can satisfy all three of the following:\n\n1. Be an average of individual-level treatment effects.\n2. Be invariant to rescaling of units.\n3. Be point-identified from observed data.\n\nThis means researchers must choose which property to sacrifice. If you want unit-invariance (percentage effects), you can’t use log-like ATEs. If you want point identification and interpretability, you may need to abandon simple averaging.\n\n---\n\n### Alternative Approaches\n\n#### Percentage Effects in Levels (Poisson Regression)\n\nOne solution is to return to **level-based** treatment effects, such as:\n\n$$\n\\theta_{\\text{level-percentage}} = \\frac{\\mathbb{E}[Y(1) - Y(0)]}{\\mathbb{E}[Y(0)]}.\n$$\n\nThis can be estimated via **Poisson regression** or by directly scaling the level-based ATE.\n\n#### Explicit Weighting of Margins\n\nIf both the extensive (zero to nonzero) and intensive (positive changes) margins are of interest, researchers can specify how much they value a zero-to-one change relative to continuous changes. This requires subjective calibration but makes assumptions transparent.\n\n#### Separate Estimates for Intensive and Extensive Margins\n\nAlternatively, estimate separate treatment effects for:\n\n- The probability of having $Y > 0$ (extensive margin).\n- The mean of $\\log(Y)$ conditional on $Y > 0$ (intensive margin).\n\nThis aligns with methods like **Lee bounds** or other partial identification strategies.\n\n---\n\n### The Huntington-Klein Adjustment: Linear Rescaling\n\nHuntington-Klein (2023) offers a complementary insight: even when using standard logs, **interpretation often relies on sloppy approximations** (e.g., interpreting $p$ units in $\\log(X)$ as $p \\times 100\\%$ change in $X$).\n\nHe proposes using **logarithms with alternative bases** (log\\(_{1+p}\\)) where a one-unit change corresponds exactly to a $p$-percent increase. This method avoids approximation error and can be written directly into regression tables for clarity.\n\nWhile this doesn't fix the zero problem per se, it improves interpretability when outcomes are positive.\n\n## An Example\n\n::::{.panel-tabset}\n\n### R\n\n```r\nset.seed(123)\nn <- 1000\nd <- rbinom(n, 1, 0.5)\ny0 <- rgamma(n, shape = 2, rate = 1)\ny1 <- y0 + d * rgamma(n, shape = 1, rate = 0.5)\ny <- ifelse(d == 1, y1, y0)\nlog1p_y <- log1p(y)\nmodel <- lm(log1p_y ~ d)\nsummary(model)\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(123)\nn = 1000\nd = np.random.binomial(1, 0.5, n)\ny0 = np.random.gamma(2, 1, n)\ny1 = y0 + d * np.random.gamma(1, 2, n)\ny = np.where(d == 1, y1, y0)\nlog1p_y = np.log1p(y)\nX = sm.add_constant(d)\nmodel = sm.OLS(log1p_y, X).fit()\nprint(model.summary())\n```\n\n::::\n\n## Bottom Line\n\n- Log transformations with zeros pose serious interpretational challenges for causal inference.\n\n- Log-like ATEs are arbitrarily sensitive to the scaling of the outcome variable.\n\n- Alternative strategies like Poisson regression, explicit weighting of margins, or separate modeling of intensive and extensive effects provide better-grounded estimates.\n\n- Linear rescaling (choosing alternative log bases) improves interpretation accuracy but does not solve the zero problem alone.\n\n## Where to Learn More\n\nThe must-read paper on this topic is Chen and Roth (2024), which provides both the formal theory and empirical illustrations of these issues. Huntington-Klein (2023) adds valuable advice on how to interpret log coefficients cleanly. Related discussions appear in work on Poisson models, two-part models, and Lee bounds for partial identification.\n\n## References\n\n- Chen, J., & Roth, J. (2024). Logs with Zeros? Some Problems and Solutions. *Quarterly Journal of Economics*, 139(2), 891–936.\n\n- Huntington-Klein, N. (2023). Linear Rescaling to Accurately Interpret Logarithms. *Journal of Economic Methods*, 12(1), 139–147.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"causal-inference-log-transformations.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Causal Inference with Log Transformations","date":"2025-00-00","categories":["causal-inference","log-transformations"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}