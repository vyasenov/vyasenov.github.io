{"title":"Data Fission: A New Approach to Model Selection and Inference","markdown":{"yaml":{"title":"Data Fission: A New Approach to Model Selection and Inference","date":"2025-00-00","categories":["statistical inference"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nWhen conducting statistical modeling and hypothesis testing, analysts often face the challenge of **model selection bias**: if we use the same data both to select a model and to conduct inference, our confidence intervals and p-values may no longer be valid. Traditionally, the go-to solution has been **data splitting** — partitioning the dataset into two separate parts: one for model selection and one for inference.\n\nHowever, what if we don't have the luxury of a large dataset? What if splitting would waste valuable information, especially when rare events or high-leverage points dominate? Enter **data fission**, a recent innovation proposed by Leiner et al. (2025), which offers a more efficient, flexible, and assumption-lean alternative to data splitting.\n\nThis article introduces the concept of data fission, how it works, and where it outperforms traditional methods like data splitting and data carving. We'll also walk through examples and highlight its applications in linear regression, GLMs, trend filtering, and post-selection inference.\n\n## Notation\n\nAssume we observe a random variable $X$ drawn from a distribution $P$ with unknown parameter $\\theta$. The core idea of data fission is to decompose $X$ into two parts:\n\n$$\nf(X), \\quad g(X)\n$$\n\nsuch that:\n- Neither $f(X)$ nor $g(X)$ alone can fully reconstruct $X$,\n- But together they determine $X$,\n- The joint distribution of $(f(X), g(X))$ is known or tractable.\n\nFor example, if $X \\sim N(\\mu, \\sigma^2)$, fission is achieved by:\n\n$$\nf(X) = X + Z, \\quad g(X) = X - Z, \\quad Z \\sim N(0, \\sigma^2).\n$$\n\n## A Closer Look\n\n### Why Fission Instead of Splitting?\n\nThe motivation behind data fission is to keep **all data points \"alive\" in both selection and inference**, but only share part of the information from each data point with each stage. This allows the analyst to:\n- Hedge their bets on influential points,\n- Maintain flexibility in model selection,\n- Retain valid inferential guarantees.\n\nFission can be viewed as a **continuous analog to data splitting**, trading off information between selection and inference smoothly, rather than discretely cutting the data in half.\n\n---\n\n### Gaussian Example: The Simplest Case\n\nIf $X \\sim N(\\mu, \\sigma^2)$, one can construct independent random noise $Z \\sim N(0, \\sigma^2)$ and define:\n\n$$\nf(X) = X + \\tau Z, \\quad g(X) = X - \\frac{1}{\\tau} Z.\n$$\n\nThe parameter $\\tau$ controls how much information is allocated to $f(X)$ versus $g(X)$. When $\\tau \\to \\infty$, $f(X)$ becomes uninformative; when $\\tau \\to 0$, $g(X)$ becomes uninformative.\n\n---\n\n### Generalized Fission via Conjugate Prior Reversal\n\nBeyond the Gaussian case, data fission leverages **conjugate prior relationships** to achieve similar decompositions:\n\n- **Poisson Example**: If $X \\sim \\text{Poisson}(\\mu)$, set $f(X) = Z \\sim \\text{Binomial}(X, p)$, $g(X) = X - Z$.\n- **Bernoulli Example**: If $X \\sim \\text{Bernoulli}(\\theta)$, draw $Z \\sim \\text{Bernoulli}(p)$ and set $f(X) = X(1 - Z) + (1 - X)Z$.\n\nThese constructions maintain a tractable joint distribution between $f(X)$ and $g(X)$ while achieving the desired splitting of information.\n\n---\n\n### Efficiency: Why Fission Outperforms Splitting\n\nOne of the most compelling results from Leiner et al. is that fission, on average, yields **tighter confidence intervals** and **higher power** than data splitting. This advantage is particularly pronounced when:\n\n- The sample size is small,\n- Certain data points have disproportionate influence (high leverage),\n- Covariates are fixed (non-random).\n\nThe reason: data splitting introduces randomness into which points are selected for inference, which lowers efficiency. Data fission avoids this by deterministically splitting the information within each point.\n\n---\n\n### Applications\n\nThe authors demonstrate the usefulness of data fission across several settings:\n\n1. **Post-selection inference after multiple testing**: Construct valid confidence intervals even after adaptive selection of hypotheses.\n2. **Linear regression with feature selection**: Use fissioned data to select variables with LASSO and then conduct inference without sacrificing power.\n3. **Generalized linear models (GLMs)**: Extend the approach to non-Gaussian responses like Poisson and Binomial models.\n4. **Trend filtering and nonparametric regression**: Enable valid uncertainty quantification in adaptive smoothing problems.\n\n### Discussion\n\nThis paper generated tremendous amount of discussion in the Journal of the American Statistical Association. Here is a brief summary of the twelve or so comments published alongisde the original paper. The discussion papers on data fission explore its potential as a novel statistical method for splitting a single data point into two components for model selection and inference. ​ Many contributors praise its innovative use of Bayesian ideas and conjugate models, highlighting its advantages in selective inference and applications like clustering, Gaussian Process regression, and single-cell RNA-seq analysis. However, several critiques emerge, including concerns about its reliance on parametric assumptions, sensitivity to data-specific properties, and potential overfitting in small sample sizes. ​ Some authors suggest extending data fission to nonparametric settings, leveraging tools like the Dirichlet Process, or exploring debiasing methods for complex dependencies in unsupervised learning tasks. ​\n\nOthers propose practical improvements, such as asymptotic guarantees for broader selection rules, new decomposition strategies for dependent data, and connections to empirical Bayes frameworks. ​ Comparisons with data splitting and data carving reveal that while data fission excels in high-dimensional problems and inference tasks, it may underperform in small data contexts or when selection events are tractable. The authors of the original paper acknowledge these critiques and emphasize the need for further research into extending data fission to dependent data, improving computational accessibility, and exploring its applications in risk estimation and clustering. Overall, the discussions highlight both the promise and challenges of data fission as a versatile tool in modern statistics. \n\n## An Example\n\n::::{.panel-tabset}\n\n### R\n\n```r\nset.seed(123)\nn <- 500\nx <- rnorm(n)\ny <- 1 + 2 * x + rnorm(n)\n\n# Fission step: add independent noise\nz <- rnorm(n)\ntau <- 1\nf_y <- y + tau * z\ng_y <- y - z / tau\n\n# Model selection on f_y (e.g., LASSO, here just OLS for simplicity)\nselection_model <- lm(f_y ~ x)\nselected_coef <- summary(selection_model)$coefficients\n\n# Inference on g_y\ninference_model <- lm(g_y ~ x)\nsummary(inference_model)\n```\n\n### Python\n\n```python\nimport numpy as np\nimport statsmodels.api as sm\n\nnp.random.seed(123)\nn = 500\nx = np.random.randn(n)\ny = 1 + 2 * x + np.random.randn(n)\n\nz = np.random.randn(n)\ntau = 1\nf_y = y + tau * z\ng_y = y - z / tau\n\nX = sm.add_constant(x)\nselection_model = sm.OLS(f_y, X).fit()\nprint(selection_model.summary())\n\ninference_model = sm.OLS(g_y, X).fit()\nprint(inference_model.summary())\n```\n\n::::\n\n## Bottom Line\n\n- Data fission splits the *information* within each data point rather than the data points themselves.\n\n- It offers higher efficiency and flexibility than traditional data splitting.\n\n- Particularly advantageous with small samples or high-leverage points.\n\n- Extends naturally to regression, GLMs, and nonparametric smoothing.\n\n## Where to Learn More\n\nThe foundational article by Leiner et al. (2025) is the key resource on data fission. It offers detailed proofs, simulation studies, and practical examples. Related work on data splitting, data carving, and post-selection inference helps contextualize this method within the broader landscape of selective inference.\n\n## References\n\n- Leiner, J., Duan, B., Wasserman, L., & Ramdas, A. (2025). Data Fission: Splitting a Single Data Point. *Journal of the American Statistical Association*, 120(549), 135–146.\n","srcMarkdownNoYaml":"\n\n## Background\n\nWhen conducting statistical modeling and hypothesis testing, analysts often face the challenge of **model selection bias**: if we use the same data both to select a model and to conduct inference, our confidence intervals and p-values may no longer be valid. Traditionally, the go-to solution has been **data splitting** — partitioning the dataset into two separate parts: one for model selection and one for inference.\n\nHowever, what if we don't have the luxury of a large dataset? What if splitting would waste valuable information, especially when rare events or high-leverage points dominate? Enter **data fission**, a recent innovation proposed by Leiner et al. (2025), which offers a more efficient, flexible, and assumption-lean alternative to data splitting.\n\nThis article introduces the concept of data fission, how it works, and where it outperforms traditional methods like data splitting and data carving. We'll also walk through examples and highlight its applications in linear regression, GLMs, trend filtering, and post-selection inference.\n\n## Notation\n\nAssume we observe a random variable $X$ drawn from a distribution $P$ with unknown parameter $\\theta$. The core idea of data fission is to decompose $X$ into two parts:\n\n$$\nf(X), \\quad g(X)\n$$\n\nsuch that:\n- Neither $f(X)$ nor $g(X)$ alone can fully reconstruct $X$,\n- But together they determine $X$,\n- The joint distribution of $(f(X), g(X))$ is known or tractable.\n\nFor example, if $X \\sim N(\\mu, \\sigma^2)$, fission is achieved by:\n\n$$\nf(X) = X + Z, \\quad g(X) = X - Z, \\quad Z \\sim N(0, \\sigma^2).\n$$\n\n## A Closer Look\n\n### Why Fission Instead of Splitting?\n\nThe motivation behind data fission is to keep **all data points \"alive\" in both selection and inference**, but only share part of the information from each data point with each stage. This allows the analyst to:\n- Hedge their bets on influential points,\n- Maintain flexibility in model selection,\n- Retain valid inferential guarantees.\n\nFission can be viewed as a **continuous analog to data splitting**, trading off information between selection and inference smoothly, rather than discretely cutting the data in half.\n\n---\n\n### Gaussian Example: The Simplest Case\n\nIf $X \\sim N(\\mu, \\sigma^2)$, one can construct independent random noise $Z \\sim N(0, \\sigma^2)$ and define:\n\n$$\nf(X) = X + \\tau Z, \\quad g(X) = X - \\frac{1}{\\tau} Z.\n$$\n\nThe parameter $\\tau$ controls how much information is allocated to $f(X)$ versus $g(X)$. When $\\tau \\to \\infty$, $f(X)$ becomes uninformative; when $\\tau \\to 0$, $g(X)$ becomes uninformative.\n\n---\n\n### Generalized Fission via Conjugate Prior Reversal\n\nBeyond the Gaussian case, data fission leverages **conjugate prior relationships** to achieve similar decompositions:\n\n- **Poisson Example**: If $X \\sim \\text{Poisson}(\\mu)$, set $f(X) = Z \\sim \\text{Binomial}(X, p)$, $g(X) = X - Z$.\n- **Bernoulli Example**: If $X \\sim \\text{Bernoulli}(\\theta)$, draw $Z \\sim \\text{Bernoulli}(p)$ and set $f(X) = X(1 - Z) + (1 - X)Z$.\n\nThese constructions maintain a tractable joint distribution between $f(X)$ and $g(X)$ while achieving the desired splitting of information.\n\n---\n\n### Efficiency: Why Fission Outperforms Splitting\n\nOne of the most compelling results from Leiner et al. is that fission, on average, yields **tighter confidence intervals** and **higher power** than data splitting. This advantage is particularly pronounced when:\n\n- The sample size is small,\n- Certain data points have disproportionate influence (high leverage),\n- Covariates are fixed (non-random).\n\nThe reason: data splitting introduces randomness into which points are selected for inference, which lowers efficiency. Data fission avoids this by deterministically splitting the information within each point.\n\n---\n\n### Applications\n\nThe authors demonstrate the usefulness of data fission across several settings:\n\n1. **Post-selection inference after multiple testing**: Construct valid confidence intervals even after adaptive selection of hypotheses.\n2. **Linear regression with feature selection**: Use fissioned data to select variables with LASSO and then conduct inference without sacrificing power.\n3. **Generalized linear models (GLMs)**: Extend the approach to non-Gaussian responses like Poisson and Binomial models.\n4. **Trend filtering and nonparametric regression**: Enable valid uncertainty quantification in adaptive smoothing problems.\n\n### Discussion\n\nThis paper generated tremendous amount of discussion in the Journal of the American Statistical Association. Here is a brief summary of the twelve or so comments published alongisde the original paper. The discussion papers on data fission explore its potential as a novel statistical method for splitting a single data point into two components for model selection and inference. ​ Many contributors praise its innovative use of Bayesian ideas and conjugate models, highlighting its advantages in selective inference and applications like clustering, Gaussian Process regression, and single-cell RNA-seq analysis. However, several critiques emerge, including concerns about its reliance on parametric assumptions, sensitivity to data-specific properties, and potential overfitting in small sample sizes. ​ Some authors suggest extending data fission to nonparametric settings, leveraging tools like the Dirichlet Process, or exploring debiasing methods for complex dependencies in unsupervised learning tasks. ​\n\nOthers propose practical improvements, such as asymptotic guarantees for broader selection rules, new decomposition strategies for dependent data, and connections to empirical Bayes frameworks. ​ Comparisons with data splitting and data carving reveal that while data fission excels in high-dimensional problems and inference tasks, it may underperform in small data contexts or when selection events are tractable. The authors of the original paper acknowledge these critiques and emphasize the need for further research into extending data fission to dependent data, improving computational accessibility, and exploring its applications in risk estimation and clustering. Overall, the discussions highlight both the promise and challenges of data fission as a versatile tool in modern statistics. \n\n## An Example\n\n::::{.panel-tabset}\n\n### R\n\n```r\nset.seed(123)\nn <- 500\nx <- rnorm(n)\ny <- 1 + 2 * x + rnorm(n)\n\n# Fission step: add independent noise\nz <- rnorm(n)\ntau <- 1\nf_y <- y + tau * z\ng_y <- y - z / tau\n\n# Model selection on f_y (e.g., LASSO, here just OLS for simplicity)\nselection_model <- lm(f_y ~ x)\nselected_coef <- summary(selection_model)$coefficients\n\n# Inference on g_y\ninference_model <- lm(g_y ~ x)\nsummary(inference_model)\n```\n\n### Python\n\n```python\nimport numpy as np\nimport statsmodels.api as sm\n\nnp.random.seed(123)\nn = 500\nx = np.random.randn(n)\ny = 1 + 2 * x + np.random.randn(n)\n\nz = np.random.randn(n)\ntau = 1\nf_y = y + tau * z\ng_y = y - z / tau\n\nX = sm.add_constant(x)\nselection_model = sm.OLS(f_y, X).fit()\nprint(selection_model.summary())\n\ninference_model = sm.OLS(g_y, X).fit()\nprint(inference_model.summary())\n```\n\n::::\n\n## Bottom Line\n\n- Data fission splits the *information* within each data point rather than the data points themselves.\n\n- It offers higher efficiency and flexibility than traditional data splitting.\n\n- Particularly advantageous with small samples or high-leverage points.\n\n- Extends naturally to regression, GLMs, and nonparametric smoothing.\n\n## Where to Learn More\n\nThe foundational article by Leiner et al. (2025) is the key resource on data fission. It offers detailed proofs, simulation studies, and practical examples. Related work on data splitting, data carving, and post-selection inference helps contextualize this method within the broader landscape of selective inference.\n\n## References\n\n- Leiner, J., Duan, B., Wasserman, L., & Ramdas, A. (2025). Data Fission: Splitting a Single Data Point. *Journal of the American Statistical Association*, 120(549), 135–146.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"data-fission.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Data Fission: A New Approach to Model Selection and Inference","date":"2025-00-00","categories":["statistical inference"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}