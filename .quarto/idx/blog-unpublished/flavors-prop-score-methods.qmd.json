{"title":"The Many Flavors of Propensity Score Methods for Causal Inference","markdown":{"yaml":{"title":"The Many Flavors of Propensity Score Methods for Causal Inference","date":"2025-04-27","categories":["causal inference","propensity scores"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nIf matching is the hammer in the causal inference toolbox, the **propensity score** is the blueprint that helps you decide where to swing it. Introduced by Rosenbaum and Rubin in 1983, the propensity score—the probability of receiving treatment given observed covariates—has become the workhorse for handling confounding in observational studies.\n\nBut here’s the thing: the propensity score itself is just the starting point. There are **many ways to use** propensity scores. You can match on them, stratify your sample, weight your observations, or plug them into doubly robust estimators that combine modeling of both the treatment and the outcome. You can tweak how you weight the units—downweighting those with extreme scores or focusing on the region where treated and control groups overlap.\n\nIn this post, we’ll explore the many flavors of propensity score methods: when to use them, how they work, and what their pros and cons are. The focus is on intuition, math, and practice—not code.\n\n## Notation\n\nWe’re back in the familiar causal inference setup:\n\n- $D_i \\in \\{0, 1\\}$: treatment indicator.\n- $X_i$: observed covariates.\n- $Y_i(1), Y_i(0)$: potential outcomes.\n\nThe **propensity score** is:\n$$\ne(X_i) = \\mathbb{P}(D_i = 1 \\mid X_i).\n$$\n\nThe key result from Rosenbaum and Rubin (1983):\n$$\n(Y(1), Y(0)) \\perp D \\mid e(X),\n$$\nmeaning that, conditional on the propensity score, treatment assignment is as good as random.\n\n## A Closer Look\n\n### Nearest Neighbor Matching on Propensity Score\n\nThis is often the first method people try after estimating the propensity score. Once $e(X)$ is estimated, treated units are matched to control units with the **closest propensity scores** (nearest neighbor). You can match one-to-one, one-to-many, with or without replacement.\n\n**When to use it?** When the number of controls is large enough to find good matches for treated units.\n\n**Strengths:** Simple and intuitive; reduces high-dimensional matching to one dimension.\n\n**Weaknesses:** Balance on the propensity score doesn’t guarantee balance on covariates; sensitive to poor matches.\n\n---\n\n### Caliper Matching\n\nCaliper matching adds a threshold: only match treated and control units if their propensity scores are within a specified distance (the caliper). Often the caliper is set to **0.2 times the standard deviation of the logit of the propensity score**, following Rosenbaum and Rubin’s recommendation.\n\n**When to use it?** To avoid bad matches in nearest neighbor matching.\n\n**Strengths:** Prevents extreme mismatches; improves balance.\n\n**Weaknesses:** May discard treated units if no control is close enough.\n\n---\n\n### Stratification (Subclassification) on the Propensity Score\n\nHere, the range of propensity scores is divided into $K$ strata (often quintiles), and treatment effects are estimated **within each stratum**, then averaged across strata.\n\n**When to use it?** When matching isn’t feasible or you prefer a more aggregate approach.\n\n**Strengths:** Easy to implement, balances on average within strata.\n\n**Weaknesses:** Coarse adjustment; may not fully eliminate bias within strata.\n\n---\n\n### Inverse Probability Weighting (IPW)\n\nIPW turns the propensity score into weights:\n$$\nw_i = \\frac{D_i}{e(X_i)} + \\frac{1 - D_i}{1 - e(X_i)}.\n$$\nThis reweights the sample so that treated and control groups resemble each other on observed covariates.\n\n**When to use it?** When you want to use the whole dataset and avoid discarding units.\n\n**Strengths:** Simple and fully utilizes all observations.\n\n**Weaknesses:** Sensitive to extreme propensity scores near 0 or 1, which can lead to huge weights and unstable estimates.\n\n---\n\n### Augmented IPW (AIPW) / Doubly Robust Estimators\n\nAIPW combines IPW with **outcome modeling** (regression adjustment). The key appeal: if either the propensity score model **or** the outcome model is correct (but not necessarily both), the estimator is consistent. This is called the **doubly robust property**.\n\nThe AIPW estimator for the ATE looks like:\n$$\n\\hat{\\tau}_{\\text{AIPW}} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{D_i (Y_i - \\hat{m}_1(X_i))}{e(X_i)} - \\frac{(1 - D_i) (Y_i - \\hat{m}_0(X_i))}{1 - e(X_i)} + \\hat{m}_1(X_i) - \\hat{m}_0(X_i) \\right],\n$$\nwhere $\\hat{m}_d(X)$ is the predicted outcome for treatment group $d$.\n\n**When to use it?** When you want robust estimation and are unsure about model correctness.\n\n**Strengths:** Doubly robust consistency. Efficient use of data.\n\n**Weaknesses:** Computational complexity; requires both models to be estimated.\n\n---\n\n### Covariate Balancing Propensity Score (CBPS)\n\nCBPS, introduced by Imai and Ratkovic (2014), directly estimates the propensity score **while optimizing covariate balance**. Instead of fitting a logistic regression and then checking balance, CBPS ensures balance is achieved *as part of the estimation process*.\n\n**When to use it?** When standard propensity score estimation leads to poor balance.\n\n**Strengths:** Good balance without iterative tuning; works directly toward the matching goal.\n\n**Weaknesses:** More complex to implement; less widely available in standard packages.\n\n---\n\n### Overlap Weights\n\nOverlap weighting focuses on the **region of common support**—where treated and control units both exist—by assigning weights:\n$$\nw_i = D_i (1 - e(X_i)) + (1 - D_i) e(X_i).\n$$\nThis downweights units with extreme scores near 0 or 1 and emphasizes comparability.\n\n**When to use it?** When you want to avoid extrapolation and focus on the units where treatment and control overlap.\n\n**Strengths:** Naturally avoids instability from extreme weights; targets the \"overlap population.\"\n\n**Weaknesses:** Estimates effects for the overlap population, not necessarily ATE or ATT.\n\n---\n\n### Entropy Balancing\n\nEntropy balancing directly reweights the control group so that the **moments of the covariates (mean, variance, etc.) match exactly** between treated and control groups. Instead of matching or stratifying, this solves a constrained optimization problem that minimizes the Kullback-Leibler divergence of weights subject to balance constraints.\n\n**When to use it?** When balance is hard to achieve with traditional weighting.\n\n**Strengths:** Guarantees exact balance on chosen covariate moments. Fully utilizes the data.\n\n**Weaknesses:** Requires specifying which moments to balance; can be sensitive to that choice.\n\n---\n\n## Bottom Line\n\n- **Matching** methods (nearest neighbor, caliper) are intuitive and interpretable but can discard data.\n- **Stratification** offers simplicity and full data use but may not fully balance covariates.\n- **IPW** uses all data but can suffer from instability due to extreme weights.\n- **AIPW** gives the best of both worlds with double robustness.\n- **CBPS** directly targets balance in the propensity score estimation.\n- **Overlap weights** avoid the problem of extreme scores and focus on the common support.\n- **Entropy balancing** guarantees exact covariate balance via weighting without matching or stratification.\n\n## Where to Learn More\n\nFor the original introduction to propensity scores, see Rosenbaum and Rubin’s (1983) landmark paper. Imai and Ratkovic’s (2014) work on CBPS is a must-read for understanding balance-focused estimation. The textbook *Causal Inference for Statistics, Social, and Biomedical Sciences* by Imbens and Rubin (2015) provides excellent coverage of these methods. There are also great tutorials and vignettes in R packages like `MatchIt`, `twang`, and `WeightIt`.\n\n## References\n\n- Rosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. *Biometrika*, 70(1), 41–55.\n\n- Imai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 76(1), 243–263.\n\n- Imbens, G. W., & Rubin, D. B. (2015). *Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction*. Cambridge University Press.\n\n- Hainmueller, J. (2012). Entropy balancing for causal effects. *Political Analysis*, 20(1), 25–46.\n\n---\n---\n\nPropensity Score Methods Applied to the Iris Dataset\n\nThe following examples apply several popular propensity score methods to the Iris dataset using both R and Python. For demonstration, we define an artificial binary treatment (`D`) based on petal length. The outcome variable is `Sepal.Length`, and the predictors are the remaining covariates.\n\nPropensity Score Estimation (Logistic Regression)\n\n::::{.panel-tabset}\n\n#### R\n```r\nlibrary(MatchIt)\ndata(iris)\niris$D <- ifelse(iris$Petal.Length > 3, 1, 0)\nps_model <- glm(D ~ Sepal.Width + Petal.Width, data = iris, family = binomial)\nsummary(ps_model)\n```\n\n#### Python\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n\niris = load_iris(as_frame=True).frame\niris['D'] = (iris['petal length (cm)'] > 3).astype(int)\nX = iris[['sepal width (cm)', 'petal width (cm)']]\ny = iris['D']\nmodel = LogisticRegression().fit(X, y)\nmodel.coef_, model.intercept_\n```\n::::\n\nNearest Neighbor Matching\n\n::::{.panel-tabset}\n\n#### R\n```r\nmatchit_nn <- matchit(D ~ Sepal.Width + Petal.Width, data = iris, method = \"nearest\")\nsummary(matchit_nn)\n```\n\n#### Python\n```python\nfrom causalinference import CausalModel\ncm = CausalModel(iris[['sepal width (cm)', 'petal width (cm)']].values, iris['D'].values, iris['sepal length (cm)'].values)\ncm.est_via_matching()\ncm.estimates\n```\n::::\n\nCaliper Matching\n\n::::{.panel-tabset}\n\n#### R\n```r\nmatchit_caliper <- matchit(D ~ Sepal.Width + Petal.Width, data = iris, method = \"nearest\", caliper = 0.2)\nsummary(matchit_caliper)\n```\n\n#### Python\n```python\n# caliper matching in Python is not built-in; would require manual implementation or use DoWhy or related packages\n```\n::::\n\n\nStratification (Subclassification)\n\n::::{.panel-tabset}\n\n#### R\n```r\nmatchit_strat <- matchit(D ~ Sepal.Width + Petal.Width, data = iris, method = \"subclass\", subclass = 5)\nsummary(matchit_strat)\n```\n\n#### Python\n```python\n# Stratification would require binning the propensity score and estimating within strata manually\n```\n::::\n\n::::{.panel-tabset}\n\nInverse Probability Weighting (IPW)\n\n#### R\n```r\niris$ps <- predict(ps_model, type = \"response\")\niris$weights <- ifelse(iris$D == 1, 1 / iris$ps, 1 / (1 - iris$ps))\nsummary(iris$weights)\n```\n\n#### Python\n```python\niris['ps'] = model.predict_proba(X)[:,1]\niris['weights'] = np.where(iris['D'] == 1, 1 / iris['ps'], 1 / (1 - iris['ps']))\niris['weights'].describe()\n```\n::::\n\nAugmented IPW (AIPW) / Doubly Robust\n\n::::{.panel-tabset}\n\n#### R\n```r\n# Requires additional outcome modeling and manual implementation\n# Use packages like AIPW or drtmle for robust estimators\n```\n\n#### Python\n```python\n# Requires DoWhy, EconML, or custom implementation for AIPW\n```\n::::\n\nCovariate Balancing Propensity Score (CBPS)\n\n::::{.panel-tabset}\n\n#### R\n```r\nlibrary(CBPS)\ncbps_fit <- CBPS(D ~ Sepal.Width + Petal.Width, data = iris)\nsummary(cbps_fit)\n```\n\n#### Python\n```python\n# CBPS not readily available in sklearn; typically done via R or custom implementation\n```\n::::\n\nOverlap Weights\n\n::::{.panel-tabset}\n\n#### R\n```r\niris$overlap_weights <- ifelse(iris$D == 1, 1 - iris$ps, iris$ps)\nsummary(iris$overlap_weights)\n```\n\n#### Python\n```python\niris['overlap_weights'] = np.where(iris['D'] == 1, 1 - iris['ps'], iris['ps'])\niris['overlap_weights'].describe()\n```\n::::\n\nEntropy Balancing\n\n::::{.panel-tabset}\n\n#### R\n```r\nlibrary(ebal)\ncontrol_idx <- which(iris$D == 0)\ntreated_idx <- which(iris$D == 1)\nX_control <- iris[control_idx, c(\"Sepal.Width\", \"Petal.Width\")]\neg <- ebalance(Treatment = rep(0, length(control_idx)), X = as.matrix(X_control), target.margins = colMeans(iris[treated_idx, c(\"Sepal.Width\", \"Petal.Width\")]))\nsummary(eg)\n```\n\n#### Python\n```python\n# Entropy balancing in Python requires specialized packages or manual convex optimization\n```\n::::\n\n","srcMarkdownNoYaml":"\n\n## Background\n\nIf matching is the hammer in the causal inference toolbox, the **propensity score** is the blueprint that helps you decide where to swing it. Introduced by Rosenbaum and Rubin in 1983, the propensity score—the probability of receiving treatment given observed covariates—has become the workhorse for handling confounding in observational studies.\n\nBut here’s the thing: the propensity score itself is just the starting point. There are **many ways to use** propensity scores. You can match on them, stratify your sample, weight your observations, or plug them into doubly robust estimators that combine modeling of both the treatment and the outcome. You can tweak how you weight the units—downweighting those with extreme scores or focusing on the region where treated and control groups overlap.\n\nIn this post, we’ll explore the many flavors of propensity score methods: when to use them, how they work, and what their pros and cons are. The focus is on intuition, math, and practice—not code.\n\n## Notation\n\nWe’re back in the familiar causal inference setup:\n\n- $D_i \\in \\{0, 1\\}$: treatment indicator.\n- $X_i$: observed covariates.\n- $Y_i(1), Y_i(0)$: potential outcomes.\n\nThe **propensity score** is:\n$$\ne(X_i) = \\mathbb{P}(D_i = 1 \\mid X_i).\n$$\n\nThe key result from Rosenbaum and Rubin (1983):\n$$\n(Y(1), Y(0)) \\perp D \\mid e(X),\n$$\nmeaning that, conditional on the propensity score, treatment assignment is as good as random.\n\n## A Closer Look\n\n### Nearest Neighbor Matching on Propensity Score\n\nThis is often the first method people try after estimating the propensity score. Once $e(X)$ is estimated, treated units are matched to control units with the **closest propensity scores** (nearest neighbor). You can match one-to-one, one-to-many, with or without replacement.\n\n**When to use it?** When the number of controls is large enough to find good matches for treated units.\n\n**Strengths:** Simple and intuitive; reduces high-dimensional matching to one dimension.\n\n**Weaknesses:** Balance on the propensity score doesn’t guarantee balance on covariates; sensitive to poor matches.\n\n---\n\n### Caliper Matching\n\nCaliper matching adds a threshold: only match treated and control units if their propensity scores are within a specified distance (the caliper). Often the caliper is set to **0.2 times the standard deviation of the logit of the propensity score**, following Rosenbaum and Rubin’s recommendation.\n\n**When to use it?** To avoid bad matches in nearest neighbor matching.\n\n**Strengths:** Prevents extreme mismatches; improves balance.\n\n**Weaknesses:** May discard treated units if no control is close enough.\n\n---\n\n### Stratification (Subclassification) on the Propensity Score\n\nHere, the range of propensity scores is divided into $K$ strata (often quintiles), and treatment effects are estimated **within each stratum**, then averaged across strata.\n\n**When to use it?** When matching isn’t feasible or you prefer a more aggregate approach.\n\n**Strengths:** Easy to implement, balances on average within strata.\n\n**Weaknesses:** Coarse adjustment; may not fully eliminate bias within strata.\n\n---\n\n### Inverse Probability Weighting (IPW)\n\nIPW turns the propensity score into weights:\n$$\nw_i = \\frac{D_i}{e(X_i)} + \\frac{1 - D_i}{1 - e(X_i)}.\n$$\nThis reweights the sample so that treated and control groups resemble each other on observed covariates.\n\n**When to use it?** When you want to use the whole dataset and avoid discarding units.\n\n**Strengths:** Simple and fully utilizes all observations.\n\n**Weaknesses:** Sensitive to extreme propensity scores near 0 or 1, which can lead to huge weights and unstable estimates.\n\n---\n\n### Augmented IPW (AIPW) / Doubly Robust Estimators\n\nAIPW combines IPW with **outcome modeling** (regression adjustment). The key appeal: if either the propensity score model **or** the outcome model is correct (but not necessarily both), the estimator is consistent. This is called the **doubly robust property**.\n\nThe AIPW estimator for the ATE looks like:\n$$\n\\hat{\\tau}_{\\text{AIPW}} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{D_i (Y_i - \\hat{m}_1(X_i))}{e(X_i)} - \\frac{(1 - D_i) (Y_i - \\hat{m}_0(X_i))}{1 - e(X_i)} + \\hat{m}_1(X_i) - \\hat{m}_0(X_i) \\right],\n$$\nwhere $\\hat{m}_d(X)$ is the predicted outcome for treatment group $d$.\n\n**When to use it?** When you want robust estimation and are unsure about model correctness.\n\n**Strengths:** Doubly robust consistency. Efficient use of data.\n\n**Weaknesses:** Computational complexity; requires both models to be estimated.\n\n---\n\n### Covariate Balancing Propensity Score (CBPS)\n\nCBPS, introduced by Imai and Ratkovic (2014), directly estimates the propensity score **while optimizing covariate balance**. Instead of fitting a logistic regression and then checking balance, CBPS ensures balance is achieved *as part of the estimation process*.\n\n**When to use it?** When standard propensity score estimation leads to poor balance.\n\n**Strengths:** Good balance without iterative tuning; works directly toward the matching goal.\n\n**Weaknesses:** More complex to implement; less widely available in standard packages.\n\n---\n\n### Overlap Weights\n\nOverlap weighting focuses on the **region of common support**—where treated and control units both exist—by assigning weights:\n$$\nw_i = D_i (1 - e(X_i)) + (1 - D_i) e(X_i).\n$$\nThis downweights units with extreme scores near 0 or 1 and emphasizes comparability.\n\n**When to use it?** When you want to avoid extrapolation and focus on the units where treatment and control overlap.\n\n**Strengths:** Naturally avoids instability from extreme weights; targets the \"overlap population.\"\n\n**Weaknesses:** Estimates effects for the overlap population, not necessarily ATE or ATT.\n\n---\n\n### Entropy Balancing\n\nEntropy balancing directly reweights the control group so that the **moments of the covariates (mean, variance, etc.) match exactly** between treated and control groups. Instead of matching or stratifying, this solves a constrained optimization problem that minimizes the Kullback-Leibler divergence of weights subject to balance constraints.\n\n**When to use it?** When balance is hard to achieve with traditional weighting.\n\n**Strengths:** Guarantees exact balance on chosen covariate moments. Fully utilizes the data.\n\n**Weaknesses:** Requires specifying which moments to balance; can be sensitive to that choice.\n\n---\n\n## Bottom Line\n\n- **Matching** methods (nearest neighbor, caliper) are intuitive and interpretable but can discard data.\n- **Stratification** offers simplicity and full data use but may not fully balance covariates.\n- **IPW** uses all data but can suffer from instability due to extreme weights.\n- **AIPW** gives the best of both worlds with double robustness.\n- **CBPS** directly targets balance in the propensity score estimation.\n- **Overlap weights** avoid the problem of extreme scores and focus on the common support.\n- **Entropy balancing** guarantees exact covariate balance via weighting without matching or stratification.\n\n## Where to Learn More\n\nFor the original introduction to propensity scores, see Rosenbaum and Rubin’s (1983) landmark paper. Imai and Ratkovic’s (2014) work on CBPS is a must-read for understanding balance-focused estimation. The textbook *Causal Inference for Statistics, Social, and Biomedical Sciences* by Imbens and Rubin (2015) provides excellent coverage of these methods. There are also great tutorials and vignettes in R packages like `MatchIt`, `twang`, and `WeightIt`.\n\n## References\n\n- Rosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. *Biometrika*, 70(1), 41–55.\n\n- Imai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 76(1), 243–263.\n\n- Imbens, G. W., & Rubin, D. B. (2015). *Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction*. Cambridge University Press.\n\n- Hainmueller, J. (2012). Entropy balancing for causal effects. *Political Analysis*, 20(1), 25–46.\n\n---\n---\n\nPropensity Score Methods Applied to the Iris Dataset\n\nThe following examples apply several popular propensity score methods to the Iris dataset using both R and Python. For demonstration, we define an artificial binary treatment (`D`) based on petal length. The outcome variable is `Sepal.Length`, and the predictors are the remaining covariates.\n\nPropensity Score Estimation (Logistic Regression)\n\n::::{.panel-tabset}\n\n#### R\n```r\nlibrary(MatchIt)\ndata(iris)\niris$D <- ifelse(iris$Petal.Length > 3, 1, 0)\nps_model <- glm(D ~ Sepal.Width + Petal.Width, data = iris, family = binomial)\nsummary(ps_model)\n```\n\n#### Python\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n\niris = load_iris(as_frame=True).frame\niris['D'] = (iris['petal length (cm)'] > 3).astype(int)\nX = iris[['sepal width (cm)', 'petal width (cm)']]\ny = iris['D']\nmodel = LogisticRegression().fit(X, y)\nmodel.coef_, model.intercept_\n```\n::::\n\nNearest Neighbor Matching\n\n::::{.panel-tabset}\n\n#### R\n```r\nmatchit_nn <- matchit(D ~ Sepal.Width + Petal.Width, data = iris, method = \"nearest\")\nsummary(matchit_nn)\n```\n\n#### Python\n```python\nfrom causalinference import CausalModel\ncm = CausalModel(iris[['sepal width (cm)', 'petal width (cm)']].values, iris['D'].values, iris['sepal length (cm)'].values)\ncm.est_via_matching()\ncm.estimates\n```\n::::\n\nCaliper Matching\n\n::::{.panel-tabset}\n\n#### R\n```r\nmatchit_caliper <- matchit(D ~ Sepal.Width + Petal.Width, data = iris, method = \"nearest\", caliper = 0.2)\nsummary(matchit_caliper)\n```\n\n#### Python\n```python\n# caliper matching in Python is not built-in; would require manual implementation or use DoWhy or related packages\n```\n::::\n\n\nStratification (Subclassification)\n\n::::{.panel-tabset}\n\n#### R\n```r\nmatchit_strat <- matchit(D ~ Sepal.Width + Petal.Width, data = iris, method = \"subclass\", subclass = 5)\nsummary(matchit_strat)\n```\n\n#### Python\n```python\n# Stratification would require binning the propensity score and estimating within strata manually\n```\n::::\n\n::::{.panel-tabset}\n\nInverse Probability Weighting (IPW)\n\n#### R\n```r\niris$ps <- predict(ps_model, type = \"response\")\niris$weights <- ifelse(iris$D == 1, 1 / iris$ps, 1 / (1 - iris$ps))\nsummary(iris$weights)\n```\n\n#### Python\n```python\niris['ps'] = model.predict_proba(X)[:,1]\niris['weights'] = np.where(iris['D'] == 1, 1 / iris['ps'], 1 / (1 - iris['ps']))\niris['weights'].describe()\n```\n::::\n\nAugmented IPW (AIPW) / Doubly Robust\n\n::::{.panel-tabset}\n\n#### R\n```r\n# Requires additional outcome modeling and manual implementation\n# Use packages like AIPW or drtmle for robust estimators\n```\n\n#### Python\n```python\n# Requires DoWhy, EconML, or custom implementation for AIPW\n```\n::::\n\nCovariate Balancing Propensity Score (CBPS)\n\n::::{.panel-tabset}\n\n#### R\n```r\nlibrary(CBPS)\ncbps_fit <- CBPS(D ~ Sepal.Width + Petal.Width, data = iris)\nsummary(cbps_fit)\n```\n\n#### Python\n```python\n# CBPS not readily available in sklearn; typically done via R or custom implementation\n```\n::::\n\nOverlap Weights\n\n::::{.panel-tabset}\n\n#### R\n```r\niris$overlap_weights <- ifelse(iris$D == 1, 1 - iris$ps, iris$ps)\nsummary(iris$overlap_weights)\n```\n\n#### Python\n```python\niris['overlap_weights'] = np.where(iris['D'] == 1, 1 - iris['ps'], iris['ps'])\niris['overlap_weights'].describe()\n```\n::::\n\nEntropy Balancing\n\n::::{.panel-tabset}\n\n#### R\n```r\nlibrary(ebal)\ncontrol_idx <- which(iris$D == 0)\ntreated_idx <- which(iris$D == 1)\nX_control <- iris[control_idx, c(\"Sepal.Width\", \"Petal.Width\")]\neg <- ebalance(Treatment = rep(0, length(control_idx)), X = as.matrix(X_control), target.margins = colMeans(iris[treated_idx, c(\"Sepal.Width\", \"Petal.Width\")]))\nsummary(eg)\n```\n\n#### Python\n```python\n# Entropy balancing in Python requires specialized packages or manual convex optimization\n```\n::::\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"flavors-prop-score-methods.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"The Many Flavors of Propensity Score Methods for Causal Inference","date":"2025-04-27","categories":["causal inference","propensity scores"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}