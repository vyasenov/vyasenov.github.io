{"title":"How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster's δ Method","markdown":{"yaml":{"title":"How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster's δ Method","date":"2025-00-00","categories":["causal inference","regression"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nIn regression analysis, especially in social science and economics, a key question often arises: how sensitive are our estimates to the inclusion (or exclusion) of covariates? Are we picking up a genuine causal relationship, or is our coefficient of interest just soaking up omitted variable bias? Two influential papers—Gelbach (2016) and Oster (2019)—tackle this head-on but from different angles. Gelbach focuses on decomposing changes in coefficient estimates, while Oster proposes a method for assessing robustness to unobserved confounding. In this article, we explore both approaches and highlight how they can inform our understanding of variable importance and robustness.\n\n## Notation\n\nLet’s consider a standard linear regression model:\n\n$$\ny = X\\beta + Z\\gamma + \\varepsilon\n$$\n\nHere:\n- $y$ is the outcome,\n- $X$ is a variable of interest (say, a treatment),\n- $Z$ is a set of controls,\n- $\\varepsilon$ is the error term.\n\nSuppose we estimate a \"short\" regression without $Z$, and then a \"long\" regression with $Z$. How does the inclusion of $Z$ affect the estimate of $\\beta$?\n\n## A Closer Look\n\n### Gelbach Decomposition\n\nGelbach (2016) proposes a formal method to break down the change in your coefficient of interest when you add additional covariates. Instead of just eyeballing the before-and-after change, Gelbach tells you exactly how much of the change is due to each new variable (or group of variables).\n\nThe key result comes from the omitted variable bias formula. For a regression of the form:\n\n$$\ny = X_1 \\beta_1 + X_2 \\beta_2 + \\varepsilon,\n$$\n\nwhere $X_1$ is the variable of interest (e.g., treatment) and $X_2$ is the set of additional controls, the difference between the coefficient on $X_1$ in the “short” model (without $X_2$) and the “long” model (with $X_2$) can be written as:\n\n$$\n\\hat{\\beta}^{short}_1 - \\hat{\\beta}^{long}_1 = (X_1' X_1)^{-1}X_1'X_2 \\hat{\\beta_2}.\n$$\n\nThis formula lets you attribute the coefficient change to each control variable in $X_2$. Crucially, this decomposition is order-invariant—unlike the naive practice of sequentially adding controls (which can produce wildly different results depending on the order of inclusion, as Gelbach demonstrates with wage gap studies).\n\nIntuition: Think of the decomposition as answering the question: Which control variables are responsible for the observed shift in my estimate? And by how much?\n\n### Gelbach Decomposition 2\n\nGelbach (2016) offers a way to formally decompose the change in $\\hat{\\beta}$ when controls are added. The key result is:\n\n$$\n\\hat{\\beta}_{\\text{long}} - \\hat{\\beta}_{\\text{short}} = \\hat{\\delta}'(\\hat{\\gamma}_{\\text{aux}})\n$$\n\nWhere:\n- $\\hat{\\delta}$ comes from regressing $Z$ on $X$ (auxiliary regression),\n- $\\hat{\\gamma}_{\\text{aux}}$ are the coefficients from regressing $y$ on $Z$ while controlling for $X$.\n\nIntuitively, this tells us exactly how much of the change in $\\hat{\\beta}$ is due to each added covariate in $Z$.\n\n### Oster's $\\delta$ Method\n\nWhile Gelbach helps you understand what’s happening with your observed controls, Oster (2019) tackles the next big question: What about the stuff I can’t observe?\n\nOster extends ideas from Altonji, Elder, and Taber (2005) and formalizes the relationship between coefficient stability and omitted variable bias. But here’s the critical insight: coefficient stability alone is not enough. You also need to look at changes in $R^2$.\n\nIf adding controls barely budges your coefficient and dramatically increases your $R^2$, that suggests the included controls are genuinely explaining a lot of variation—and unobserved factors may not be a huge threat. On the other hand, if the $R^2$ hardly changes, even a stable coefficient might not mean much.\n\nThe Oster bounding formula allows you to compute what your estimate would be if you could observe everything. The adjusted coefficient is given by:\n\n$$\n\\tilde{\\beta} = \\hat{\\beta}_{\\text{long}} - \\delta(\\hat{\\beta}_{\\text{short}} - \\hat{\\beta}_{\\text{long}}) \\cdot \\left(\\frac{R^2_{\\text{max}} - R^2_{\\text{long}}}{R^2_{\\text{long}} - R^2_{\\text{short}}}\\right),\n$$\n\nwhere:\n\n- $R^2_{\\text{short}}$ is the fit of the model without controls,\n- $R^2_{\\text{long}}$ is the fit with controls,\n- $R^2_{\\text{max}}$ is the hypothetical fit if all confounders were observed,\n- $\\delta$ is the key assumption: the relative importance of selection on unobservables versus observables.\n\nOster (2019) suggests that assuming $\\delta=1$ (equal selection) is a reasonable upper bound, but this can be adjusted based on context.\n\n### Oster's $\\delta$ Method 2\n\nOster (2019) tackles a different but related question: what if we’re worried about selection on unobservables? Her approach uses coefficient stability and changes in $R^2$ to bound the effect of unobserved confounding.\n\nThe core idea is to calculate a value $\\delta$ such that:\n\n$$\n\\frac{\\Delta\\beta}{\\Delta R^2} = \\delta\n$$\n\nIf this ratio is stable as you add more controls, and assuming selection on unobservables is not much worse than selection on observables, you can project how much $\\hat{\\beta}$ would change if you could observe everything. She formalizes this with the adjusted coefficient formula:\n\n$$\n\\tilde{\\beta} = \\hat{\\beta}_{\\text{long}} - \\delta(\\hat{\\beta}_{\\text{short}} - \\hat{\\beta}_{\\text{long}}) \\cdot \\left(\\frac{R^2_{\\text{max}} - R^2_{\\text{long}}}{R^2_{\\text{long}} - R^2_{\\text{short}}}\\right)\n$$\n\nThis gives a way to estimate bounds on $\\beta$ under assumptions about how much more could be explained if we had access to the unobservables.\n\n### Intuition and Contrast\n\nGelbach's method is like a post-mortem dissection: it tells you exactly how much each covariate changed your result. Oster’s method is more of a risk assessment: given the changes you've seen, how scared should you be of the things you can't see?\n\nBoth are powerful tools, but they address different kinds of uncertainty—Gelbach focuses on observable confounding, while Oster extends this to unobservable confounding.\n\n### Why Coefficient Stability Alone Can Mislead\n\nBoth papers caution against over-interpreting coefficient stability on its own:\n\n- Gelbach shows that apparent robustness may depend on which controls you added and in what order.\n\n- Oster shows that stability without $R^2$ movement is meaningless—if your controls aren’t explaining much of the outcome, their failure to shift the coefficient tells you little.\n\nOster’s illustrative example of wage returns to education highlights this issue clearly: if you add a weak control (say, a poor proxy for ability), the coefficient on education may appear stable, but that doesn’t mean there’s no bias—it just means your control wasn’t very good.\n\n## An Example\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nlibrary(hdm)\nlibrary(oaxaca)\n\n# Simulated data\ndf <- data.frame(\n  y = rnorm(1000),\n  x = rnorm(1000),\n  z1 = rnorm(1000),\n  z2 = rnorm(1000)\n)\n\n# Short model\nshort <- lm(y ~ x, data = df)\n\n# Long model\nlong <- lm(y ~ x + z1 + z2, data = df)\n\n# Gelbach decomposition using Oaxaca-Blinder (as approximation)\nlibrary(oaxaca)\noaxaca(y ~ x | z1 + z2, data = df, R = 30)\n\n# Oster bounds (simplified)\nlibrary(psacalc)\npsa(y = df$y, d = df$x, X = df[, c(\"z1\", \"z2\")], R2max = 0.9)\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Simulated data\ndf = pd.DataFrame({\n    'y': np.random.randn(1000),\n    'x': np.random.randn(1000),\n    'z1': np.random.randn(1000),\n    'z2': np.random.randn(1000)\n})\n\n# Short model\nX_short = sm.add_constant(df['x'])\nmodel_short = sm.OLS(df['y'], X_short).fit()\n\n# Long model\nX_long = sm.add_constant(df[['x', 'z1', 'z2']])\nmodel_long = sm.OLS(df['y'], X_long).fit()\n\nprint(model_short.params)\nprint(model_long.params)\n\n# No direct analog to Gelbach in Python, but differences in coefficients can be manually computed.\n```\n\n::::\n\n## Bottom Line\n\n- Gelbach decomposition explains *why* coefficients change when you add covariates.\n\n- Oster’s method helps you assess how robust your findings are to *unobserved* variables.\n\n- Both approaches require strong assumptions—but when applied carefully, they offer insight into what’s driving your regression results.\n\n- Don't blindly include controls—use tools like these to make sense of what they’re doing to your estimates.\n\n## Where to Learn More\n\nIf you're into causal inference and want to go beyond just “throwing in controls,” both these papers are must-reads. For applied guidance, check out Emily Oster’s book *“Uncontrolled”* and Guido Imbens' lecture notes on sensitivity analysis. You might also explore sensitivity tools like `sensemakr` in R, which is built around similar ideas to Oster's method.\n\n## References\n\n- Gelbach, J. B. (2016). When do covariates matter? And which ones, and how much? *Journal of Labor Economics*, 34(2), 509–543.\n\n- Oster, E. (2019). Unobservable selection and coefficient stability: Theory and evidence. *Journal of Business & Economic Statistics*, 37(2), 187–204.\n\n","srcMarkdownNoYaml":"\n\n## Background\n\nIn regression analysis, especially in social science and economics, a key question often arises: how sensitive are our estimates to the inclusion (or exclusion) of covariates? Are we picking up a genuine causal relationship, or is our coefficient of interest just soaking up omitted variable bias? Two influential papers—Gelbach (2016) and Oster (2019)—tackle this head-on but from different angles. Gelbach focuses on decomposing changes in coefficient estimates, while Oster proposes a method for assessing robustness to unobserved confounding. In this article, we explore both approaches and highlight how they can inform our understanding of variable importance and robustness.\n\n## Notation\n\nLet’s consider a standard linear regression model:\n\n$$\ny = X\\beta + Z\\gamma + \\varepsilon\n$$\n\nHere:\n- $y$ is the outcome,\n- $X$ is a variable of interest (say, a treatment),\n- $Z$ is a set of controls,\n- $\\varepsilon$ is the error term.\n\nSuppose we estimate a \"short\" regression without $Z$, and then a \"long\" regression with $Z$. How does the inclusion of $Z$ affect the estimate of $\\beta$?\n\n## A Closer Look\n\n### Gelbach Decomposition\n\nGelbach (2016) proposes a formal method to break down the change in your coefficient of interest when you add additional covariates. Instead of just eyeballing the before-and-after change, Gelbach tells you exactly how much of the change is due to each new variable (or group of variables).\n\nThe key result comes from the omitted variable bias formula. For a regression of the form:\n\n$$\ny = X_1 \\beta_1 + X_2 \\beta_2 + \\varepsilon,\n$$\n\nwhere $X_1$ is the variable of interest (e.g., treatment) and $X_2$ is the set of additional controls, the difference between the coefficient on $X_1$ in the “short” model (without $X_2$) and the “long” model (with $X_2$) can be written as:\n\n$$\n\\hat{\\beta}^{short}_1 - \\hat{\\beta}^{long}_1 = (X_1' X_1)^{-1}X_1'X_2 \\hat{\\beta_2}.\n$$\n\nThis formula lets you attribute the coefficient change to each control variable in $X_2$. Crucially, this decomposition is order-invariant—unlike the naive practice of sequentially adding controls (which can produce wildly different results depending on the order of inclusion, as Gelbach demonstrates with wage gap studies).\n\nIntuition: Think of the decomposition as answering the question: Which control variables are responsible for the observed shift in my estimate? And by how much?\n\n### Gelbach Decomposition 2\n\nGelbach (2016) offers a way to formally decompose the change in $\\hat{\\beta}$ when controls are added. The key result is:\n\n$$\n\\hat{\\beta}_{\\text{long}} - \\hat{\\beta}_{\\text{short}} = \\hat{\\delta}'(\\hat{\\gamma}_{\\text{aux}})\n$$\n\nWhere:\n- $\\hat{\\delta}$ comes from regressing $Z$ on $X$ (auxiliary regression),\n- $\\hat{\\gamma}_{\\text{aux}}$ are the coefficients from regressing $y$ on $Z$ while controlling for $X$.\n\nIntuitively, this tells us exactly how much of the change in $\\hat{\\beta}$ is due to each added covariate in $Z$.\n\n### Oster's $\\delta$ Method\n\nWhile Gelbach helps you understand what’s happening with your observed controls, Oster (2019) tackles the next big question: What about the stuff I can’t observe?\n\nOster extends ideas from Altonji, Elder, and Taber (2005) and formalizes the relationship between coefficient stability and omitted variable bias. But here’s the critical insight: coefficient stability alone is not enough. You also need to look at changes in $R^2$.\n\nIf adding controls barely budges your coefficient and dramatically increases your $R^2$, that suggests the included controls are genuinely explaining a lot of variation—and unobserved factors may not be a huge threat. On the other hand, if the $R^2$ hardly changes, even a stable coefficient might not mean much.\n\nThe Oster bounding formula allows you to compute what your estimate would be if you could observe everything. The adjusted coefficient is given by:\n\n$$\n\\tilde{\\beta} = \\hat{\\beta}_{\\text{long}} - \\delta(\\hat{\\beta}_{\\text{short}} - \\hat{\\beta}_{\\text{long}}) \\cdot \\left(\\frac{R^2_{\\text{max}} - R^2_{\\text{long}}}{R^2_{\\text{long}} - R^2_{\\text{short}}}\\right),\n$$\n\nwhere:\n\n- $R^2_{\\text{short}}$ is the fit of the model without controls,\n- $R^2_{\\text{long}}$ is the fit with controls,\n- $R^2_{\\text{max}}$ is the hypothetical fit if all confounders were observed,\n- $\\delta$ is the key assumption: the relative importance of selection on unobservables versus observables.\n\nOster (2019) suggests that assuming $\\delta=1$ (equal selection) is a reasonable upper bound, but this can be adjusted based on context.\n\n### Oster's $\\delta$ Method 2\n\nOster (2019) tackles a different but related question: what if we’re worried about selection on unobservables? Her approach uses coefficient stability and changes in $R^2$ to bound the effect of unobserved confounding.\n\nThe core idea is to calculate a value $\\delta$ such that:\n\n$$\n\\frac{\\Delta\\beta}{\\Delta R^2} = \\delta\n$$\n\nIf this ratio is stable as you add more controls, and assuming selection on unobservables is not much worse than selection on observables, you can project how much $\\hat{\\beta}$ would change if you could observe everything. She formalizes this with the adjusted coefficient formula:\n\n$$\n\\tilde{\\beta} = \\hat{\\beta}_{\\text{long}} - \\delta(\\hat{\\beta}_{\\text{short}} - \\hat{\\beta}_{\\text{long}}) \\cdot \\left(\\frac{R^2_{\\text{max}} - R^2_{\\text{long}}}{R^2_{\\text{long}} - R^2_{\\text{short}}}\\right)\n$$\n\nThis gives a way to estimate bounds on $\\beta$ under assumptions about how much more could be explained if we had access to the unobservables.\n\n### Intuition and Contrast\n\nGelbach's method is like a post-mortem dissection: it tells you exactly how much each covariate changed your result. Oster’s method is more of a risk assessment: given the changes you've seen, how scared should you be of the things you can't see?\n\nBoth are powerful tools, but they address different kinds of uncertainty—Gelbach focuses on observable confounding, while Oster extends this to unobservable confounding.\n\n### Why Coefficient Stability Alone Can Mislead\n\nBoth papers caution against over-interpreting coefficient stability on its own:\n\n- Gelbach shows that apparent robustness may depend on which controls you added and in what order.\n\n- Oster shows that stability without $R^2$ movement is meaningless—if your controls aren’t explaining much of the outcome, their failure to shift the coefficient tells you little.\n\nOster’s illustrative example of wage returns to education highlights this issue clearly: if you add a weak control (say, a poor proxy for ability), the coefficient on education may appear stable, but that doesn’t mean there’s no bias—it just means your control wasn’t very good.\n\n## An Example\n\n:::: {.panel-tabset}\n\n### R\n\n```r\nlibrary(hdm)\nlibrary(oaxaca)\n\n# Simulated data\ndf <- data.frame(\n  y = rnorm(1000),\n  x = rnorm(1000),\n  z1 = rnorm(1000),\n  z2 = rnorm(1000)\n)\n\n# Short model\nshort <- lm(y ~ x, data = df)\n\n# Long model\nlong <- lm(y ~ x + z1 + z2, data = df)\n\n# Gelbach decomposition using Oaxaca-Blinder (as approximation)\nlibrary(oaxaca)\noaxaca(y ~ x | z1 + z2, data = df, R = 30)\n\n# Oster bounds (simplified)\nlibrary(psacalc)\npsa(y = df$y, d = df$x, X = df[, c(\"z1\", \"z2\")], R2max = 0.9)\n```\n\n### Python\n\n```python\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Simulated data\ndf = pd.DataFrame({\n    'y': np.random.randn(1000),\n    'x': np.random.randn(1000),\n    'z1': np.random.randn(1000),\n    'z2': np.random.randn(1000)\n})\n\n# Short model\nX_short = sm.add_constant(df['x'])\nmodel_short = sm.OLS(df['y'], X_short).fit()\n\n# Long model\nX_long = sm.add_constant(df[['x', 'z1', 'z2']])\nmodel_long = sm.OLS(df['y'], X_long).fit()\n\nprint(model_short.params)\nprint(model_long.params)\n\n# No direct analog to Gelbach in Python, but differences in coefficients can be manually computed.\n```\n\n::::\n\n## Bottom Line\n\n- Gelbach decomposition explains *why* coefficients change when you add covariates.\n\n- Oster’s method helps you assess how robust your findings are to *unobserved* variables.\n\n- Both approaches require strong assumptions—but when applied carefully, they offer insight into what’s driving your regression results.\n\n- Don't blindly include controls—use tools like these to make sense of what they’re doing to your estimates.\n\n## Where to Learn More\n\nIf you're into causal inference and want to go beyond just “throwing in controls,” both these papers are must-reads. For applied guidance, check out Emily Oster’s book *“Uncontrolled”* and Guido Imbens' lecture notes on sensitivity analysis. You might also explore sensitivity tools like `sensemakr` in R, which is built around similar ideas to Oster's method.\n\n## References\n\n- Gelbach, J. B. (2016). When do covariates matter? And which ones, and how much? *Journal of Labor Economics*, 34(2), 509–543.\n\n- Oster, E. (2019). Unobservable selection and coefficient stability: Theory and evidence. *Journal of Business & Economic Statistics*, 37(2), 187–204.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"how-much-controls-matter.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster's δ Method","date":"2025-00-00","categories":["causal inference","regression"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}