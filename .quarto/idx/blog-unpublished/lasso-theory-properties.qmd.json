{"title":"Theoretical Properties of Lasso","markdown":{"yaml":{"title":"Theoretical Properties of Lasso","date":"2025-00-00","categories":["lasso","statitical inference"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nLasso (Least Absolute Shrinkage and Selection Operator) is widely used not only for its variable selection ability but also for its appealing theoretical properties in high-dimensional regression. Over the past two decades, a rich literature has characterized the behavior of Lasso estimators under various conditions — especially when the number of predictors $p$ may exceed the number of observations $n$.\n\nThis article walks through the key theoretical properties of Lasso, explaining what they mean, when they hold, and why they matter for estimation, prediction, and inference.\n\n## A Closer Look\n\n### 1. Convergence Rates (Estimation Consistency)\n\nUnder sparsity and restricted eigenvalue conditions:\n$$\n\\| \\hat{\\beta} - \\beta_0 \\|_1 = O_p\\left( s \\sqrt{ \\frac{\\log p}{n} } \\right),\n$$\nwhere $s$ is the sparsity level (number of true nonzero coefficients).\n\nPrediction error:\n$$\n\\| X ( \\hat{\\beta} - \\beta_0 ) \\|_2^2 / n = O_p\\left( s \\frac{\\log p}{n} \\right).\n$$\nThis is near-oracle optimal up to log factors.\n\n### 2. Support Recovery (Sparsistency)\n\nLasso can recover the correct set of nonzero coefficients with high probability (sparsistency) if:\n- The **irrepresentable condition** holds (Zhao and Yu, 2006),\n- Signal strength is sufficiently large.\n\nOtherwise, Lasso may fail to perfectly select the true model, especially under high correlations or weak signals.\n\n### 3. Prediction Consistency\n\nLasso can achieve prediction consistency:\n$$\n\\mathbb{E} \\left[ \\| X (\\hat{\\beta} - \\beta_0) \\|_2^2 \\right] \\to 0 \\quad \\text{as} \\quad n \\to \\infty,\n$$\neven if exact support recovery fails.\n\n### 4. Oracle Inequalities\n\nLasso satisfies oracle inequalities comparing its performance to an ideal estimator that knows the true support:\n$$\n\\| X (\\hat{\\beta} - \\beta_0) \\|_2^2 / n \\leq C \\cdot \\inf_{\\beta: \\| \\beta \\|_0 \\leq s} \\left\\{ \\| X (\\beta - \\beta_0) \\|_2^2 / n + \\lambda^2 s \\right\\}.\n$$\n\n### 5. Bias of the Estimates\n\nLasso is biased due to the L1 penalty. The bias remains unless the penalty $\\lambda$ shrinks appropriately with $n$.\n\nBias motivates the use of:\n- Post-Lasso OLS,\n- Relaxed Lasso,\n- Adaptive Lasso,\n- Debiased Lasso.\n\n### 6. Asymptotic Normality (Debiased Lasso)\n\nStandard Lasso does not yield asymptotically normal estimates. However, **Debiased (Desparsified) Lasso** allows for valid inference:\n$$\n\\sqrt{n} (\\hat{\\beta}_j^{\\text{debiased}} - \\beta_{0j}) \\overset{d}{\\to} N(0, \\sigma_j^2).\n$$\n\n### 7. Valid Inference and Confidence Intervals\n\nValid $p$-values and confidence intervals require adjustments:\n- Debiased Lasso,\n- Selective inference (e.g., Lee et al., 2016),\n- Double selection (Belloni et al., 2014).\n\n### 8. Double Robustness and Semi-parametric Efficiency\n\nIn causal inference settings (e.g., DML), Lasso can estimate high-dimensional nuisance components, yielding:\n- $\\sqrt{n}$-consistent estimates for low-dimensional targets,\n- Asymptotic normality under orthogonality and cross-fitting.\n\n## Summary Table\n\n| Property                    | Achieved by Lasso?               | Conditions Required                             |\n|-----------------------------|----------------------------------|--------------------------------------------------|\n| Convergence rates (L1/L2)    | Yes, near-oracle (up to log factor) | Sparsity, restricted eigenvalues                 |\n| Support recovery (sparsistency) | Sometimes (hard in correlated designs) | Irrepresentable condition or compatibility     |\n| Prediction consistency       | Yes                             | Restricted eigenvalues, compatibility            |\n| Oracle inequality            | Yes                             | Standard sparsity assumptions                    |\n| Bias                         | Yes (biased toward zero)        | Bias remains unless corrected                    |\n| Asymptotic normality         | No (unless debiased)            | Debiasing, desparsification required             |\n| Valid inference (CI, $p$-values) | Not directly (needs debiased Lasso) | Debiased Lasso, post-selection inference         |\n| Double robustness / efficiency | Only when combined (e.g., DML)  | Orthogonal scores, cross-fitting                 |\n\n## Bottom Line\n\n- Lasso achieves strong theoretical guarantees for estimation and prediction under sparsity.\n\n- Support recovery requires stringent conditions, and Lasso may miss variables under collinearity.\n\n- Bias correction techniques like debiasing or post-selection OLS help achieve valid inference.\n\n- Double/debiased machine learning and orthogonalization frameworks leverage Lasso for high-dimensional nuisance estimation.\n\n## Where to Learn More\n\nKey papers include Tibshirani (1996), Zhao and Yu (2006), Bickel et al. (2009), van de Geer et al. (2014), Javanmard and Montanari (2014), and Belloni et al. (2014). For debiased methods and selective inference, see recent reviews by Bühlmann and van de Geer.\n\n## References\n\n[TO ADD]\n\n","srcMarkdownNoYaml":"\n\n## Background\n\nLasso (Least Absolute Shrinkage and Selection Operator) is widely used not only for its variable selection ability but also for its appealing theoretical properties in high-dimensional regression. Over the past two decades, a rich literature has characterized the behavior of Lasso estimators under various conditions — especially when the number of predictors $p$ may exceed the number of observations $n$.\n\nThis article walks through the key theoretical properties of Lasso, explaining what they mean, when they hold, and why they matter for estimation, prediction, and inference.\n\n## A Closer Look\n\n### 1. Convergence Rates (Estimation Consistency)\n\nUnder sparsity and restricted eigenvalue conditions:\n$$\n\\| \\hat{\\beta} - \\beta_0 \\|_1 = O_p\\left( s \\sqrt{ \\frac{\\log p}{n} } \\right),\n$$\nwhere $s$ is the sparsity level (number of true nonzero coefficients).\n\nPrediction error:\n$$\n\\| X ( \\hat{\\beta} - \\beta_0 ) \\|_2^2 / n = O_p\\left( s \\frac{\\log p}{n} \\right).\n$$\nThis is near-oracle optimal up to log factors.\n\n### 2. Support Recovery (Sparsistency)\n\nLasso can recover the correct set of nonzero coefficients with high probability (sparsistency) if:\n- The **irrepresentable condition** holds (Zhao and Yu, 2006),\n- Signal strength is sufficiently large.\n\nOtherwise, Lasso may fail to perfectly select the true model, especially under high correlations or weak signals.\n\n### 3. Prediction Consistency\n\nLasso can achieve prediction consistency:\n$$\n\\mathbb{E} \\left[ \\| X (\\hat{\\beta} - \\beta_0) \\|_2^2 \\right] \\to 0 \\quad \\text{as} \\quad n \\to \\infty,\n$$\neven if exact support recovery fails.\n\n### 4. Oracle Inequalities\n\nLasso satisfies oracle inequalities comparing its performance to an ideal estimator that knows the true support:\n$$\n\\| X (\\hat{\\beta} - \\beta_0) \\|_2^2 / n \\leq C \\cdot \\inf_{\\beta: \\| \\beta \\|_0 \\leq s} \\left\\{ \\| X (\\beta - \\beta_0) \\|_2^2 / n + \\lambda^2 s \\right\\}.\n$$\n\n### 5. Bias of the Estimates\n\nLasso is biased due to the L1 penalty. The bias remains unless the penalty $\\lambda$ shrinks appropriately with $n$.\n\nBias motivates the use of:\n- Post-Lasso OLS,\n- Relaxed Lasso,\n- Adaptive Lasso,\n- Debiased Lasso.\n\n### 6. Asymptotic Normality (Debiased Lasso)\n\nStandard Lasso does not yield asymptotically normal estimates. However, **Debiased (Desparsified) Lasso** allows for valid inference:\n$$\n\\sqrt{n} (\\hat{\\beta}_j^{\\text{debiased}} - \\beta_{0j}) \\overset{d}{\\to} N(0, \\sigma_j^2).\n$$\n\n### 7. Valid Inference and Confidence Intervals\n\nValid $p$-values and confidence intervals require adjustments:\n- Debiased Lasso,\n- Selective inference (e.g., Lee et al., 2016),\n- Double selection (Belloni et al., 2014).\n\n### 8. Double Robustness and Semi-parametric Efficiency\n\nIn causal inference settings (e.g., DML), Lasso can estimate high-dimensional nuisance components, yielding:\n- $\\sqrt{n}$-consistent estimates for low-dimensional targets,\n- Asymptotic normality under orthogonality and cross-fitting.\n\n## Summary Table\n\n| Property                    | Achieved by Lasso?               | Conditions Required                             |\n|-----------------------------|----------------------------------|--------------------------------------------------|\n| Convergence rates (L1/L2)    | Yes, near-oracle (up to log factor) | Sparsity, restricted eigenvalues                 |\n| Support recovery (sparsistency) | Sometimes (hard in correlated designs) | Irrepresentable condition or compatibility     |\n| Prediction consistency       | Yes                             | Restricted eigenvalues, compatibility            |\n| Oracle inequality            | Yes                             | Standard sparsity assumptions                    |\n| Bias                         | Yes (biased toward zero)        | Bias remains unless corrected                    |\n| Asymptotic normality         | No (unless debiased)            | Debiasing, desparsification required             |\n| Valid inference (CI, $p$-values) | Not directly (needs debiased Lasso) | Debiased Lasso, post-selection inference         |\n| Double robustness / efficiency | Only when combined (e.g., DML)  | Orthogonal scores, cross-fitting                 |\n\n## Bottom Line\n\n- Lasso achieves strong theoretical guarantees for estimation and prediction under sparsity.\n\n- Support recovery requires stringent conditions, and Lasso may miss variables under collinearity.\n\n- Bias correction techniques like debiasing or post-selection OLS help achieve valid inference.\n\n- Double/debiased machine learning and orthogonalization frameworks leverage Lasso for high-dimensional nuisance estimation.\n\n## Where to Learn More\n\nKey papers include Tibshirani (1996), Zhao and Yu (2006), Bickel et al. (2009), van de Geer et al. (2014), Javanmard and Montanari (2014), and Belloni et al. (2014). For debiased methods and selective inference, see recent reviews by Bühlmann and van de Geer.\n\n## References\n\n[TO ADD]\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"lasso-theory-properties.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Theoretical Properties of Lasso","date":"2025-00-00","categories":["lasso","statitical inference"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}