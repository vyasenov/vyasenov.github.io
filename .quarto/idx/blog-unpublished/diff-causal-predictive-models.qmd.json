{"title":"Causal vs. Predictive Modeling: Why the Difference Matters More Than You Think","markdown":{"yaml":{"title":"Causal vs. Predictive Modeling: Why the Difference Matters More Than You Think","date":"2025-04-24","categories":["causality","machine-learning"]},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nIt’s one of the most common mix-ups I see among data scientists—especially those coming from a machine learning background: confusing causal modeling with predictive modeling. On the surface, they look similar. You build a model, you include some variables, you fit it, and then you do... something with the results. But under the hood, these two approaches serve fundamentally different goals and require very different mindsets.\n\nPredictive modeling is about building a machine that can forecast outcomes. Causal modeling is about understanding how the world works. And mixing them up can lead to some really bad decisions—like launching a product based on a spurious correlation or controlling for the wrong variables and wiping out your treatment effect.\n\nThis post is for all the data scientists who’ve ever wondered, “Why can’t I just throw everything into my causal model like I do with my random forest?” Let’s unpack it.\n\n## Notation\n\nLet’s fix some notation for clarity. Suppose we have:\n\n- $Y$: the outcome\n- $T$: a treatment or exposure variable\n- $X$: a set of covariates (features)\n\nIn **predictive modeling**, the goal is to estimate:\n\n$$\n\\mathbb{E}[Y \\mid X]\n$$\n\nIn **causal modeling**, we care about quantities like:\n\n$$\n\\mathbb{E}[Y(1) - Y(0)]\n$$\n\nwhere $Y(1)$ is the potential outcome under treatment, and $Y(0)$ is the outcome under control. This is the average treatment effect (ATE). The tricky part? We never observe both $Y(1)$ and $Y(0)$ for the same unit.\n\nSo causal inference becomes a game of counterfactuals—and that’s where all the complexity comes in.\n\n## A Closer Look\n\n### Predictive Modeling: Just Give Me Accuracy\n\nLet’s start with what most machine learning folks are familiar with: predictive models.\n\nIn predictive modeling, you’re judged by how well you can forecast $Y$. That’s it. You can (and often do) throw in everything and the kitchen sink—lagged outcomes, future values of other variables (careful though!), variables that are correlated with the outcome but not necessarily meaningful in a causal sense.\n\nIt’s all good *as long as* it helps you reduce RMSE, increase AUC, or minimize cross-entropy loss. Data leakage is your main enemy, but otherwise, the bar for “what goes in the model” is pretty low.\n\nNo one cares *why* your model works, only that it does.\n\n### Causal Modeling: Think Harder, Control Smarter\n\nNow, enter the world of causal inference. The rules are completely different.\n\nIn causal modeling, the goal is not prediction, but isolation of the effect of $T$ on $Y$. And to do that, you need to control for confounders—variables that affect both the treatment and the outcome. But here's the catch: **not all variables should be controlled for**.\n\nThis is where the concept of **bad controls** comes in—variables that are affected by the treatment (post-treatment variables), or colliders that open up backdoor paths and induce spurious associations.\n\nIn other words, in causal inference:\n\n- **Including the wrong variable can make things worse.**\n- **You must think hard about the causal structure of your data.**\n- **Domain knowledge is critical.**\n\nThrowing in “everything” like in a predictive model? That can completely destroy your estimate.\n\n### Propensity Scores: Where Predictive and Causal Worlds Collide\n\nOne place where this confusion often plays out is in propensity score modeling.\n\nTo recap, the propensity score $e(X) = P(T = 1 \\mid X)$ is the probability of receiving treatment given covariates. It’s often estimated via a logistic regression or ML model. Then, you use this score to adjust for differences between treated and control groups (e.g., via weighting or matching).\n\nAnd here’s the key point: **your goal is not to get the best prediction of treatment.** Your goal is to use the propensity score to balance covariates between groups. That’s it.\n\nSo even if a fancy XGBoost model gives you higher prediction accuracy, it may overfit or fail to achieve covariate balance—which defeats the purpose. In fact, some of the best-performing PS models (for causal purposes) may have terrible predictive accuracy but excel at achieving balance.\n\nThere’s a trade-off here:\n\n- Predictive ML models focus on minimizing error.\n- Propensity score models should optimize **covariate balance**.\n\nAnd that trade-off is why a more accurate model is not necessarily better for causal inference.\n\n## Bottom Line\n\n- Predictive models are about forecasting outcomes; causal models are about estimating effects.\n\n- In causal inference, you must think carefully about what to include in the model—“bad controls” can bias results.\n\n- Propensity scores should be judged by how well they balance covariates, not by how well they predict treatment.\n\n- More context and domain knowledge is usually required for causal models than for predictive ones.\n\n## Where to Learn More\n\nFor causal inference, the gold standard is *Causal Inference: The Mixtape* by Scott Cunningham or *Mostly Harmless Econometrics* by Angrist and Pischke. For a more technical treatment, check out Hernán and Robins’ *Causal Inference*. Judea Pearl’s *Book of Why* adds more philosophical background. For those working with propensity scores specifically, papers by Peter Austin and Elizabeth Stuart are a great starting point. If you're trying to navigate the blurry line between prediction and causation, Andrew Gelman’s blog has a wealth of insights too.\n\n## References (if applicable)\n\nHernán, M. A., & Robins, J. M. (2020). *Causal Inference: What If*. \n\nCunningham, S. (2021). *Causal Inference: The Mixtape*.  \n\nAngrist, J. D., & Pischke, J.-S. (2009). *Mostly Harmless Econometrics*.  \n\nPearl, J., & Mackenzie, D. (2018). *The Book of Why: The New Science of Cause and Effect*.\n","srcMarkdownNoYaml":"\n\n## Background\n\nIt’s one of the most common mix-ups I see among data scientists—especially those coming from a machine learning background: confusing causal modeling with predictive modeling. On the surface, they look similar. You build a model, you include some variables, you fit it, and then you do... something with the results. But under the hood, these two approaches serve fundamentally different goals and require very different mindsets.\n\nPredictive modeling is about building a machine that can forecast outcomes. Causal modeling is about understanding how the world works. And mixing them up can lead to some really bad decisions—like launching a product based on a spurious correlation or controlling for the wrong variables and wiping out your treatment effect.\n\nThis post is for all the data scientists who’ve ever wondered, “Why can’t I just throw everything into my causal model like I do with my random forest?” Let’s unpack it.\n\n## Notation\n\nLet’s fix some notation for clarity. Suppose we have:\n\n- $Y$: the outcome\n- $T$: a treatment or exposure variable\n- $X$: a set of covariates (features)\n\nIn **predictive modeling**, the goal is to estimate:\n\n$$\n\\mathbb{E}[Y \\mid X]\n$$\n\nIn **causal modeling**, we care about quantities like:\n\n$$\n\\mathbb{E}[Y(1) - Y(0)]\n$$\n\nwhere $Y(1)$ is the potential outcome under treatment, and $Y(0)$ is the outcome under control. This is the average treatment effect (ATE). The tricky part? We never observe both $Y(1)$ and $Y(0)$ for the same unit.\n\nSo causal inference becomes a game of counterfactuals—and that’s where all the complexity comes in.\n\n## A Closer Look\n\n### Predictive Modeling: Just Give Me Accuracy\n\nLet’s start with what most machine learning folks are familiar with: predictive models.\n\nIn predictive modeling, you’re judged by how well you can forecast $Y$. That’s it. You can (and often do) throw in everything and the kitchen sink—lagged outcomes, future values of other variables (careful though!), variables that are correlated with the outcome but not necessarily meaningful in a causal sense.\n\nIt’s all good *as long as* it helps you reduce RMSE, increase AUC, or minimize cross-entropy loss. Data leakage is your main enemy, but otherwise, the bar for “what goes in the model” is pretty low.\n\nNo one cares *why* your model works, only that it does.\n\n### Causal Modeling: Think Harder, Control Smarter\n\nNow, enter the world of causal inference. The rules are completely different.\n\nIn causal modeling, the goal is not prediction, but isolation of the effect of $T$ on $Y$. And to do that, you need to control for confounders—variables that affect both the treatment and the outcome. But here's the catch: **not all variables should be controlled for**.\n\nThis is where the concept of **bad controls** comes in—variables that are affected by the treatment (post-treatment variables), or colliders that open up backdoor paths and induce spurious associations.\n\nIn other words, in causal inference:\n\n- **Including the wrong variable can make things worse.**\n- **You must think hard about the causal structure of your data.**\n- **Domain knowledge is critical.**\n\nThrowing in “everything” like in a predictive model? That can completely destroy your estimate.\n\n### Propensity Scores: Where Predictive and Causal Worlds Collide\n\nOne place where this confusion often plays out is in propensity score modeling.\n\nTo recap, the propensity score $e(X) = P(T = 1 \\mid X)$ is the probability of receiving treatment given covariates. It’s often estimated via a logistic regression or ML model. Then, you use this score to adjust for differences between treated and control groups (e.g., via weighting or matching).\n\nAnd here’s the key point: **your goal is not to get the best prediction of treatment.** Your goal is to use the propensity score to balance covariates between groups. That’s it.\n\nSo even if a fancy XGBoost model gives you higher prediction accuracy, it may overfit or fail to achieve covariate balance—which defeats the purpose. In fact, some of the best-performing PS models (for causal purposes) may have terrible predictive accuracy but excel at achieving balance.\n\nThere’s a trade-off here:\n\n- Predictive ML models focus on minimizing error.\n- Propensity score models should optimize **covariate balance**.\n\nAnd that trade-off is why a more accurate model is not necessarily better for causal inference.\n\n## Bottom Line\n\n- Predictive models are about forecasting outcomes; causal models are about estimating effects.\n\n- In causal inference, you must think carefully about what to include in the model—“bad controls” can bias results.\n\n- Propensity scores should be judged by how well they balance covariates, not by how well they predict treatment.\n\n- More context and domain knowledge is usually required for causal models than for predictive ones.\n\n## Where to Learn More\n\nFor causal inference, the gold standard is *Causal Inference: The Mixtape* by Scott Cunningham or *Mostly Harmless Econometrics* by Angrist and Pischke. For a more technical treatment, check out Hernán and Robins’ *Causal Inference*. Judea Pearl’s *Book of Why* adds more philosophical background. For those working with propensity scores specifically, papers by Peter Austin and Elizabeth Stuart are a great starting point. If you're trying to navigate the blurry line between prediction and causation, Andrew Gelman’s blog has a wealth of insights too.\n\n## References (if applicable)\n\nHernán, M. A., & Robins, J. M. (2020). *Causal Inference: What If*. \n\nCunningham, S. (2021). *Causal Inference: The Mixtape*.  \n\nAngrist, J. D., & Pischke, J.-S. (2009). *Mostly Harmless Econometrics*.  \n\nPearl, J., & Mackenzie, D. (2018). *The Book of Why: The New Science of Cause and Effect*.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../code/styles.css"],"toc":true,"include-in-header":[{"text":"<script src=\"../code/open-links-new-tab.js\"></script>\n"}],"output-file":"diff-causal-predictive-models.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","resources":["../code/open-links-new-tab.js"],"theme":{"light":"cosmo","dark":"cyborg"},"page-layout":"full","title":"Causal vs. Predictive Modeling: Why the Difference Matters More Than You Think","date":"2025-04-24","categories":["causality","machine-learning"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}