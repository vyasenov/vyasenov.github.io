[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Staff Data Scientist, Adobe (2021 – )\nData Scientist, Immigration Policy Lab, Stanford University (2020 – )\nConsultant, Ministry of Employment, Copenhagen, Denmark (2017)\nResearch Intern, Ministry of Labor and Social Policy, Sofia, Bulgaria (2014)"
  },
  {
    "objectID": "cv.html#employment",
    "href": "cv.html#employment",
    "title": "CV",
    "section": "",
    "text": "Staff Data Scientist, Adobe (2021 – )\nData Scientist, Immigration Policy Lab, Stanford University (2020 – )\nConsultant, Ministry of Employment, Copenhagen, Denmark (2017)\nResearch Intern, Ministry of Labor and Social Policy, Sofia, Bulgaria (2014)"
  },
  {
    "objectID": "cv.html#education-training",
    "href": "cv.html#education-training",
    "title": "CV",
    "section": "Education / Training",
    "text": "Education / Training\n\nPostdoctoral Fellow, Immigration Policy Lab, Stanford University (2018 – 2020)\nPostdoctoral Fellow, Goldman School of Public Policy, UC Berkeley (2017 – 2018)\nPh.D. Economics, UC Davis (2017)\nM.A. Economics, UC Davis (2013)\nB.S. Mathematics-Economics, UC San Diego (magna cum laude) (2012)\nH.S. Diploma, “Св. Княз Борис I”, Asenovgrad, Bulgaria (2007)"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "CV",
    "section": "Teaching",
    "text": "Teaching\nI have taught lectures on applications of quasi-experimental methods at:\n\nYale University (2021)\nStanford University (2020, 2021)\nUC Berkeley (2018)\nUC Davis (2017)"
  },
  {
    "objectID": "cv.html#awards",
    "href": "cv.html#awards",
    "title": "CV",
    "section": "Awards",
    "text": "Awards\n\nUS 2050 Initiative Grant, Peter G. Peterson Foundation, and Ford Foundation (2018)\nEarly Career Research Grant, W.E. Upjohn Institute for Employment Research (2018)\nRay Beaumont Memorial Award, Economics Department, UC Davis (2017)\nExcellence in Joint Mathematics-Economics Major Award Nominee, UC San Diego (2012)"
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#background",
    "href": "blog/flavors-correlation-coef.html#background",
    "title": "Nonlinear Correlations and Chatterjee’s Coefficient",
    "section": "Background",
    "text": "Background\nMuch of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford published a paper outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\nIn this article I will go over the basics of Chatterjee’s correlation measure. Before we get there, let’s first review some of the more traditional approaches in assessing bivariate relationship strength.\nFor simplicity, let’s assume away ties. Let’s also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#a-closer-look",
    "href": "blog/flavors-correlation-coef.html#a-closer-look",
    "title": "Nonlinear Correlations and Chatterjee’s Coefficient",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLinear Relationships\nWhen we simply say “correlation” we refer to the so-called Pearson correlation coefficient. Virtually everyone working with data is familiar with it. Given a random sample \\((X_1,Y_1),\\dots,(X_n, Y_n)\\) of two random variables \\(X\\) and \\(Y\\) it is computed by:\n\\[corr^{P}(X,Y) = \\frac{ \\sum_i(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}},\\]\nwhere an upper bar denotes a sample mean.\nThis coefficient lives in the \\([-1,1]\\) interval. Larger values indicate stronger relationship between \\(X\\) and \\(Y\\), be it positive or negative. At the extreme, the Pearson coefficient will equal \\(1\\) when all observations can be perfectly lined up on a upward sloping line. Yes, you guessed it – when it equals \\(-1\\) the line is sloping down.\nYou can easily calculate the Pearson correlation in R and python:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'pearson', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr\n\npearson_corr, pearson_pval = pearsonr(x, y)\n\n\n\nThis measure, while widely popular, suffers from a few shortcomings. First, outliers have an outsized impact in skewing its value. Sample means vulnerable to outliers are a key ingredient in the calculation, rendering the measure sensitive to data anomalies. Second, it is designed to detect only linear relationships. Two variables might have a strong but non-linear relationship which this measure will not detect. Lastly, it is not transformation-invariant, meaning that applying a monotone transformation to either of the variables will change the correlation value.\nLet’s discuss some improvements to the Pearson correlation measure. Enter Spearman correlation.\n\n\nMonotone Relationships\nSpearman correlation is the Pearson correlation among the ranks of \\(X\\) and \\(Y\\):\n\\[corr^{S}(X,Y) = corr^{P}(R(X),R(Y)),\\]\nwhere \\(R(\\cdot)\\) denotes an observation’s rank (or order) in the sample.\nSpearman correlation is thus a rank correlation measure. As such, it quantifies how well the relationship between \\(X\\) and \\(Y\\) can be described using a monotone (and not necessarily a linear) function. It is therefore a more flexible measure of association. Again, intuitively, Spearman correlation will take on a large positive value when the \\(X\\) and \\(Y\\) observations have similar ranks. This value will be negative when the ranks tend to go in opposite directions.\nSpearman correlation addresses some of the shortcomings associated with Pearson correlation. It is not easily influenced by outliers, and it does not change if we apply a monotone transformation of \\(X\\) and/or \\(Y\\). These benefits come at the expense of a loss in interpretation and potential issues when tied ranks are common, a scenario I ignore here.\nCalculating it is just as simple:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import spearmanr\n\nspearman_corr, spearman_pval = spearmanr(x, y)\n\n\n\nSpearman correlation is not the only rank correlation coefficient out there. А popular alternative is the Kendall rank coefficient which is computed slightly differently. Let’s define a pair of observations \\((X_i, Y_i)\\) and \\((X_j, Y_j)\\) to be agreeing (the technical term is concordant) if the differences \\((X_i - X_j)\\) and \\((Y_i - Y_j)\\) have the same sign (i.e., either both \\(X_i &gt; X_j\\) and $Y_i &gt; Y_j $or both \\(X_i &lt; X_j\\) and \\(Y_i &lt; Y_j\\) ).\nThen, Kendall’s coefficient is expressed as:\n\\[corr^{K} = \\frac{\\text{number of agreeing pairs} - \\text{number of disagreeing pairs}} {\\text{total number of pairs}}.\\]\nSo, it quantifies the degree of agreement between the ranks of \\(X\\) and \\(Y\\). Like the other coeffients described above, its range is \\([-1, 1]\\) and values away from zero indicate stronger relationship.\nAgain, this coefficient is similarly computed in R and python:\n\nRPython\n\n\ncor(x,y, method = 'kendall')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\nkendall_corr, kendall_pval = kendalltau(x, y)\n\n\n\nKendall’s measure improves on some of the shortcomings baked in the Spearman’s coefficients – it has a clearer interpretation and it is less sensitive to rank ties.\nHowever, none of these rank correlation coefficients can detect non-monotonic relationships. For instance, \\(X\\) and \\(Y\\) can have a parabola- or wave-like pattern when plotted against each other. We would like a correlation measure flexible enough to capture such non-linear relationships.\nThis is where Chatterjee’s coefficient comes in.\n\n\nMore General Relationships\nChatterjee recently proposed a new correlation coefficient designed to detect non-monotonic relationships. He discovered a novel estimator of a population quantity first proposed by Dette et al. (2013).\nLet’s start with the formula. The Chatterjee correlation coefficient is calculated as follows:\n\\[corr^{C}(X,Y) = 1-\\frac{3\\sum_{i=1}^{n-1}|R(Y_{k:R(X_k)=i+1}) - R(Y_{k:R(X_k)=i})|}{n^2-1},\\]\nwhere n is the sample size. This looks complicated, so let’s try to simplify the numerator. Let’s sort the data in an ascending order of \\(X\\) so that we have \\((X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})\\), where \\(X_{(1)}&lt;X_{(2)}&lt;\\dots &lt;X_{(n)}\\). Also, denote \\(R(Y_i)\\) be the rank of \\(Y_{(i)}\\). Then:\n\\[corr^{C}(X,Y) = 1 - \\frac{3\\sum_{i=1}^{n-1}|R(Y_{i+1})-R(Y_i)|}{n^2-1}.\\]\nSo, this new coefficient is a scaled version of the sum of the absolute differences in the consecutive ranks of \\(Y\\) when ordered by \\(X\\). It is perhaps best to think about Chatterjee’s method as a measure of dependence and not strictly a correlation coefficient.\nThere are some major difference compared to the previous correlation measures. Chatterjee’s correlation coefficient lies in the \\([0,1]\\) interval. It is equal to zero if and only if \\(X\\) and \\(Y\\) are independent and to one if one of them is a function of the other. Unlike the coefficients described above, it is not symmetric in \\(X\\) and \\(Y\\), meaning \\(corr^{C}(X,Y) \\neq corr^{C}(Y,X)\\). This is understandable since we are interested in whether \\(X\\) is a function of \\(Y\\), which does not imply the opposite. The author also develops asymptotic theory for calculating p-values although some researchers have raised concerns about the coefficient’s power.\nHere is a sample code to calculate its value:\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\nn &lt;- 1000\nx &lt;- runif(n) \ny &lt;- 5 * sin(x) + rnorm(n)\n\ndata &lt;- data.frame(x=x, y=y)\ndata$R &lt;- rank(data$y)\ndata &lt;- data[order(data$x), ]\n\n1 - 3 * sum(abs(diff(data$R))) / (n^2-1)\n&gt;[1] 0.4093024\n\n\nimport numpy as np\n\nnp.random.seed(1988)\nn = 1000\nx = np.random.uniform(size=n)\ny = 5 * np.sin(x) + np.random.normal(size=n)\n\ndata = np.array(sorted(zip(x, y), key=lambda pair: pair[0]))\nranks = np.argsort(np.argsort(data[:, 1]))  # Rank of y\nchatterjee_corr = 1 - 3 * np.sum(np.abs(np.diff(ranks))) / (n**2 - 1)\nprint(f\"Chatterjee's correlation: {chatterjee_corr:.4f}\")\n&gt; Chatterjee's correlation: 0.4050\n\n\n\nSoftware Package: XICOR.\nThere you have it. You are now well-equipped to dive deeper into your datasets and find new exciting relationships."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#bottom-line",
    "href": "blog/flavors-correlation-coef.html#bottom-line",
    "title": "Nonlinear Correlations and Chatterjee’s Coefficient",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThere are numerous ways of measuring association between two variables.\nThe most common methods measure only linear or monotonic relationships. These are often useful but do not capture more complex, non-linear associations.\nA new correlation measure, Chatterjee’s coefficient, is designed to go beyond monotonicity and assess more general bivariate relationships."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#where-to-learn-more",
    "href": "blog/flavors-correlation-coef.html#where-to-learn-more",
    "title": "Nonlinear Correlations and Chatterjee’s Coefficient",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia has detailed entries on correlation, rank correlation, and Kendall’s coefficient which I found helpful. The R bloggers platform has articles exploring the Chatterjee’s correlation coefficient in detail. The more technically oriented folks will find Chatterjee’s original paper helpful."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#references",
    "href": "blog/flavors-correlation-coef.html#references",
    "title": "Nonlinear Correlations and Chatterjee’s Coefficient",
    "section": "References",
    "text": "References\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\nDette, H., Siburg, K. F., & Stoimenov, P. A. (2013). A Copula‐Based Non‐parametric Measure of Regression Dependence. Scandinavian Journal of Statistics, 40(1), 21-41.\nShi, H., Drton, M., & Han, F. (2022). On the power of Chatterjee’s rank correlation. Biometrika, 109(2), 317-333.\nhttps://www.r-bloggers.com/2021/12/exploring-the-xi-correlation-coefficient/"
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#background",
    "href": "blog/filling-missing-data-mcmc.html#background",
    "title": "Filling in Missing Data with MCMC",
    "section": "Background",
    "text": "Background\nEvery dataset inevitably contains missing or incomplete values. Practitioners then face the dilemma of how to address these missing observations. A common approach, though potentially problematic, is to simply ignore them. I am guilty of doing this all too often. While convenient, ignoring missing data can introduce bias into analyses, particularly if the missingness is not entirely random. Moreover, throwing away data usually results in loss of statistical precision. Traditional methods for handling missing data, such as mean or median imputation, usually oversimplify the underlying data-generating process. Regression-based adjustments offer some improvement, but they rely on the linearity assumption.\nThis article introduces Markov Chain Monte Carlo (MCMC) as a robust and theoretically sound methodology for addressing missing data. Unlike arbitrary imputation methods, MCMC leverages the inherent information within the dataset to generate plausible values for the missing observations. The core principle of MCMC involves treating missing data as random variables and employing the algorithm to sample from their posterior distribution, thereby capturing the uncertainty and built-in structure within the data. The use of MCMC to draw observations repeatedly in the context of missing data is often referred to as Multiple Imputation (MI).\nLet’s break this down step by step, explore the underlying intuition, as well as an illustrative example, assuming a basic understanding of probability theory and Bayesian methods."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#notation",
    "href": "blog/filling-missing-data-mcmc.html#notation",
    "title": "Filling in Missing Data with MCMC",
    "section": "Notation",
    "text": "Notation\nTo keep things precise, let’s set up some notation. Let Y be the complete dataset, which we wish we had. We observe some, but not all observations of Y. Let’s split it into observed data \\(Y_{\\text{obs}}\\), and missing (or incomplete) data \\(Y_{\\text{miss}}\\), so that \\(Y = \\left( Y_{\\text{obs}}, Y_{\\text{miss}} \\right)\\).\nAssume a model for the data parameterized by \\(\\theta\\), that is \\(Y \\sim f(Y \\mid \\theta)\\). A simple example would be that a univariate \\(Y\\) is Gaussian with some unspecified mean and variance. Our goal is to estimate and fill in \\(Y_{\\text{miss}}\\) by sampling from the posterior distribution\n\\[P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta).\\]\nThe results and intuition hold also conditional on some covariates \\(X\\), but for simplicity’s sake, I will keep that out of the notation for now."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#a-closer-look",
    "href": "blog/filling-missing-data-mcmc.html#a-closer-look",
    "title": "Filling in Missing Data with MCMC",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nMarkov Chain Monte Carlo is a powerful computational technique designed to draw observations from complex probability distributions that are difficult to directly sample from. This might happen because they do not have a nice closed-form analytical expression, or they do, but it’s too messy. Such distributions often arise in Bayesian statistics, where we aim to estimate the posterior distribution of parameters given observed data.\nThe “magic” of MCMC lies in its iterative nature. It begins with an initial guess for the parameter \\(\\theta\\). Then, a sophisticated sampling algorithm, such as the Metropolis-Hastings or Gibbs sampler, is employed to generate a sequence of observations. These observations are not independent but are related to each other in a specific way, forming a Markov chain. Crucially, under certain conditions, this Markov chain will eventually converge to the true target distribution.\nIn the context of missing data, MCMC iteratively alternates between the \\(I\\)- and the \\(P\\)-steps. At the \\(t\\)-th iteration with current guess for \\(\\theta\\) denoted \\(\\theta^t\\), these steps are:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nThe \\(I\\)-step (imputation): Draw \\(Y_{\\text{miss}}\\) from \\(P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta^t)\\). That is, from its conditional distribution given the observed data and current parameter estimates.\nThe \\(P\\)-step (posterior): Draw \\(\\theta^{t+1}\\) from \\(P(\\theta \\mid Y_{\\text{obs}}, Y_{\\text{miss}}^{t+1})\\). This is its posterior distribution given the observed data and the newly imputed \\(Y_{\\text{miss}}\\).\n\n\n\nThis back-and-forth dance ensures that the imputed values reflect the uncertainty and structure of the data. And with enough iterations (large \\(t\\)) the chain will converge to our target, \\(P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta)\\).\nSoftware Packages: mcmc, MCMCPack, mice.\n\n\nPractical Considerations\nConvergence diagnostics are crucial to ensure the MCMC chains have reached a stable equilibrium, as the initial values can significantly influence the results. In simple words, the chain should run long enough so that the posterior distribution does not change significantly after each additional iteration. It is also common to discard (or “burn”) an initial batch of values since they do not come from the final, stable posterior distribution. Additionally, computational costs can be a significant factor, especially for large datasets or complex models but efficient algorithms and parallel processing can help. Lastly, model specification is critical, as the choice of imputation model directly impacts the quality of the imputed values."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#an-example",
    "href": "blog/filling-missing-data-mcmc.html#an-example",
    "title": "Filling in Missing Data with MCMC",
    "section": "An Example",
    "text": "An Example\nLet’s walk through an example using a simple dataset with missing values. Suppose you have a dataset with two variables, \\(X\\) and \\(Y\\), where \\(Y \\sim N(\\beta_0 + \\beta_1 X, \\sigma^2)\\), and some values of \\(Y\\) are missing. We assume the following relationship:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\nwhere \\(\\epsilon\\) is an error term. We impose priors on \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) (which collectively comprise \\(\\theta\\) in this example).\nWe begin with generating some fake data and introduce missingness in \\(Y\\).\nrm(list=ls())\nset.seed(1988)\nlibrary(mice)\n\n# generate fake data\nn &lt;- 100                  \nc &lt;- 0.2                 \nX &lt;- rnorm(n, mean = 5, sd = 2)\nbeta0 &lt;- 2                 \nbeta1 &lt;- 1.5              \nepsilon &lt;- rnorm(n, mean = 0, sd = 1)  \nY &lt;- beta0 + beta1 * X + epsilon      \n\n# introduce missingness in Y\nmissing_indices &lt;- sample(1:n, size = n * c, replace = FALSE)\nY[missing_indices] &lt;- NA  \n\n# combine data into a data frame\ndata &lt;- data.frame(X = X, Y = Y)\nhead(data)\n\n# perform imputation\nimputed_data &lt;- mice(data, \n                    m = 5, \n                    method = \"norm\", \n                    seed = 1988)\nmodels &lt;- with(imputed_data, lm(Y ~ X))\n\n# print results\nsummary(pool(models))\n\n         term estimate std.error statistic       df      p.value\n1 (Intercept) 1.730583 0.3008046  5.753179 49.51038 5.435802e-07\n2           X 1.546689 0.0544164 28.423207 52.02756 2.313802e-33\nThis is clearly a silly example since \\(Y\\) is missing at random, suggesting that missing data does not result in bias. Anyway, for illustration purposes we run the Bayesian Regression algorithm to fill in the missing \\(Y\\) and proceed with a linear regression of \\(Y\\) on \\(X\\).\nSpecifically, the code below uses normal (linear regression) imputation to fill in the missing values. For each missing point the algorithm fits a linear regression model predicting \\(Y\\) from \\(X\\) using the complete data and then use this model to predict or impute the missing \\(Y\\). This process is repeated \\(m\\) times (hence the name multiple imputation), creating \\(m\\) different versions of the dataset with the missing values filled in.\nBoth coefficients fall in the expected respective regions."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#where-to-learn-more",
    "href": "blog/filling-missing-data-mcmc.html#where-to-learn-more",
    "title": "Filling in Missing Data with MCMC",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFollowing some computational innovations, Bayesian methods have experienced somewhat of a revival in the last fifteen years. Consequently, there are plenty of high-quality materials online. Takahashi (2017) is an accessible resource on MCMC and Multiple Imputation which I used extensively."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#bottom-line",
    "href": "blog/filling-missing-data-mcmc.html#bottom-line",
    "title": "Filling in Missing Data with MCMC",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMissing data is an ever-present issue in practice.\nStandard approaches to dealing with missing information include ignoring it or imputing it with mean or predicted values.\nMCMC leverages the full joint distribution of the data, making it a robust imputation method.\nBy alternating between imputing missing values and updating parameters, MCMC aligns imputations with the observed data’s structure."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#references",
    "href": "blog/filling-missing-data-mcmc.html#references",
    "title": "Filling in Missing Data with MCMC",
    "section": "References",
    "text": "References\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman and Hall/CRC.\nRubin, D B 1987 Multiple Imputation for Nonresponse in Surveys. New York, NY: John Wiley & Sons. DOI: https://doi.org/10.1002/9780470316696\nSchafer, J L 1997 Analysis of Incomplete Multivariate Data. Boca Raton, FL: Chapman & Hall/CRC. DOI: https://doi.org/10.1201/9781439821862\nScheuren, F 2005 Multiple imputation: How it began and continues. The American Statistician, 59(4): 315–319.\nTakahashi, M. (2017). Statistical inference in missing data by MCMC and non-MCMC multiple imputation algorithms: Assessing the effects of between-imputation iterations. Data Science Journal, 16, 37-37."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#background",
    "href": "blog/bootstrap-limitations.html#background",
    "title": "The Bootstrap and its Limitations",
    "section": "Background",
    "text": "Background\nThe bootstrap is a powerful resampling technique used to estimate the sampling distribution of a statistic. By repeatedly drawing observations with replacement from the original dataset, it enables practitioners to perform tasks like hypothesis testing, computing standard errors, and constructing confidence intervals—without relying on strong parametric assumptions about the underlying population.\nIt is particularly valuable when analytical expressions for the variance of an estimator are unavailable or computationally complex. Moreover, the bootstrap is grounded in robust statistical theory and offers versatile adaptations, such as the wild bootstrap, which is commonly used for estimating cluster-robust variance. This combination of methodological flexibility and statistical rigor has established the bootstrap as a central tool in modern data science.\nHowever, like any statistical method, the bootstrap has its limitations. This article examines scenarios where the bootstrap will yield unreliable results."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#a-closer-look",
    "href": "blog/bootstrap-limitations.html#a-closer-look",
    "title": "The Bootstrap and its Limitations",
    "section": "A Closer Look",
    "text": "A Closer Look\nConsider the following scenarios:\n\nVery Small Sample Sizes – The bootstrap relies on resampling the observed data to approximate the population distribution. With very small samples, there is not enough variability in the data to accurately capture the underlying distribution, leading to unreliable estimates.\nParameter at the Edge of the Parameter Space – When the parameter being estimated lies at or near a boundary (e.g., estimating a proportion close to \\(0\\) or \\(1\\)), the bootstrap may fail to reflect the true sampling distribution. The resampling process cannot fully mimic the constraints of the parameter space. This includes situations in which we are interested in learning more about the minimum or maximum value of some statistic.\nPresence of Outliers – Outliers can heavily influence bootstrap resamples, leading to biased or overly variable estimates.\nDependence in the Data – The bootstrap assumes the data are independent and identically distributed (i.i.d.). For time series or spatial data where observations are dependent, naive application of the bootstrap can yield incorrect inferences unless adapted for the structure (e.g., block bootstrap).\nExtreme Skewness or Rare Events – When the data distribution is highly skewed or dominated by rare events, the bootstrap may struggle to approximate the tails of the distribution accurately, affecting confidence interval coverage and tail probability estimates.\nMisspecified Models – If the bootstrap is applied to a statistic derived from a poorly specified model, the resulting inferences will inherit the same flaws. The bootstrap cannot correct for model misspecification.\n\nIn some of these cases theoretical approximation methods can provide analytical solutions that bypass the resampling challenges. The parametric bootstrap is like a more structured cousin of the standard bootstrap, generating samples based on a known probability distribution. It’s particularly helpful when you’ve got a good sense of what your data looks like. Bayesian methods take things a step further, folding in prior knowledge to handle tricky statistical scenarios with flexibility.\nWhile the bootstrap is a versatile and often reliable tool, awareness of these limitations can help you avoid potential pitfalls and ensure more robust statistical analyses."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#an-example",
    "href": "blog/bootstrap-limitations.html#an-example",
    "title": "The Bootstrap and its Limitations",
    "section": "An Example",
    "text": "An Example\nLet’s illustrate this failure mode with a simple example in R and Python focusing on estimating the maximum value of a random variable.\n\nRPython\n\n\n# Clear workspace and generate data\nrm(list=ls())\nset.seed(123)\nx &lt;- runif(30, 0, 1)\n\n# True max and standard error via simulation\ntrue_max_se &lt;- sd(replicate(10000, max(runif(30))))\n\n# Bootstrap\nB &lt;- 1000\nboot_max &lt;- replicate(B, max(sample(x, replace = TRUE)))\nboot_se &lt;- sd(boot_max)\n\n# Results\ncat(\"True SE of max:\", round(true_max_se, 4), \"\\n\")\n&gt; True SE of max: 0.0311 \ncat(\"Bootstrap SE of max:\", round(boot_se, 4), \"\\n\")\n&gt; Bootstrap SE of max: 0.021 \n\n\n# Clear workspace and generate fake data\nimport numpy as np\nnp.random.seed(1988)\nx = np.random.uniform(0, 1, 30)\n\n# True SE via simulation\ntrue_max_se = np.std([np.max(np.random.uniform(0, 1, 30)) for _ in range(10000)])\n\n# Bootstrap\nB = 1000\nboot_max = [np.max(np.random.choice(x, size=30, replace=True)) for _ in range(B)]\nboot_se = np.std(boot_max, ddof=1)\n\n# Results\nprint(f\"True SE of max: {true_max_se:.4f}\")\n&gt; True SE of max: 0.0313\nprint(f\"Bootstrap SE of max: {boot_se:.4f}\")\n&gt; Bootstrap SE of max: 0.0240"
  },
  {
    "objectID": "blog/bootstrap-limitations.html#bottom-line",
    "href": "blog/bootstrap-limitations.html#bottom-line",
    "title": "The Bootstrap and its Limitations",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe bootstrap is incredible versatile, but it has its limitations.\nBeware in relying on it when facing any of the situations described above."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#where-to-learn-more",
    "href": "blog/bootstrap-limitations.html#where-to-learn-more",
    "title": "The Bootstrap and its Limitations",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great place to start. If you have nailed the basics and are looking for a technical challenge on the bootstrap, Efron and Hastie (2021) is a hidden gem on all things statistical inference, especially the bootstrap. Econometrics geeks might want to dive into James Mackinnon’s papers cited below for seriously deep details."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#references",
    "href": "blog/bootstrap-limitations.html#references",
    "title": "The Bootstrap and its Limitations",
    "section": "References",
    "text": "References\nEfron, B. (2000). The bootstrap and modern statistics. Journal of the American Statistical Association, 95(452), 1293-1296.\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press.\nEfron, B., & Tibshirani, R. J. (1994). An introduction to the bootstrap. Chapman and Hall/CRC.\nMacKinnon, J. G. (2006). Bootstrap methods in econometrics. Economic Record, 82, S2-S18.\nMacKinnon, J. G., & Webb, M. D. (2017). Wild bootstrap inference for wildly different cluster sizes. Journal of Applied Econometrics, 32(2), 233-254.\nMacKinnon, J. G., & Webb, M. D. (2020). Clustering methods for statistical inference. Handbook of labor, human resources and population economics, 1-37."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#background",
    "href": "blog/ml-based-adjustments.html#background",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "Background",
    "text": "Background\nRandomized experiments are the gold standard when interested in measuring causal relationships with data. In settings with small treatment effects or underpowered designs, a major focus falls on decreasing the variance. In simple low-dimensional settings a common attempt to do that is to include a bunch of covariates and their interaction with the treatment variable in an OLS regression. Under standard assumptions, the coefficient on the treatment variable is still asymptotically unbiased (albeit not in finite samples) and including the interactions guarantees that this estimator does not have higher asymptotic variance than the simple difference-in-means.\nIn high-dimensional settings, however, this can easily lead to overfitting and new tools for variance reduction are needed. In this article, I will focus on two ways Machine Learning (ML) can be helpful with this problem when we have access to a bunch of covariates. In the first set of methods, we use a ML algorithm (such as the lasso) to directly estimate the treatment effect. Alternatively, we can first use ML to predict the outcome and then feed that prediction in an OLS regression.\nA helpful benchmark with which to compare these methods is the simple (non-parametric) difference-in-means estimator. Under certain conditions, both approaches guarantee smaller or equal variance."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#notation",
    "href": "blog/ml-based-adjustments.html#notation",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "Notation",
    "text": "Notation\nI use \\(\\bar{Y}^T\\) and \\(\\bar{Y}^C\\) to denote the sample average outcomes for the treatment and control groups respectively. \\(X\\) is the covariate vector and its deviations from the average are \\(\\tilde{X}\\). The benchmark estimator can be expressed as:\n\\[\\hat{ATE}^{simple} = \\bar{Y}^T - \\bar{Y}^C.\\]"
  },
  {
    "objectID": "blog/ml-based-adjustments.html#a-closer-look",
    "href": "blog/ml-based-adjustments.html#a-closer-look",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "A Closer Look",
    "text": "A Closer Look\nBroadly speaking, there are two ML Methods for Variance Reduction.\n\nUsing ML Regression Directly\nThe simplest and most natural way to incorporate covariates is to add them to a linear model (along with the treatment variable and their interactions with the treatment variable). Bloniarz et al. (2015) show we can directly use Lasso regression instead of OLS.\nTo guarantee that the lasso does not omit the treatment variable, we can run two separate regressions, one for each (treatment) group. Then the estimator can be formulated as:\n\\[\\hat{ATE}^{lasso} = (\\bar{Y}^T-\\tilde{X}^T\\beta^{T}_{lasso}) - (\\bar{Y}^C-\\tilde{X}^C\\beta^{C}_{lasso}),\\]\nwhere \\(\\beta^{i}_{lasso}\\) is the coefficient vector from the lasso regressions on observations in group \\(i\\in\\{T,C\\}\\). The authors also give a conservative formula for computing the variance of \\(\\hat{ATE}^{lasso}\\). When the two lasso regressions select different sets of covariates (which is probably common in practice), this is no longer guaranteed to yield equal or lower asymptotic variance compared to the benchmark.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nFor the treatment and control groups separately, run lasso regression of \\(Y\\) on \\(\\tilde{X}\\) go get \\(\\hat{\\beta}^T_{lasso}\\) and \\(\\hat{\\beta}^C_{lasso}\\).\nCalculate the treatment effect estimate \\(\\hat{ATE}^{lasso}\\) using the above formula.\nCalculate the estimate of the variance of \\(\\hat{ATE}^{lasso}\\) using the formula in Blonarz et al. (2015).\n\n\n\nThe authors also propose the lasso+OLS estimator which first uses \\(L1\\) regularization as above to select the covariates and then plugs those in OLS to get the treatment effect estimate.\nA similar idea has also been studied by Wager et al (2016). They show that when additionally, assuming Gaussian data (along with a bunch of regularity assumptions), we can use any “risk consistent” ML estimator such as ridge, elastic net, etc. “Risk consistent” here means as we give the algorithm more data, it gets closer to the truth. The lower the risk the higher the variance reduction gains compared to the simple difference-in-means estimator. The authors also propose a simple cross-fitting approach to calculate confidence intervals.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nSplit the data into \\(k\\) equal sized folds.\nFor each fold \\(k\\):\n\n\ncalculate \\(\\bar{Y}^k, \\tilde{X}^k\\).\nget the coefficients \\(\\hat{\\beta}_{lasso}^{-k}\\) based on regressions on all other \\(k-1\\) folds.\ncombine both quantities and calculate \\(\\hat{ATE}^{lasso}\\).\ncalculate its standard error.\n\n\nGet the final estimates \\(\\hat{ATE}^{lasso}\\) and its standard error by taking weighted averages across all \\(k\\) folds.\n\n\n\nThis concludes the discussion of using a ML-type linear regression model to reduce the variance in A/B tests. Let’s now move on to the second method.\n\n\nUsing ML Regression Indirectly\nAn alternative approach first uses ML to predict \\(Y\\) and then plugs that prediction into an OLS regression of the outcome on the treatment variable. One can then use cross-fitting to do the prediction which ensures the “naïve” OLS confidence intervals remain valid. The authors call this procedure MLRATE (machine learning regression-adjusted treatment effect estimator).\nHere is a rough version of the algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nSplit the data in \\(k\\) equal-sized folds.\nFor each fold \\(k\\):\n\n\nPredict \\(Y\\) by applying a ML algorithm to all other \\(k-1\\) folds. Call this prediction \\(\\bar{Y}_k\\).\n\n\nGet a final prediction \\(\\bar{Y}=\\sum_k\\bar{Y}_k\\).\nRun OLS of \\(Y\\) on \\(T\\), \\(\\bar{Y}_k\\) and \\((\\bar{Y}_k-\\bar{Y}) \\times T\\) and use the associated standard errors and \\(p\\)-values."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#bottom-line",
    "href": "blog/ml-based-adjustments.html#bottom-line",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nRegression adjustments are a commonly used tool to reduce variance in A/B tests.\nThe machine learning toolbox offers possibly more powerful algorithms in this space.\nThere are two main ML approaches. Both can be shown under certain conditions to be at least as good as the simple difference-in-means estimator.\nThe first approach uses ML regression algorithms directly.\nThe second method, instead, uses ML to predict the outcome and adds that in an OLS regression."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#references",
    "href": "blog/ml-based-adjustments.html#references",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "References",
    "text": "References\nBelloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse models. Bernoulli 19(2): 521-547\nBloniarz, A., Liu, H., Zhang, C. H., Sekhon, J. S., & Yu, B. (2016). Lasso adjustments of treatment effect estimates in randomized experiments. Proceedings of the National Academy of Sciences, 113(27), 7383-7390.\nGuo, Y., Coey, D., Konutgan, M., Li, W., Schoener, C., & Goldman, M. (2021). Machine learning for variance reduction in online experiments. Advances in Neural Information Processing Systems, 34, 8637-8648.\nLin, W. (2013). Agnostic notes on regression adjustments to experimental data: Reexamining Freedman’s critique. Ann. Appl. Stat. 7(1): 295-318\nList, J. A., Muir, I., & Sun, G. K. (2022). Using Machine Learning for Efficient Flexible Regression Adjustment in Economic Experiments (No. w30756). National Bureau of Economic Research.\nNegi, A., & Wooldridge, J. M. (2021). Revisiting regression adjustment in experiments with heterogeneous treatment effects. Econometric Reviews, 40(5), 504-534.\nPoyarkov, A., Drutsa, A., Khalyavin, A., Gusev, G., & Serdyukov, P. (2016, August). Boosted decision tree regression adjustment for variance reduction in online controlled experiments. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 235-244).\nWager, S., Du, W., Taylor, J., & Tibshirani, R. J. (2016). High-dimensional regression adjustments in randomized experiments. Proceedings of the National Academy of Sciences, 113(45), 12673-12678."
  },
  {
    "objectID": "blog/ks-test-one-sample.html#background",
    "href": "blog/ks-test-one-sample.html#background",
    "title": "The Kolmogorov–Smirnov Test as a Goodness-of-fit",
    "section": "Background",
    "text": "Background\nThe Kolmogorov–Smirnov (KS) test is a staple in the statistical toolbox for checking how well data fit a hypothesized distribution. It comes in both a one-sample and a two-sample version. A common application in causal inference is covariates distribution balance checks between the treatment and control groups. It’s nonparametric, straightforward to compute, and widely implemented just about every statistical software. But—and this is a big but—using the KS test naively can lead to some serious misinterpretations, especially when parameters are estimated from the data.\nThis article is based on the 2024 paper by Zeimbekakis, Schifano, and Yan, which takes a hard look at the common misuses of the one-sample KS test. We’ll walk through what the KS test is supposed to do, when it goes wrong, and how to think more clearly about assessing goodness-of-fit."
  },
  {
    "objectID": "blog/ks-test-one-sample.html#notation",
    "href": "blog/ks-test-one-sample.html#notation",
    "title": "The Kolmogorov–Smirnov Test as a Goodness-of-fit",
    "section": "Notation",
    "text": "Notation\nLet \\(X_1, \\dots, X_n\\) be i.i.d. random variables with unknown distribution function \\(F\\). We want to test whether \\(F = F_0\\), for some known distribution function \\(F_0\\).\nThe empirical distribution function (EDF) is: \\[F_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i \\leq x)\\]\nYou are probably familiar with this. It’s is a step function that estimates the true cumulative distribution function of a random variable based on a sample. At any point \\(x\\), the ECDF gives the proportion of observations in the sample that are less than or equal to \\(x\\). It is the nonparametric maximum likelihood estimator of the cumulative distribution function (CDF).\nThe KS statistic is: \\[D_n = \\sup_{x \\in \\mathbb{R}} |F_n(x) - F_0(x)|\\]\nUnder the null hypothesis, this test statistic converges to the Kolmogorov distribution, a distribution with no closed-form density but a known CDF. This is under the assumption that \\(F_0\\) is fully specified, i.e., no parameters have been estimated from the data."
  },
  {
    "objectID": "blog/ks-test-one-sample.html#a-closer-look",
    "href": "blog/ks-test-one-sample.html#a-closer-look",
    "title": "The Kolmogorov–Smirnov Test as a Goodness-of-fit",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nA Refresher on KS\nIntuitively, the KS test statistic measuries the largest vertical distance between the EDF and the hypothesized CDF \\(F_0\\). It is sensitive to discrepancies in the CDF. This gives you a global measure of discrepancy, not a local one—so it’s less powerful for detecting issues like tail misspecification or multimodality. This is important because in many applications, tail behavior is critically important, such as in risk modeling or extreme value analysis.\nA well known limitation of the KS test is that with small samples, it has limited power to detect distributional differences, while with very large samples, it may detect statistically significant but practically trivial deviations from the hypothesized distribution. This problem in the contxt of “big data” is obviously broader and goes beyond the KS test.\n\n\nThe Problem\nHere’s the catch: the null distribution of the KS statistic assumes \\(F_0\\) is fully known. But in practice, people often use the test to evaluate model fit after estimating parameters—e.g., fitting a normal distribution by MLE and then checking fit with KS.\nThat invalidates the test.\nWhy? Because the theoretical distribution of \\(D_n\\) changes when parameters are estimated. The true distribution of the test statistic becomes conditional on the data, and the critical values are no longer accurate. This leads to an deflated Type I error rate: you’re less likely to incorrectly reject the null. In other words, the test is too conservative.\n\n\nBetter Alternatives\nWhen parameters are estimated, we need modified procedures:\n\nLilliefors test: An adaptation of the KS test that adjusts the null distribution when testing for normality with estimated parameters.\nParametric bootstrap: Simulate the null distribution of the test statistic by repeatedly fitting the model and computing \\(D_n\\) on simulated data.\nOther GOF tests: Anderson-Darling and Cramér-von Mises tests have versions that handle estimated parameters more gracefully."
  },
  {
    "objectID": "blog/ks-test-one-sample.html#bottom-line",
    "href": "blog/ks-test-one-sample.html#bottom-line",
    "title": "The Kolmogorov–Smirnov Test as a Goodness-of-fit",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe KS test is a popular and flexible method for estimating differences between statistical disitributions.\nIt assumes no parameters are estimated—violating this leads to invalid inference.\nEstimating parameters from the same data used in the test inflates Type I error.\nUse alternatives like the Lilliefors test or bootstrap methods when parameters are estimated."
  },
  {
    "objectID": "blog/ks-test-one-sample.html#references",
    "href": "blog/ks-test-one-sample.html#references",
    "title": "The Kolmogorov–Smirnov Test as a Goodness-of-fit",
    "section": "References",
    "text": "References\nLilliefors, H. W. (1967). On the Kolmogorov-Smirnov test for normality with mean and variance unknown. Journal of the American statistical Association, 62(318), 399-402.\nZeimbekakis, A., Schifano, E. D., & Yan, J. (2024). On Misuses of the Kolmogorov–Smirnov Test for One-Sample Goodness-of-Fit. The American Statistician, 78(4), 481-487."
  },
  {
    "objectID": "blog/ci-residualized-reg.html#background",
    "href": "blog/ci-residualized-reg.html#background",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Background",
    "text": "Background\nThe Frisch-Waugh-Lovell (FWL) theorem offers an elegant alternative to standard multivariate linear regression when estimating causal effects. Instead of running a full regression with treatment and control variables, FWL demonstrates that we can obtain identical treatment coefficient estimates by first “residualizing” both the outcome and treatment variable(s) with respect to controls, then regressing these residuals against each other. This approach provides both computational advantages and conceptual clarity.\nThis theorem effectively decomposes multiple regression into simpler components, helping data scientists understand what happens “under the hood” of regression models. By residualizing variables—removing the components explained by control variables—we can isolate and estimate treatment effects more intuitively, essentially peeling away confounding layers to reveal the core relationship of interest.\nMy PhD advisor often emphasized that the magnitude of a treatment effect’s standard error can matter more than the effect itself. A related but underexplored issue is how standard errors behave in this setting. Fortunately, a recent paper by Peng Ding (2021) extends the Frisch–Waugh–Lovell (FWL) theorem to show that various standard errors—homoskedastic, heteroskedastic-robust (EHW), cluster-robust, and HAC—are either equivalent or differ only by degrees-of-freedom corrections."
  },
  {
    "objectID": "blog/ci-residualized-reg.html#notation",
    "href": "blog/ci-residualized-reg.html#notation",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Notation",
    "text": "Notation\nConsider a standard linear model:\n\\[Y = \\alpha + \\tau D + X\\beta + \\varepsilon, \\]\nwhere:\n\n\\(Y\\) is the outcome variable (e.g., earnings),\n\\(D\\) is the treatment variable (e.g., whether a person attended a job training program),\n\\(X\\) is a vector of control variables (e.g., age, education, experience),\n\\(\\tau\\) is the treatment effect we want to estimate,\n\\(\\beta\\) represents the coefficients on the controls,\n\\(\\varepsilon\\) is the error term.\n\nWe assume that \\(X\\) includes all relevant confounders, the linear model is correct, etc. so that this model has a causal interpretation. In econometrics jargon, this is a “structural” model."
  },
  {
    "objectID": "blog/ci-residualized-reg.html#a-closer-look",
    "href": "blog/ci-residualized-reg.html#a-closer-look",
    "title": "Causal Inference with Residualized Regressions",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe FWL Theorem\nThe OLS estimate of \\(\\tau\\) in the full regression includes both \\(D\\) and \\(X\\). However, FWL tells us that we can obtain the same estimate of \\(\\tau\\) by following these steps:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRegress \\(Y\\) on \\(X\\) and collect the residuals \\(\\tilde{Y}\\).\nRegress \\(D\\) on \\(X\\) and collect the residuals \\(\\tilde{D}\\).\nRegress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) (without an intercept). The coefficient on \\(\\tilde{D}\\) is exactly \\(\\tau\\).\n\n\n\n\n\nIntuition\nFWL is simple yet profound. When we regress \\(Y\\) on \\(X\\), we strip out the variation in \\(Y\\) that is explained by \\(X\\), leaving only the part orthogonal to (i.e., unexplained by) \\(X\\). Similarly, regressing \\(D\\) on \\(X\\) removes the influence of \\(X\\) on \\(D\\), isolating the component of \\(D\\) that is independent of \\(X\\). Since \\(X\\) has been accounted for in both cases, the regression of \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) retrieves the direct relationship between \\(D\\) and \\(Y\\), net of \\(X\\).\nMathematically, the key result of FWL is:\n\\[\\hat{\\tau} = (D' M_X D)^{-1} D' M_X Y,\\]\nwhere \\(M_X = I - X(X'X)^{-1}X'\\) is the projection matrix that residualizes variables with respect to \\(X\\). This shows that the estimate of remains unchanged whether we use the full regression or the residualized regression.\nThink of it this way: we’re first “adjusting” both our treatment and outcome variables by removing the predictable parts based on our controls. Then we’re examining how the “adjusted” treatment relates to the “adjusted” outcome. This residual-on-residual regression gives us our causal estimate.\n\n\nGeometric Interpretation\nGeometrically, we can think of the FWL theorem in terms of projections in vector space. The residuals \\(\\tilde{D}\\) and \\(\\tilde{\\mathbf{y}}\\)​ are what remain after projecting \\(D\\) and \\(Y\\) onto the orthogonal complement of the space spanned by \\(Z\\).\nIn other words, we’re looking at the components of \\(D\\) and \\(Y\\) that are orthogonal to (i.e., cannot be explained by) the control variables. The relationship between these orthogonal components gives us our treatment effect estimate.\n\n\nVariance and Standard Errors\nThe precision of our treatment effect estimate depends on how much variation in the treatment remains after accounting for the controls. If the treatment is highly collinear with the controls (meaning \\(\\tilde{D}\\) has little variation), our estimate will be imprecise.\nThis highlights the “curse of dimensionality” in causal inference with observational data. As we include more control variables to reduce omitted variable bias, we may inadvertently limit the residual variation in the treatment variable after adjusting for those controls. This reduction in effective variation makes it harder to isolate the treatment effect, leading to wider standard errors and less precise estimates. In extreme cases, the treatment can become nearly collinear with the controls, undermining our ability to learn anything meaningful from the data.\n\n\nPractical Implications\n\nConceptual clarity: FWL emphasizes that controlling for \\(X\\) means adjusting both \\(Y\\) and \\(D\\) before examining their relationship.\nComputational benefits: In high-dimensional settings, it is often more efficient to work with residualized variables rather than estimating the full model.\nInstrumental variables and two-stage regression: The residualization step is analogous to first-stage regressions in instrumental variable estimation.\nTwo separate data sources: In some cases, possibly due to data privacy concerns, the three variables \\(Y\\), \\(X\\) and \\(D\\) might live in two separate datasets - one with \\(Y\\) and \\(X\\), and the other one with \\(D\\) and \\(X\\). The traditional multivariate regression approach is then not available."
  },
  {
    "objectID": "blog/ci-residualized-reg.html#an-example",
    "href": "blog/ci-residualized-reg.html#an-example",
    "title": "Causal Inference with Residualized Regressions",
    "section": "An Example",
    "text": "An Example\nLet’s go through an example in R and python. We will generate synthetic data where the treatment effect is nonzero and show that both the full regression and the residualized approach give the same estimate.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\nn &lt;- 1000\nX &lt;- matrix(rnorm(n * 3), ncol = 3)  # Three control variables\nD &lt;- 0.5 * X[,1] + 0.3 * X[,2] + rnorm(n)  # Treatment depends on controls\nY &lt;- 2 * D + 1.5 * X[,1] - 0.5 * X[,2] + 0.3 * X[,3] + rnorm(n)  # Outcome model\n\n# Full Regression\nfull_model &lt;- lm(Y ~ D + X)\n\n### Residualized Regression \nY_resid &lt;- residuals(lm(Y ~ X)) \nD_resid &lt;- residuals(lm(D ~ X)) \nresid_model &lt;- lm(Y_resid ~ D_resid - 1)  # No intercept \n\n# Print Results\nsummary(full_model)$coefficients[\"D\",]  \n&gt;Estimate  Std. Error     t value    Pr(&gt;|t|) \n&gt;1.99338755  0.03095204 64.40246546  0.00000000 \nsummary(resid_model)$coefficients[\"D_resid\",]\n&gt;Estimate  Std. Error     t value    Pr(&gt;|t|) \n&gt;1.99338755  0.03089001 64.53178781  0.00000000 \n\n\n# Load libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nnp.random.seed(1988)\n\n# Generate synthetic data\nn = 1000\nX = np.random.normal(size=(n, 3))  # Three control variables\nD = 0.5 * X[:, 0] + 0.3 * X[:, 1] + np.random.normal(size=n)  # Treatment depends on controls\nY = 2 * D + 1.5 * X[:, 0] - 0.5 * X[:, 1] + 0.3 * X[:, 2] + np.random.normal(size=n)  # Outcome model\n\n# Full Regression\nfull_model = LinearRegression()\nfull_model.fit(np.column_stack((D, X)), Y)\nfull_coef = full_model.coef_[0]  # Coefficient for D\nprint(f\"Full Regression Coefficient for D: {full_coef}\")\n\n# Residualized Regression\n# Residualize Y with respect to X\nY_resid_model = LinearRegression()\nY_resid_model.fit(X, Y)\nY_resid = Y - Y_resid_model.predict(X)\n\n# Residualize D with respect to X\nD_resid_model = LinearRegression()\nD_resid_model.fit(X, D)\nD_resid = D - D_resid_model.predict(X)\n\n# Regress residualized Y on residualized D\nresid_model = LinearRegression(fit_intercept=False)\nresid_model.fit(D_resid.reshape(-1, 1), Y_resid)\nresid_coef = resid_model.coef_[0]\n\n# Print results\nprint(f\"Residualized Regression Coefficient for D: {resid_coef}\")\n\n&gt;Full Regression Coefficient for D: 2.020\n&gt;Residualized Regression Coefficient for D: 2.020\n\n\n\nThe coefficient on \\(D\\) in the full model and the coefficient on \\(D_{resid}\\) in the residualized model are identical up to a degrees of freedom correction. This demonstrates that controlling for \\(X\\) can be done implicitly by working with residuals."
  },
  {
    "objectID": "blog/ci-residualized-reg.html#bottom-line",
    "href": "blog/ci-residualized-reg.html#bottom-line",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe FWL theorem shows that controlling for variables in a regression can be done by residualizing first.\nThis approach helps conceptually separate the treatment effect from confounding influences.\nResidualization is particularly useful in high-dimensional settings and instrumental variables estimation.\nWhether you use the full regression or the residualized approach, you get the same treatment effect estimate."
  },
  {
    "objectID": "blog/ci-residualized-reg.html#references",
    "href": "blog/ci-residualized-reg.html#references",
    "title": "Causal Inference with Residualized Regressions",
    "section": "References",
    "text": "References\nDing, P. (2021). The Frisch–Waugh–Lovell theorem for standard errors. Statistics & Probability Letters, 168, 108945."
  },
  {
    "objectID": "blog/variance-ps-matching.html#background",
    "href": "blog/variance-ps-matching.html#background",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Background",
    "text": "Background\nPropensity score matching (PSM) is among the most popular methods for estimating causal effects with observational data. It lends its fame to both its power and simplicity. Under a certain set of commonly invoked assumptions, controlling for the propensity score alone (as opposed to the full set of covariates) is enough to remove bias between the treatment and control groups. The underlying idea of matching on the propensity score is incredibly simple and most data scientists can quickly estimate a treatment effect even from scratch.\nWhile estimating treatment effects is common and straightforward, calculating their variance along with the associated confidence intervals and p-values can be more challenging. So, how exactly do you compute the variance of PSM methods? In this article I will describe the most popular ways of doing that. My goal is to cover the intuition and I will spare the technical details. Interested readers can refer to the References section below for more detailed expositions.\nStatistical methods based on the propensity score function come in different flavors (e.g., weighting, matching, subclassification, etc.), but today I will focus on \\(1:1\\) or \\(1:N\\) matching. There are also several potential estimands of interest, such as ATE (average treatment effect), ATT (ATE for the treatment group) but I will talk specifically about the latter."
  },
  {
    "objectID": "blog/variance-ps-matching.html#a-closer-look",
    "href": "blog/variance-ps-matching.html#a-closer-look",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn brief, the idea behind treatment effect estimation with PSM is:\n\nEstimate the propensity score (i.e., the probability of being in the treatment group).\nFor each user in the treatment group find the user(s) in the control group with the most similar predicted propensity score value.\nAnalyze the outcome differences in each pair to arrive at a final ATT (or ATE) estimate.\n\nThroughout the article, I will assume a standard, well-behaved setting with SUTVA, unconfoundedness, and overlap assumptions in place. Similarly, I will ignore many important aspects of PSM methods such as trimming, propensity score and even treatment effect estimation. I will also use standard notation (e.g., \\(Y\\) and \\(D\\) denote outcome and treatment; and \\(\\tau\\) is the ATT) without setting up the entire framework.\nWe can classify the methods for estimating the PSM variance in three distinct buckets.\n\nAsymptotic Approximations\nRecall the Law of Total Variance stating that, given a conditioning variable \\(X\\), we can always decompose a random variable \\(Y\\)’s variance, into two components – the expectation of the conditional variance and the variance of the conditional expectation: \\[Var(Y) = Var(E(Y|X)) + E(Var(Y|X)).\\]\nAbadie and Imbens (2006) use this idea to derive the asymptotic variance formula for one-to-one and one-to-many matching estimators. They provide an analytical expression for the variance as \\(n \\to \\infty\\) (and hence, this is an approximation) using the matching covariates as conditioning variables. I will not be going into their work in detail, but a couple of notes are worth discussing.\nThe original formulation assumes the true propensity score function is known. This is rarely true and in practice we rely on an estimate of it. Practitioners then replace this true propensity score with the estimated one. This is OK, but not great – it brings me to the second issue.\nOne needs to account for the fact that this propensity score is estimated. As such, it comes with uncertainty, which, if ignored, can potentially understate the variance of your estimator. Hello, type 1 errors!\nIn follow-up work, Abadie and Imbens (2016) propose correction which accounts for this uncertainty. Interestingly, in a work that I will discuss below, Bodory et al. (2020) report that in practice this correction might increase or decrease the ATT variance.\n\n\nApproximation Based on Weights\nWe can express all treatment effect estimators as a difference of weighted means:\n\\[\\hat{\\tau} = \\frac{1}{n} \\sum\\hat{w} D Y - \\frac{1}{n} \\sum\\hat{w} (1-D) Y,\\]\nwhere the weights \\(\\hat{w}\\) may depend on the propensity score.\nLechner (2002) suggested calculating the variance of PSM based on this formula, assuming the weights are non-stochastic. In other words, the approach ignores the fact that the propensity score is estimated in a first step. The PSM variance is then equal to the sum of two terms – the one for the treated and the one for the control.\n\n\nBootstrap\nBootstrap is the go-to method for estimating variances in complex settings where analytical expressions are too messy or even do not exist. PSM seems like a natural environment where the bootstrap can help. Not so fast!\nAbadie and Imbens (2006) show that the standard nonparametric bootstrap is inconsistent for non-smooth estimators (such as propensity score matching) with fixed number of matches and continuous covariates. Nevertheless, practitioners routinely ignore this result. While the theory says we should not use the bootstrap here, the extent to which this is of practical relevance is unclear. For instance, if this only makes a difference in the fourth decimal, we might be willing to sacrifice some bias for ease of computation.\nThere are at least two ways one can go around using the bootstrap here. The first one is the standard plain vanilla approach.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRandomly draw \\(B\\) samples of size \\(n\\). Alternatively, you can also sample directly from the matched pairs which works better in certain cases.\nCompute the ATT each time.\nCompute the confidence interval: \\[ \\hat{\\tau}\\pm \\sqrt{\\hat{V}(\\hat{\\tau}) \\times c}, \\]\n\nwhere \\(\\hat{V}(\\tau) = \\frac{1}{B-1}\\sum_{b=1}^B(\\hat{\\tau}^b-\\frac{1}{B}\\sum_b\\hat{\\tau}^b)^2\\) is the bootstrap variance of \\(\\hat{\\tau}\\) and \\(c\\) is the critical value associated with a confidence level \\(\\alpha\\).\n\n\nAlternatively, in this last step we can directly use the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution of \\(\\hat{\\tau}\\).\nThe second approach uses the well-known fact that the bootstrap behaves better when it uses so called asymptotically pivotal statistics – i.e., ones which asymptotic distribution does not depend on unknown quantities. In this setting a candidate would be the \\(t\\)-statistic which, as we know, under certain conditions has a standard normal asymptotic distribution. We proceed in three steps:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nCompute the \\(t\\)-stat in the main sample using one of the variance approximations.\nDraw \\(B\\) random samples of size \\(n\\) and we compute the recentered \\(t\\)-stat with respect to \\(\\hat{\\tau}\\) in the main sample, \\[T^b = \\frac{\\hat{\\tau}^b - \\hat{\\tau}}{\\sqrt{\\hat{V}(\\tau^b)}}.\\]\nThe \\(p\\)-value is the share of (absolute value) bootstrap \\(t\\)-stats larger than the absolute value of the \\(t\\)-stat in the main sample \\[p{\\text{-value}} = 1 - \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|\\leq |T|) = \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|&gt; |T|),\\]\n\nwhere \\(T\\) is the \\(t\\)-stat from the main sample.\n\n\nThere are also newer (wild) bootstrap ideas which are still making their way into the mainstream.\n\n\nMonte Carlo Simulations\nBodory et al. (2020) compare the finite sample performance of several of these methods (plus others). Their analysis is more thorough in the sense that they include other ATT estimators besides PSM, such as PS weighting or radius matching.\nOverall, their results do not indicate a clear winner which performs best in all scenarios. Interestingly, even some of the bootstrap methods often outperform the asymptotic approximations of Abadie and Imbens (2006) or Lechner (2002)."
  },
  {
    "objectID": "blog/variance-ps-matching.html#where-to-learn-more",
    "href": "blog/variance-ps-matching.html#where-to-learn-more",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nI based this post on Bodory et al. (2020)’s paper, so that is a natural place to start. Many other studies have focused on the performance of propensity score treatment effect estimators (as opposed to their variance) – Huber et al. (2013) and Busso et al. (2014) are great starting points. Lastly, Imbens (2015) is a great resource for an overview of matching methods more generally along with some of the problems they solve as well as the pitfalls they entail."
  },
  {
    "objectID": "blog/variance-ps-matching.html#bottom-line",
    "href": "blog/variance-ps-matching.html#bottom-line",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThere is no shortage of methods when it comes to estimating the variance of propensity score matching estimators.\nAbadie and Imbens (2006) showed that the standard bootstrap is inconsistent in this setting with continuous covariates and formally derived an asymptotic approximation of the true variance.\nMonte Carlo simulations, however, show that bootstrap methods offer a good tradeoff between simplicity and performance."
  },
  {
    "objectID": "blog/variance-ps-matching.html#references",
    "href": "blog/variance-ps-matching.html#references",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "References",
    "text": "References\nAbadie, A., & Imbens, G. W. (2006). Large sample properties of matching estimators for average treatment effects. Econometrica, 74(1), 235-267.\nAbadie, A., & Imbens, G. W. (2008). On the failure of the bootstrap for matching estimators. Econometrica, 76(6), 1537-1557.\nAustin, P. C., & Small, D. S. (2014). The use of bootstrapping when using propensity‐score matching without replacement: a simulation study. Statistics in medicine, 33(24), 4306-4319.\nBodory, H., Camponovo, L., Huber, M., & Lechner, M. (2020). The finite sample performance of inference methods for propensity score matching and weighting estimators. Journal of Business & Economic Statistics, 38(1), 183-200.\nBusso, M., DiNardo, J., & McCrary, J. (2014). New evidence on the finite sample properties of propensity score reweighting and matching estimators. Review of Economics and Statistics, 96(5), 885-897.\nCaliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of propensity score matching. Journal of economic surveys, 22(1), 31-72.\nHuber, M., Lechner, M., & Wunsch, C. (2013). The performance of estimators based on the propensity score. Journal of Econometrics, 175(1), 1-21.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nLechner, M. (2002). Some practical issues in the evaluation of heterogeneous labour market programmes by matching methods. Journal of the Royal Statistical Society: Series A (Statistics in Society), 165(1), 59-82."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#background",
    "href": "blog/conformal-inference-var-selection.html#background",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Background",
    "text": "Background\nMany machine learning (ML) methods operate as opaque systems, generating predictions when given a dataset as input. Identifying which variables have the greatest impact on these predictions is often crucial. This adds a touch of interpretability and transparency and aids stakeholders in better understanding the relevant context. Examples abound. For instance, identifying the house attributes most important for predicting home prices, the school or hospital characteristics most strongly associated with better students’ and patients’ outcomes, etc.\nConformal inference offers a novel way of measuring variable importance in ML. In an earlier article I introduced conformal inference as a tool for generating confidence intervals when making predictions for new observations, and here I will describe how we can adapt it to the context of feature importance. The approach is thus similar in spirit to the Gini Importance-based methods mentioned above."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#notation",
    "href": "blog/conformal-inference-var-selection.html#notation",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Notation",
    "text": "Notation\nLet’s begin by setting up some notation. We have a size \\(n\\) i.i.d. random sample of a feature vector \\(X\\) and an outcome \\(Y\\). The focus of conformal inference is on constructing a “confidence interval” for predicting a new observation \\(Y_{n+1}\\) given a new feature realization \\(X_{n+1}\\). I denote the estimate of the mean function by \\(\\hat{\\mu}\\) and the same estimate when removing feature \\(j\\) from $X $ by \\(\\hat{\\mu}_{-j}\\).\nPlease refer to my previous article for more details on the conformal inference framework, methodology and its properties."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#a-closer-look",
    "href": "blog/conformal-inference-var-selection.html#a-closer-look",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Variable Importance\nThe idea of measuring which variables contribute most to a prediction model is not new. The data scientist’s toolbox contains some useful techniques designed to measure variable importance in ML models. Popular choices include:\n\nGini Importance and Information Gain in tree-based models (e.g., random forest, gradient boosting) measure the decrease in various within-leaf impurity indexes caused by excluding a certain variable. The larger the loss, the more important the variable.\nSHAP Values use a cooperative game-theoretic approach to measure each variable’s contribution to the final model’s prediction.\nPermutation Importance assesses a variable’s significance by randomly shuffling its values and comparing the change in the model’s performance. The larger the drop, the more important the variable.\nVariable Coefficients in linear ML models (e.g., Lasso, Ridge) can directly signal importance. This requires an appropriate standardization before fitting the model (to make sure all features are on a level playing field).\n\n\n\nVariable Importance with Conformal Inference\nWe can measure the prediction error associated with dropping a feature \\(j\\) when predicting a new observation \\(Y_{n+1}\\) by:\n\\[\\Delta_j^{n+1} = |Y_{n+1} - \\hat{\\mu}_{-j}(X_{n+1})| - |Y_{n+1}-\\hat{\\mu}(X_{n+1})|.\\]\nThe main idea is to use conformal inference ideas to construct a confidence interval for this prediction loss, \\(\\Delta_j^{n+1}\\), as a signal whether that variable is relevant in predicting the outcome.\nSpecifically, let \\(CI(\\cdot)\\) denote the conformal inference interval for \\(Y_{n+1}\\) given \\(X_{n+1}\\). Then, the interval\n\\[S_j(x)=\\{ |y-\\hat{\\mu}_{-j}(x)|-|y-\\hat{\\mu}(x)| : y \\in CI(x) \\}.\\]\nhas a valid finite-sample coverage in the sense that:\n\\[ P(\\Delta_j^{n+1} \\in S_j(X_{n+1})) \\geq 1-\\alpha, \\]\nwhere \\(\\alpha\\) is a pre-specified significance level. This holds for all \\(j\\).\nWe can plot the confidence intervals \\(S_j(X_i)\\) for \\(i=1 \\dots n\\) and roughly interpret them as measuring variable importance. The closer the intervals are to zero, the less important the variable is for predicting new outcomes. The opposite is true as well. The further and more often it is away from zero, the more important the variable.\nAnother, more global, approach to using conformal inference for variable importance focuses on the distribution of \\(\\Delta_j(X_{n+1}, Y_{n+1})\\) and conducts hypothesis testing on its median or mean. Intuitively, failing to reject a hypothesis that these statistics are non-zero is evidence that variable \\(j\\) does not play a significant role in predicting \\(Y\\)."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#bottom-line",
    "href": "blog/conformal-inference-var-selection.html#bottom-line",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWhile many ML methods act as black boxes, attention often falls on measuring individual variable importance.\nConformal inference offers a new way for data scientists to quantify the influence of each variable to the model performance.\nThe main idea is to use conformal inference to construct a confidence interval for the loss in prediction accuracy associated with removing a feature from the dataset."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#where-to-learn-more",
    "href": "blog/conformal-inference-var-selection.html#where-to-learn-more",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nSee Section 6 in Lei et al. (2018) and the references therein."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#references",
    "href": "blog/conformal-inference-var-selection.html#references",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "References",
    "text": "References\nLei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094-1111.\nLei, J., Rinaldo, A., & Wasserman, L. (2015). A conformal prediction approach to explore functional data. Annals of Mathematics and Artificial Intelligence, 74, 29-43.\nShafer, G., & Vovk, V. (2008). A Tutorial on Conformal Prediction. Journal of Machine Learning Research, 9(3).\nVovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world (Vol. 29). New York: Springer."
  },
  {
    "objectID": "blog/delta-method.html#background",
    "href": "blog/delta-method.html#background",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "Background",
    "text": "Background\nYou’ve likely encountered this scenario: you’ve calculated an estimate for a particular parameter, and now you require a confidence interval. Seems straightforward, doesn’t it? However, the task becomes considerably more challenging if your estimator is a nonlinear function of other random variables. Whether you’re dealing with ratios, transformations, or intricate functional relationships, directly deriving the variance for your estimator can feel incredibly daunting. In some instances, the bootstrap might offer a solution, but it can also be computationally demanding.\nEnter the Delta Method, a technique that harnesses the power of Taylor series approximations to assist in calculating confidence intervals within complex scenarios. By linearizing a function of random variables around their mean, the Delta Method provides a way to approximate their variance (and consequently, confidence intervals). This effectively transforms a convoluted problem into a more manageable one. Let’s delve deeper together, assuming you already have a foundational understanding of hypothesis testing."
  },
  {
    "objectID": "blog/delta-method.html#notation",
    "href": "blog/delta-method.html#notation",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "Notation",
    "text": "Notation\nBefore diving into the technical weeds, let’s set up some notation to keep things grounded. Let \\(X=(x_1, \\dots, x_k)\\) be a random vector of dimension \\(k\\), with mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\) (or simply a scalar \\(\\sigma^2\\) when \\(k=1\\)). Suppose you have a continuous, differentiable function \\(g(\\cdot)\\), and you’re interested in approximating the variance of \\(g(X)\\), denoted as \\(\\text{Var}(g(X))\\)."
  },
  {
    "objectID": "blog/delta-method.html#a-closer-look",
    "href": "blog/delta-method.html#a-closer-look",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "A Closer Look",
    "text": "A Closer Look\nThe Delta Method builds on a simple premise: for a smooth function \\(g(\\cdot)\\), we can approximate \\(g(X)\\) around its mean \\(\\mu\\) using a first-order Taylor expansion:\n\\[g(X) \\approx g(\\mu) + \\nabla g(\\mu)^T (X - \\mu),\\]\nwhere \\(\\nabla g(\\mu)\\) is the gradient of \\(g(\\cdot)\\) evaluated at \\(\\mu\\), i.e., a \\(k\\times1\\) vector of partial derivatives:\n\\[\\nabla g(\\mu) = \\left[ \\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2}, \\dots, \\frac{\\partial g}{\\partial x_k} \\right]^T.\\]\nBy substituting this into the approximation, the variance of \\(g(X)\\) becomes:\n\\[\\begin{align*} \\text{Var}(g(X)) & = \\text{Var}(g(\\mu) + \\nabla g(\\mu)^T (X - \\mu)) \\\\ & = \\text{Var}(g(\\mu) + \\nabla g(\\mu)^T X -  \\nabla g(\\mu)^T  \\mu) \\\\  &= \\text{Var}(g(\\mu)^T X)  \\\\ &=  \\nabla g(\\mu)^T \\Sigma \\nabla g(\\mu).  \\end{align*}\\]\nIn the univariate \\(k=1\\) case, we have:\n\\[\\text{Var}(g(X)) = \\sigma^2 [g(\\cdot)']^2.\\]\nIf \\(X\\) is a sample-based estimator (e.g., sample mean, regression coefficients), then \\(\\Sigma\\) would be its estimated covariance matrix, and the Delta Method gives us an approximate standard error for \\(g(X)\\). This approximation works well for large samples but may break down when variances are high or sample sizes are small."
  },
  {
    "objectID": "blog/delta-method.html#an-example",
    "href": "blog/delta-method.html#an-example",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "An Example",
    "text": "An Example\nLet’s walk through an example to make this concrete. Suppose you’re studying the ratio of two independent random variables: \\(R = \\frac{X_1}{X_2}\\), where \\(X_1 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim N(\\mu_2, \\sigma_2^2)\\). I know some of you want specific numbers, so we can set \\(\\mu_1 = 5\\), \\(\\mu_2 = 10\\), \\(\\sigma_1 = 2\\), and \\(\\sigma_2=1\\).\nWe want to approximate the variance of \\(R\\) using the Delta Method. Here is the step-by-step procedure to get there.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nDefine \\(g(X)\\) and obtain its gradient. Here, \\(g(X) = \\frac{X_1}{X_2}\\) and the gradient is: \\[\\nabla g(\\mu) = \\left[ \\frac{1}{\\mu_2}, -\\frac{\\mu_1}{\\mu_2^2} \\right]^T.\\]\nEvaluate \\(g(\\mu)\\) at _1 and _2. In our example \\[\\nabla g(\\mu) = [0.1, -0.5]^T.\\]\nCompute the variance approximation. We have \\[\\Sigma = \\begin{bmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_2^2 \\end{bmatrix} = \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix}.\\] Thus, the approximate variance of \\(R\\) is: \\[\\text{Var}(R) \\approx \\nabla g(\\mu)^T \\Sigma \\nabla g(\\mu) = \\frac{\\sigma_1^2}{\\mu_2^2} + \\frac{\\mu_1^2 \\sigma_2^2}{\\mu_2^4}=\\frac{4}{100}+\\frac{25}{625}=0.08.\\]\n\n\n\nAnd that’s it. We used the Delta Method to compute the approximate variance of \\(R = \\frac{X_1}{X_2}\\)."
  },
  {
    "objectID": "blog/delta-method.html#bottom-line",
    "href": "blog/delta-method.html#bottom-line",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe Delta Method is a generic way of computing confidence intervals in non-standard situations.\nIt works by linearizing nonlinear functions to approximate variances and standard errors.\nThis technique works for any smooth function, making it a go-to tool in econometrics, biostatistics, and machine learning."
  },
  {
    "objectID": "blog/delta-method.html#references",
    "href": "blog/delta-method.html#references",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "References",
    "text": "References\nCasella, G., & Berger, R. L. (2002). Statistical Inference.\nGreene, W. H. (2018). Econometric Analysis."
  },
  {
    "objectID": "blog/binscatter.html#background",
    "href": "blog/binscatter.html#background",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Background",
    "text": "Background\nIn the realm of data visualization, the classical scatter plot has long been a staple for exploring bivariate relationships. However, as datasets grow larger and more complex, traditional scatter plots can become cluttered and less informative. Privacy concerns may also limit the ability to plot raw data, and simple bivariate plots often fail to reveal causal relationships. This is where binscatter, or binned scatter plots, come into play.\nBinscatter offers a cleaner, more interpretable way to visualize the relationship between two variables, especially when dealing with large datasets. By aggregating data points into bins and plotting the average outcome within each bin, binscatter simplifies the visualization, making it easier to discern patterns and trends. It’s particularly useful for:\n\nIntuitive visualization for large datasets by grouping data into bins.\nHighlighting trends and relationship between variables effectively.\nExtending these ideas to control for covariates.\n\nIn this article, I will introduce binscatter, explore its mathematical foundation, and demonstrate its utility with an example in R and python."
  },
  {
    "objectID": "blog/binscatter.html#notation",
    "href": "blog/binscatter.html#notation",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Notation",
    "text": "Notation\nTo formalize binscatter, let’s define the following:\n\n\\(X\\): The independent/predictor variable.\n\\(Y\\): The dependent/outcome/response variable.\n\\(n\\): The number of observations in the dataset.\n\\(K\\): The number of bins into which \\(X\\) is divided.\n\\(\\bar{Y}_k\\): The mean of \\(Y\\) for observations falling in the \\(k\\)-th bin of \\(X\\). Similarly for \\(\\bar{X}_k\\).\n\\(B_k\\)​: The observations falling in the \\(k\\)-th bin.\n\\(W\\): The covariate to be controlled. This can be a vector too."
  },
  {
    "objectID": "blog/binscatter.html#a-closer-look",
    "href": "blog/binscatter.html#a-closer-look",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFormal Definition\nA binscatter plot is constructed by partitioning the range of the independent variable \\(X\\) into a fixed number of \\(K\\) bins, \\(B_1,\\dots,B_K\\) typically using empirical quantiles. This ensures each bin is of roughly the same size. Within each bin, the average value of the dependent variable \\(Y\\) is calculated. These averages are then plotted against the midpoint of each bin, \\(\\bar{X}\\), resulting in a series of points that represent an estimate of conditional mean of \\(Y\\) given \\(X\\), \\(E[Y\\mid X]\\).\nIn technical jargon binscatter provides a nonparametric estimate of the conditional mean function, offering a visual summary of the relationship between the two variables. The resulting graph allows assessment of linearity, monotonicity, convexity, etc.\n\n\nThe Algorithm\nHere is the step-by-step recipe for constructing a binscatter plot.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nBin construction: Divide the range of \\(X\\) into \\(K\\) equal-width bins, or use quantile-based bins for equal sample sizes within bins. For example, with \\(K=10\\), the observations in \\(B_1\\) would be those between the minimum value of \\(X\\) and that of its tenth percentile.\nMean calculation: Compute the mean of \\(Y\\) within each bin:\n\n\\[\\bar{Y}_k= \\frac{1}{|B_k|} \\sum_{i \\in B_k} Y_i,\\]\nwhere \\(|B_k|\\) is the number of observations in bin \\(B_k\\)​.\n\nPlotting: Plot \\(\\bar{Y}_k\\) against the midpoints of each bin, \\(\\bar{X}_k\\).\n\n\n\nSoftware Package: binsreg.\nQuite simple, right? Let’s explore certain useful extensions of this idea.\n\n\nAdjusting for Covariates: The Wrong Way\nIn many applications, it is essential to control for additional covariates \\(W\\) to isolate the relationship between the primary variables of interest. The object of interest then becomes the conditional mean \\(E[Y\\mid W,X]\\). An example would be focusing on the relationship between income (\\(Y\\)) and education level (\\(X\\)) when controlling for parental education (\\(W\\)).\nA common but flawed approach to incorporating covariates in binscatter is residualized binscatter. This method involves first regressing separately both \\(Y\\) and \\(X\\) on the covariates \\(W\\) to obtain residuals \\(\\hat{u}_Y\\)​ and \\(\\hat{u}_X\\)​, and then applying the binscatter method to these residuals:\n\\[\\bar{\\hat{u}}_{Y,k} = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\hat{u}_{X,i}.\\]\nWhile this approach is motivated by the Frisch-Waugh-Lovell theorem in linear regression, it can lead to incorrect conclusions in more general settings. The residualized binscatter may not accurately reflect the true conditional mean function, especially if the underlying relationship is nonlinear. Therefore, it is generally not recommended for empirical work.\n\n\nAdjusting for Covariates: The Right Way\nInstead, this should be done using a semi-parametric partially linear regression model. This is achieved by modeling the conditional mean function as\n\\[Y = \\mu_0(X) + W \\gamma_0 + \\varepsilon,\\]\nwhere \\(\\mu_0(X)\\) captures the main effect of \\(X\\), and \\(W' \\gamma_0\\) adjusts for the influence of additional covariates. Rather than residualizing, we estimate \\(\\mu_0(X)\\) using the least-squares approach:\n\\[(\\hat{\\beta}, \\hat{\\gamma}) = \\arg\\min_{\\beta, \\gamma} \\sum (Y- b(X)' \\beta - W' \\gamma)^2,\\]\nwhere \\(b(X)\\) represents the binning basis functions. The final binscatter plot displays the estimated conditional mean function\n\\[\\hat{\\mu}(X_k) = b(X_k)' \\hat{\\beta}\\]\nagainst \\(\\bar{X}_k\\), ensuring a correct visualization of the relationship between \\(X\\) and \\(Y\\) after accounting for the covariates \\(W\\).\n\n\nPractical Considerations\nA key decision is the choice of the number of bins \\(K\\). Too few bins can oversmooth the data, masking important features, while too many bins can lead to undersmoothing, resulting in a noisy and less interpretable plot. An optimal choice of \\(K\\) balances bias and variance, often determined using data-driven methods. To address this, Cattaneo et al. (2024) propose an adaptive, Integrated Mean Squared Error (IMSE)-optimal choice of \\(K\\) for which get a plug-in formula.\nThoughtful data scientist always have variance in their mind. If, for instance, we see some linear relationship between \\(Y\\) and \\(X\\), how can we determine whether it is statistically significant? Quantifying the uncertainty around binscatter estimates is crucial. The authors also discuss constructing confidence bands, which can be added to the plot to visually represent estimation uncertainty, enhancing both interpretability and reliability."
  },
  {
    "objectID": "blog/binscatter.html#an-example",
    "href": "blog/binscatter.html#an-example",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "An Example",
    "text": "An Example\nAs an example let’s examine the relationship between the variables Sepal.Length and Petal.Length in the popular iris dataset. We will use a fixed number of ten bins. Alternatively, the package binsreg will automatically calculate the optimal \\(K\\).\n\nRPython\n\n\n# clear the workspace and load libraries\nrm(list=ls())\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(binsreg)\ndata(iris)\n\n# define the number of bins\nbins &lt;- 10\n\n# create binned data\niris_binned &lt;- iris %&gt;%\n  mutate(bin = cut(Sepal.Length, breaks = bins, include.lowest = TRUE)) %&gt;%\n  group_by(bin) %&gt;%\n  summarize(\n    bin_mid = mean(as.numeric(as.character(bin))),\n    mean_petal_length = mean(Petal.Length)\n  )\n\n# Add a panel label for the raw scatter plot\niris_raw &lt;- iris %&gt;% \n    mutate(panel = \"1. Raw Scatter Plot\")\n\n# Add a panel label for the binned scatter plot\niris_binned &lt;- iris_binned %&gt;%\n  mutate(panel = \"2. Binned Scatter Plot\")\n\n# Combine raw and binned data into a single dataset for plotting\nplot_data &lt;- bind_rows(\niris_raw %&gt;% rename(x = Sepal.Length, y = Petal.Length),\n  iris_binned %&gt;% rename(x = bin_mid, y = mean_petal_length)\n)\n\n# Create the plot\nggplot(plot_data, aes(x = x, y = y)) +\n  geom_point() +\n  facet_wrap(~ panel, scales = \"free_x\", ncol = 2) +\n  labs(title = \"Comparison of Raw and Binned Scatter Plots\",\n  x = \"Sepal Length\",\n  y = \"Petal Length\") +\n  theme_minimal()\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Number of bins\nbins = 10\n\n# Create binned data\niris['bin'] = pd.cut(iris['Sepal.Length'], bins=bins, include_lowest=True)\niris_binned = iris.groupby('bin').agg(\n    bin_mid=('Sepal.Length', lambda x: (x.min() + x.max()) / 2),\n    mean_petal_length=('Petal.Length', 'mean')\n).reset_index()\n\n# Add panel labels\niris_raw = iris[['Sepal.Length', 'Petal.Length']].copy()\niris_raw['panel'] = \"1. Raw Scatter Plot\"\n\niris_binned = iris_binned.rename(columns={'bin_mid': 'Sepal.Length', 'mean_petal_length': 'Petal.Length'})\niris_binned['panel'] = \"2. Binned Scatter Plot\"\n\n# Combine raw and binned data\nplot_data = pd.concat([iris_raw, iris_binned], ignore_index=True)\n\n# Plot\nsns.set_theme(style=\"whitegrid\")\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Raw scatter plot\nsns.scatterplot(\n    data=plot_data[plot_data['panel'] == \"1. Raw Scatter Plot\"],\n    x='Sepal.Length', y='Petal.Length', ax=axes[0]\n)\naxes[0].set_title(\"1. Raw Scatter Plot\")\naxes[0].set_xlabel(\"Sepal Length\")\naxes[0].set_ylabel(\"Petal Length\")\n\n# Binned scatter plot\nsns.scatterplot(\n    data=plot_data[plot_data['panel'] == \"2. Binned Scatter Plot\"],\n    x='Sepal.Length', y='Petal.Length', ax=axes[1]\n)\naxes[1].set_title(\"2. Binned Scatter Plot\")\naxes[1].set_xlabel(\"Sepal Length\")\n\n# Adjust layout\nplt.suptitle(\"Comparison of Raw and Binned Scatter Plots\")\nplt.tight_layout()\nplt.show()\n\n\n\nHere is the resulting image. The left scatter plot displays the raw data and the right one shows the binscatter. Binscatter removes some of the clutter and highlights the linear relationship more directly."
  },
  {
    "objectID": "blog/binscatter.html#bottom-line",
    "href": "blog/binscatter.html#bottom-line",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBinscatter simplifies scatterplots by aggregating data into bins and plotting means.\nIt is a powerful tool for visualizing relationships in large or noisy datasets.\nConditional and residualized binscatter extend its utility to controlling for covariates.\nWhile intuitive, binscatter is sensitive to binning choices and may obscure nuances."
  },
  {
    "objectID": "blog/binscatter.html#where-to-learn-more",
    "href": "blog/binscatter.html#where-to-learn-more",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nBoth papers cited below are relatively accessible and will answer your questions. Start with Starr and Goldfarb (2020)."
  },
  {
    "objectID": "blog/binscatter.html#references",
    "href": "blog/binscatter.html#references",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "References",
    "text": "References\nCattaneo, M. D., Crump, R. K., Farrell, M. H., & Feng, Y. (2024). On Binscatter Regressions. American Economic Review, 111(3), 718–748.\nStarr, E., & Goldfarb, B. (2020). Binned scatterplots: A simple tool to make research easier and better. Strategic Management Journal, 41(12), 2261-2274."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#background",
    "href": "blog/limits-nonparametric-models.html#background",
    "title": "The Limits of Nonparametric Models",
    "section": "Background",
    "text": "Background\nNonparametric statistics offers a powerful toolkit for data analysis when the underlying data-generating process is too complex or unknown to be captured by parametric models. Unlike parametric methods, which assume a specific form for the underlying distribution or functional relationship, nonparametric approaches let the data “speak for itself.”\nIn earlier articles, we explored parametric and semiparametric models, where the focus was on attaining the lowest possible variance. However, in nonparametric statistics, this goal is unsuitable, as prioritizing minimal variance often introduces significant bias. The spotlight falls instead on selecting the optimal tuning parameter: the bandwidth. This article explains the various considerations and mathematical details associated with selecting the optimal bandwidth value in Kernel Density (KD) and Kernel Regression (KR) estimation."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#notation",
    "href": "blog/limits-nonparametric-models.html#notation",
    "title": "The Limits of Nonparametric Models",
    "section": "Notation",
    "text": "Notation\nWe begin by establishing the mathematical terminology and notations essential for our analysis.\n\nKernels\nA kernel function \\(K(\\cdot)\\) is a non-negative, symmetric function that integrates to \\(1\\) over its domain. It acts as a weighting function that determines how observations contribute to the estimate at any given point, effectively spreading the mass of each data point over a local neighborhood.\n\n\nKernel Density\nKD is sort of like creating a smooth histogram. The kernel density estimator for a sample of n observations \\(X_1, X_2, \\dots, X_n \\in \\mathbb{R}\\) is given by:\n\\[\\hat{f}_h (x) = \\frac{1}{n h} \\sum_i K\\left(\\frac{x - X_i}{h}\\right),\\]\nwhere \\(h &gt; 0\\) is the bandwidth, a positive scalar controlling the smoothness of the estimate. Its function is to determine the window length around x in which the observations get positive weight.\nThe value \\(\\hat{f}_h (x)\\) will be large when there are many data points around \\(x\\), and small otherwise. Some popular kernel functions like Epanechnikov, Gaussian or triangular assign higher weights to observations closer to \\(x\\) and decaying importance to data further away.\nDensity estimation can also be performed in higher dimensions where the curse of dimensionality lurks in the background. In two dimensions, these estimates typically take the form of heat map-type figures.\n\n\nKernel Regression\nKernel regression, also known as local regression, is much like fitting a flexible smooth curve through data points. In this context, we have access to \\(n\\) observations of an outcome variable \\(Y\\). The objective is to estimate the conditional mean function at some point X=x:\n\\[m(x) = \\mathbb{E}[Y \\mid X = x].\\]\nAmong various kernel regression methods, the Nadaraya-Watson (NW) estimator is particularly popular. It is defined as:\n\\[\\hat{m}_h (x) = \\frac{\\sum_i K\\left(\\frac{x - X_i}{h}\\right) Y_i}{\\sum_i K\\left(\\frac{x - X_i}{h}\\right)}.\\]\nThe NW method fits a local constant around \\(x\\) equal to the average \\(Y\\) in that region (determined by the bandwidth). This approach is also known as local constant regression. More sophisticated variants include local linear and quadratic methods, which extend this fundamental idea but require more data. Across all these approaches, the bandwidth \\(h\\) serves as the key parameter that determines the trade-off between bias and variance, a relationship we will explore in detail below."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#a-closer-look",
    "href": "blog/limits-nonparametric-models.html#a-closer-look",
    "title": "The Limits of Nonparametric Models",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn practice, the choice of kernel is not as consequential. The primary focus of nonparametric statistics is selecting the optimal bandwidth \\(h\\).\n\nThe Bias-Variance Tradeoff\nAt the heart of bandwidth selection is the bias-variance tradeoff. Intuitively:\n\nSmall bandwidth (“overfitting”): Captures fine details but also noise, leading to high variance and low bias.\nLarge bandwidth (“underfitting”): Smoothes over the noise but can miss important structure, leading to high bias and low variance.\nFor nonparametric curve estimation, the mean integrated squared error (MISE) is a common loss function for evaluating performance:\n\n\\[\\text{MISE}(h) = \\mathbb{E}\\left[ \\int \\left( \\hat{f}_h(x) - f(x) \\right)^2 dx \\right].\\]\nThis decomposes into bias and variance terms:\n\\[\\text{MISE}(h) =  \\int \\text{Bias}^2(\\hat{f}_h(x)) dx + \\int \\text{Var}(\\hat{f}_h(x)) dx.\\]\nMISE is analogous to the mean squared error (MSE) commonly used in machine learning, but integrated, or summed, across the support of the data. Minimizing the MISE yields the optimal bandwidth by balancing the bias-variance tradeoff, a fundamental concept in statistical estimation theory.\n\n\nOptimal \\(h\\) for Kernel Density\nSilverman’s rule of thumb provides a practical, closed-form approximation for \\(h\\) in KDE, assuming the data is roughly Gaussian. The expression is:\n\\[h^* = \\left( \\frac{4\\sigma^5}{3n} \\right) ^{\\frac{1}{5}}  = 1.06 \\sigma n^{-1/5},\\]\nwhere \\(\\sigma\\) is the standard deviation of the data. This formula emerges from theoretical analysis of the bias-variance tradeoff and serves as an effective initial bandwidth choice. While it performs well for many datasets, particularly those with approximately normal distributions, it may not be optimal for multimodal or heavily skewed distributions.\n\n\nOptimal \\(h\\) for Kernel Regression\nFor Nadaraya-Watson regression, several methods exist for selecting the bandwidth h. Cross-validation provides reliable results but can be computationally demanding. Theoretical analysis based on MISE minimization for common kernel functions suggests that the optimal bandwidth follows the relationship:\n\\[h^* \\propto n^{-1/5}.\\]\nThis expression highlights a fundamental characteristic of nonparametric methods: their convergence rate is slower than that of parametric methods. This represents one manifestation of the curse of dimensionality in nonparametric estimation. Furthermore, this relationship only specifies the rate at which the optimal bandwidth should decrease with sample size (proportionality to \\(n^{-1/5}\\)), rather than providing an exact value. Without the proportionality constant, its direct practical application is limited."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#an-example",
    "href": "blog/limits-nonparametric-models.html#an-example",
    "title": "The Limits of Nonparametric Models",
    "section": "An Example",
    "text": "An Example\nLet’s illustrate bandwidth selection with a simple example.\n\nRPython\n\n\n# clear workspace\nrm(list=ls())\n\n# load data\nrequire(graphics)\n\n# plot\nwith(cars, {\n    plot(speed, dist)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 2), col = 2)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 5), col = 3)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 10), col = 4)\n})\n\n\n# same idea, but with the iris dataset\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom statsmodels.nonparametric.kernel_regression import KernelReg\n\n# Load iris dataset\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n# Choose two continuous variables\nx = df['sepal length (cm)'].values\ny = df['petal length (cm)'].values\n\n# Define a function to plot kernel smoothed lines with varying bandwidths\ndef plot_kernel_smoothing(x, y, bandwidths):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(x, y, alpha=0.5, label='Data', color='black')\n\n    x_grid = np.linspace(x.min(), x.max(), 200)\n    \n    colors = ['red', 'green', 'blue']\n    for bw, color in zip(bandwidths, colors):\n        kr = KernelReg(endog=[y], exog=[x], var_type='c', bw=[bw])\n        mean, _ = kr.fit(x_grid)\n        plt.plot(x_grid, mean, label=f'bandwidth={bw}', color=color)\n\n    plt.xlabel('Sepal Length (cm)')\n    plt.ylabel('Petal Length (cm)')\n    plt.title('Kernel Smoothing with Varying Bandwidths')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Call function with different bandwidths\nplot_kernel_smoothing(x, y, bandwidths=[0.2, 0.5, 1.0])\n\n\n\nThis yields a simplified version of the following figure:\n\nThe green line corresponds to the Nadaraya-Watson regression with an optimal bandwidth and features some degree of curvature. The red line has a smaller bandwidth (higher variance, lower bias), and the blue line is undersmoothed (higher bandwidth)."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#bottom-line",
    "href": "blog/limits-nonparametric-models.html#bottom-line",
    "title": "The Limits of Nonparametric Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBandwidth selection is critical for nonparametric methods to balance bias and variance.\nSilverman’s rule of thumb offers a simple yet effective starting point for KD.\nFor commonly used second-order kernels, the optimal bandwidth in KR scales as \\(n^{-1/5}\\).\nPractical implementation often relies on cross-validation or plug-in methods and is limited by the curse of dimensionality."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#references",
    "href": "blog/limits-nonparametric-models.html#references",
    "title": "The Limits of Nonparametric Models",
    "section": "References",
    "text": "References\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.\nLi, Q., & Racine, J. S. (2023). Nonparametric econometrics: theory and practice. Princeton University Press.\nSilverman, B. W. (1986). Density Estimation for Statistics and Data Analysis. Wand, M. P., & Jones, M. C. (1995). Kernel Smoothing.\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#background",
    "href": "blog/hypothesis-testing-all-data.html#background",
    "title": "Hypothesis Testing with Population Data",
    "section": "Background",
    "text": "Background\nClassical statistical theory is built on the idea of working with a sample of data from a given population of interest. Our software packages compute confidence intervals to reflect precisely this – we observe only a small part of that population.\nIn modern times, however, we often work with all data points, and not just random samples. Examples abound, especially in the tech industry. Companies store all sales and website activity, the FBI records all homicides, and school records contain information on all students.\nHow can we interpret confidence intervals when we work with such datasets? More generally, how do we think about uncertainty in these settings? We know exactly how many items are sold or how many homicides occur each year; nothing is uncertain about that.\nThe short answer is that the confidence intervals in this setting have a fundamentally different interpretation – one reflecting parameters of an underlying metaphorical population."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#a-closer-look",
    "href": "blog/hypothesis-testing-all-data.html#a-closer-look",
    "title": "Hypothesis Testing with Population Data",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nAn Example\nLet’s focus on a specific example – homicides in the US. According to the Crime in the US report published by the FBI, in 2018, there were \\(14,123\\) homicides, and for 2019 this number was \\(13,927\\). This is a decrease of \\(196\\) cases, or roughly equivalent to a \\(1.4\\%\\) drop.\nMother nature and the world around us are incredibly complex, so numbers around us can go up and down for no obvious reason. So, does this drop reflect a real change in the underlying crime rate?\nTo answer this question, it is helpful to model annual homicides as coming from a Poisson distribution from a figurative population of alternative US histories. This distribution has a mean \\(\\lambda\\) equal to the hypothetical true underlying homicide rate. We want to know whether \\(\\lambda\\) changed from 2018 to 2019.\nThe confidence interval for the change in this underlying homicide rate is:\n\\[ (14,123-13,927) \\pm 1.96 \\times \\sqrt{14,123+13,927}=(-132.26, 524.26). \\]\nThis interval clearly contains \\(0\\), so we cannot conclude that there was a real drop in the crime rate between 2018 and 2019. In other words, the \\(1.4\\%\\) drop in homicides between 2018 and 2019 was within the range consistent with the noise in our world. It should not be confused with increased underlying safety in the US.\n\n\nOne More Thing\nThis type of thinking is also helpful in a slightly different context. Let’s focus on 2018, when there were \\(14,123\\) homicides in the US, corresponding to an average daily rate of about \\(38.7\\) cases.\nImagine someone asked us to calculate the probability that there would be less than \\(25\\) cases on a given day, but no such day took place in 2018. It would still be naïve to conclude that the probability of this event was zero.\nWe can look at the left tail of the Poisson distribution with mean \\(\\lambda = 38.7\\) to answer this question:\n\nRPython\n\n\nppois(25, lambda=38.7)\n&gt;[1] 0.01270669\n\n\nfrom scipy.stats import poisson\n\n# Define the mean of the Poisson distribution\nlambda_value = 38.7\n\n# Calculate the cumulative probability for less than 25 cases\nprobability = poisson.cdf(25, mu=lambda_value)\nprint(probability)\n&gt;[1] 0.01270669\n\n\n\nThis gives us a \\(1.27%\\) probability of such an event, suggesting that, on average, there should be about \\(4.6\\) such days per year. Data would not help answer this question."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#bottom-line",
    "href": "blog/hypothesis-testing-all-data.html#bottom-line",
    "title": "Hypothesis Testing with Population Data",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWorking with all data eliminates the uncertainty that usually arises in random samples.\nConfidence intervals in such settings are still meaningful – they represent uncertainty associated with the underlying parameters of a metaphorical population."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#where-to-learn-more",
    "href": "blog/hypothesis-testing-all-data.html#where-to-learn-more",
    "title": "Hypothesis Testing with Population Data",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThis article was inspired by “The Art of Statistics” (2019) which beautifully explains an impressively wide range of statistical topics in an engaging way. It is a non-technical read accessible to everyone interested in combining statistics and data to make inferences about the world."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#references",
    "href": "blog/hypothesis-testing-all-data.html#references",
    "title": "Hypothesis Testing with Population Data",
    "section": "References",
    "text": "References\nThe FBI (2018) Crime in the US.\nThe FBI (2019) Crime in the US.\nSpiegelhalter, D. (2019). The art of statistics: Learning from data. Penguin UK."
  },
  {
    "objectID": "blog/correlation-transitive.html#background",
    "href": "blog/correlation-transitive.html#background",
    "title": "Correlation is Not (Always) Transitive",
    "section": "Background",
    "text": "Background\nAt first, I found this really puzzling. \\(X\\) is correlated (Pearson) with Y, and Y is correlated with \\(Z\\). Does this mean X is necessarily correlated with \\(Z\\)? Intuitively, this totally makes sense. The answer, however, is “no.”\nPerhaps the strangest thing is how easy it is to rationalize this “puzzle.” I drink more beer (\\(X\\)) and read more books (\\(Z\\)) when I am on a vacation (Y). That is, both pairs – \\(X\\) and \\(Y\\) and \\(Z\\) and \\(Y\\) – are positively correlated. But I do not drink more beer when I read more books – \\(X\\) and \\(Z\\) are not correlated. It is now obvious that correlation is not (always) transitive, but a second ago, this sounded bizarre."
  },
  {
    "objectID": "blog/correlation-transitive.html#a-closer-look",
    "href": "blog/correlation-transitive.html#a-closer-look",
    "title": "Correlation is Not (Always) Transitive",
    "section": "A Closer Look",
    "text": "A Closer Look\nLet’s denote the respective correlations between \\(X\\), \\(Y\\) and \\(Z\\) by \\(cor(X,Y)\\), \\(cor(X,Z)\\), and \\(cor(Y,Z)\\). For simplicity (and without loss of generality), let’s work with standardized versions of these variables – that is, means of \\(0\\) and variances of \\(1\\). This implies, \\(cov(X,Y) = cor(X,Y)\\) for any pair.\nWe can write the linear projections of \\(X\\) and \\(Z\\) on \\(Y\\) as follows:\n\\[ X = cor(X,Y)Y + \\epsilon^{X,Y}, \\]\n\\[ Z = cor(Z,Y)Y + \\epsilon^{Z,Y}. \\]\nThen, we have:\n\\[ cor(X,Z)=cor(X,Y)cor(Z,Y)+cor(\\epsilon^{X,Y},\\epsilon^{Z,Y}).\\]\nWe can use the Cauchy-Schwarz inequality to bound the last term, which gives the final range of possible values for cor(X,Z):\n\\[cor(X,Y)cor(Z,Y) - \\sqrt{(1-cor(X,Y)^2) (1-cor(Z,Y)^2)}\\]\n\\[\\leq cor(X,Z) \\leq  \\]\n\\[cor(X,Y)cor(Z,Y) + \\sqrt{(1-cor(X,Y)^2) (1-cor(Z,Y)^2)}\\]\nFor instance, if we set \\(cor(X,Y)=cor(Z,Y)=0.6\\), then we get:\n\\[-.28 \\leq cor(X,Z) \\leq 1.\\]\nThat is, \\(cor(X,Z)\\) can be negative.\n\nExamples\n\n\nExample: City Lifestyle\nLet’s go through three examples of non-transitive correlations.\nLet \\(X\\) represent the number of hours a person spends commuting daily in a city, \\(Y\\) represent their monthly public transit expenses, and \\(Z\\) represent their daily step count. \\(X\\) and \\(Y\\) are correlated because longer commutes (\\(X\\)) typically involve more frequent or longer public transit use, increasing transit expenses (\\(Y\\)). Similarly, \\(X\\) and \\(Z\\) are correlated since longer commutes (\\(X\\)) often involve more walking to and from transit stops, boosting step count (\\(Z\\)). However, \\(Y\\) and \\(Z\\) are not correlated because transit expenses (\\(Y\\)) depend on fare structures and trip frequency, while step count (\\(Z\\)) is influenced by walking habits unrelated to cost, such as choosing to walk shorter distances or using different transit routes, resulting in no direct relationship between the two.\nConsider \\(X\\) as the number of hours a person exercises per week, \\(Y\\) as their muscle mass, and \\(Z\\) as their resting heart rate. \\(X\\) and \\(Y\\) are correlated because more exercise (\\(X\\)) typically increases muscle mass (\\(Y\\)). Likewise, \\(X\\) and \\(Z\\) are correlated since regular exercise (\\(X\\)) tends to lower resting heart rate (\\(Z\\)). However, \\(Y\\) and \\(Z\\) are not correlated because muscle mass (\\(Y\\)) and resting heart rate (\\(Z\\)) are influenced by different physiological mechanisms—muscle mass depends on strength training, while heart rate is more tied to cardiovascular fitness—and thus show no direct relationship.\nPerhaps the simplest example to illustrate this mathematically is:\n\n\\(X\\) and \\(Z\\) are independent random variables,\n\\(Y=X+Z\\). The result follows.\n\nThe following code sets up this example in R and python.\n\nRPython\n\n\nrm(list=ls())\nset.seed(68493)\n\nx &lt;- runif(n=1000)\nz &lt;- runif(n=1000)\ny &lt;- x + z\n\ncor(y, x)\ncor(y, z)\ncor(z, x)\n\ncor.test(y, x, alternative='two.sided', method='pearson')\ncor.test(y, z, alternative='two.sided', method='pearson')\ncor.test(z, x, alternative='two.sided', method='pearson')\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr\n\n# Set seed for reproducibility\nnp.random.seed(68493)\n\n# Generate random variables\nx = np.random.uniform(size=1000)\nz = np.random.uniform(size=1000)\ny = x + z\n\n# Compute correlations\nprint(\"cor(y, x):\", np.corrcoef(y, x)[0, 1])\nprint(\"cor(y, z):\", np.corrcoef(y, z)[0, 1])\nprint(\"cor(z, x):\", np.corrcoef(z, x)[0, 1])\n\n# Perform correlation tests\nprint(\"cor.test(y, x):\", pearsonr(y, x))\nprint(\"cor.test(y, z):\", pearsonr(y, z))\nprint(\"cor.test(z, x):\", pearsonr(z, x))\n\n\n\nBelow is a table with correlation coefficients and \\(p\\)-values associated with the null hypotheses that they are equal to zero.\n\n\n\n\nvars\ncor. coef.\n\\(p\\)-value\n\n\n\n\n\\(cor(X,Y)\\)\n0.68\n0.00\n\n\n\\(cor(Z,Y)\\)\n0.70\n0.00\n\n\n\\(cor(X,Z)\\)\n-0.05\n0.15"
  },
  {
    "objectID": "blog/correlation-transitive.html#when-is-correlation-transitive",
    "href": "blog/correlation-transitive.html#when-is-correlation-transitive",
    "title": "Correlation is Not (Always) Transitive",
    "section": "When Is Correlation Transitive",
    "text": "When Is Correlation Transitive\nFrom the equation above it follows that when both \\(cor(X,Y)\\) and \\(cor(Z,Y)\\) are sufficiently large, then \\(cor(X,Z)\\) is sure to be positive (i.e., bounded below by \\(0\\)).\nIn the example above, if we fix \\(cor(X,Y)=.6\\), then we need \\(cor(Z,Y)&gt;.8\\) to guarantee that \\(cor(X,Z)&gt;0\\)."
  },
  {
    "objectID": "blog/correlation-transitive.html#where-to-learn-more",
    "href": "blog/correlation-transitive.html#where-to-learn-more",
    "title": "Correlation is Not (Always) Transitive",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMultiple Stack Overflow threads explain this phenomenon from various angles. Olkin (1981) derives some further mathematical results related to transitivity in higher dimensions."
  },
  {
    "objectID": "blog/correlation-transitive.html#bottom-line",
    "href": "blog/correlation-transitive.html#bottom-line",
    "title": "Correlation is Not (Always) Transitive",
    "section": "Bottom Line",
    "text": "Bottom Line\n\n\\(X\\) and \\(Z\\) both being correlated with \\(Y\\) does not guarantee that \\(X\\) and \\(Z\\) are correlated with each other.\nThis is the case when the former two correlations are “large enough.”"
  },
  {
    "objectID": "blog/correlation-transitive.html#references",
    "href": "blog/correlation-transitive.html#references",
    "title": "Correlation is Not (Always) Transitive",
    "section": "References",
    "text": "References\nOlkin, I. (1981). Range restrictions for product-moment correlation matrices. Psychometrika, 46, 469-472. doi:10.1007/BF02293804"
  },
  {
    "objectID": "blog/limits-parametric-models.html#background",
    "href": "blog/limits-parametric-models.html#background",
    "title": "The Limits of Parametric Models: The Cramér-Rao Bound",
    "section": "Background",
    "text": "Background\nObtaining the lowest possible variance is a primary goal for anyone working with statistical models. Efficiency (or precision), as is the jargon, is a cornerstone of statistics and econometrics, guiding us toward estimators that extract the maximum possible information from the data. It can make or break a data project.\nThe Cramér-Rao lower bound (CRLB) plays a pivotal role in this context by establishing a theoretical limit on the variance of unbiased estimators. Unbiased estimators are those that yield the true answer (on average), rendering them a highly attractive class of methods. The CRLB highlights the best achievable precision for parameter estimation based on the Fisher information in the data. This article explores the theoretical foundation of the CRLB, its computation, and its implications for practical estimation.\nIn what follows, I am concerned with unbiased estimators, a common practice that should not be taken for granted. As a counterexample, consider the James-Stein estimator —a biased but attractive technique."
  },
  {
    "objectID": "blog/limits-parametric-models.html#notation",
    "href": "blog/limits-parametric-models.html#notation",
    "title": "The Limits of Parametric Models: The Cramér-Rao Bound",
    "section": "Notation",
    "text": "Notation\nBefore diving in, let’s establish a unified notation to structure the mathematical discussion:\n\nLet X denote the observed data, with \\(X_1, X_2, \\dots, X_n\\) being n independent and identically distributed (i.i.d.) observations.\nThe model governing the data is characterized by a (finite-dimensional) parameter \\(\\theta \\in \\mathbb{R}^d\\) which we aim to estimate.\nThe likelihood of the data is \\(f(x; \\theta)\\), fully specified by the parameter \\(\\theta\\)."
  },
  {
    "objectID": "blog/limits-parametric-models.html#a-closer-look",
    "href": "blog/limits-parametric-models.html#a-closer-look",
    "title": "The Limits of Parametric Models: The Cramér-Rao Bound",
    "section": "A Closer Look",
    "text": "A Closer Look\nThe Cramér-Rao lower bound provides a theoretical benchmark for how precise an unbiased estimator can be. It sets the minimum variance that any unbiased estimator of a parameter \\(\\theta\\) can achieve, given a specific data-generating process.\n\nThe CRLB Formula\nFor a parameter \\(\\theta\\) in a parametric model with likelihood \\(f(x; \\theta)\\), the CRLB is expressed as:\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{I(\\theta)},\\]\nwhere \\(I(\\theta)\\) is the Fisher information (FI), defined as:\n\\[I(\\theta) = \\mathbb{E}\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 \\right].\\]\n\n\nIntuition\nTo understand the CRLB, we must delve into the concept of Fisher information named after one of the modern fathers of statistics R.A. Fisher. Intuitively, FI quantifies how much information the observed data carries about the parameter \\(\\theta\\).\nThink of the likelihood function \\(f(x; \\theta)\\) as describing the probability of observing a given dataset \\(x\\) for a particular value of \\(\\theta\\). If the likelihood changes sharply with \\(\\theta\\) (i.e., \\(\\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta)\\) is large), small changes in \\(\\theta\\) lead to noticeable differences in the likelihood. This variability reflects high information: the data can “pinpoint” \\(\\theta\\) with greater precision. Conversely, if the likelihood changes slowly with \\(\\theta\\), the data offers less information about its true value.\nMathematically, the Fisher information \\(I(\\theta)\\) is the variance of the the partial derivative\n\\[\\frac{\\partial}{\\partial \\theta} logf(x;\\theta),\\]\nwhich we refer to as the score function. This score measures how sensitive the likelihood function is to changes in \\(\\theta\\). Higher variance in the score corresponds to more precise information about \\(\\theta\\).\n\n\nPractical Application\nThe CRLB provides a benchmark for evaluating the performance of estimators. For example, if you propose an unbiased estimator \\(\\hat{\\theta}\\), you can compare its variance to the CRLB. If \\(\\text{Var}(\\hat{\\theta}) = \\frac{1}{I(\\theta)}\\), we say the estimator is efficient. However, if the variance is higher, there may be room to improve the estimation method.\nMoreover, the CRLB also offers insight into the difficulty of estimating a parameter. If \\(I(\\theta)\\) is “small”, so that the bound on the variance is high, then no unbiased estimator can achieve high precision with the available data. It is possible to develop a biased estimator for \\(\\theta\\) with lower variance, but it is not clear why you would do that."
  },
  {
    "objectID": "blog/limits-parametric-models.html#an-example",
    "href": "blog/limits-parametric-models.html#an-example",
    "title": "The Limits of Parametric Models: The Cramér-Rao Bound",
    "section": "An Example",
    "text": "An Example\nImagine you are estimating the mean of a normal distribution, where \\(X \\sim N(\\mu, \\sigma^2)\\), and \\(\\sigma^2\\) is known. The likelihood for a single observation \\(x_i\\) is:\n\\[f(x_i;\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2 \\sigma^2}}.\\]\nUsing the Fisher information definition given above, taking the derivative and simplifying, we find:\n\\[I(\\mu)=  \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 = \\frac{1}{\\sigma^2}.\\]\nFor n independent observations, this expression becomes:\n\\[I(\\mu)=\\frac{n}{\\sigma^2}.\\]\nThe CRLB for the variance of any unbiased estimator of is:\n\\[\\text{Var}(\\hat{\\mu})\\geq \\frac{\\sigma^2}{n}\\]\nThis result aligns with our intuition: as n increases, the precision of our estimate improves. In other words, more data leads to more informative results."
  },
  {
    "objectID": "blog/limits-parametric-models.html#where-to-learn-more",
    "href": "blog/limits-parametric-models.html#where-to-learn-more",
    "title": "The Limits of Parametric Models: The Cramér-Rao Bound",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAny graduate econometrics textbook will do. Personally, my grad school nightmares were induced by Greene’s textbook (cited below). It can be dry but certainly contains what you need to know."
  },
  {
    "objectID": "blog/limits-parametric-models.html#bottom-line",
    "href": "blog/limits-parametric-models.html#bottom-line",
    "title": "The Limits of Parametric Models: The Cramér-Rao Bound",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe CRLB establishes a theoretical lower limit on the variance of unbiased estimators, serving as a benchmark for efficiency.\nFisher information measures the sensitivity of the likelihood to changes in the parameter \\(\\theta\\), linking the amount of information in the data to the precision of estimation.\nEfficient estimators achieve the CRLB and are optimal under the given model assumptions."
  },
  {
    "objectID": "blog/limits-parametric-models.html#references",
    "href": "blog/limits-parametric-models.html#references",
    "title": "The Limits of Parametric Models: The Cramér-Rao Bound",
    "section": "References",
    "text": "References\nGreene, William H. “Econometric analysis”. New Jersey: Prentice Hall (2000): 201-215."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html#background",
    "href": "blog/gen-vars-predefined-corr.html#background",
    "title": "Generating Variables with Predefined Correlation",
    "section": "Background",
    "text": "Background\nSuppose you are working on a project where the relationship between two variables is influenced by an unobserved confounder, and you want to simulate data that reflects this dependency. Standard random number generators often assume independence between variables, making them unsuitable for this task. Instead, you need a method to introduce specific correlations into your data generation process.\nA powerful and efficient way to achieve this is through Cholesky decomposition. By decomposing a correlation matrix into its triangular components, you can transform independent random variables into correlated ones. This approach is versatile, efficient, and mathematically grounded, making it ideal for simulating realistic datasets with predefined (linear) relationships."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html#a-closer-look",
    "href": "blog/gen-vars-predefined-corr.html#a-closer-look",
    "title": "Generating Variables with Predefined Correlation",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Algorithm\nAssume we want to generate a vector Y with n observations and p variables with a target correlation matrix \\(\\Sigma\\). The algorithm to obtain \\(Y\\) is as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nStart with Independent Variables: Create a matrix \\(X\\) of dimensions \\(n \\times p\\), where each column is independently drawn from N(0,1): \\[ X = \\begin{bmatrix}x_{11} & x_{12} & \\cdots & x_{1p} \\\\x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\x_{n1} & x_{n2} & \\cdots & x_{np}.\\end{bmatrix} \\]\nDecompose the Target Matrix: Perform Cholesky decomposition on the target correlation matrix \\(\\Sigma\\) as: \\[\\Sigma = LL^T,\\] where \\(L\\) is a lower triangular matrix.\nTransform the Independent Variables: Multiply the independent variable matrix \\(X\\) by \\(L\\) to obtain the correlated variables: \\[Y = XL.\\]\n\n\n\nHere \\(Y\\) is an \\(n\\times p\\) matrix where the columns have the desired correlation structure defined by \\(\\Sigma\\). To ensure that \\(\\Sigma\\) is a valid correlation matrix, it must be positive-definite. This condition guarantees the success of Cholesky decomposition and the correctness of the resulting correlated variables.\n\n\nMathematical Explanation\nLet’s examine how and why this approach works. We know that \\(\\Sigma = LL^T\\) and \\(E(XX^T)=I\\) by definition. We want to show that \\(E(YY^T)=LL^T\\). Here is the simplest way to get there:\n\\[\\begin{align*}\nE(YY^T) &= E((LX)(LX)^T) \\\\\n        &= E(LXX^TL^T) \\\\\n        &= LE(XX^T)L^T \\\\\n        &= LL^T.\n\\end{align*}\\]\nThere you have it – the algorithm outlined above is mathematically grounded. The covariance matrix of \\(Y\\) is indeed equal to \\(\\Sigma\\). Let’s now look at an example."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html#an-example",
    "href": "blog/gen-vars-predefined-corr.html#an-example",
    "title": "Generating Variables with Predefined Correlation",
    "section": "An Example",
    "text": "An Example\nLet’s implement this in R and python with \\(p=3\\) and \\(n=1,000\\). Our target correlation matrix defines the desired relationships between the variables in \\(Y\\). In our example, we have pairwise correlations equal to \\(0.8\\) (b/w \\(y_1\\) and \\(y_2\\)), \\(0.5\\) (b/w \\(y_1\\) and \\(y_3\\)), and \\(0.3\\) (b/w \\(y_2\\) and \\(y_3\\)).\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\n# Generate X, independent standard normal variables\nn &lt;- 1000 \np &lt;- 3   \nx &lt;- matrix(rnorm(n * p), nrow = n, ncol = p)\n\n# Define Sigma, the target correlation matrix\nsigma &lt;- matrix(c(\n  1.0, 0.8, 0.5,\n  0.8, 1.0, 0.3,\n  0.5, 0.3, 1.0\n), nrow = p, byrow = TRUE)\n\n# Cholesky decomposition\nL &lt;- t(chol(sigma))\ndiag &lt;- diag(c(1,1,1))\ny &lt;- t(diag %*% L %*% t(x))\n\n# Print the results\nprint(cor(y))\n          [,1]      [,2]      [,3]\n[1,] 1.0000000 0.7875707 0.5111323\n[2,] 0.7875707 1.0000000 0.3008518\n[3,] 0.5111323 0.3008518 1.0000000\n\n\nimport numpy as np\nnp.random.seed(1988)\n\n# Generate X, independent standard normal variables\nn = 1000\np = 3\nx = np.random.normal(size=(n, p))\n\n# Define Sigma, the target correlation matrix\nsigma = np.array([\n    [1.0, 0.8, 0.5],\n    [0.8, 1.0, 0.3],\n    [0.5, 0.3, 1.0]\n])\n\n# Cholesky decomposition\nL = np.linalg.cholesky(sigma)\ndiag = np.diag([1, 1, 1])\ny = (diag @ L @ x.T).T\n\n# Print results\n[[1.         0.78702913 0.48132289]\n [0.78702913 1.         0.27758356]\n [0.48132289 0.27758356 1.        ]]\n\n\n\nUsing our notation above we have:\n\\[\\Sigma = \\begin{bmatrix}1.0 & 0.8 & 0.5 \\\\0.8 & 1.0 & 0.3 \\\\ 0.5 & 0.3 &1.0\\end{bmatrix}. \\]\nThe chol function in R decomposes the matrix into a lower triangular matrix. In our example:\n\\[L^T = \\begin{bmatrix}1 & 0.8 & 0.5 \\\\0 & 0.6 & -0.17 \\\\0 & 0.0 & 0.85 \\end{bmatrix}. \\]\nMultiplying the independent variables \\(X\\) by the transpose of \\(L\\) ensures the output \\(Y\\) matches the specified correlation structure.\nThe cor function checks whether the generated data conforms to the target correlation matrix.\nThe two matrices match almost exactly. We can also visualize the three variables in a scatter plot matrix. Notice that higher correlation values (e.g., b/w \\(y_1\\) and \\(y_2\\)) correspond to stronger linear associations between."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html#bottom-line",
    "href": "blog/gen-vars-predefined-corr.html#bottom-line",
    "title": "Generating Variables with Predefined Correlation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nA common data practitioner’s need is to generate variables with a predefined correlation structure.\nCholesky decomposition offers a powerful and efficient way to achieve this."
  },
  {
    "objectID": "blog/diff-causal-predictive-models.html#background",
    "href": "blog/diff-causal-predictive-models.html#background",
    "title": "Causal vs. Predictive Modeling: Subtle, but Crucial Differences",
    "section": "Background",
    "text": "Background\nIt’s one of the most common mix-ups I see among data scientists—especially those coming from a machine learning background: confusing causal modeling with predictive modeling. On the surface, they look similar. You build a model, you include some variables, you fit it, and then you do… something with the results. But under the hood, these two approaches serve fundamentally different goals and require very different mindsets.\nPredictive modeling is about building a model that can forecast outcomes. Causal modeling is about understanding how the world works. And mixing them up can lead to some really bad decisions—like launching a product based on a spurious correlation or controlling for the wrong variables and wiping out your treatment effect.\nThis post is for all the data scientists who’ve ever wondered any of the following: - “Why can’t I just throw everything into my causal model like I do with my random forest?” - “This causal model is great. Can’t we just use it for prediction as well?” - “What exactly is the difference between the two?”\nLet’s unpack it."
  },
  {
    "objectID": "blog/diff-causal-predictive-models.html#a-closer-look",
    "href": "blog/diff-causal-predictive-models.html#a-closer-look",
    "title": "Causal vs. Predictive Modeling: Subtle, but Crucial Differences",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nPredictive Modeling\nLet’s start with what more folks are familiar with: predictive models.\nIn predictive modeling, you’re judged by how well you can forecast an outcome, \\(Y\\). That’s it. You can (and often do) throw in everything and the kitchen sink—lagged outcomes, future values of other variables (careful though!), variables that are correlated with the outcome but not necessarily meaningful in a causal sense.\nIt’s all good as long as it helps you reduce Root Mean Squared Error (RMSE), increase Area Under the Curve (AUC), or minimize cross-entropy loss. Data leakage is your main enemy, but otherwise, the bar for “what goes in the model” is pretty low. Setting data leakage and interpretation aside, just throw anything you have in there. You can often get away with building a decent model without deep institutional or context knowledge. The complex algorithms take care of that for you.\nNo one cares why your model works, only that it does.\n\n\nCausal Modeling\nNow, enter the world of causal inference. The rules are completely different. Demonstrating the challenging nature of determining causality, the ancient Greek philosopher Democritus famously said:\n\n“I would rather understand one cause than be King of Persia.”\n\nIn causal modeling, the goal is not prediction, but isolation of the effect of a treatment \\(T\\) on \\(Y\\). And to do that, you need to control for confounders—variables that affect both the treatment and the outcome. But here’s the catch: not all variables should be controlled for.\nThis is where the concept of bad controls comes in—variables that are affected by the treatment (post-treatment variables), or colliders that open up backdoor paths and induce spurious associations.\nIn other words, in causal inference:\n\nIncluding the wrong variable can make things worse.\nYou must think hard about the causal structure of your data.\nDomain knowledge is critical.\n\nThrowing in “everything” like in a predictive model? That can completely destroy your estimate. Furthermore, in causal inference you can almost never get away without deep knowledge of every aspect of your analysis - environment, intervention, sample, etc. Thus, in many ways causality is significantly more challenging than prediction.\n\n\nPropensity Scores\nOne place where this confusion often plays out is in propensity score modeling.\nTo recap, the propensity score \\(e(X) = P(T = 1 \\mid X)\\) is the probability of receiving treatment given covariates. It’s often estimated via a logistic regression or ML model. Then, you use this score to adjust for differences between treated and control groups (e.g., via weighting or matching).\nAnd here’s the key point: your goal is not to get the best prediction of treatment. Your goal is to use the propensity score to balance covariates between groups. That’s it.\nSo even if a fancy XGBoost model gives you higher prediction accuracy, it may overfit or fail to achieve covariate balance—which defeats the purpose. In fact, some of the best-performing PS models (for causal purposes) may have terrible predictive accuracy but excel at achieving balance.\nThere’s a trade-off here:\n\nPredictive ML models focus on minimizing error.\nPropensity score models should optimize covariate balance.\n\nAnd that trade-off is why a more accurate model is not necessarily better for causal inference."
  },
  {
    "objectID": "blog/diff-causal-predictive-models.html#an-example",
    "href": "blog/diff-causal-predictive-models.html#an-example",
    "title": "Causal vs. Predictive Modeling: Subtle, but Crucial Differences",
    "section": "An Example",
    "text": "An Example\nThe distinction between machine learning and causal inference is best illustrated with an example. Suppose we build a causal model to understand what drives student success (\\(Y\\)), and we find that hard work (\\(T\\)) has a causal effect on academic achievement. This means that if we were to increase a given student’s effort, we would expect—on average—a corresponding improvement in that student’s success. Causality is about what will happen if we intervene (i.e., manipulate \\(T\\)). Intervene is the key word here. Here’s the crazy thing - in observational data, student success and effort might not even be correlated!\nSo, this doesn’t necessarily help us predict with high accuracy which students will be most successful. For that task, other variables—like parental income (\\(X_1\\)), neighborhood quality (\\(X_2\\)), or prior test scores (\\(X_3\\))—might outperform hard work as predictors, even if they are not causes. In short, causal inference answers what-if questions about interventions, while machine learning focuses on associational patterns that are useful for prediction, regardless of whether they are causal."
  },
  {
    "objectID": "blog/diff-causal-predictive-models.html#bottom-line",
    "href": "blog/diff-causal-predictive-models.html#bottom-line",
    "title": "Causal vs. Predictive Modeling: Subtle, but Crucial Differences",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nPredictive models are about forecasting outcomes; causal models are about estimating effects.\nIn causal inference, you must think carefully about what to include in the model—“bad controls” can bias results.\nPropensity scores should be judged by how well they balance covariates, not by how well they predict treatment.\nMore context and domain knowledge is usually required for causal models than for predictive ones."
  },
  {
    "objectID": "blog/diff-causal-predictive-models.html#references",
    "href": "blog/diff-causal-predictive-models.html#references",
    "title": "Causal vs. Predictive Modeling: Subtle, but Crucial Differences",
    "section": "References",
    "text": "References\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If.\nCunningham, S. (2021). Causal Inference: The Mixtape.\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly Harmless Econometrics.\nPearl, J., & Mackenzie, D. (2018). The Book of Why: The New Science of Cause and Effect."
  },
  {
    "objectID": "blog/conformal-inference.html#background",
    "href": "blog/conformal-inference.html#background",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Background",
    "text": "Background\nTraditional confidence intervals estimate the range in which a population parameter, such as a mean or regression coefficient, is likely to fall with a specified level of confidence (e.g., 95%). They reflect the uncertainty in estimating a fixed but unknown parameter based on observing only a sample of the population. But what if we are interested in measuring uncertainty in the context of making predictions? Can we use traditional methods, or do we need a new framework?\nPrediction intervals provide a range within which a future individual observation is expected to fall with a given probability. Since they account for both the uncertainty in estimating the mean and the inherent variability of individual observations, they are typically wider than confidence intervals. While a confidence interval narrows as sample size increases, a prediction interval remains relatively wide due to the irreducible noise in individual data points. For simplicity, I will use both terms interchangeably.\nConformal inference offers a formalized framework for building such prediction intervals in machine learning models. This article provides a gentle introduction to the main idea behind conformal inference, presenting a new way of thinking about uncertainty in the context of machine learning (i.e., prediction) models.\nAs a teaser, the basic idea behind the method rests on a simple result about sample quantiles. Let me explain."
  },
  {
    "objectID": "blog/conformal-inference.html#notation",
    "href": "blog/conformal-inference.html#notation",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Notation",
    "text": "Notation\nLet’s imagine a size n i.i.d. sample of an outcome variable \\(Y\\) and a covariate vector \\(X\\), \\((X_1, Y_1) \\dots (X_n, Y_n)\\). Conformal inference is concerned with building a “confidence interval” for a new outcome observation \\(Y_{n+1}\\) from a new feature realization \\(X_{n+1}\\).\nImportantly, this interval should be valid:\n\nin finite samples (i.e., non-asymptotically),\nwithout assumptions on the data generating process, and\nfor any estimator of the regression function, \\(\\mu(x)=E[Y \\mid X=x]\\).\n\nIn mathematical notation, given a significance level , we want to construct a confidence interval \\(CI(X_{n+1})\\) satisfying the above properties and such that:\n\\[P(Y_{n+1} \\in CI(X_{n+1})) \\geq 1-\\alpha.\\]"
  },
  {
    "objectID": "blog/conformal-inference.html#a-closer-look",
    "href": "blog/conformal-inference.html#a-closer-look",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Sample Quantiles\nI will start with reviewing sample quantiles. Given an i.i.d. sample, \\(U_1, \\dots, U_n\\), the (\\(1-\\alpha\\))th quantile is the value \\(\\hat{q}_{1-\\alpha}\\) such that approximately \\((1-\\alpha)\\times100\\%\\) of the data is smaller than it. For instance, the \\(95\\)th quantile (sometimes also called percentile) is the value for which \\(95\\%\\) of the observations are at least as small.\nSo, given a new observation \\(U_{n+1}\\), we know that:\n\\[P(U_{n+1}\\leq \\hat{q}_{1-\\alpha})\\geq 1-\\alpha.\\]\n\n\nThe Naïve Approach\nLet’s turn back to the regression example with Y and X. We are given a new observation \\(X_{n+1}\\) and our focus is on \\(Y_{n+1}\\). Following the fact described above, a naïve way to construct a confidence interval for \\(Y_{n+1}\\) is as follows:\n\\[CI^{\\text{naïve}}_{1-\\alpha}(X_{n+1}) = \\left[ \\hat{\\mu}(X_{n+1}) \\pm \\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha} \\right].\\]\nHere \\(\\mu(\\cdot)\\) is an estimate of the regression function \\(E[Y \\mid X]\\), and \\(\\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha}\\) is the \\((1-\\alpha)\\)th quantile of empirical distribution function of the fitted residuals \\(\\mid Y-\\hat{\\mu}(X) \\mid\\).\nPut simply, we can look at an interval around our best prediction for \\(Y_{n+1}\\) (i.e., \\(\\hat{\\mu}(X_{n+1})\\)) defined by the residuals estimated on the original data.\nIt turns out this interval is too narrow. In a series of papers Vladimir Vovk and co-authors show that the empirical distribution function of the fitted residuals is often biased downward and hence this interval is invalid. This is where conformal inference comes in.\n\n\nConformal Inference\nConsider the following strategy. For each \\(y\\) we fit a regression $_y $ on the sample \\((Y_1, X_1),\\dots (Y_n, X_n), (y, X_{n+1})\\). We calculate the residuals \\(R^y_i\\) for \\(i=1,\\dots,n\\) and \\(R^y_{n+1}\\) and count the proportion of \\(R^y_i\\)’s smaller than \\(R^y_{n+1}\\). Let’s call this number \\(\\sigma(y)\\). That is,\n\\[\\sigma(y) = \\frac{1}{n+1}\\sum_{i=1}^{n+1} I (R^y_i \\leq R^y_{n+1}),\\]\nwhere \\(I(\\cdot)\\) is the indicator function equal to one when the statement in the parenthesis is true and 0 if when it is not.\nThe test statistic \\(\\sigma({Y_{n+1}})\\) is uniformly distributed over the set \\(\\{ \\frac{1}{n+1}, \\frac{2}{n+1},\\dots, 1\\}\\), implying we can use \\(1-\\sigma({Y_{n+1}})\\) as a valid p-value for testing the null that \\(Y_{n+1}=y.\\) Then, using the sample quantiles logic outlined above we arrive at the following confidence interval for \\(Y_{n+1}\\):\n\\[ CI^{\\text{conformal}}_{1-\\alpha}(X_{n+1}) \\approx \\{ y\\in \\mathbb{R} : \\sigma(y)\\leq 1-\\alpha \\}.\\]\nThis is summarized in the following procedure:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nFor each value \\(y\\):\n\n\nFit the regression function \\(\\mu(\\cdot)\\) on \\((X_1, Y_1), \\dots, (X_n, Y_n), (X_{n+1}, y)\\) using your favorite estimator/learner.\nCalculate the \\(n+1\\) residuals.\nCalculate the proportion \\(\\sigma(y)\\).\n\n\nConstruct \\(CI = \\{y: \\sigma(y) \\leq (1-\\alpha)\\}\\).\n\n\n\nSoftware Package: conformalInference\nTwo notes. First, conformal inference guarantees unconditional coverage. This is conceptually different and should not be confused with the conditional statement \\(P(Y_{n+1}\\in CI(x) \\mid X_{n+1}=x)\\geq 1-\\alpha\\). The latter is stronger and more difficult to assert, requiring additional assumptions such as consistency of our estimator of \\(\\mu(\\cdot)\\).\nSecond, this procedure can be computationally expensive. For a given value \\(X_{n+1}\\) we need to fit a regression model and compute residuals for every \\(y\\) which we consider including in the confidence interval. This is where split conformal inference comes in.\n\n\nSplit Conformal Inference\nSplit conformal inference is a modification of the original algorithm that requires significantly less computation power. The idea is to split the fitting and ranking steps, so that the former is done only once. Here is the algorithm.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRandomly split the data in two equal-sized bins.\nGet \\(\\hat{\\mu}\\) on the first bin.\nCalculate the residuals for each observation in the second bin.\nLet \\(d\\) be the \\(s\\)-th smallest residual, where \\(s=(\\frac{n}{2}+1)(1-\\alpha)\\).\nConstruct \\(CI^{\\text{split}}=[\\hat{\\mu}-d,\\hat{\\mu}+d]\\).\n\n\n\nA downside of this splitting approach is the introduction of extra randomness. One way to mitigate this is to perform the split multiple times and construct a final confidence interval by taking the intersection of all intervals. The aggregation decreases the variability from a single data split and, as this paper shows, still remains valid. Similar random split aggregation has also been used in the context of statistical significance in high-dimensional models."
  },
  {
    "objectID": "blog/conformal-inference.html#an-example",
    "href": "blog/conformal-inference.html#an-example",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "An Example",
    "text": "An Example\nI used the popular iris dataset to try out the R package conformalInference. Like most of my data demos, this is meant to be a mere illustration and you should not take the results seriously.\nThe outcome variable was Sepal.Length, and the matrix \\(X\\) included sepal.width, petal.length, petal.width, species_setosa, species_versicolor, and species_virginica. Some of these were categorical in which case I converted them to a bunch of binary variables. I used the first \\(148\\) observations to estimate the regression function \\(\\mu(X)\\) using lasso and the \\(149\\)th row to form the prediction (i.e., the test set).\nHere is the code.\n# clear workspace and load packages\nrm(list=ls())\nlibrary(conformalInference)\nlibrary(glmnet)\nlibrary(tidyverse)\n\n# load the iris dataset\ndata &lt;- iris\n\n# clean data\ncolnames(data) &lt;- tolower(colnames(data))\n\n# one-hot encode the species variable\ndata &lt;- data %&gt;%\n  mutate(species_setosa = as.integer(species == 'setosa'),\n         species_versicolor = as.integer(species == 'versicolor'),\n         species_virginica = as.integer(species == 'virginica')) %&gt;%\n  dplyr::select(-species)\n\n# check for missing values (none in iris, but included for completeness)\ndata &lt;- na.omit(data)\n\n# split training/test data\ndata0 &lt;- data %&gt;% filter(row_number() == nrow(data))  # last row as test set\ndata &lt;- data %&gt;% filter(row_number() &lt; nrow(data))   # remaining rows as training set\n\n# select variables X, Y\ny &lt;- data$sepal.length  # target variable\nx &lt;- data %&gt;% dplyr::select(-sepal.length)  # predictors\nx &lt;- as.matrix(x)\nx0 &lt;- data0 %&gt;% dplyr::select(-sepal.length)  # test predictors\nx0 &lt;- as.matrix(x0)\nn &lt;- nrow(x)\n\n# use lasso to estimate mu\nout.gnet = glmnet(x, y, nlambda=100, lambda.min.ratio=1e-3)\nlambda = min(out.gnet$lambda)\nfuns = lasso.funs(lambda=lambda)\n\n# run conformal inference\nout.conf = conformal.pred(x, y, x0, \n                          alpha=0.1,\n                          train.fun=funs$train, \n                          predict.fun=funs$predict, \n                          verb=TRUE)\n\n# run split conformal inference\nout.split = conformal.pred.split(x, y, x0, \n                                 alpha=0.1,\n                                 train.fun=funs$train, \n                                 predict.fun=funs$predict, \n                                 verb=TRUE)\n\n# print results\npaste('The lower bound is', out.conf$lo, 'and the upper bound is', out.conf$up)\n&gt; [1] \"The lower bound is 5.89 and the upper bound is 6.68\"\nout.conf$pred\n&gt;         [,1]\n&gt; [1,] 6.316882\n\n# print results for split conformal inference\npaste('The lower bound is', out.split$lo, 'and the upper bound is', out.split$up)\n&gt; [1] \"The lower bound is 5.74 and the upper bound is 6.93\"\nout.split$pred\n&gt;          [,1]\n&gt; [1,] 6.33556\nThe actual age value in the test set was \\(6.2\\) while the conformal inference approach computed a confidence interval (\\(5.88, 6.68\\)). The splitting algorithm gave similar results."
  },
  {
    "objectID": "blog/conformal-inference.html#bottom-line",
    "href": "blog/conformal-inference.html#bottom-line",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nConformal inference offers a novel approach for constructing valid finite-sample prediction intervals in machine learning models."
  },
  {
    "objectID": "blog/conformal-inference.html#where-to-learn-more",
    "href": "blog/conformal-inference.html#where-to-learn-more",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nConformal inference in machine learning is an ongoing research topic and I do not know of any review papers or textbook treatments of the subject. If you are interested in learning more, check the paper referenced below."
  },
  {
    "objectID": "blog/conformal-inference.html#references",
    "href": "blog/conformal-inference.html#references",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "References",
    "text": "References\nLei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094-1111.\nLei, J., Rinaldo, A., & Wasserman, L. (2015). A conformal prediction approach to explore functional data. Annals of Mathematics and Artificial Intelligence, 74, 29-43.\nShafer, G., & Vovk, V. (2008). A Tutorial on Conformal Prediction. Journal of Machine Learning Research, 9(3).\nVovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world (Vol. 29). New York: Springer."
  },
  {
    "objectID": "blog/new-dev-fdr.html#background",
    "href": "blog/new-dev-fdr.html#background",
    "title": "New Developments in False Discovery Rate",
    "section": "Background",
    "text": "Background\nA while back I wrote an article summarizing various approaches to correcting for multiple hypothesis testing. The dominant framework, False Discovery Rate (FDR), controls the share of hypotheses that are incorrectly rejected at a pre-specified level . Its foundations were laid out in 1995 by Benjamini and Hochberg (BH) and to date, their method remains the most popular approach for controlling FDR. Since then, the literature has gone in a few directions.\nOne strand of research generalizes the BH procedure to accommodate cases in which there is a dependency (i.e., correlation) among the hypotheses being tested. Another group of papers makes use of covariates that carry information about whether a given hypothesis is likely to be false. While intuitive in theory, in practice this idea is of limited use as such covariates are not often available.\nFinally, a relatively new class of methods builds on the notion of “knockoff” (or fake) variables and performs variable selection while controlling FDR. The underlying idea is based on creating a fake variable and comparing its test statistics to that of the original variable. Since the fake one is, by definition, null, a small discrepancy between the two test statistics signals the original variable does not belong in the model. The baseline model-X knockoff method requires knowledge of the joint distribution of all covariates. Recent simulations show if this distribution is unknown and misspecified (which in practice it almost always is) there is a loss of statistical power and FDR increase.\nIn this article I will discuss a few new papers which aim to build on and improve the knockoff method. Like knockoffs, they are based on “mirror statistics”, but unlike them they do not require exact knowledge or consistent estimation of any distribution. Specifically, I will discuss Gaussian Mirrors and Data Splitting for FDR control."
  },
  {
    "objectID": "blog/new-dev-fdr.html#notation",
    "href": "blog/new-dev-fdr.html#notation",
    "title": "New Developments in False Discovery Rate",
    "section": "Notation",
    "text": "Notation\nAlthough many of the results generalize to more complex settings, I will work with the simple linear model:\n\\[Y = X\\beta + \\epsilon.\\]\nWe have n observations of an outcome \\(Y\\), and a covariate vector \\(X\\in \\mathbb{R}^p\\) with \\(p &lt; n\\). (Again, some of these results generalize to high-dimensional settings, but let’s keep it simple here.) I will index the variables in \\(X\\) by \\(j\\). My goal is to find a subset of relevant features from $ X$ while controlling the FDR at some level \\(\\alpha\\). In other words, I wil be testing the series of \\(p\\) null hypotheses of the kind \\(\\beta_j=0\\)."
  },
  {
    "objectID": "blog/new-dev-fdr.html#a-closer-look",
    "href": "blog/new-dev-fdr.html#a-closer-look",
    "title": "New Developments in False Discovery Rate",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFDR Control with Mirror Statistics\nThe building block of these methods are the so-called mirror statistics, \\(M_j\\). They have the following two properties:\n\nVariables with larger mirror statistics (\\(M_j\\)’s) are more likely to be relevant.\nTheir distribution under the null hypothesis is (asymptotically) symmetric around \\(0\\).\n\nThese properties are simple and intuitive. For instance, the commonly used t-statistic for hypothesis testing in the linear model satisfies both. The first property suggests we can order the features and select ones with a mirror statistic exceeding some pre-defined threshold. The second one leads to an approximate upper bound on the number of false positives for any cutoff \\(t\\):\n\\[FDP(t) = \\frac{\\#\\{j: \\text{j is irrelevant, but } M_j &gt; t\\}}{\\# \\{j:M_j &gt; t\\}} \\leq \\frac{\\# \\{j:M_j &lt;- t\\}}{\\# \\{j:M_j &gt; t\\}}. \\]\nNow that we know the mirror statistics’ properties, I will discuss various ways of calculating them.\n\n\nConstructing the Mirror Statistics\nThe mirror statistics \\(M_j\\) take the following general form:\n\\[M_j = sign(\\tilde{\\beta}_j^1, \\tilde{\\beta}_j^2) f(|\\tilde{\\beta}_j^1|, |\\tilde{\\beta}_j^2|),\\]\nwhere the \\(\\tilde{\\beta}\\) denote (standardized) estimates of the true coefficient \\(\\beta\\) and \\(f(\\cdot)\\) is a nonnegative, exchangeable and monotonically increasing function. For instance, convenient choices for \\(f(\\cdot)\\) include \\(f(a,b) = 2min(a,b)\\) (Xing et al. 2019), \\(f(a,b) = ab\\), and \\(f(a,b) = a+b\\) (Dai et al. 2022).\nLet’s now turn to calculating the \\(\\tilde{\\beta}\\)’s.\n\n\nConstructing the Regression Coefficients\nThe coefficients \\(\\tilde{\\beta}\\) ought to satisfy the following two conditions:\n\nIndependence – The two regression coefficients are (asymptotically) independent.\nSymmetry – Under the null hypothesis, the (marginal) distribution of either of the two coefficients is (asymptotically) symmetric around zero.\n\nI will now describe two approaches in constructing them.\n\nMethod #1 – Gaussian Mirrors\nSoftware Package: GM.\nThe main idea is to create a set of two perturbed mirror features for each variable \\(X_j\\). Namely,\n\\[X_j^+ = X_j + a_jZ_j, \\text{      and      } X_j^-=X_j -a_jZ_j,\\]\nwhere \\(a_j\\) is a scalar and \\(Z_j \\approx N(0,1)\\). The authors provide some guidance on how to select \\(a_j\\), but I will not get into that here.\nWhile it is possible to generate the mirror features for all columns in \\(X\\) simultaneously, the one-fit-per-feature approach shows better performance in simulations. So, the \\(\\tilde{\\beta}\\) are the estimates of \\(\\beta\\) in the following model:\n\\[ y = \\frac{\\beta_j}{2}X_j^+ +\\frac{\\beta_j}{2}X_j^- + X_{\\text{non-j}}\\beta_{\\text{non-j}} + \\epsilon. \\]\n\n\nMethod #2 – Data Splitting\nAn alternative approach for getting two independent coefficient estimates \\(\\tilde{\\beta}\\) is through data splitting. When estimating the linear model, we can get \\(\\tilde{\\beta}^1\\) from one half of the data and $ ^2$ from the other half of the data. While this is simple and intuitive it can result in loss of statistical power. To alleviate this concern, we can do repeated data splitting and aggregate the results in the end. This is reminiscent of the procedure suggested by Meinheusen et al. (2012) in the context of hypothesis testing in for high-dimensional regression. We can then determine the feature importance based on the share of data splits in which it ends up being included. I will omit the technical details here.\nThere is a technical wrinkle I have omitted – the regression coefficients have to be standardized so that the \\(M_j\\)’s have comparable variances across variables. Check the original papers for details on exactly how to do that. Instead, I will now turn to the final algorithm for variable selection with FDR control using the approaches outlined above.\n\n\n\nPutting it All Together\nThis framework sets the stage for the following general algorithm for variable selection with FDR control:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nCalculate the \\(j\\) mirror statistics, \\(M_j\\).\nGiven a FDR level \\(\\alpha\\), set a threshold \\(\\tau(\\alpha)\\) such that \\[\\tau(\\alpha)= min\\{t &gt; 0 : \\hat{FDP}(t) \\leq \\alpha\\}.\\]\nSelect the features \\(\\{ j : M_j &gt;  \\tau(\\alpha) \\}\\).\n\n\n\nIn words, given \\(\\alpha\\) calculate the \\(M_j\\)’s, find the magical threshold \\(\\tau(\\alpha)\\) and include the variables with \\(M_j &gt; \\tau(\\alpha)\\)."
  },
  {
    "objectID": "blog/new-dev-fdr.html#bottom-line",
    "href": "blog/new-dev-fdr.html#bottom-line",
    "title": "New Developments in False Discovery Rate",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe Benjamini-Hochberg approach is still the most popular way to control FDR.\nI discuss two novel approaches for variable selection and FDR control aimed at improving the knockoff filter."
  },
  {
    "objectID": "blog/new-dev-fdr.html#references",
    "href": "blog/new-dev-fdr.html#references",
    "title": "New Developments in False Discovery Rate",
    "section": "References",
    "text": "References\nBarber, R. F., & Candès, E. J. (2018). Controlling the false discovery rate via knockoffs. Annals of Statistics\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300.\nBenjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of statistics, 1165-1188.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2022). False discovery rate control via data splitting. Journal of the American Statistical Association, 1-18.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2023). A scale-free approach for false discovery rate control in generalized linear models. Journal of the American Statistical Association, 1-15.\nIgnatiadis, N., Klaus, B., Zaugg, J. B., & Huber, W. (2016). Data-driven hypothesis weighting increases detection power in genome-scale multiple testing. Nature methods, 13(7), 577-580.\nKorthauer, K., Kimes, P. K., Duvallet, C., Reyes, A., Subramanian, A., Teng, M., … & Hicks, S. C. (2019). A practical guide to methods controlling false discoveries in computational biology. Genome biology, 20(1), 1-21.\nScott, J. G., Kelly, R. C., Smith, M. A., Zhou, P., & Kass, R. E. (2015). False discovery rate regression: an application to neural synchrony detection in primary visual cortex. Journal of the American Statistical Association, 110(510), 459-471.\nXing, X., Zhao, Z., & Liu, J. S. (2023). Controlling false discovery rate using gaussian mirrors. Journal of the American Statistical Association, 118(541), 222-241."
  },
  {
    "objectID": "blog/foci.html#background",
    "href": "blog/foci.html#background",
    "title": "FOCI: A New Variable Selection Method",
    "section": "Background",
    "text": "Background\nIn our data-abundant world, we often have access to tens, hundreds, or even thousands of variables. Most of these features are usually irrelevant or redundant, leading to increased computational complexity and potentially overfit models. Variable selection algorithms are designed to mitigate these challenges by selecting the subset of columns that is most relevant to the problem at hand. This, in turn, can result in simplified, more efficient, and interpretable machine learning (ML) models. Examples of such methods abound—lasso, ridge, elastic net utilizing \\(L1\\) and \\(L2\\) regularization or a linear combination of both; forward/backward stepwise regression algorithms, etc.\nIn this article, I will describe a new variable selection method built on the idea of Chatterjee’s correlation coefficient, which has been a hot topic of discussion among statisticians. The new method goes by the abbreviation FOCI, which stands for Feature Ordering by Conditional Independence. It is somewhat similar to forward stepwise regression but overcomes some of the critiques most often associated with it.\nLet’s start with setting up some basic notation. We are focused on studying the correlation between (\\(Y\\)) (e.g., income) and (\\(X\\)) (education level) while interested in conditioning on/controlling for (\\(Z\\)) (parental education and other factors). As always, we are armed with a random sample of size n of all these variables and assume everything is well-behaved. If you have not had a chance, I recommend you first read my earlier post on Chatterjee’s correlation measure, which lays some of the foundations necessary to understand the algorithm under the hood."
  },
  {
    "objectID": "blog/foci.html#a-closer-look",
    "href": "blog/foci.html#a-closer-look",
    "title": "FOCI: A New Variable Selection Method",
    "section": "A Closer Look",
    "text": "A Closer Look\nI will be being with taking you on a tour of conditional and unconditional statistics. What’s the deal with conditional versus unconditional correlation, and is one of them always a better choice than the other?\n\nConditional and Unconditional Correlation\nMost times, when we talk about the correlation between \\(X\\) and \\(Y\\), knowingly or not, we mean unconditional correlation. This is the correlation between two variables without considering any additional or contextual factors. As such, it describes their relationship “in general.” In my previous post, I illustrated several ways to measure such unconditional correlations, including the well-known Spearman correlation coefficient.\nIn some cases, however, we like to go one step further and consider contextual factors. This is where conditional correlation comes in. It considers the influence of one or more additional variables when measuring the relationship between \\(X\\) and \\(Y\\). Thus, it provides information on how their relationship changes under specific conditions.\nConditional correlation is hard (pun intended); it’s even NP-hard in some contexts.\nUnder certain assumptions, it might be possible to go from conditional to unconditional correlation. Simplifying the example above, let’s say that \\(Z\\) (parental education) can take on two values—low and high. If we have measures of the correlation between \\(X\\) and \\(Y\\) for each of these values (i.e., for families with low and high parental education), we can weigh these by their relative proportion and arrive at an estimate of the unconditional correlation between \\(X\\) and \\(Y\\).\nTo answer my own question above, it’s difficult to say that one is always better than the other. It really depends on your goal. If you want to know how \\(X\\) and \\(Y\\) vary in general, unconditional correlation is your friend; if, instead, you are interested in incorporating contextual information or simply controlling for other factors, you really want conditional correlation. My take is that most often we are after conditional correlations but use tools for measuring unconditional ones.\n\n\nQuantile Regression\n\n\n\n\n\n\nSide Note on Quantile Regression\n\n\n\nQuantile regression is another setting where most often people confuse conditional and unconditional statistical inference. Interestingly, though, the situation is flipped. The classical quantile regression estimator, which most people utilize, measures the impact of \\(X\\) on the conditional quantile of \\(Y\\), but there results are commonly interpreted as having unconditional interpretation.\n\n\n\n\nNew Coefficient of Conditional Independence\nAzadkia and Chatterjee (2021) develop a new coefficient of conditional correlation. The math behind is in extremely involved and I will not even be discussing its formula here. For simplicity, let’s just call it \\(T(\\cdot)\\):\n\\[T(Y,X|Z) = \\text{corr} (Y, X | Z).\\]\n\\(T(\\cdot)\\) enjoys the following attractive properties:\n\nIt is an extension of Chatterjee’s unconditional correlation coefficient.\nIt is non-parametric.\nIt has no tuning parameters.\nIt can be estimated quickly, in \\(O(n \\text{ log}n)\\) time.\nAsymptotically converges to a limit in \\([0,1]\\). It’s limit is \\(0\\) when \\(Y\\) and \\(X\\) are conditionally independent, and it is \\(1\\) when \\(Y\\) is a measurable function of \\(X\\), conditionally.\nIt is a nonlinear generalization of the partial \\(R^2\\) coefficient in a linear regression of \\(Y\\) on \\(X\\) and \\(Z\\).\n\nAll this is to say – \\(T(\\cdot)\\) has many things going for it, and it is a pretty good measure of conditional correlation.\n\n\nThe Variable Selection Algorithm\nThe key idea is to integrate \\(T(\\cdot)\\) into a forward stepwise variable selection procedure. Let’s add the \\(Z\\) variables into \\(X\\), so we only have \\(Y\\in \\mathbb{R}\\) and \\(X\\in \\mathbb{R}^p\\). Then the algorithm goes as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nStart with the index \\(j\\) that maximizes \\(T(Y \\mid X_j)\\).\nGiven \\(j_1, \\dots j_k\\), select \\(j_{k+1}\\) as the index \\(\\notin (j_1, \\dots j_k)\\) that maximizes \\(T(Y,X_j \\mid X_{j_1}, \\dots, X_{j_k})\\).\nContinue until finding the first \\(k\\) such that \\(T(Y, X_{j_{k+1}} \\mid X_{j_1}, \\dots, X_{j_k}) \\leq 0\\).\nDeclare the set of selected variables \\(\\hat{S} ={X_{j_1}, \\dots, X_{j_k}}\\).\n\n\n\nSoftware Package: FOCI\nIn words, we start with the variable j that maximizes \\(T(Y\\mid X_j)\\). In each subsequent step we select the variable that has not yet been selected and has the highest \\(T(Y \\mid \\cdot)\\) value up until \\(T(Y \\mid \\cdot)\\) is positive. That’s it. We then have the set of FOCI selected variables.\nAlthough it is not required theoretically, the predictor variables be standardized before running the algorithm. If computational time is not an issue, one can try to add \\(m \\geq  2\\) variables at each step instead of just one.\nThere you have it. Grab your data and see whether and how much FOCI improves on your favorite feature selection method."
  },
  {
    "objectID": "blog/foci.html#bottom-line",
    "href": "blog/foci.html#bottom-line",
    "title": "FOCI: A New Variable Selection Method",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nAzadkia and Chatterjee (2021) develop a new coefficient of conditional correlation featuring a host of attractive properties.\nThis coefficient can be used in a stepwise inclusion fashion as a variable selection algorithm potentially improving on well-established methods in the field."
  },
  {
    "objectID": "blog/foci.html#where-to-learn-more",
    "href": "blog/foci.html#where-to-learn-more",
    "title": "FOCI: A New Variable Selection Method",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMy earlier post on Chatterjee’s bivariate coefficient of (unconditional) correlation is a good starting point. Data scientists more deeply interested in FOCI should read Azadkia and Chatterjee’s paper which describes in detail the mathematics behind the new algorithm."
  },
  {
    "objectID": "blog/foci.html#references",
    "href": "blog/foci.html#references",
    "title": "FOCI: A New Variable Selection Method",
    "section": "References",
    "text": "References\nAlejo, J., Favata, F., Montes-Rojas, G., & Trombetta, M. (2021). Conditional vs Unconditional Quantile Regression Models: A Guide to Practitioners. Economia, 44(88), 76-93.\nAzadkia, M., & Chatterjee, S. (2021). A simple measure of conditional dependence. The Annals of Statistics, 49(6), 3070-3102.\nBorah, B. J., & Basu, A. (2013). Highlighting differences between conditional and unconditional quantile regression approaches through an application to assess medication adherence. Health economics, 22(9), 1052-1070.\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\nDagum, P., & Luby, M. (1993). Approximating probabilistic inference in Bayesian belief networks is NP-hard. Artificial intelligence, 60(1), 141-153.\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional quantile regressions. Econometrica, 77(3), 953-973.\nKoenker, R. (2017). Quantile regression: 40 years on. Annual review of economics, 9, 155-176."
  },
  {
    "objectID": "blog/causation-without-correlation.html#background",
    "href": "blog/causation-without-correlation.html#background",
    "title": "Causation without Correlation",
    "section": "Background",
    "text": "Background\nWhile most people understand that correlation doesn’t imply causation, it might surprise many to learn that causation doesn’t always result in correlation. In the absence of randomization, causal relationships do not require observable correlation. This counterintuitive concept challenges our natural tendency to expect that when one variable causes change in another, we should see a clear (linear) relationship between them. The core idea is that confounding variables or other statistical phenomena can obscure the causal link. Let’s explore this concept through a few examples."
  },
  {
    "objectID": "blog/causation-without-correlation.html#a-closer-look",
    "href": "blog/causation-without-correlation.html#a-closer-look",
    "title": "Causation without Correlation",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn the popular book Causal Inference: The Mixtape, Scott Cunningham gives an example of a sailor steering a boat in stormy waters. The wind may be so strong as to offset the boat’s natural moving direction. For instance, the sailor might steer (treatment, \\(T\\)) the boat north, while a southward wind (confounder, \\(Z\\)) causes the boat to move east (outcome, \\(Y\\)). An onlooker would not observe any direct relationship between \\(T\\) and \\(Y\\), even though \\(T\\) causes \\(Y\\).\nAt first, this sounds counterintuitive. On second thought, such patterns are everywhere. Consider the following.\n\nExample 1: Parenting Styles and Children’s Behavior\nA parent might adopt a stricter parenting style (\\(T\\)) in response to a child’s behavioral issues (\\(Y\\)). However, other influences, like peer pressure or school environment (\\(Z\\)), may also shape the child’s behavior, sometimes overriding the parent’s efforts. The net observable outcome could show no correlation between stricter parenting and improved behavior, even though the stricter parenting is causally effective in certain contexts.\nThis idea can be taken one step further. An observable relationship might even appear positive when the causal relationship is negative.\n\n\nExample 2: Ice Cream Sales and Shark Attacks\nImagine two beaches with vastly different safety protocols (\\(Z\\)): one has lifeguards trained to prevent shark attacks, while the other does not. On the safer beach, higher ice cream sales (\\(T\\)) correlate positively with shark attacks (\\(Y\\)), because more people visit the beach when safety protocols are in place. This hides the fact that proper safety protocols causally reduce shark attacks. The observed positive correlation between \\(T\\) and \\(Y\\) masks the negative causal relationship.\n\n\nExample 3: Nonlinearity\nA more trivial scenario leading to the lack of correlation in causal relationships is non-linearity. I do not find this scenario too insightful simply because it can be avoided by using more sophisticated measures of correlation. See my earlier post on the Chatterjee correlation coefficient.\nExamples of such relationships abound. Consider a parabolic relationship, where increasing a drug’s dosage initially improves patient outcomes but becomes harmful at higher doses. Despite a clear causal relationship, the (Pearson) correlation coefficient might be close to zero because the relationship is not linear.\n\n\nExample 4: Threshold Effects and Phase Transitions\nTake the classic example of temperature and water’s state. Increasing temperature causes water to change state at exactly 100°C. Below and above this point, temperature changes cause minimal effects on the water’s state. Aggregating these observations leads to a weak correlation, despite the temperature being the direct cause of the phase transition.\nOther fascinating scenarios include Lord’s Paradox, and Simpson’s Paradox, where a causal relationship can appear to reverse or disappear when data is aggregated.\n\n\nExample 5: Hospital Mortality Rates\nSuppose two hospitals treat patients with different levels of severity. Hospital \\(A\\) specializes in high-risk patients, while Hospital \\(B\\) treats mostly low-risk cases. When comparing raw mortality rates (\\(Y\\)), Hospital \\(A\\) might appear worse, even though it provides superior care (\\(T\\)). Disaggregating the data by risk level reveals the causal effect of Hospital \\(A\\)’s superior treatment within each group."
  },
  {
    "objectID": "blog/causation-without-correlation.html#bottom-line",
    "href": "blog/causation-without-correlation.html#bottom-line",
    "title": "Causation without Correlation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nIn observational data causation does not require correlation.\nCorrelation—or the lack thereof—can obscure our understanding of causal relationships.\nWith the right tools and frameworks, we can disentangle the true causal effects, even when correlation gives us a wrong answer."
  },
  {
    "objectID": "blog/causation-without-correlation.html#references",
    "href": "blog/causation-without-correlation.html#references",
    "title": "Causation without Correlation",
    "section": "References",
    "text": "References\nCunningham, S. (2021). Causal inference: The mixtape. Yale university press."
  },
  {
    "objectID": "blog/paradox-simpson.html#background",
    "href": "blog/paradox-simpson.html#background",
    "title": "Simpson’s Paradox: A Simple Illustration",
    "section": "Background",
    "text": "Background\nSimpson’s paradox is one of the most counterintuitive phenomena in data analysis. It describes situations where a trend observed within groups disappears—or even reverses—when the data is aggregated. The underlying cause is often a confounding variable that distorts the overall trend. Let’s examine a concrete example."
  },
  {
    "objectID": "blog/paradox-simpson.html#a-closer-look",
    "href": "blog/paradox-simpson.html#a-closer-look",
    "title": "Simpson’s Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nAn Example\nImagine two hospitals, \\(A\\) and \\(B\\), treating patients for a particular condition with two treatment options, \\(T_1\\) and \\(T_2\\). Hospital \\(A\\), located in a higher-income neighborhood, primarily receives healthier patients, while Hospital \\(B\\), in a lower-income neighborhood, tends to treat sicker patients. The effectiveness of the treatments is measured as improvement in a continuous health score.\nWe are interested in examining whether one of the treatment options leads to better health outcomes. Consider the following data gathered across both hospitals.\n\n\nHealth improvement by hospital and treatment type. Both treatments \\(T_1\\) and \\(T_2\\) are equally effective within each hospital.\n\n\nHospital\nTreatment\nHealth Improvement\nN\n\n\n\n\nA\n\\(T_1\\)\n20\n90\n\n\nA\n\\(T_2\\)\n20\n10\n\n\nB\n\\(T_1\\)\n10\n10\n\n\nB\n\\(T_2\\)\n10\n90\n\n\n\n\nLet’s now look at what happens when we combine the data from both hospitals.\n\n\nHealth improvement by treatment type. Treatment \\(T_1\\) is more effective overall.\n\n\nTreatment\nN\nHealth Improvement\n\n\n\n\n\\(T_1\\)\n100\n19 = 20 * .9 + 10 * .1\n\n\n\\(T_2\\)\n100\n11 = 10 * .9 + 20 * .1\n\n\n\n\nWithin each hospital, the data shows that both treatments are equally effective. However, combining the data across both hospitals reveals that treatment \\(T_1\\) appears to be significantly more effective overall. Why does this happen?\nThe confounding variable here is the underlying health status of patients. Hospital \\(A\\) treats mostly healthier patients, while Hospital \\(B\\) handles more severe cases. This difference in patient distribution influences the overall success rates of the treatments, even though both treatments perform identically within each hospital.\n\n\nA Visualization\nTo illustrate, imagine a scatter plot where each dot represents a patient. The color of the dot indicates the treatment they received, and the horizontal lines represent the average health improvement for each group. The vertical axis depicts the outcome variable (health improvement).\n\nRPython\n\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\n# Set random seed for reproducibility\nset.seed(4904)\n\n# Define data dimensions\nn_a &lt;- 200\nn_b &lt;- 20\n\n# Create the dataset\ndata &lt;- data.frame(\n  Hospital = rep(c(\"Hospital A\", \"Hospital B\"), each = n_a + n_b),\n  Treatment = c(\n    rep(\"Treatment 1\", n_a), rep(\"Treatment 2\", n_b),  # Hospital A\n    rep(\"Treatment 1\", n_b), rep(\"Treatment 2\", n_a)   # Hospital B\n  ),\n  Improvement = c(\n    rnorm(n_a, mean = 20, sd = 2), rnorm(n_b, mean = 20, sd = 2),  # Hospital A\n    rnorm(n_b, mean = 10, sd = 2), rnorm(n_a, mean = 10, sd = 2)   # Hospital B\n  ),\n  Aggregated = \"Overall\"\n)\n\n# Calculate mean improvement for each treatment and hospital\nmean_improvement &lt;- data %&gt;%\n  group_by(Hospital, Treatment) %&gt;%\n  summarize(Mean_Improvement = mean(Improvement), .groups = \"drop\")\n\n# Calculate overall mean improvement for each treatment\noverall_mean &lt;- data %&gt;%\n  group_by(Treatment) %&gt;%\n  summarize(Mean_Improvement = mean(Improvement), .groups = \"drop\")\n\n# Create the disaggregated plot\nplot1 &lt;- ggplot(data, aes(x = Hospital, y = Improvement, color = Treatment)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.7) +\n  geom_hline(data = mean_improvement, aes(yintercept = Mean_Improvement, color = Treatment), linetype = \"solid\") +\n  labs(\n    title = \"By Hospital\",\n    x = NULL,\n    y = \"Health Improvement\",\n    color = \"Treatment\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Create the aggregated plot\nplot2 &lt;- ggplot(data, aes(x = Aggregated, y = Improvement, color = Treatment)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.7) +\n  geom_hline(data = overall_mean, aes(yintercept = Mean_Improvement, color = Treatment), linetype = \"solid\") +\n  labs(\n    title = \"Overall\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")\n\n# Combine the plots with a common legend and title\nfinal_plot &lt;- (plot1 + plot2) +\n  plot_annotation(\n    title = \"Simpson's Paradox: Treatment Effectiveness Within Hospitals and Overall\",\n    theme = theme(plot.title = element_text(hjust = 0.5))\n  ) &\n  theme(legend.position = \"bottom\")\n\n# Display the combined plot\nprint(final_plot)\n\n\n# load libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.random.seed(1988)\n\n# Define data dimensions\nn_a = 200\nn_b = 20\n\n# Create the dataset\nhospital = ['Hospital A'] * (n_a + n_b) + ['Hospital B'] * (n_a + n_b)\ntreatment = (\n    ['Treatment 1'] * n_a + ['Treatment 2'] * n_b +  # Hospital A\n    ['Treatment 1'] * n_b + ['Treatment 2'] * n_a   # Hospital B\n)\nimprovement = (\n    list(np.random.normal(20, 2, n_a)) + list(np.random.normal(20, 2, n_b)) +  # Hospital A\n    list(np.random.normal(10, 2, n_b)) + list(np.random.normal(10, 2, n_a))   # Hospital B\n)\naggregated = ['Overall'] * (2 * (n_a + n_b))\n\ndata = pd.DataFrame({\n    'Hospital': hospital,\n    'Treatment': treatment,\n    'Improvement': improvement,\n    'Aggregated': aggregated\n})\n\n# Calculate mean improvement for each treatment and hospital\nmean_improvement = data.groupby(['Hospital', 'Treatment'], as_index=False)['Improvement'].mean()\nmean_improvement.rename(columns={'Improvement': 'Mean_Improvement'}, inplace=True)\n\n# Calculate overall mean improvement for each treatment\noverall_mean = data.groupby(['Treatment'], as_index=False)['Improvement'].mean()\noverall_mean.rename(columns={'Improvement': 'Mean_Improvement'}, inplace=True)\n\n# Create the disaggregated plot\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.stripplot(data=data, x='Hospital', y='Improvement', hue='Treatment', jitter=0.2, alpha=0.7, dodge=True)\nfor _, row in mean_improvement.iterrows():\n    plt.axhline(y=row['Mean_Improvement'], color=sns.color_palette('Set1')[0 if row['Treatment'] == 'Treatment 1' else 1], linestyle='solid')\nplt.title(\"By Hospital\")\nplt.xlabel(None)\nplt.ylabel(\"Health Improvement\")\nplt.legend(title=\"Treatment\", loc='upper right')\nplt.grid(True)\n\n# Create the aggregated plot\nplt.subplot(1, 2, 2)\nsns.stripplot(data=data, x='Aggregated', y='Improvement', hue='Treatment', jitter=0.2, alpha=0.7, dodge=True)\nfor _, row in overall_mean.iterrows():\n    plt.axhline(y=row['Mean_Improvement'], color=sns.color_palette('Set1')[0 if row['Treatment'] == 'Treatment 1' else 1], linestyle='solid')\nplt.title(\"Overall\")\nplt.xlabel(None)\nplt.ylabel(None)\nplt.legend([], [], frameon=False)\nplt.grid(True)\n\n# Add a common title\nplt.suptitle(\"Simpson's Paradox: Treatment Effectiveness Within Hospitals and Overall\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\nIn the aggregated data, \\(T_1\\) shows a higher average improvement, creating the illusion of greater effectiveness. But when disaggregated by hospital, the averages for \\(T_1\\) and \\(T_2\\) are identical."
  },
  {
    "objectID": "blog/paradox-simpson.html#where-to-learn-more",
    "href": "blog/paradox-simpson.html#where-to-learn-more",
    "title": "Simpson’s Paradox: A Simple Illustration",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nStart with the Wikipedia entry, where you will find all necessary additional resources."
  },
  {
    "objectID": "blog/paradox-simpson.html#bottom-line",
    "href": "blog/paradox-simpson.html#bottom-line",
    "title": "Simpson’s Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nSimpson’s paradox manifests when an observable pattern within groups disappears if the data is aggregated.\nIt is a reminder of the critical role confounding variables play in data analysis.\nIt underscores the importance of stratifying data by meaningful subgroups and carefully considering the context before drawing conclusions from aggregated statistics."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#background",
    "href": "blog/flavors-ml-methods-ci.html#background",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Background",
    "text": "Background\nThe most exciting trend in causal inference over the last decade has been the infusion of machine learning (ML) techniques. Supervised machine learning is designed to find complex patterns in data and as such, it is merely occupied with prediction. Causal inference, on the other hand, pays close attention to statistical precision and inference based on asymptotic properties like consistency and normality. The two worlds are, thus, fundamentally different.\nIt should be no surprise then that machine learning and causal inference do not naturally speak to each other, and some modifications are required to marry them. The good news is recent innovations have led to a bunch of ways in which ML models can be used in isolating causal effects especially in settings with many covariates (also called “high dimensional” settings).\nIn this article, I will briefly describe the ways in which we can use ML when looking for causal relationships. I will indeed be concise, and I will avoid diving deeper into technical details. This blog post will look more like a laundry list with references to papers and software packages than a tutorial."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#notation",
    "href": "blog/flavors-ml-methods-ci.html#notation",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Notation",
    "text": "Notation\nIt is helpful to quickly summarize some features of the potential outcome framework. Imagine we have a i.i.d. random sample of a binary treatment indicator \\(D\\), outcome variable \\(Y\\) and a vector of covariates \\(X\\). Assume the potential outcomes \\(Y(0)\\) and \\(Y(1)\\) are unrelated to the binary treatment status \\(D\\) which is often referred to as the unconfoundedness or ignorability.\nA common estimand of interest is the Average Treatment Effect (ATE)\n\\[ATE = E[Y(1) - Y(0)],\\]\nwhere \\(Y(d)\\) is the potential outcome under treatment regime \\(D=d\\). Another popular estimand is the Conditional ATE (CATE),\n\\[CATE(X) = E[Y(1) - Y(0) | X],\\]\nwhich is the ATE for a particular group of units with a fixed covariates level (e.g., women, men, new users, etc.).\nThe ATE can be expressed in at least three useful ways:\n\\[\\begin{align*} ATE & = \\mathbf{E} \\left[ \\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(outcome model only)} \\\\ & = \\mathbf{E}\\left[ \\frac{YD}{e(X)} - \\frac{Y(1-D)}{1-e(X)} \\right] \\hspace{1cm} \\text{(prop. score model only)} \\\\ & = \\mathbf{E} \\left[ \\frac{[Y-\\mu(1,X)D]}{e(X)} - \\frac{[Y-\\mu(0,X)](1-D)}{1-e(X)} \\right] \\\\ & + \\mathbf{E} \\left[\\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(both models)} \\end{align*}\\]\nwhere\n\\[\\mu(D,X) = \\mathbf{E}[Y|D,X]\\]\nis the outcome model and\n\\[e(x)=\\mathbf{E}[D|X]\\]\nis the propensity score.\nThis formulation is helpful because it naturally splits the types of treatment effect estimation methods into three separate categories – (i) those that require only estimation of \\(\\mu(D,X)\\), (ii) those that use only \\(e(X)\\), and (iii) those that need both.\nOne can think of the propensity score (PS) and the outcome models as nuisance functions – ones that are not of direct interest but play a part in treatment effect estimation. ML methods are attractive candidates for estimating these nuisance functions flexibly."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#a-closer-look",
    "href": "blog/flavors-ml-methods-ci.html#a-closer-look",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nCovariate Balancing Methods\nUnder the ignorability assumption, all confounding bias comes from differences in the covariates \\(X\\) between the treatment and the control groups. Intuitively, balancing these is enough to guarantee unbiasedness. One line of research develops methods to do exactly that – directly equate covariates between the two groups of interest. These approaches circumvent estimation of the two nuisance functions mentioned above.\nThese are inspired by the ML view of data analysis framed as an optimization problem. Examples include Entropy Balancing, Genetic Matching, Stable Weights, and Residual Balancing. The last approach combines balancing with a regression adjustment to reduce extrapolation when estimating the counterfactuals for the treatment group. Some of these methods were designed with a low dimensional setting in mind, but they still carry the spirit of ML type of thinking.\nSoftware Packages: MatchIt, Ebal, BalanceHD.\n\n\nML Methods for the Propensity Score Model\nPropensity score methods rely on correctly specifying the PS model. In low-dimensional settings, it is possible to estimate it nonparametrically. In practice, however, this is unrealistic when data scientists have access to continuous or even bunch of discrete covariates. Can ML methods come to the rescue?\nIn principle, yes. A major challenge in this context, however, is the choice of a loss function. In the ML world loss functions target measures of fit (e.g., Root Mean Squared Error, log likelihood, etc.) but these would be problematic here as they do not aim at balancing covariates important to reduce bias. Thus, these methods do not perform very well unless used with much caution.\nImai and Ratkovic (2014) propose a PS method that directly balances covariates. Another choice is the Boosted CART implementation. As its name suggests, it iteratively forms a bunch of tree models and averages them, but with an appropriately chosen loss function. A series of simulation studies analyze the performance of various ML algorithms used to estimate the PS, but overall, these methods are nowadays dominated by some of the doubly robust approaches described below.\nSoftware Packages: TWANG, CBPS.\n\n\nML Methods for the Outcome Model\nWe can also estimate treatment effects directly by modelling the outcome variable. This also requires correct model specification, and even then, it is prone to extrapolation in finite samples. Examples include Bayesian Additive Regression Trees (BART) and other ensemble methods.\nBelloni et al. (2014) show that the set of features optimal when estimating the outcome model, is not necessarily optimal for estimating treatment effects. The issue is omitting a variable that is correlated with the treatment even if its correlation with the outcome is only modest, can introduce considerable bias. Moreover, typically, the rate of convergence in this context when using ML models will be slower than \\(\\sqrt{n}\\), meaning that you will need much more data to get a good treatment effect estimate.\nOverall, there is no statistical theory of why ML methods should work well here, but some methods tend to perform well empirically. This brings us to the doubly robust approach.\nSoftware Packages: BART, rBART, BayesTree.\n\n\nML Methods for Both Models & Doubly Robust Methods\nMethods combining models for both the propensity score and the outcome have long been advocated. Intuitively, the propensity score can be seen as a balancing step after which regression adjustment can remove any remaining bias. Imbens (2015), for instance, promotes this type of thinking in matching methods specifically.\nDoubly robust (DR) estimators use both nuisance models and have the amazing property of being consistent even if only one of the two models is correctly specified. You can think of the bias term as a product of the biases in the two nuisance models – if one of them is equal to zero, the entire term vanishes. Additionally, if both models are correctly specified, some methods are semiparametrically efficient, (i.e., “the best” in a large class of flexible models). A simple DR method is the Augmented Inverse Probability Weighting (AIPW) estimator which in linear models comes down to running weighted OLS regression of \\(Y\\) on \\(D\\) and \\(X\\) with the estimated (inverse) propensity score as weights.\nThere is more good news. Amazingly, DR methods can still converge at a rate \\(\\sqrt{N}\\) even if the underlying nuisance models converge at slower rates. The formal requirement is that the nuisance models must belong to something called a Donsker class. In simple words, they should not be too complex and prone to overfit.\nOne line of research has developed the Double ML framework. This work has been so influential that it deserves a blog post of its own. Without going into technical details, the authors of the original paper show that naïve application of ML methods when estimating both nuisance functions results in two types of biases – regularization and overfitting. DoubleML makes use of something called Neyman orthogonalization (think of the Frisch–Waugh–Lovell theorem) to remove the former, and sample splitting to avoid the latter. In simple settings, this method combines the residuals from regressions of \\(Y\\) on \\(X\\) and \\(Y\\) on \\(D\\), but in general it can take more complicated forms.\nAnother line of research has developed the Double Post Lasso approach. The idea here is simpler – use Lasso to select covariates relevant to the outcome regression and then again to select ones relevant to the propensity score. Lastly, use OLS to regress \\(Y\\) on the union of the covariates selected previously. This procedure removes confounding or regularization bias that might be present in methods using ML models to estimate only one of the nuisance models.\nSoftware Packages: DoubleML, hdm, dlsr.\n\n\nHeterogeneous Treatment Effect Estimation\nMining for heterogeneous treatment effects has been a particularly fruitful field for ML methods. A leading example is the causal tree method developed by Athey and Imbens (2016). It resembles the traditional CART algorithm, but it uses a different criterion for splitting the data: instead of focusing on Mean Squared Error (MSE) for the outcome, it uses MSE for treatment effect. The result is a decision tree, in which units in each leaf have similar treatment effects. The method also features “honest” sample splitting for obtaining variance estimates – one half of the data is used to determine the optimal tree, and the other half to estimate the treatment effects.\nBuilding on this idea, Wager and Athey (2018) propose a random forest-based method which generates a bunch of causal trees and averages the results to induce smoothness in the treatment effect’s function. Magically, the authors show the predictions are asymptotically normal and centered around the true value for each unit! This is exciting as it allows for standard methods for statistical inference.\nOther methods include BART, a Bayesian version of random forests mentioned above, Imai and Ratkovic (2013) who propose adding treatment indicators interacted with covariates in a LASSO regression to determine which variables are important for treatment effect heterogeneity. Similarly, Tian et al. (2014) suggest modifying the covariates in a straightforward way and running a regression of the outcome on the modified variables without an intercept. Other methods include the \\(R\\)-learner of Nie and Wager (2021) which relies on estimating the two nuisance models described above and using a special loss function. Künzel et al. (2019) propose a \\(X\\)-learner metaalogrithm.\nSoftware Packages: FindIt, rlearner, grf, causalToolbox.\n\n\nOthers\nA by-product of estimating treatment effect heterogeneity is that we can determine which units should be treated. Intuitively, if the treatment effect is close to zero (or even negative) for some users, there is not much to be gained from the exposure. Kitagawa and Tetenov (2018) analyze a setting with limited complexity, and Athey and Wager (2021) develop the DoubleML framework discussed above for choosing whom to treat. ML has also been used for variance reduction in randomized experiments via regression adjustments. See, for instance, Wager et al. (2016), Bloniarz et al. (2016), and List et al. (2022)."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#bottom-line",
    "href": "blog/flavors-ml-methods-ci.html#bottom-line",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods are slowly becoming an indispensable part of data scientists’ toolkit for estimating causal relationships. There is an abundance of methods aiding practitioners in both ATE and CATE estimation.\nDoubly robust approaches offer better theoretical guarantees than methods relying on estimating either the outcome or the propensity score models.\nThe leading approaches for estimating ATEs are Double ML and Double Post Lasso.\nThe leading approach for estimating CATEs is the causal forest method."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#where-to-learn-more",
    "href": "blog/flavors-ml-methods-ci.html#where-to-learn-more",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMore technical data scientists will find the following review papers useful:\n\nAthey and Imbens (2019)\nAthey and Imbens (2017)\nVarian (2014)\nKreif and DiazOrdaz (2019)\nMullainathan and Spiess (2017)\nHu (2023)\n\nThere are a few major Python frameworks for using ML in causal inference estimation. More practically-oriented folks might like their documentation:\n\nCausalML\nEconML\nDoubleML"
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#references",
    "href": "blog/flavors-ml-methods-ci.html#references",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "References",
    "text": "References\nAthey, S., & Imbens, G. (2016). Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27), 7353-7360.\nAthey, S., & Imbens, G. W. (2017). The state of applied econometrics: Causality and policy evaluation. Journal of Economic perspectives, 31(2), 3-32.\nAthey, S., & Imbens, G. W. (2019). Machine learning methods that economists should know about. Annual Review of Economics, 11, 685-725.\nAthey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 80(4), 597-623.\nAthey, S., & Wager, S. (2021). Policy learning with observational data. Econometrica, 89(1), 133-161.\nAustin, P. C. (2012). Using ensemble-based methods for directly estimating causal effects: an investigation of tree-based G-computation. Multivariate behavioral research, 47(1), 115-135.\nBelloni, A., Chernozhukov, V., & Hansen, C. (2014). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2), 608-650.\nBloniarz, A., Liu, H., Zhang, C. H., Sekhon, J. S., & Yu, B. (2016). Lasso adjustments of treatment effect estimates in randomized experiments. Proceedings of the National Academy of Sciences, 113(27), 7383-7390.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal.\nDiamond, A., & Sekhon, J. S. (2013). Genetic matching for estimating causal effects: A general multivariate matching method for achieving balance in observational studies. Review of Economics and Statistics, 95(3), 932-945.\nHahn, P. R., Murray, J. S., & Carvalho, C. M. (2020). Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion). Bayesian Analysis, 15(3), 965-1056.\nHainmueller, J. (2012). Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20(1), 25-46.\nHill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240.\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. Annals of Applied Statistics\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B: Statistical Methodology, 243-263.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nKitagawa, T., & Tetenov, A. (2018). Who should be treated? empirical welfare maximization methods for treatment choice. Econometrica, 86(2), 591-616.\nKreif, N., & DiazOrdaz, K. (2019). Machine learning in policy evaluation: new tools for causal inference. arXiv preprint arXiv:1903.00402.\nKünzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165.\nLee, B. K., Lessler, J., & Stuart, E. A. (2010). Improving propensity score weighting using machine learning. Statistics in medicine, 29(3), 337-346.\nList, J. A., Muir, I., & Sun, G. K. (2022). Using Machine Learning for Efficient Flexible Regression Adjustment in Economic Experiments (No. w30756). National Bureau of Economic Research.\nMcCaffrey, D. F., Ridgeway, G., & Morral, A. R. (2004). Propensity score estimation with boosted regression for evaluating causal effects in observational studies. Psychological methods, 9(4), 403.\nMullainathan, S., & Spiess, J. (2017). Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2), 87-106.\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\nRobins, J. M., Rotnitzky, A., & Zhao, L. P. (1994). Estimation of regression coefficients when some regressors are not always observed. Journal of the American statistical Association, 89(427), 846-866.\nSetoguchi, S., Schneeweiss, S., Brookhart, M. A., Glynn, R. J., & Cook, E. F. (2008). Evaluating uses of data mining techniques in propensity score estimation: a simulation study. Pharmacoepidemiology and drug safety, 17(6), 546-555.\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\nVarian, H. R. (2014). Big data: New tricks for econometrics. Journal of Economic Perspectives, 28(2), 3-28.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nWager, S., Du, W., Taylor, J., & Tibshirani, R. J. (2016). High-dimensional regression adjustments in randomized experiments. Proceedings of the National Academy of Sciences, 113(45), 12673-12678.\nWestreich, D., Lessler, J., & Funk, M. J. (2010). Propensity score estimation: neural networks, support vector machines, decision trees (CART), and meta-classifiers as alternatives to logistic regression. Journal of clinical epidemiology, 63(8), 826-833.\nWyss, R., Ellis, A. R., Brookhart, M. A., Girman, C. J., Jonsson Funk, M., LoCasale, R., & Stürmer, T. (2014). The role of prediction modeling in propensity score estimation: an evaluation of logistic regression, bCART, and the covariate-balancing propensity score. American journal of epidemiology, 180(6), 645-655.\nZivich, P. N., & Breskin, A. (2021). Machine learning for causal inference: on the use of cross-fit estimators. Epidemiology (Cambridge, Mass.), 32(3), 393.\nZubizarreta, J. R. (2015). Stable weights that balance covariates for estimation with incomplete outcome data. Journal of the American Statistical Association, 110(511), 910-922."
  },
  {
    "objectID": "childrenbook.html",
    "href": "childrenbook.html",
    "title": "Causal Inference for Toddlers",
    "section": "",
    "text": "English (pdf)   Български (pdf)   Amazon (paperback)"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "“The Labor Market Effects of Mexican Repatriations: Longitudinal Evidence from the 1930s” (w/ JongKwan Lee and Giovanni Peri)\n\nJournal of Public Economics (2022) 25, 104558; latest version; replication package\nMedia: The Washington Post,The New York Times, The Economist, Bloomberg, This American Life, The Guardian, Forbes\nSummary: A large-scale program of repatriating Mexicans and Mexican Americans during the Great Depression did not achieve its goal – more jobs for native workers.\n\n\n\n\n\n“The Role of Labor Market Institutions in the Impact of Immigration on Wages and Employment” (w/ Mette Foged and Linea Hasager)\n\nScandinavian Journal of Economics (2022) 124(1), 164-213; latest version; replication package\nSummary: Labor market institutions may reduce the wage inequality effect of immigration while weakening the associated economic benefits.\n\n\n\n\n\n“Does Halting Refugee Resettlement Reduce Crime? Evidence from the US Refugee Ban” (2020) (w/ Daniel Masterson)\n\nAmerican Political Science Review (2021) 115(3), 1066-1073; latest version; replication package; appendix\nMedia: Bloomberg, The World Bank Blog\nSummary: The refugee ban introduced by Trump in early 2017 did not lower local crime rates.\n\n\n\n\n\n\n“Association between Health Care Utilization and Immigration Enforcement Events in San Francisco” (w/ J. Hainmueller, M. Hotard, D. Lawrence, L. Gottlieb, and J. Torres)\n\nJAMA Network Open (2020) 3(11), e2025065-e2025065; replication code; appendix; pre-analysis plan\nSummary: Healthcare utilization among likely undocumented immigrants in San Francisco did not respond to local ICE raids or anti-immigration rhetoric and policies.\n\n\n\n\n\n“Public Health Insurance Expansion for Immigrant Children and Interstate Migration of Low-income Immigrants” (w/ D. Lawrence, F. Mendoza, and J. Hainmueller)\n\nJAMA Pediatrics (2020) Vol 174(1), pp22-28.; replication package\nMedia: Vox, Reuters, Yahoo! News\nSummary: Public health insurance expansion for recent immigrants did not lead to an increased interstate in-migration among eligible foreign-born.\n\n\n\n\n\n“Standardizing the Fee-Waiver Application Increased Naturalization Rates of Low-Income Immigrants” (w/ M. Hotard, D. Lawrence, J. Hainmueller, and D. Laitin)\n\nPNAS Vol 116(34), pp16768-16772 (2019); latest version; replication package\nMedia: The Washington Post, Fortune\nSummary: Standardizing the fee waiver for citizenship applications raised naturalization rates among low-income immigrants.\n\n\n\n\n\n“Does Schedule Irregularity Affect Productivity? Evidence from Random Assignment into College Classes” (w/ Lester Lusher and Phuc Luong)\n\nLabour Economics Vol 60, pp115-128 (2019); latest version; replication package\nSummary: More volatile school start schedules throughout the week do not lead to lower college test scores.\n\n\n\n\n\n“The Labor Market Effects of a Refugee Wave: Synthetic Control Method Meets the Mariel Boatlift” (w/ Giovanni Peri)\n\nJournal of Human Resources\nMedia: The Wall Street Journal, The Economist, Vox, The Atlantic, Bloomberg (#1, #2), The Chicago Tribune, CBS News, Newsweek (#1, #2), Business Insider, Yahoo! Finance\nSummary: The Mariel Boatlift of 1980 did not result in large, statistically detectable wage or employment changes among low-skilled Miamians.\n\n\n\n\n\n“Gender Performance Gaps: Quasi-Experimental Evidence on the Role of Gender Differences in Sleep Cycles” (w/ Lester Lusher)\n\nEconomic Inquiry Vol 56(1), pp252-262 (2018); latest version; replication package\nMedia: The Washington Post, The Independent\nSummary: Girls benefit from earlier school start times relative to boys, partially contributing to the observed gender performance gap.\n\n\n\n\n\n“Double-Shift Schooling and Student Success: Quasi-experimental Evidence from Europe” (w/ Lester Lusher)\n\nEconomics Letters Vol.139, pp36-39 (2016); latest version; replication package\nSummary: Students achieve slightly higher grades when taking classes in the morning compared to the afternoon."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "",
    "text": "“The Labor Market Effects of Mexican Repatriations: Longitudinal Evidence from the 1930s” (w/ JongKwan Lee and Giovanni Peri)\n\nJournal of Public Economics (2022) 25, 104558; latest version; replication package\nMedia: The Washington Post,The New York Times, The Economist, Bloomberg, This American Life, The Guardian, Forbes\nSummary: A large-scale program of repatriating Mexicans and Mexican Americans during the Great Depression did not achieve its goal – more jobs for native workers.\n\n\n\n\n\n“The Role of Labor Market Institutions in the Impact of Immigration on Wages and Employment” (w/ Mette Foged and Linea Hasager)\n\nScandinavian Journal of Economics (2022) 124(1), 164-213; latest version; replication package\nSummary: Labor market institutions may reduce the wage inequality effect of immigration while weakening the associated economic benefits.\n\n\n\n\n\n“Does Halting Refugee Resettlement Reduce Crime? Evidence from the US Refugee Ban” (2020) (w/ Daniel Masterson)\n\nAmerican Political Science Review (2021) 115(3), 1066-1073; latest version; replication package; appendix\nMedia: Bloomberg, The World Bank Blog\nSummary: The refugee ban introduced by Trump in early 2017 did not lower local crime rates.\n\n\n\n\n\n\n“Association between Health Care Utilization and Immigration Enforcement Events in San Francisco” (w/ J. Hainmueller, M. Hotard, D. Lawrence, L. Gottlieb, and J. Torres)\n\nJAMA Network Open (2020) 3(11), e2025065-e2025065; replication code; appendix; pre-analysis plan\nSummary: Healthcare utilization among likely undocumented immigrants in San Francisco did not respond to local ICE raids or anti-immigration rhetoric and policies.\n\n\n\n\n\n“Public Health Insurance Expansion for Immigrant Children and Interstate Migration of Low-income Immigrants” (w/ D. Lawrence, F. Mendoza, and J. Hainmueller)\n\nJAMA Pediatrics (2020) Vol 174(1), pp22-28.; replication package\nMedia: Vox, Reuters, Yahoo! News\nSummary: Public health insurance expansion for recent immigrants did not lead to an increased interstate in-migration among eligible foreign-born.\n\n\n\n\n\n“Standardizing the Fee-Waiver Application Increased Naturalization Rates of Low-Income Immigrants” (w/ M. Hotard, D. Lawrence, J. Hainmueller, and D. Laitin)\n\nPNAS Vol 116(34), pp16768-16772 (2019); latest version; replication package\nMedia: The Washington Post, Fortune\nSummary: Standardizing the fee waiver for citizenship applications raised naturalization rates among low-income immigrants.\n\n\n\n\n\n“Does Schedule Irregularity Affect Productivity? Evidence from Random Assignment into College Classes” (w/ Lester Lusher and Phuc Luong)\n\nLabour Economics Vol 60, pp115-128 (2019); latest version; replication package\nSummary: More volatile school start schedules throughout the week do not lead to lower college test scores.\n\n\n\n\n\n“The Labor Market Effects of a Refugee Wave: Synthetic Control Method Meets the Mariel Boatlift” (w/ Giovanni Peri)\n\nJournal of Human Resources\nMedia: The Wall Street Journal, The Economist, Vox, The Atlantic, Bloomberg (#1, #2), The Chicago Tribune, CBS News, Newsweek (#1, #2), Business Insider, Yahoo! Finance\nSummary: The Mariel Boatlift of 1980 did not result in large, statistically detectable wage or employment changes among low-skilled Miamians.\n\n\n\n\n\n“Gender Performance Gaps: Quasi-Experimental Evidence on the Role of Gender Differences in Sleep Cycles” (w/ Lester Lusher)\n\nEconomic Inquiry Vol 56(1), pp252-262 (2018); latest version; replication package\nMedia: The Washington Post, The Independent\nSummary: Girls benefit from earlier school start times relative to boys, partially contributing to the observed gender performance gap.\n\n\n\n\n\n“Double-Shift Schooling and Student Success: Quasi-experimental Evidence from Europe” (w/ Lester Lusher)\n\nEconomics Letters Vol.139, pp36-39 (2016); latest version; replication package\nSummary: Students achieve slightly higher grades when taking classes in the morning compared to the afternoon."
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Research",
    "section": "Working Papers",
    "text": "Working Papers\n\n\n“Who Can Work from Home?’ (2020)\n\nIZA WP No. 13197; replication package."
  },
  {
    "objectID": "research.html#non-academic-publications",
    "href": "research.html#non-academic-publications",
    "title": "Research",
    "section": "Non-Academic Publications",
    "text": "Non-Academic Publications\n\n\n“Immigration in Local US Economies was Associated with Strong Native Wage Growth for 40 Years” (w/ Giovanni Peri) Global Migration Center (UC Davis) 03/2020\n\n\n\n\n“New Evidence on Immigration and Jobs” (w/ Giovanni Peri) The Wall Street Journal, 01/2016"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi there!",
    "section": "",
    "text": "While my real name is Vasil, most people call me Vasco. I am an applied statistician."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#background",
    "href": "blog/two-types-weights-causality.html#background",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Background",
    "text": "Background\nCausal inference fundamentally seeks to answer: What is the effect of a treatment or intervention? The challenge lies in ensuring that the comparison groups—treated versus untreated—are balanced in terms of their characteristics, so differences in outcomes can be attributed solely to the intervention. This idea, often referred to as “like-with-like” or “apples-to-apples” comparison, is as simple as it is intuitive.\nBalance is thus fundamental to causal inference. Weighting methods, central to achieving this balance, have evolved to include two primary types: inverse propensity score weights and covariate balancing weights. This article briefly describes these weights, their mathematical foundations, and the intuition behind them, with an example in R to bring it all together."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#notation",
    "href": "blog/two-types-weights-causality.html#notation",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Notation",
    "text": "Notation\nTo set the stage, we begin by establishing the potential outcome framework and laying out the notation for the discussion:\n\n\\(Y(0), Y(1)\\): Potential outcomes under treatment and control.\n\\(W\\): Treatment indicator (\\(1\\) for treated, \\(0\\) for untreated).\n\\(X\\): Vector of covariates (i.e., control variables).\n\\(e(X)=P(W=1 \\mid X)\\): Propensity score, the probability of receiving treatment given covariates.\n\\(\\mu_1​=E[Y(1)]\\): Mean outcome under treatment.\n\\(\\tau=\\mu_1 - \\mu_0\\): Average treatment effect (ATE), the main object of interest.\n\\(n\\): number of observations in the sample.\n\\(n_T\\): number of observations in the treatment group.\n\nWe observe a random sample of size \\(n\\), of \\(\\{Y_i, W_i, X_i \\}\\), where \\(i\\) indexes units (e.g., individuals, firms, schools etc.). Under the assumptions of strong ignorability—unconfoundedness \\(W \\perp (Y(0), Y(1)) \\mid X\\) and overlap \\((0&lt;e(X)&lt;1)\\) — the ATE can be identified and estimated."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#a-closer-look",
    "href": "blog/two-types-weights-causality.html#a-closer-look",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nInverse Propensity Score Weights\nInverse propensity score (IPS) weights rely on the estimated propensity score (X). The weights for treated and untreated groups are defined as:\n\\[ \\gamma_{\\text{IPS}}(X) = \\begin{cases} 1 / \\hat{e}(X) & \\text{if } W = 1, \\\\ 1 / (1 - \\hat{e}(X)) & \\text{if } W = 0. \\end{cases} \\]\nIPS weights intuitively correct for the unequal probability of treatment assignment. If an individual has a low probability of treatment \\(e(X)\\approx 0\\) but is treated, their weight is large, amplifying their influence in the analysis. Conversely, individuals with high treatment probabilities are downweighted to prevent overrepresentation.\nA few notes. First, these weights adjust the observed data such that the distribution of covariates in the weighted treated and control groups resembles that of the overall population. This helps mitigate selection bias, ensuring that comparisons between treated and control groups reflect a treatment effect rather than confounded differences in covariates (e.g., the control group is older).\nA second key property is that the weighted average of the outcomes for the treated group is an unbiased estimator for the mean outcome under treatment, \\(\\mu_1\\). This can be expressed mathematically as:\n\\[ \\hat{\\mu}_1(X)=\\frac{\\sum_i W_i Y_i/ \\hat{e}(X)}{ \\sum_i W_i / \\hat{e}(X)}=\\frac{1}{n_T}\\sum_i  \\gamma_{\\text{IPS}}(X_i) W_i Y_i. \\]\nThis equality is particularly useful in the next step, when estimating the treatment effect \\(\\tau\\).\nThird, these IPS weights are generally unknown and have to be estimated from the data. A leading exception is controlled randomized experiments where the researcher determines the probability of treatment. This is, however, uncommon. Traditionally, practitioners rely on methods like logistic regression to first obtain \\(\\hat{e}(X)\\) and then take its reciprocal to construct the weights.\nUsing IPS weights entails two primary challenges. When \\(\\hat{e}(X)\\) is close to \\(0\\) or \\(1\\), weights can become excessively large, leading to high variance in estimates. Practitioners then turn to trim observations with “too large” or “too small” \\(\\hat{e}(X)\\) values. Moreover, model misspecification in \\(\\hat{e}(X)\\) can lead to poor covariate balance, introducing bias. More flexible methods such as machine learning models can alleviate this problem.\nSoftware Packages: MatchIt, Matching.\n\n\nCovariate Balancing Weights\nAn alternative approach seeks weights that directly balance the dataset at hand. Typically this is framed as a constrained optimization problem with placing restrictions on the maximum imbalance in X between the treatment and control groups.\nThe weights \\(\\gamma_{\\text{CB}}(X)\\) are obtained by solving:\n\\[ \\min_{\\gamma} \\text{Imbalance}(\\gamma) + \\lambda \\text{Penalty}(\\gamma),\\]\nwhere “Imbalance” measures covariate discrepancies between groups, and “Penalty” controls for extreme weights. A common formulation balances means:\n\\[ \\frac{1}{n} \\sum_{i=1}^n W_i \\gamma(X_i) f(X_i) \\approx \\frac{1}{n} \\sum_{i=1}^n f(X_i), \\]\nwith \\(f(x_i)=x\\). The same idea can be applied to higher moments of \\(X\\) like variance or skewness. Examples methods relying on this type of weights include include Hainmueller (2012), Chan et al. (2016), and Athey et al. (2018), among many others.\nCovariate balancing weights bypass the need to explicitly estimate \\(e(X)\\). Instead, they solve for weights that ensure the treated and control groups are balanced across predefined covariates or their transformations. This method aligns closely with the goal of causal inference: achieving balance. The main challenge is selecting the right set of covariates to be balanced. Variance estimation can also be more involved, although modern statistical packages take care of it.\nSoftware Packages: MatchIt, Matching.\n\n\nHybrid Approach\nImai and Ratkovic (2013) developed a hybrid method that combines elements of covariate balancing and inverse propensity score methods. Their method, known as Covariate Balancing Propensity Score (CBPS), aims to estimate propensity scores while simultaneously achieving covariate balance. Instead of relying on a tuning parameter, CBPS ensures balance by solving the following moment conditions:\n\\[ \\sum_{i}\\left[ W_i - e(X_i; \\gamma) \\right] X_i = 0. \\]\nAdditionally, CBPS estimates by solving the standard likelihood score equation:\n\\[ \\sum_{i} \\left[ W_i - e(X_i; \\gamma) \\right] \\frac{\\partial e(X_i; \\gamma)}{\\partial \\gamma} = 0. \\]\nThe CBPS is operationalized in the Genralized Method of Moments (GMM) framework from the econometrics literature. This approach ensures that the estimated propensity scores lead to balanced covariates, potentially improving the robustness of causal inference. CBPS can be implemented using available GMM software packages and is particularly useful when traditional propensity score models fail to achieve adequate balance.\nSoftware Packages: CBPS."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#an-example",
    "href": "blog/two-types-weights-causality.html#an-example",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "An Example",
    "text": "An Example\nLet’s illustrate these methods in practice with R. Consider a dataset with treatment \\(W\\), outcome \\(Y\\), and covariates \\(X_1, X_2​\\). We estimate the ATE using both IPS, entropy balancing weights and CBPS. The exercise starts with generating some synthetic data.\nrm(list=ls())\nlibrary(MASS)\nlibrary(WeightIt)\nset.seed(1988)\n\n# generate fake data\nn &lt;- 1000\nX1 &lt;- rnorm(n)\nX2 &lt;- rnorm(n)\nW &lt;- rbinom(n, 1, plogis(0.5 * X1 - 0.25 * X2))\nY &lt;- 3 + 2 * W + X1 + X2 + rnorm(n)\ndata = data.frame(Y, W, X1, X2)\n\n# define functions that will calculate the weights and the associated Average Treatment Effects.\n\ncompute_weights &lt;- function(method) {\n    weightit(W ~ X1 + X2, method = method, data = data)$weights\n}\n\ncompute_ate &lt;- function(weights) {\n     weighted.mean(Y[W == 1], weights = weights[W == 1]) - \n        weighted.mean(Y[W == 0], weights = weights[W == 0])\n}\n\n# calcualte the three types of estimates.\nips_weights &lt;- compute_weights(\"glm\")\nebal_weights &lt;- compute_weights(\"ebal\")\ncbps_weights &lt;- compute_weights(\"cbps\")\n\n# we estimate the average treatment effect and print the results.\nips_ate &lt;- compute_ate(ips_weights)\nebal_ate &lt;- compute_ate(ebal_weights)\ncbps_ate &lt;- compute_ate(cbps_weights)\n\ncat(\"ATE (IPS Weights):\", ips_ate, \"\\n\")\n&gt;ATE (IPS Weights): 2.287048 \ncat(\"ATE (Entropy Balance Weights):\", ebal_ate, \"\\n\")\n&gt;ATE (Entropy Balance Weights): 2.287048 \n\ncat(\"ATE (CBPS Weights):\", cbps_ate, \"\\n\")\n&gt;ATE (CBPS Weights): 2.287048 \nThe weights are all very highly correlated with each other (not shown above), so they yield nearly identical results. For simplicity, I have ignored variance estimation and confidence intervals."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#where-to-learn-more",
    "href": "blog/two-types-weights-causality.html#where-to-learn-more",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThis article was inspired by Ben-Michael et al. (2021) and Chattopadhyay et al. (2020). Both references are great starting points. There are plenty of accessible materials on the topic online. My favorite is Imbens (2015). For more in-depth content turn to Imbens and Rubin (2015)’s seminal textbook."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#bottom-line",
    "href": "blog/two-types-weights-causality.html#bottom-line",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nCovariate balance between the treatment and control groups is at the core of causal inference.\nThere are two broad classes of weights that achieve such balance.\nIPS weights adjust for treatment probability but can be unstable.\nCovariate balancing weights directly target balance X, bypassing propensity score estimation."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#references",
    "href": "blog/two-types-weights-causality.html#references",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "References",
    "text": "References\nAthey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing: debiased inference of average treatment effects in high dimensions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 80(4), 597-623.\nBen-Michael, E., Feller, A., Hirshberg, D. A., & Zubizarreta, J. R. (2021). The balancing act in causal inference. arXiv preprint arXiv:2110.14831.\nChan, K. C. G., Yam, S. C. P., & Zhang, Z. (2016). Globally efficient non-parametric inference of average treatment effects by empirical balancing calibration weighting. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(3), 673-700.\nChattopadhyay, A., Hase, C. H., & Zubizarreta, J. R. (2020). Balancing vs modeling approaches to weighting in practice. Statistics in Medicine, 39(24), 3227-3254.\nHainmueller, J. (2012). Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20(1), 25-46.\nHirshberg, D. A., & Wager, S. (2018). Augmented minimax linear estimation for treatment and policy evaluation.\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society Series B: Statistical Methodology, 76(1), 243-263.\nImbens, G. W., & Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical sciences. Cambridge university press.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nLi, F., Morgan, K. L., & Zaslavsky, A. M. (2018). Balancing covariates via propensity score weighting. Journal of the American Statistical Association, 113(521), 390-400.\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41-55."
  },
  {
    "objectID": "blog/weights-statistics.html#background",
    "href": "blog/weights-statistics.html#background",
    "title": "Weights in Statistical Analyses",
    "section": "Background",
    "text": "Background\nWeights in statistical analyses offer a way to assign varying importance to observations in a dataset. Although powerful, they can be quite confusing due to the various types of weights available. In this article, I will unpack the details behind the most common types of weights used in data science.\nMost types of statistical analyses can be performed with weights. These include calculating summary statistics, regression models, bootstrap, etc. Even maximum likelihood estimation is minimally affected. In this article, I will keep it simple and only discuss mean and variance estimation.\nThe two primary types of weights encountered in practice are sampling weights and frequency weights. I will explore each in turn and conclude with a comparative example.\nTo begin, let’s imagine a well-behaved random variable X of which we have an iid sample of size n. I will use w to denote the relevant weighting variable."
  },
  {
    "objectID": "blog/weights-statistics.html#a-closer-look",
    "href": "blog/weights-statistics.html#a-closer-look",
    "title": "Weights in Statistical Analyses",
    "section": "A Closer Look",
    "text": "A Closer Look\nMean estimation does not depend on the type of weights. We have:\n\\[ \\bar{X} = \\frac{\\sum_i wX}{\\sum_i w}. \\]\nVariance estimation depends on the weight type.\nRemember that we estimate the standard error of \\(\\bar{X}\\) as:\n\\[ SE(\\bar{X})=\\frac{s}{\\sqrt{n}}, \\hspace{.3cm} \\text{where} \\hspace{.3cm} s=\\sqrt{\\frac{1}{N-1}\\sum_i(X-\\bar{X})^2} \\]\nis an estimate of the standard deviation of \\(X\\). I will explain below how estimation of \\(SE(\\bar{X})\\) differs for sampling and frequency weights. Importantly, s_w will denote the weighted version of this standard error.\n\nSampling Weights\nSampling weights measure the inverse probability of an observation entering the sample. They are particularly relevant in surveys when some units in the population are sampled more frequently than others. These weights adjust for sampling discrepancies to ensure that survey estimates are representative of the entire population. Sampling weights are also sometimes called probability weights.\nIntuitively, if we want to use the survey to accurately represent the population, we should assign higher importance to observations that are less likely to be sampled and lower importance to those more likely to appear in our data. This is exactly what sampling weights achieve. Sampling weights are, thus, inversely proportional to the probability of selection. Therefore, a smaller weight indicates a higher probability of being sampled, and vice versa.\nFor example, households in rural areas might be less likely to enter a survey due to the increased resources required to reach remote locations. Conversely, urban households are typically easier to sample and thus more likely to appear in the data.\nThis weighting approach is also common in causal inference analyses using propensity score methods. Similarly, in machine learning, weighting is often employed to adjust for unbalanced samples in the context of rare events (e.g., fraud detection, cancer diagnosis).\nLet’s now get back to the discussion on variance estimation. With sampling weights, we have:\n\\[ SE^{\\text{sampling}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{S}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\frac{(\\sum_i w)^2}{\\sum_i w^2}}}\\]\nThe numerator is the weighted version of the sum of squared deviations from the mean, emphasizing observations with higher weights. The denominator normalizes the standard error based on the total sum of the weights. When the weights differ greatly—high weights make some observations much more influential, increasing uncertainty about the population mean and so the sampling-weighted standard error is often larger.\nI will move on to describing frequency weights.\nSoftware Package: survey\n\n\nFrequency Weights\nFrequency weights measure the number of times an observation should be counted in the analysis. They are most relevant when there are multiple identical observations or when duplicating observations is possible. Naturally, higher weights correspond to observations that appear more frequently. Frequency weights are common in aggregated datasets. For instance, with market- or city-level data, we might want to weight the rows by market size, thus assigning more importance to larger units.\nWe can gain intuition from a linear algebra perspective. In a regression context, the design matrix X must be of full rank (to ensure invertibility), which implies no two rows can be identical. We thus need to collapse X to keep only distinct rows and record the number of times each row appears in the original data. This record constitutes the frequency weights.\nThe formula for estimating the standard error of X with frequency weights is:\n\\[ SE^{\\text{frequency}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{F}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\sum_i w}}.\\]\nHere \\(n_{\\text{eff}}\\) is the effective sample size (i.e., the sum of all weights) and \\(s_w\\) is the weighted standard deviation of \\(X\\).\nThe frequency-weighted standard error is typically the smaller because it increases the effective sample size without introducing variability in the contribution of different observations. In contrast, the sampling-weighted standard error could be larger if some observations are given much higher weights, increasing the variability and lowering the precision.\nSoftware Package: survey.\n\n\nOne More Thing\nThere is actually one more type of weights which are not so commonly used in practice, precision weights. They represent the precision (\\(1/\\text{variance}\\)) of observations. A weight of 5, for example, reflects 5 times the precision of a weight of 1, originally based on averaging 5 replicate observations. Precision weights often come up in statistical theory in places such as Generalized Least Squares where they promise efficiency gains (i.e., lower variance) relative to traditional Ordinary Least Squares estimation. In practice, precision weights are in fact frequency weights normalized to sum to n. Lastly, Stata’s user guide refers to them as analytic weights."
  },
  {
    "objectID": "blog/weights-statistics.html#an-example",
    "href": "blog/weights-statistics.html#an-example",
    "title": "Weights in Statistical Analyses",
    "section": "An Example",
    "text": "An Example\nLet’s see all of this in practice. We begin by creating a fake dataset of a variable \\(X\\) with both sampling and frequency weights. The weights are randomly created and hence have no underlying meaning, so think of this example as a tutorial, without a strong focus on the results.\n\nRPython\n\n\n# clear workspace and load libraries\nlibrary(survey)\nrm(list=ls())\nset.seed(681)\n\n# generate fake data\nn &lt;- 100000\ndata &lt;- data.frame(\n  x = rnorm(n),\n  prob_selection = runif(n, .1, .9),\n  freq_weight = rpois(n, 3)\n)\ndata$samp_weight &lt;- 1 / data$prob_selection\n\n# calculate the average value of $X$ using both types of weights.\ndesign_unweight &lt;- svydesign(ids = ~1, data = data, weights = ~1)\ndesign_samp &lt;- svydesign(ids = ~1, data = data, weights = ~samp_weight)\ndesign_freq &lt;- svydesign(ids = ~1, data = data, weights = ~freq_weight)\n\nmean_unweight &lt;- svymean(~x, design_unweight)\nmean_samp &lt;- svymean(~x, design_samp)\nmean_freq &lt;- svymean(~x, design_freq)\n\n# print results\nprint(round(mean_unweight, digits=3))\n&gt;    mean     SE\n&gt; x -0.002 0.0032\nprint(round(mean_samp, digits=3))\n&gt;  mean     SE\n&gt; x    0 0.0038\nprint(round(mean_freq, digits=3))\n&gt;    mean     SE\n&gt; x -0.002 0.0037\n\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.weightstats import DescrStatsW\n\n# Set seed for reproducibility\nnp.random.seed(681)\n\n# Generate fake data\nn = 100000\ndata = pd.DataFrame({\n    \"x\": np.random.normal(size=n),\n    \"prob_selection\": np.random.uniform(0.1, 0.9, size=n),\n    \"freq_weight\": np.random.poisson(3, size=n)\n})\ndata[\"samp_weight\"] = 1 / data[\"prob_selection\"]\n\n# Calculate the average value of X using both types of weights\n# Unweighted mean\nmean_unweight = DescrStatsW(data[\"x\"]).mean, DescrStatsW(data[\"x\"]).std_mean\n\n# Sampling-weighted mean\nmean_samp = DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).std_mean\n\n# Frequency-weighted mean\nmean_freq = DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).std_mean\n\n# Print results\nprint(\"Unweighted Mean and SE:\", np.round(mean_unweight, 3))\n&gt; Unweighted Mean and SE: [-0.004  0.003]\nprint(\"Sampling-Weighted Mean and SE:\", np.round(mean_samp, 3))\n&gt; Sampling-Weighted Mean and SE: [-0.002  0.002]\nprint(\"Frequency-Weighted Mean and SE:\", np.round(mean_freq, 3))\n&gt; Frequency-Weighted Mean and SE: [-0.004  0.002]\n\n\n\nThe unweighted and frequency-weighted means match exactly, (I am not sure why the sampling-weighted mean is slightly lower.) while the variances are different. The variance with frequency weights is lower than that of sampling weights."
  },
  {
    "objectID": "blog/weights-statistics.html#bottom-line",
    "href": "blog/weights-statistics.html#bottom-line",
    "title": "Weights in Statistical Analyses",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWeights are one of the most confusing aspects of working with data.\nSampling and frequency weights are the most common types of weights found in practice.\nThe former measure the inverse probability of being sampled, while the latter represent the number of times an observation enters the sample.\nWhile weighting usually does not impact point estimates (e.g., regression coefficients, means), incorrect usage of weights can lead to inaccurate confidence intervals and p-values.\nThis is relevant only if (i) your dataset contains weights, and (ii) you are interested in population-level statistics."
  },
  {
    "objectID": "blog/weights-statistics.html#where-to-learn-more",
    "href": "blog/weights-statistics.html#where-to-learn-more",
    "title": "Weights in Statistical Analyses",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nGoogle is a great starting place. Lumley’s blog post titled Weights in Statistics was incredibly helpful in preparing this article. Stata’s manuals which are publicly available contain more detailed information on various types of weighting schemes. See also Solon et al. (2015) for using weights in causal inference."
  },
  {
    "objectID": "blog/weights-statistics.html#references",
    "href": "blog/weights-statistics.html#references",
    "title": "Weights in Statistical Analyses",
    "section": "References",
    "text": "References\nDupraz, Y. (2013). Using Weights in Stata. Memo, 54(2)\nLumley, T. (2020), Weights in Statistics, Blog Post\nSolon, G., Haider, S. J., & Wooldridge, J. M. (2015). What are we weighting for?. Journal of Human resources, 50(2), 301-316.\nStata User’s Guide (2023)"
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#background",
    "href": "blog/flavors-gradient-boosting.html#background",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Background",
    "text": "Background\nGradient boosting has emerged as one of the most powerful techniques for predictive modeling. In its simplest form, we can think of gradient boosting like having a team of detectives working in sequence, where each new detective specifically focuses on solving the cases where their predecessors stumbled. Each detective contributes their findings to the investigation, with earlier ones catching obvious clues and later ones piecing together the subtle evidence that was initially missed. The final solution emerges from combining all their work.\nWhile the term “gradient boosting” is often used generically, there are notable differences among its implementations—each with unique strengths, weaknesses, and practical applications. Understanding these nuances is essential for advanced data scientists aiming to choose the best method for their specific datasets and computational constraints.\nThis article aims to provide a brief overview of the most popular gradient boosting methods, delving into their mathematical foundations and unique characteristics. By the end, you will have a clearer understanding of when and how to use each method to achieve the best results in your predictive modeling tasks. At the end of the article, I’ll provide a step-by-step Python example that implements these algorithms."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#notation",
    "href": "blog/flavors-gradient-boosting.html#notation",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Notation",
    "text": "Notation\nI assume a mathematical familiarity with machine learning (ML) basics and some minimal previous exposure to gradient boosting. If you need a refresher, grab any introductory ML textbook.\nBefore diving into the specifics of each method, let’s establish some common notation that will be used throughout this article:\n\n\\(\\mathbf{X}\\): Covariates/features matrix\n\\(\\mathbf{y}\\): Outcome/target variable\n\\(f(x)\\): Predictive model\n\\(L(y, \\hat{y})\\): Loss function\n\\(\\hat{y}\\): Predicted outcome/target value\n\\(\\gamma\\): Learning rate\n\\(n\\): Number of observations/instances\n\\(M\\): Number of algorithm iterations."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#a-closer-look",
    "href": "blog/flavors-gradient-boosting.html#a-closer-look",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Gradient Boosting\nGradient boosting is an ensemble machine learning technique that builds models sequentially, each new model attempting to correct the errors of the previous ones. The general idea is to minimize a loss function by adding weak learners (typically short decision trees) in a stage-wise manner to arrive at a single strong learner. This iterative process allows the model to improve its performance gradually, making it highly effective for complex datasets. Gradient boosting methods are versatile in that they can be used both for regression and classifications problems. In the most common case when the weak learners are decision trees, the algorithm is known as gradient tree boosting.\nMathematically, the model is built as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m = 1\\) to \\(M\\):\n\nCompute the pseudo-residuals: \\(r_{im} = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}\\) for \\(i=1,\\dots,n\\). In regression tasks this is simply \\(y-\\hat{y}\\).\nFit a weak learner such as a tree, \\(h_m(x)\\), on \\(\\{(x_i, r_{im}) \\}_{i=1}^n.\\)\nCompute \\(\\gamma\\) by solving: \\(\\gamma=\\arg\\min\\sum_i L(y_i, f_{m-1}(x_i)+\\gamma h_m(x_i)).\\)\nUpdate the model: \\(f_m(x) = f_{m-1}(x) + \\gamma h_m(x).\\)\n\nThe final model is \\(f_M(x).\\)\n\n\n\nThis is the most generic recipe for a gradient boosting algorithm. Let’s now focus on more specific implementations of this idea.\n\n\nAdaBoost\nAdaBoost, short for Adaptive Boosting, is one of the earliest boosting algorithms. It adjusts the weights of incorrectly classified observations so that subsequent learners focus more on difficult ones. In other words, it works by learning from its mistakes – after each round of predictions, it identifies which observations it got wrong and pays special attention to them (i.e., gives them a higher weight) in the next round. Thus, AdaBoost is not strictly speaking a gradient boosting algorithm, in the modern sense of the term.\nLet’s consider a binary classification problem where the outcome variable takes values in \\(\\{ -1, 1\\}\\). Here is the general AdaBoost algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nInitialize weights \\(w_i = \\frac{1}{n}\\) for \\(i = 1, \\ldots, n.\\)\nFor \\(m = 1 \\dots M\\):\n\nTrain a weak learner \\(h_m(x)\\) using weights \\(w_i\\).\nCompute the weighted error: \\(\\epsilon_m = \\frac{\\sum_{i=1}^{n} w_i \\mathbb{I}(y_i \\neq h_m(x_i))}{\\sum_{i=1}^{n} w_i}\\), where \\(\\mathbb{I}(\\cdot)\\) is the indicator function.\nCompute the model weight:$ _m = ()$.\nUpdate and \\(w_i^{m+1}= w_i^{m} \\exp(-\\alpha_m y_i h_m(x_i))\\), and \\(w_i^{m+1} = \\frac{ w_i^{m+1}}{\\sum_j  w_j^{m+1}}\\) for \\(i=1,\\dots,n\\).\n\nThe final model is \\(f(x)=sign \\left( \\sum_m \\alpha_m h_m(x) \\right)\\).\n\n\n\nAdaBoost is simple to implement while being relatively resistant to overfitting, making it especially effective for problems with clean, well-structured data. Its adaptive nature means it can identify and focus on the most challenging observations. However, AdaBoost has notable weaknesses: it’s highly sensitive to noisy data and outliers (since it increases weights on misclassified examples), can perform poorly when working with insufficient training data, and tends to be computationally intensive compared to newer, simpler algorithms.\nSoftware Packages: adabag, gbm, scikit-learn.\n\n\nXGBoost\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting designed for speed and performance. It revolutionizes the method by using the Newton-Raphson method in function space, setting it apart from traditional gradient boosting’s simpler gradient descent approach. At its core, XGBoost leverages both the first and second derivatives of the loss function through a second-order Taylor approximation. This sophisticated approach enables faster convergence and more accurate predictions by making better-informed decisions about how to improve the model at each step. When building decision trees, XGBoost considers both the expected improvement and the uncertainty of potential splits, while simultaneously applying regularization to prevent overfitting.\nWith some simplifications, the regression version of the XGBoost is implemented as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m=1, \\dots M\\):\n\nCompute the gradients \\[grad_m(x_i)=\\left( \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right)_{f(x)=f_{m-1}(x)}\\] and hessians \\[hess_m(x_i)=\\left( \\frac{\\partial^2 L(y_i, f(x_i))}{\\partial^2 f(x_i)^2}\\right)_{f(x)=f_{m-1}(x)}\\] of the loss function for \\(i=1,\\dots,n\\).\nFit a weak learner such as a tree, \\(h_m(x)\\) on$ {(x_i, ) }_{i=1}^n$.\nUpdate the model \\(f_m(x)=f_{(m-1)} - \\alpha h_m(x).\\)\n\nThe final model is \\(f_M(x)=\\sum_m f_m(x)\\).\n\n\n\nXGBoost is ideal for large datasets and competitions where model performance is critical. It is also a good choice when you need a highly optimized and scalable solution as it is highly efficient, supports parallel and distributed computing. Nevertheless, this algorithm can be complex to tune, and may require significant computational resources for very large datasets.\nSoftware Packages: xgboost.\n\n\nCatBoost\nCatBoost, short for Categorical Boosting, is a gradient boosting library that handles categorical features efficiently. It uses ordered boosting to reduce overfitting and supports GPU training, making it both powerful and versatile. CatBoost modifies the standard gradient boosting algorithm by incorporating two novel techniques – ordered boosting and target statistics for categorical features. Let’s examine each one in turn.\nRather than requiring preprocessing like one-hot encoding, CatBoost encodes categorical values based on the distribution of the target variable without introducing data leakage. This is achieved by encoding each data point as if it were unseen, preventing overfitting. Additionally, ordered boosting is a method that builds each new tree while treating each data point as “out-of-fold” for itself. This helps reduce overfitting, particularly in small datasets, by preventing over-reliance on individual observations during training.\nSoftware Packages: catboost.\n\n\nLightGBM\nLightGBM shares many of XGBoost’s benefits, such as support for sparse data, parallel training, multiple loss functions, regularization, and early stopping, but it also introduces a bunch of new features and improvements.\nFirst, rather than growing trees level-wise (row by row) as in most implementations, LightGBM grows trees leaf-wise, selecting the leaf that provides the greatest reduction in loss. Second, it does not use the typical sorted-based approach for finding split points, which sorts feature values to locate the best split. Instead, it relies on an efficient histogram-based method that significantly improves both speed and memory efficiency. Third, LightGBM incorporates the so-called Gradient-Based One-Side Sampling, which speeds up training by focusing on the most informative samples. Lastly, the algorithm uses Exclusive Feature Bundling which can group features to reduce dimensionality, enabling faster and more accurate model training.\nSoftware Packages: lightgbm.\n\n\nChallenges\nGradient boosting methods comes with a few common practical challenges. When a predictive model learns the training data too precisely, it may perform poorly on new, unseen data – a problem called overfitting. To combat this, various regularization methods are used to add helpful constraints to the learning process. A main parameter that regulates the model’s learning precision is the number of boosting rounds (\\(M\\)), which determines how many base models are created. While using more rounds reduces training errors, it also increases overfitting risk. To find the optimal \\(M\\) value, it’s common to use cross validation. The depth of decision trees is another important control parameter in tree boosting. Deeper trees can capture more complex patterns but are more prone to memorizing the training data rather than learning generalizable patterns.\nThe improved predictive performance of gradient boosting relative to simpler models comes at a cost of reduced model transparency. While a single decision tree’s reasoning can be easily traced and understood, tracking the decision-making process across numerous trees becomes extremely complex and challenging. Gradient boosting is an excellent example of the inherent tradeoff between model simplicity and performance.\nLet’s now look at how can we use these methods in practice."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#an-example",
    "href": "blog/flavors-gradient-boosting.html#an-example",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "An Example",
    "text": "An Example\nHere is some sample python code illustrating the implementation of each algorithm described above on a common dataset. Let’s look at it in detail.\n# loading the necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\n# we load the data and split it into training and test parts.\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# define and implement the boosting algorithms. \nclassifiers = {\n    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n    \"XGBoost\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42),\n    \"LightGBM\": LGBMClassifier(n_estimators=100, random_state=42),\n    \"CatBoost\": CatBoostClassifier(n_estimators=100, verbose=0, random_state=42)\n}\n\n# save results\nresults = {}\nfor name, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = accuracy\nFinally, we print the accuracy results:\n\nfor name, accuracy in results.items():\n    print(f\"{name}: {accuracy:.4f}\")\n\n# results:\n&gt;AdaBoost: 0.9737\n&gt;XGBoost: 0.9561\n&gt;LightGBM: 0.9649\n&gt;CatBoost: 0.9649\nOverall each method performed reasonably well, with accuracy ranging from \\(95.6\\)% to \\(97.4\\)%. Interestingly, Adaboost outperformed the other more complex algorithms, at least in the in terms of accuracy.\nAnd that’s it. You are now familiar with the most popular implementations of gradient boosting along with their advantages and weaknesses. You also know how to employ them in practice. Have fun incorporating XGboost and the like into your predictive modeling tasks."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#bottom-line",
    "href": "blog/flavors-gradient-boosting.html#bottom-line",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nGradient boosting is a powerful ensemble technique for predictive modeling that comes in a variety of flavors.\nAdaBoost focuses on misclassified instances by adjusting weights.\nXGBoost introduces regularization and optimization for speed and performance.\nCatBoost efficiently handles categorical features and reduces overfitting.\nLightGBM enjoys many of XGBoost’s strengths while introducing a few novelties including a different way of building the underlying weak learners.\nCommon practical challenges when implementing gradient boosting include overfitting, decreased interpretability and computational costs."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#where-to-learn-more",
    "href": "blog/flavors-gradient-boosting.html#where-to-learn-more",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great starting point, and it’s a resource I used extensively when preparing this article. “The Elements of Statistical Learning” by Hastie, Tibshirani, and Friedman is a comprehensive guide that covers the theoretical foundations of machine learning, including gradient boosting. It is the de facto bible for statistical ML. While this book is phenomenal, it can be challenging for less technical practitioners for which I recommend its lighter versions, “An Introduction to Statistical Learning” with R and Python code. All these books are available for free online. Lastly, if you want to dive even deeper into any of the algorithms describe above, consider studying the papers in the References section below."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#references",
    "href": "blog/flavors-gradient-boosting.html#references",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "References",
    "text": "References\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785-794).\nDorogush, A. V., Ershov, V., & Gulin, A. (2018). CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363.\nFreund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139.\nFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The elements of statistical learning: data mining, inference, and prediction.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An introduction to statistical learning: With applications in python. Springer Nature.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2013). An introduction to statistical learning: With applications in R. Springer Nature.\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., … & Liu, T. Y. (2017). Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30.\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: unbiased boosting with categorical features. Advances in neural information processing systems, 31."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#background",
    "href": "blog/limits-semiparam-models.html#background",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "Background",
    "text": "Background\nThe efficiency bound is a cornerstone of the academic literature on semiparametric models, and it’s easy to see why. This bound quantifies the potential loss in efficiency (i.e., increase in variance) that arises when opting for a semiparametric model over a fully parametric one. In doing so, it offers a rigorous benchmark for evaluating the asymptotic variance of any estimator. By providing insights into the trade-offs between model flexibility and statistical precision, the efficiency bound occupies a critical role in understanding the theoretical limits of estimation. Despite its importance, this concept and the broader class of semiparametric models remain underappreciated within much of the data science community.\nThis article aims to demystify the notion of the semiparametric efficiency bound and its relevance to practical applications. It unpacks the mathematical foundations underlying this concept, shedding light on its relationship with the Cramér-Rao lower bound (CRLB). I will also touch on the bound’s implications for real-world data analysis, where balancing flexibility and efficiency is often a key concern."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#notation",
    "href": "blog/limits-semiparam-models.html#notation",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "Notation",
    "text": "Notation\nBefore diving in, let’s establish the necessary notation to guide our technical discussion. The model governing the data is characterized by parameters \\(\\theta\\) and \\(\\eta\\) with likelihood \\(f(X; \\theta, \\eta)\\). Moreover:\n\n\\(\\theta \\in \\mathbb{R}^d\\) is the parameter of interest, a finite-dimensional vector we want to estimate. Often \\(\\theta\\) is a scalar. It represents the parametric component of the model.\n\\(\\eta\\) is a nuisance parameter, which is infinite-dimensional (e.g., a nonparametric density or function). It is a nuisance in the sense that it is part of the model, but we are not interested in it for its own sake. It represents the nonparametric component of the model.\n\nA leading example is the partially linear model:\n\\[Y= \\theta X+g(Z)+\\epsilon,\\]\nwhere \\(Y\\) is the outcome variable, \\(Z\\) represents a vector of covariates, \\(g(\\cdot)\\) is a function characterized by \\(\\eta\\), while \\(\\epsilon\\) is an error term. To fit this model in the likelihood notation above, think of \\(Z\\) as a component of \\(X\\). We assume we have a random i.i.d. sample of all necessary variables."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#a-closer-look",
    "href": "blog/limits-semiparam-models.html#a-closer-look",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn semiparametric models, the presence of \\(\\eta\\) complicates the estimation in that it can obscure the relationship between \\(\\theta\\) and the observed data. The semiparametric efficiency bound generalizes the CRLB by accounting for the nuisance parameter \\(\\eta\\) and isolating the information relevant to \\(\\theta\\).\n\nParametric Submodels\nLet’s take a sidestep for a minute. A parametric submodel, say \\(f(\\theta)\\), that contains \\(\\theta\\) alone, represents a subset of distributions that satisfy semiparametric assumptions and contains the true distribution \\(f(X; \\theta, \\eta)\\). For any semiparametric estimator that is consistent and asymptotically normal, its asymptotic variance can be compared to the CRLB of the parametric submodel. Since this relationship holds for all possible parametric submodels, the semiparametric estimator’s variance cannot be smaller than any submodel’s bound. In other words, the asymptotic variance of any semiparametric estimator is at least as large as the largest CRLB across all parametric submodels.\nInformally,\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\max_{\\text{{param. submodel}}} \\text{CRLB}.\\]\nThis is our first insight on the semiparametric efficiency bound, which admittedly is more of theoretical than practical significance.\n\n\nEfficient Influence Functions\nThe semiparametric efficiency bound depends on the interplay between \\(\\theta\\) and \\(\\eta\\), captured through the something called the Efficient Influence Function (EIF). Remember that the score function for \\(\\theta\\),\n\\[S_\\theta(X)=\\frac{\\partial}{\\partial \\theta} \\log f(X; \\theta, \\eta),\\]\nmeasures the sensitivity of the log-likelihood to changes in \\(\\theta\\). We can similarly define the score with respect to \\(\\eta\\):\n\\[S_\\eta(X)=\\frac{\\partial}{\\partial \\eta} \\log f(X; \\theta, \\eta).\\]\nNow enter the EIF \\(\\psi^*(X)\\) which captures the variation in \\(\\theta\\) while adjusting for the nuisance parameter \\(\\eta\\). It satisfies the orthogonality condition:\n\\[\\mathbb{E}\\left[ \\psi^*(X) \\cdot S_\\eta(X) \\right] = 0,\\]\nensuring that the influence of \\(\\eta\\) is removed from \\(\\psi(X)\\). In other words, \\(\\psi^*(X)\\) captures only information about \\(\\theta\\), uncontaminated by nuisance parameters. It is the influence function with the lowest possible variance.\n\n\nEfficient Score\nThe next piece of the puzzle is the Efficient Score \\(S^*_{\\theta}\\), the projection of \\(S_\\theta(X)\\) onto the space orthogonal to the nuisance tangent space \\(\\mathcal{T}_\\eta\\):\n\\[S^*_{\\theta}(X) = S_\\theta(X)  - \\Pi( S_\\theta(X) \\mid \\mathcal{T}_\\eta),\\]\nwhere \\(\\Pi(\\cdot)\\) is the projection operator. Here \\(\\mathcal{T}_\\eta\\) is simply the linear subspace spanned by \\(S_\\eta(X)\\). The Efficient Score is the part of the score vector that is “free” from the influence of nuisance parameters. It represents the best possible score function for estimating \\(\\theta\\) in the presence of \\(\\eta\\). (A similar technique underlies the so-called Neyman orthogonality principle in double/debiased machine learning.) We can construct the efficient influence function by appropriately scaling the efficient score to get to the optimal EIF.\n\n\nThe Semiparametric Efficiency Bound\nWe are, at last, ready to state the main result. The semiparametric efficiency bound is determined by the variance of the Efficient Score:\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{\\mathbb{E}[S_\\theta^*(X)^2]}.\\]\n​ This generalizes the Cramér-Rao lower bound for semiparametric models by incorporating the complexity introduced by the nuisance parameter \\(\\eta\\). In parametric models, this bound collapses to and is determined by the Fisher Information, while in here, it is governed by the efficient score.\nTo achieve the bound in practice, nuisance parameters are often removed through methods like regression residuals, inverse probability weighting, or targeted maximum likelihood estimation (TMLE). These techniques isolate the information about \\(\\theta\\) from \\(\\eta\\), enabling efficient estimation."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#bottom-line",
    "href": "blog/limits-semiparam-models.html#bottom-line",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nSemiparametric models blend parametric assumptions (related to a parameter of interest) with nonparametric flexibility (related to nuisance parameters).\nThe efficient influence function isolates the information about the parameter of interest, removing the impact of nuisance parameters.\nThe semiparametric efficiency bound generalizes CRLB to this class of models. It is determined by the variance of the efficient score vector.\nPractical estimation achieving this bound often involves removing nuisance effects through residualization or other adjustment techniques."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#references",
    "href": "blog/limits-semiparam-models.html#references",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "References",
    "text": "References\nBickel, P. J., Klaassen, C. A., Ritov, Y., & Wellner, J. A. (1993)\nEfficient and Adaptive Estimation for Semiparametric Models. Johns Hopkins University Press.\nGreene, William H. “Econometric analysis”. New Jersey: Prentice Hall (2000): 201-215.\nHines, O., Dukes, O., Diaz-Ordaz, K., & Vansteelandt, S. (2022). Demystifying statistical learning based on efficient influence functions. The American Statistician, 76(3), 292-304.\nIchimura, H., & Todd, P. (2007) Implementing Nonparametric and Semiparametric Estimators. In Heckman, J. & Leamer, E. (Eds.), Handbook of Econometrics (Vol. 6B).\nNewey, W. K. (1990) Semiparametric Efficiency Bounds. Journal of Applied Econometrics, 5(2), 99–135.\nTsiatis, A. (2007). Semiparametric theory and missing data. Springer Science & Business Media\nVan der Vaart, A. W. (2000) Asymptotic Statistics. Cambridge University Press."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#background",
    "href": "blog/causality-wo-experiments.html#background",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Background",
    "text": "Background\nCausality is central to many practical data-related questions. Conventional methods for isolating causal relationships rely on experimentation, assume unconfoundedness, or require instrumental variables. However, experimentation is often infeasible, costly, or ethically concerning; good instruments are notoriously difficult to find; and unconfoundedness can be an uncomfortable assumption in many settings.\nThis article highlights methods for measuring causality beyond these three paradigms. These underappreciated approaches exploit higher moments and heteroscedastic error structures (Lewbel 2012, Rigobon 2003), latent instrumental variables (IVs) (Ebbes et al. 2005), and copulas (Park and Gupta 2012). I will unite them in a common statistical framework and discuss the key assumptions underlying each one.\nThe focus will be on the ideas, intuition, and practical aspects of these methodologies, rather than technical details. Readers can find more in-depth information in the References section. This article assumes familiarity with econometric endogeneity and the basics of instrumental variables; without this background, some sections may be challenging to follow.\nNote: Regression discontinuity (RD) methods are excluded from this discussion, as they fall somewhere between instrument-based and instrument-free econometric methodologies. We know that in fuzzy RDs, the running variable can be viewed as an instrument."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#notation",
    "href": "blog/causality-wo-experiments.html#notation",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Notation",
    "text": "Notation\nLet’s begin by establishing some basic notation. We aim to analyze the impact of a binary, endogenous treatment variable \\(X_1\\) on an outcome variable \\(Y\\), in a setting with exogenous variables \\(X_2\\). We have access to a well-behaved, representative iid sample of size \\(n\\) of \\(Y\\), and \\(X:=[X_1, X_2]\\). These variables are related as follows:\n\\[ Y = \\beta X_1 + \\gamma X_2 + \\epsilon, \\]\nwhere is a mean-zero error term. Our goal is to obtain a consistent estimate of \\(\\beta\\). For simplicity, we’re using the same notation for both single- and vector-valued quantities, as \\(X_2\\) can be in \\(\\mathbb{R}^p\\) with $ p&gt;1$.\nThe challenge arises because \\(X_1\\) and \\(\\epsilon\\) are correlated, rendering the standard OLS estimator inconsistent. Even in large samples, \\(\\hat{\\beta}_{OLS}\\) will be biased, and getting more data would not help. Specifically:\n\\[\\hat{\\beta}_{OLS} := (X'X)^{-1}X'Y \\nrightarrow \\beta. \\]\nStandard instrument-based methods rely on the existence of an instrumental variable \\(Z\\) which, conditional on \\(X_2\\), correlates with \\(X_1\\) but not with . Estimation then proceeds with 2SLS, LIML, or GMM, potentially yielding good estimates of \\(\\beta\\) given appropriate assumptions. Common issues with instrumental variables include implausibility of the exclusion restriction, weak correlation with \\(X_1\\), and challenges in interpreting \\(\\hat{\\beta}_{IV}\\). Formally:\n\\[\\hat{\\beta}_{IV} := (Z'X)^{-1}Z'Y \\rightarrow \\beta. \\]\nIn this article, we focus on obtaining correct estimates of \\(\\beta\\) in settings where we don’t have access to such an instrumental variable \\(Z\\)."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#a-closer-look",
    "href": "blog/causality-wo-experiments.html#a-closer-look",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "A Closer Look",
    "text": "A Closer Look\nLet’s start with the heteroskedasticity-based approach of Lewbel (2012).\n\nHeteroskedasticity & Higher Moments\nThe main idea here is to construct valid instruments for \\(X_1\\) by using information contained in the heteroskedasticity of \\(\\epsilon\\). Intuitively, if \\(\\epsilon\\) exhibits heteroskedasticity related to $ X_2$, we can use to create instruments—specifically by interacting \\(X_2\\) with the residuals of the endogenous regressor’s reduced form equation. So, this is an IV-based method, but the instrument is “internal” to the model and does not rely on any external information.\nThe key assumptions are:\n\nThe error term in the structural equation (\\(\\epsilon\\)) is heteroskedastic. This means \\(var(\\epsilon|X_2)\\) is not constant and depends on \\(X_2\\). Moreover, we need \\(cov(X_2,\\epsilon^2) \\neq 0\\). This is an analogue of the first stage assumption in IV methods.\nThe exogenous variable (\\(X_2\\)) are uncorrelated with the product of the endogeneous variable (\\(X_1\\)) and the error term (\\(\\epsilon\\)). That is, \\(cov(X_2, X_1\\epsilon) = 0\\). This is a form of the standard exogeneity assumption in IV estimation.\n\nThe heteroskedasticity-based estimator of Lewbel (2012) proceeds in two steps:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRegress \\(X_1\\) on \\(X_2\\) and save the estimated residuals; call them \\(\\hat{u}\\). Construct an instrument for \\(X_1\\) as \\(\\tilde{Z}=(X_2-\\bar{X}_2)\\hat{u}\\), where \\(\\bar{X}_2\\) is the mean of \\(X_2\\).\nUse \\(\\tilde{Z}\\) as an instrument in a standard 2SLS estimation: \\[\\hat{\\beta}_{LEWBEL} = (X'P_{\\tilde{Z}}X)^{-1}X'P_{\\tilde{Z}}Y,\\]\n\nwhere \\(P_{\\tilde{Z}}\\) ​is the projection matrix onto the instrument.\n\n\nThis line of thought can also be extended to use higher moments as an alternative or additional way to construct instrumental variables. The original approach uses the variance of the error term, but we can also rely on skewness, kurtosis, etc. The assumptions then must be modified such that these higher moments are correlated with the endogenous variable, etc.\nSoftware Packages: REndo, ivlewbel.\n\n\nLatent IVs\nThe latent IV approach imposes distributional assumptions on the exogenous part of the endogenous variable and employs likelihood-based methods to estimate \\(\\beta\\).\nLet’s simplify the model above, so that we have:\n\\[ Y=\\beta X + \\epsilon, \\]\nwhere \\(X\\) is endogenous. The key idea is to decompose \\(X\\) into two components:\n\\[ X= \\theta + \\nu, \\]\nwith \\(cov(\\theta, \\epsilon)=0\\), \\(cov(\\theta, \\nu) = 0\\), and \\(cov(\\epsilon, \\nu)\\neq0\\). The first condition states that \\(\\theta\\) is the exogenous part of \\(X\\), and the last one gives rise to the endogeneity problem.\nWe then proceed with adding distributional assumptions. Importantly, \\(\\theta\\) must follow some discrete distribution with a finite number of mass points. A common example imposes:\n\\[ \\theta \\sim \\text{Multinomial}(\\cdot) \\]\nand\n\\[ (\\epsilon, \\nu) \\sim \\text{Gaussian}(\\cdot). \\]\nThese set of assumptions lead to analytical solutions for the conditional and unconditional distributions of \\((Y,X)\\) and all parameters of the model are identified. Maximum likelihood estimation can then give us an estimate of \\(\\beta_{LIV}\\).\nSoftware Packages: REndo.\n\n\nCopulas\nFirst, a word on copulas. A copula is a multivariate cumulative distribution function (CDF) with uniform marginals on \\([0,1]\\). An old theorem states that any multivariate CDF can be expressed with uniform marginals and a copula function that represents the relationship between the variables. Specifically, if \\(A\\) and \\(B\\) are two random variables with marginal CDFs \\(F_A\\) and \\(F_B\\) and joint CDF \\(H\\), then there exists a copula \\(C\\) such that \\(H(a,b)=C(F_A(a), F_B(b))\\).\nHow does this fit into our context and framework? Park and Gupta (2012) introduced two estimation methods for \\(\\beta\\) under the assumption that \\(\\epsilon \\sim Gaussian(\\cdot)\\). The key idea is positing a Gaussian copula to link the marginal distributions of \\(X\\) and \\(\\epsilon\\) and obtain their joint distribution. We can then estimate \\(\\beta\\) in one of two ways: either impose distributional assumptions on these marginals and derive and maximize the joint likelihood function of \\(X\\) and \\(\\epsilon\\), or use a generated regressor approach. We will focus on the latter.\nIn the linear model, endogeneity is tackled by creating a novel variable \\(\\tilde{X}\\) and adding that as a control (i.e., a generated regressor). Using our simplified model where \\(X\\) is single-valued and endogenous, we now have:\n\\[Y=\\beta X + \\mu \\tilde{X} + \\eta, \\]\nwhere \\(\\eta\\) is the error term in this augmented model.\nWe construct \\(\\tilde{X}\\) as follows:\n\\[\\tilde{X}=\\Phi^{-1}(\\hat{F}_X(X)).\\]\nHere \\(\\Phi^{-1}(\\cdot)\\) is the inverse CDF of the standard normal distribution and \\(F_X(\\cdot)\\) is the marginal CDF of \\(X\\). We can estimate the latter using the empirical CDF by sorting the observations in ascending order and calculating the proportion of rows with smaller values for each observation. As you can guess, this introduces further uncertainty into the model, so the standard errors should be estimated using bootstrap.\nSoftware Packages: REndo, copula.\n\n\nComparison\nEach statistical method has its strengths and limitations. While the methods described here circumvent the traditional unconfoundedness and external instruments-based assumptions, they do not provide a magical panacea to the endogeneity problem. Instead, they rely on their own, different assumptions. These methods are not universally superior, but should be considered when traditional approaches do not fit your context.\nThe heteroskedasticity-based approach, as the name suggests, requires a considerable degree of heteroskedasticity to perform well. Latent IVs may offer efficiency advantages but come at the cost of imposing distributional assumptions and requiring a group structure of \\(X_1\\). The copula-based approach, while simple to implement, also requires strong assumptions about the distributions of \\(X\\) and \\(Y\\), as well as their relationship.\nThat’s it. You are now equipped with a set of new methods designed to identify causal relationships in your data."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#bottom-line",
    "href": "blog/causality-wo-experiments.html#bottom-line",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nConventional methods used to tease causality rely on experiments or ambitious assumptions such as unconfoundedness or the access to valid instrumental variables.\nResearchers have developed methods aimed at measuring causality without relying on these frameworks.\nNone of these are a panacea and they rely on their own assumptions that have to be checked on a case-by-case basis."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#where-to-learn-more",
    "href": "blog/causality-wo-experiments.html#where-to-learn-more",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nEbbes, Wedel, and Bockenholt (2009), Park and Gupta (2012), Papies, Ebbes, and Heerde (2017), and Rutz and Watson (2019) provide detailed comparisons of these IV-free methods with alternative methods. Also, Qian et al. (2024) and Papadopolous (2022) and Baum and Lewbel (2019) have a practical angle that many data scientist will find accessible and attractive."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#references",
    "href": "blog/causality-wo-experiments.html#references",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "References",
    "text": "References\nBaum, C. F., & Lewbel, A. (2019). Advice on using heteroskedasticity-based identification. The Stata Journal, 19(4), 757-767.\nEbbes, P. (2004). Latent instrumental variables: a new approach to solve for endogeneity.\nEbbes, P., Wedel, M., & Böckenholt, U. (2009). Frugal IV alternatives to identify the parameter for an endogenous regressor. Journal of Applied Econometrics, 24(3), 446-468.\nEbbes, P., Wedel, M., Böckenholt, U., & Steerneman, T. (2005). Solving and testing for regressor-error (in) dependence when no IVs are available: With new evidence for the effect of education on income. Quantitative Marketing and Economics, 3, 365-392.\nErickson, T., & Whited, T. M. (2002). Two-step GMM estimation of the errors-in-variables model using high-order moments. Econometric Theory, 18(3), 776-799.\nGui, R., Meierer, M., Schilter, P., & Algesheimer, R. (2020). REndo: An R package to address endogeneity without external instrumental variables. Journal of Statistical Software.\nHueter, I. (2016). Latent instrumental variables: a critical review. Institute for New Economic Thinking Working Paper Series, (46).\nLewbel, A. (1997). Constructing instruments for regressions with measurement error when no additional data are available, with an application to patents and R&D. Econometrica, 1201-1213.\nLewbel, A. (2012). Using heteroscedasticity to identify and estimate mismeasured and endogenous regressor models. Journal of business & economic statistics, 30(1), 67-80.\nPapadopoulos, A. (2022). Accounting for endogeneity in regression models using Copulas: A step-by-step guide for empirical studies. Journal of Econometric Methods, 11(1), 127-154.\nPapies, D., Ebbes, P., & Van Heerde, H. J. (2017). Addressing endogeneity in marketing models. Advanced methods for modeling markets, 581-627.\nPark, S., & Gupta, S. (2012). Handling endogenous regressors by joint estimation using copulas. Marketing Science, 31(4), 567-586.\nRigobon, R. (2003). Identification through heteroskedasticity. Review of Economics a nd Statistics, 85(4), 777-792.\nQian, Y., Koschmann, A., & Xie, H. (2024). A Practical Guide to Endogeneity Correction Using Copulas (No. w32231). National Bureau of Economic Research.\nRigobon, R. (2003). Identification through heteroskedasticity. Review of Economics and Statistics, 85(4), 777-792.\nRutz, O. J., & Watson, G. F. (2019). Endogeneity and marketing strategy research: An overview. Journal of the Academy of Marketing Science, 47, 479-498.\nTran, K. C., & Tsionas, E. G. (2015). Endogeneity in stochastic frontier models: Copula approach without external instruments. Economics Letters, 133, 85-88."
  },
  {
    "objectID": "blog/paradox-lord.html#background",
    "href": "blog/paradox-lord.html#background",
    "title": "Lord’s Paradox: A Simple Illustration",
    "section": "Background",
    "text": "Background\nLord’s paradox presents a fascinating challenge in causal inference and statistics. It highlights how different statistical methods applied to the same data can lead to contradictory conclusions. The paradox typically arises when comparing changes over time between two groups in the context of adjusting for baseline characteristics. Despite its theoretical nature, the phenomena is deeply relevant to practical data analysis, especially in observational studies where causation is challenging to establish. Let’s look at an example."
  },
  {
    "objectID": "blog/paradox-lord.html#a-closer-look",
    "href": "blog/paradox-lord.html#a-closer-look",
    "title": "Lord’s Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nMean Differences Over Time\nTo explore Lord’s paradox, consider the following scenario: Suppose we have two groups of individuals—\\(A\\) and \\(B\\)—with their body weights measured at two time points: before and after a specific intervention. Let the weight at the initial time point be denoted as \\(W_{\\text{pre}}\\), and the weight at the final time point be \\(W_{\\text{post}}\\). We are interested in whether the intervention caused a change in weight between the two groups.\nOne approach to analyze this is to compute the (unadjusted) mean weight change for each group over time:\n\\[\\Delta = \\Delta^A - \\Delta^B.\\]\nIf this quantity is statistically significant, we might conclude that the intervention had a differential effect.\n\n\nControlling for Baseline Characteristics\nAn alternative approach involves adjusting for baseline weight \\(W_{\\text{pre}}\\) using, for example, a regression model:\n\\[W_{\\text{post}}=\\beta_1 + \\beta_2 G_A + \\beta_3 W_{\\text{pre}} + \\epsilon,\\]\nwhere \\(G\\) is a binary indicator for group \\(A\\) membership and \\(\\epsilon\\) is an error term. Here, \\(\\beta_2\\) captures the group difference in \\(W_{\\text{post}}\\), linearly controlling for baseline body weight.\nSurprisingly, these two approaches can yield conflicting results. For example, the former method might suggest no difference, while the regression adjustment indicates a significant group effect.\n\n\nExplanation\nThis contradiction arises because the two methods implicitly address different causal questions.\n\nMethod 1 asks: “Do Groups \\(A\\) and \\(B\\) gain/lose different amounts of weight?”\nMethod 2 asks: “Given the same initial weight, does any of the groups end up at different final weights?” The regression approach adjusts for baseline differences, assuming \\(W_{\\text{pre}}\\) is a confounder.\n\nThe key insight is that the choice of analysis reflects underlying assumptions about the causal structure of the data. If \\(W_{\\text{pre}}\\) is affected by group membership (e.g., due to selection bias), then adjusting for it may introduce bias rather than remove it. However, in practice we most often should control for baseline characteristics as they are critical in balancing the treatment and control groups.\n\n\nThe Simpson’s Paradox Once Again\nI recently illustrated the more commonly discussed Simpson’s paradox. Interestingly, a 2008 paper claims that two phenomena are closely related, with the Lord’s paradox being a “continuous version” of Simpson’s paradox."
  },
  {
    "objectID": "blog/paradox-lord.html#an-example",
    "href": "blog/paradox-lord.html#an-example",
    "title": "Lord’s Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nLet’s look at some code illustrating Lord’s paradox in R and python. We start with simulating a dataset where two groups have identical distributions of \\(W_{\\text{pre}}\\) and \\(W_{\\text{post}}\\), yet differing relationships between the two variables.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\nn &lt;- 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup &lt;- factor(rep(c(\"A\", \"B\"), each = n/2))\n\n# Initial weight (pre). # Group A starts with higher average weight\nweight_pre &lt;- numeric(n)\nweight_pre[group == \"A\"] &lt;- rnorm(n/2, mean = 75, sd = 10)\nweight_pre[group == \"B\"] &lt;- rnorm(n/2, mean = 65, sd = 10)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain &lt;- rnorm(n, mean = 10, sd = 5)\nweight_post &lt;- weight_pre + gain\n\n# Create data frame\ndata &lt;- data.frame(group = group, pre = weight_pre, post = weight_post)\n\n# Analysis 1: Compare change scores between groups`\nt.test(post - pre ~ group, data = data)\n&gt; p-value = 0.6107\n\n# Analysis 2: Regression Adjustment`\nmodel &lt;- lm(post ~ group + pre, data = data)\nsummary(model)\n&gt; p-value = 0.08428742\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Set seed for reproducibility\nnp.random.seed(1988)\nn = 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup = np.array([\"A\"] * (n // 2) + [\"B\"] * (n // 2))\n\n# Initial weight (pre). Group A starts with higher average weight\nweight_pre = np.zeros(n)\nweight_pre[group == \"A\"] = np.random.normal(loc=75, scale=10, size=n // 2)\nweight_pre[group == \"B\"] = np.random.normal(loc=65, scale=10, size=n // 2)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain = np.random.normal(loc=10, scale=5, size=n)\nweight_post = weight_pre + gain\n\n# Create DataFrame\ndata = pd.DataFrame({\"group\": group, \"pre\": weight_pre, \"post\": weight_post})\n\n# Analysis 1: Compare change scores between groups\ndata[\"change\"] = data[\"post\"] - data[\"pre\"]\ngroup_a_change = data[data[\"group\"] == \"A\"][\"change\"]\ngroup_b_change = data[data[\"group\"] == \"B\"][\"change\"]\nt_stat, p_value = ttest_ind(group_a_change, group_b_change)\nprint(f\"Analysis 1: p-value = {p_value:.4f}\")\n\n# Analysis 2: Regression Adjustment\nmodel = smf.ols(\"post ~ group + pre\", data=data).fit()\nprint(model.summary())\n\n\n\nThe former approach shows lack of statistically significant differences in the body weight after the intervention between the two groups (\\(p\\)-value =$ 0.6107\\(). The results from the latter method do show meaningful differences (\\)p$-value = \\(0.0842\\)).\nThis illustrates the core of Lord’s paradox – the statistical approach chosen can lead to different interpretations of the same underlying phenomenon."
  },
  {
    "objectID": "blog/paradox-lord.html#bottom-line",
    "href": "blog/paradox-lord.html#bottom-line",
    "title": "Lord’s Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLord’s paradox underscores the importance of aligning statistical methods with causal assumptions.\nDifferent methods answer different questions and may yield contradictory results if applied blindly.\nCareful consideration of the data-generating process and the role of potential confounders is crucial in choosing the appropriate analytical approach."
  },
  {
    "objectID": "blog/paradox-lord.html#references",
    "href": "blog/paradox-lord.html#references",
    "title": "Lord’s Paradox: A Simple Illustration",
    "section": "References",
    "text": "References\nLord, E. M. (1967). A paradox in the interpretation of group comparisons. Psychological Bulletin, 68, 304–305. doi:10.1037/h0025105\nLord, F. M. (1969). Statistical adjustments when comparing preexisting groups. Psychological Bulletin, 72, 336–337. doi:10.1037/h0028108\nLord, E. M. (1975). Lord’s paradox. In S. B. Anderson, S. Ball, R. T. Murphy, & Associates, Encyclopedia of Educational Evaluation (pp. 232–236). San Francisco, CA: Jossey-Bass.\nTu, Y. K., Gunnell, D., & Gilthorpe, M. S. (2008). Simpson’s Paradox, Lord’s Paradox, and Suppression Effects are the same phenomenon–the reversal paradox. Emerging themes in epidemiology, 5, 1-9."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#background",
    "href": "blog/hypothesis-testing-linear-ml.html#background",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Background",
    "text": "Background\nMachine learning models are an indispensable part of data science. They are incredibly good at what they are designed for – making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, \\(p\\)-values, or anything else related to statistical significance. Why?\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have already selected the most strongly correlated variables.\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The \\(t\\)-stats and \\(p\\)-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models – Ridge, Elastic net, SCAD, etc."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#notation",
    "href": "blog/hypothesis-testing-linear-ml.html#notation",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Notation",
    "text": "Notation\nAs a reminder, \\(\\beta^{lasso}\\) is the solution to:\n\\[\\min_{\\beta} \\frac{1}{2} || Y-x\\beta|| ^2_2 + \\lambda ||\\beta||_1. \\]\nWe are trying to predict a vector \\(Y\\in \\mathbb{R}\\) with a set of features \\(X\\in \\mathbb{R}^{pxn}\\) with \\(p\\leq n\\), and \\(\\lambda\\) is a tuning parameter. When needed, I will use \\(j\\) to index individual columns (i.e., variables) of \\(X\\)."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#a-closer-look",
    "href": "blog/hypothesis-testing-linear-ml.html#a-closer-look",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nTwo Types of Models and Parameters\nWe first need to discuss an important subtlety. There are two distinct ways of thinking about performing hypothesis testing in ML models. The traditional view is that we have a true linear model which includes all variables:\n\\[\\begin{equation} Y=X\\beta_0+\\epsilon. \\end{equation}\\]\nWe are interested in testing whether \\(\\beta_0=0\\) – that is, inference on the full model. This is certainly an intuitive target. The interpretation is that this model encapsulates all relevant causal variables for \\(Y\\). Importantly, even if a given variable $X_j $ is not selected, it still belongs to the model and has a meaningful interpretation.\nThere is an alternative way to think about the problem. Imagine we run a variable selection algorithm (e.g., lasso), which selects a subset \\(M=\\{1,\\dots,p\\}\\) of all available predictors (\\(X\\)), \\(M&lt;p\\) leading to the alternative model:\n\\[\\begin{equation} Y=X_M\\beta_M+u. \\end{equation}\\]\nNow we are interested in testing whether \\(\\beta_M=0\\) – that is, inference on the selected model. Unlike the scenario above, here \\(\\beta_{Mj}\\) is interpreted as the change in \\(Y\\) for a unit change in \\(X_j\\) when all other variables in \\(X_M\\) (as opposed to all of \\(X\\)) are kept constant.\nWhich of the two targets is more intuitive?\nStatisticians argue vehemently about this. Some claim that the full model interpretation is inherently problematic. It is too naïve and perhaps even arrogant to think that (i) mother nature can be explained by a linear equation, (ii) we can measure and include the full set of relevant predictors. On top of this, there are also technical issues with this interpretation beyond the scope of this post.\nTo overcome these challenges, relatively recently, statisticians developed the idea of inference on the selected model. This introduces major technical challenges, however.\n\n\nThe Naïve Approach: What Not to Do\nFirst things first – here is what we should not do.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRun a Lasso regression.\nRun OLS regression on the subset of selected variables.\nPerform statistical inference with the estimated \\(t\\)-stats, confidence intervals, and \\(p\\)-values.\n\n\n\nThis is bad practice. Can you see why?\nIt is simply because we ignored the fact that we already peeked at the data when we ran the Lasso regression. Lasso already chose the variables that are strongly correlated with the outcome. Intuitively, we will need to inflate the \\(p\\)-values to account for the data exploration in the first step.\nIt turns out that, in general, both the finite- and large-sample distributions of these parameters are non-Gaussian and depend on unknown parameters in weird ways. Consequently, the calculated \\(t\\)-stats and \\(p\\)-values are all wrong, and there is little hope that anything simple can be done to save this approach. And no, the standard bootstrap cannot help us either.\nBut are there special cases when this approach might work? A recent paper titled “In Defense of the Indefensible: A Very Naïve Approach to High-Dimensional Inference” argues that under very strict assumptions on \\(X\\) and \\(\\lambda\\), this method is actually kosher. The reason it works is that in the magical world of those assumptions, the set of variables that Lasso chooses is deterministic, and not random (hence circumventing the issue described above). The resulting estimator is unbiased and asymptotically normal – hence hypothesis testing is trivial.\nHere is what we should do instead.\n\n\nThe Classical Approach: Inference on the Full Model\nRoughly speaking, there are at least four ways we can go about doing hypothesis testing for \\(\\beta\\) in equation (1).\n\nData Split\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nSplit our data into two equal parts.\nRun a Lasso regression on the first part.\nRun OLS on the second part with the selected variables from Step 2.\nPerform inference using the computed \\(t\\)-stats, \\(p\\)-values, and confidence intervals.\n\n\n\nThis is simple and intuitive. The problem is that in small samples, the \\(p\\)-values can be quite sensitive to how we split the data in the first step. This is clearly undesirable, as we will be getting different results every time we run this algorithm for no apparent reason.\n\n\nMulti Split\nThis is a modification of the Data Split approach designed to solve the sensitivity issue and increase power.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRepeat \\(B\\) times:\n\n\nReshuffle data.\nRun the Data Split method.\nSave the \\(p\\)-values.\n\n\nAggregate the B \\(p\\)-values into a single final one for each variable.\n\n\n\nInstead of splitting the data into two parts only once, we can do it many times, and each time, we get a \\(p\\)-value for every variable. The aggregation goes a long way to solving the instability of the simple data split approach. There is a lot of clever mathematics behind it. For example, there is a complicated expression for aggregating the \\(p\\)-values rather than taking a simple average.\nSoftware Package: hdi.\n\n\nBias Correction\nThis approach tackles the problem from a very different angle. The idea is to directly remove the bias from the naïve Lasso procedure without any subsampling or data splitting. Somewhat magically, the resulting estimator is unbiased and asymptotically normally distributed – statistical inference is then straightforward.\nThere are multiple versions of this idea, but the general form of these estimators is:\n\\[\\hat{\\beta}^{\\text{bias cor}} = \\hat{\\beta}^{lasso} + \\hat{\\Theta} X'\\epsilon^{lasso},\\]\nWhere \\(\\hat{\\beta}^{lasso}\\) is the lasso estimator and \\(\\epsilon^{lasso}\\) are the residuals. The missing piece is the \\(\\hat{\\Theta}\\) matrix; there are several ways to estimate it depending on the setting. In its simplest form, \\(\\hat{\\Theta}\\) is the inverse of the sample variance-covariance matrix. Other examples include the matrix, which minimizes an error term related to the bias as well as the variance of its Gaussian component. Similar bias-correction methods have been developed for Ridge regression as well.\nSoftware Package: hdi.\n\n\nBootstrap\nAs in many other complicated settings for statistical inference, the bootstrap can come to the rescue. Still, the plain vanilla bootstrap will not do. Instead, here is the general idea of the leading version of the bootstrap estimator for Lasso:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRun a Lasso regression.\nKeep only \\(\\beta^{lasso}\\)’s larger than some magical threshold.\nCompute the associated residuals and center them around \\(0\\).\nRepeat B times:\n\n\ndraw random samples of these centered residuals,\ncompute new responses \\(\\dot{Y}\\) by adding them to the predictions \\(X'\\beta^{lasso}\\), and\nobtain \\(\\beta^{lasso}\\) coefficients from Lasso regressions on these new responses \\(\\dot{Y}\\).\n\n\nUse the distribution of the obtained coefficients to conduct statistical inference.\n\n\n\nThis idea can be generalized to other settings and, for instance, be combined with the bias-corrected estimator.\nThis wraps up our discussion of methods for performing hypothesis testing on equation (1) (i.e., the full model). We now move on to a more challenging topic – inference on equation (2) (i.e., the selected model).\n\n\n\nThe Novel Approach: Inference on the Selected Model\n\nPoSI (Post Selection Inference)\nThe goal of the PoSI method is to construct confidence intervals that are valid regardless of the variable selection method and the selected submodel. The benefit is that we would be reaching the correct conclusion even if we did not select the true model. This luxury comes at the expense of often being too conservative (i.e., confidence intervals are “too wide”). Let me explain how this is done.\nTo take a step back, most confidence intervals in statistics take the form:\n\\[\\hat{\\beta} \\pm m \\times \\hat{SE}(\\hat{\\beta}).\\]\nEvery data scientist has seen a similar formula before. The question is usually one about choosing the constant \\(m\\). When we work with two-sided hypotheses tests and large samples, we often use \\(m = 1.96\\) because this is roughly the \\(97.5\\)th percentile of the \\(t\\)-distribution with many degrees of freedom. This gives a \\(2.5\\%\\) false positive error on both tails of the distribution (\\(5\\%\\) in total) and hence the associated 95% confidence intervals. The larger m, the wider or more conservative the confidence interval.\nThere are a few ways to choose the constant m in the PoSI world. Vaguely speaking, PoSI says we should select this constant to equal the \\(97.5\\)th percentile of a distribution related to the largest \\(t\\)-statistic among all possible models. This is usually approximated with Monte Carlo simulations. Interestingly, we do not need the response variable to approximate the value.\n\\[m = \\max_{\\text{ \\{all models and vars\\} }} |t|.\\]\nAnother and even more conservative choice for \\(m\\) is the Scheffe constant.\n\\[m^{scheffe} = \\sqrt{rank(X) \\times F(rank(X),  n-p)},\\]\nwhere \\(F(\\dot)\\) denotes the \\(95\\)th percentile of the \\(F\\) distribution with the respective degrees of freedom.\nUnfortunately, you guessed correctly that this method does not scale well. In some sense, it is a “brute force” method that scans through all possible model combinations and all variables within each model and picks the most conservative value. The authors recommend this procedure for datasets with roughly \\(p&lt;20\\). This rules out many practical applications where machine learning is most useful.\nSoftware Package: PoSI\n\n\nEPoSI (Exact PoSI)\nOk, the name of this approach is not super original. The “E” here stands for “exact.” Unlike its cousin, this approach is valid only in the selected submodel. Because we cover fewer scenarios, the intervals will generally be narrower than the PoSI ones. EPoSI produces valid finite sample (as opposed to asymptotic) confidence intervals and \\(p\\)-values. Like all methods described here, the math behind this is extremely technical. So, I will give you only a high-level description of how this works.\nThe idea is first to get the conditional distribution of \\(\\beta\\) given the selected model. A bit magically, it turns out it is a truncated normal distribution. Really, who would have guessed this? Do you even remember truncated probability distributions (hint: they are just like regular PDFs but bounded from at least one side. This requires further scaling so that the density area sums to 1.)?\nTo dig one layer deeper, the authors show that the selection of Lasso predictors can be recast as a “polyhedral region” of the form:\n\\[ AY\\leq b. \\]\nIn English, for fixed \\(X\\) and \\(\\lambda\\), the set of alternative outcome values \\(Y^*\\) which yields the same set of selected predictors, can be expressed by the simple inequality above. In it, \\(A\\) and \\(b\\) depend do not depend on \\(Y\\). Under this new result, the distribution of \\(\\hat{\\beta}_M^{\\text{EPoSI}}\\) is now well-understood and tractable, thus enabling valid hypothesis testing.\nThen, we can use the conditional distribution function to construct a whimsical test statistic that is uniformly distributed on the \\([0,1]\\) interval. And we can finally build confidence intervals based on that statistic.\nSelective inference is currently among the hottest topic in all of statistics. There have been a myriad of extensions and improvements on the original paper. Still, this literature is painstakingly technical. What is the polyhedral selection property or a union of polytopes?\nSoftware Package: selectiveInference."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#an-example",
    "href": "blog/hypothesis-testing-linear-ml.html#an-example",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Refer to the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\nUnlike in Monte Carlo simulations, I do not know the ground truth here, so this exercise is not informative about which approaches work well and which do not. Rather, it just serves as an illustrative example.\nHere is a table displaying the number of statistically significant variables with \\(p &lt; .05\\) for various inference methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n\\(\\#\\) vars \\(p &lt; .05\\)\n7\n5\n2\n3\n2\n2\n\n\n\n\nAs expected, the naive method results in the smallest \\(p\\)-values and hence the highest number of significant predictors – seven. Data Split knocks down two of those seven variables, resulting in five significant ones. The rest are more conservative, leaving only two or three features with \\(p &lt; .05\\).\nBelow is the table with \\(p\\)-values for all variables and each method.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: \\(p\\)-values\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.01\n1.00\n\n\nage\n0.00\n0.04\n1.00\n0.01\n1.00\n1.00\n\n\nsibsp\n0.02\n0.03\n1.00\n0.21\n1.00\n1.00\n\n\nparch\n0.32\n0.64\n1.00\n1.00\n1.00\n1.00\n\n\nfare\n0.20\n0.21\n1.00\n1.00\n1.00\n1.00\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n\n\nembarkedS\n0.00\n0.01\n1.00\n0.06\n-\n1.00\n\n\ncabinA\n0.39\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinB\n0.35\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinD\n0.05\n0.27\n1.00\n0.44\n1.00\n-\n\n\ncabinE\n0.00\n0.64\n1.00\n0.06\n1.00\n1.00\n\n\ncabinF\n0.02\n0.52\n1.00\n0.21\n1.00\n1.00\n\n\nembarkedC\n-\n-\n1.00\n0.11\n1.00\n1.00\n\n\nembarkedQ\n-\n-\n1.00\n1.00\n1.00\n0.00\n\n\ncabinC\n-\n-\n1.00\n1.00\n1.00\n-\n\n\n\n\nYou can find the code for this exercise in this GitHub repo."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#bottom-line",
    "href": "blog/hypothesis-testing-linear-ml.html#bottom-line",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods mine through datasets to find strong correlations between the response and the features. Quantifying this strength is an open and challenging problem.\nThe naive approach to hypothesis testing is usually invalid.\nThere are two main approaches that work – inference on the full model or on the selected model. The latter poses more technical challenges than the former.\nIf we are interested in the full model, the Multi Split approach is general enough to cover a wide range of models and settings.\nIf we believe you have to focus on the selected model, EPoSI is the state-of-the-art.\nSimulation exercises usually show no clear winner, as none of the methods consistently outperforms the rest."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#where-to-learn-more",
    "href": "blog/hypothesis-testing-linear-ml.html#where-to-learn-more",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nTaylor and Tibshirani (2015) give a non-technical introduction to the problem space along with a description of the POSI method – a great read but focused on a single approach. Other studies both provide a relatively accessible overview of the various methods for statistical inference on the full model. For technical readers, Zhang et al. (2022) provide an excellent up-to-date review of the literature, which I used extensively."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#references",
    "href": "blog/hypothesis-testing-linear-ml.html#references",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "References",
    "text": "References\nBerk, R., Brown, L., Buja, A., Zhang, K., & Zhao, L. (2013). Valid post-selection inference. The Annals of Statistics, 802-837.\nBühlmann, P. (2013). Statistical significance in high-dimensional linear models. Bernoulli, 19(4), 1212-1242.\nDezeure, R., Bühlmann, P., Meier, L., & Meinshausen, N. (2015). High-dimensional inference: confidence intervals, p-values and R-software hdi. Statistical Science, 533-558.\nJavanmard, A., & Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15(1), 2869-2909.\nLee, J. D., Sun, D. L., Sun, Y., & Taylor, J. E. (2016). Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3), 907-927.\nLeeb, H., & Pötscher, B. M. (2005). Model selection and inference: Facts and fiction. Econometric Theory, 21(1), 21-59.\nLeeb, H., Pötscher, B. M., & Ewald, K. (2015). On various confidence intervals post-model-selection. Statistical Science, 30(2), 216-227.\nMeinshausen, N., Meier, L., & Bühlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\nTaylor, J., & Tibshirani, R. J. (2015). Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112(25), 7629-7634.\nVan de Geer, S., Bühlmann, P., Ritov, Y. A., & Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. The Annals of Statistics, 42(3), 1166-1202.\nWasserman, L., & Roeder, K. (2009). High dimensional variable selection. Annals of Statistics, 37(5A), 2178.\nZhang, D., Khalili, A., & Asgharian, M. (2022). Post-model-selection inference in linear regression models: An integrated review. Statistics Surveys, 16, 86-136.\nZhang, C. H., & Zhang, S. S. (2014). Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 217-242.\nZhao, S., Witten, D., & Shojaie, A. (2021). In defense of the indefensible: A very naive approach to high-dimensional inference. Statistical Science, 36(4), 562-577."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#background",
    "href": "blog/flavors-multiple-testing.html#background",
    "title": "Multiple Testing: Methods Overview",
    "section": "Background",
    "text": "Background\nThe abundance of data around us is a major factor making the data science field so attractive. It enables all kinds of impactful, interesting, or fun analyses. I admit this is what got me into statistics and causal inference in the first place. However, this luxury has downsides – it might require deeper methodological knowledge and attention.\nIf we are interested in measuring statistical significance, the standard frequentist framework relies on testing a single hypothesis on any given dataset. Once we start using the same data multiple times – i.e., testing several hypotheses at once – as we often do, we might have to make some additional adjustments.\nIn this article, I will walk you through the most popular multiple hypotheses (MH) testing corrections. Common examples of when these come to play include estimating the impact of several variables on a single outcome or estimating the effect of an intervention on several subgroups of users to mine for heterogeneous treatment effects. Both scenarios are ubiquitous in most data analysis projects.\nTo illustrate the problem, imagine we are testing seven hypotheses at the \\(5\\%\\) significance level. Then, the probability that we incorrectly reject at least one of these hypotheses is:\n\\[ 1 - (1-.05)^7 = .30. \\]\nThat is, roughly one in three times, we will reach a wrong conclusion.\nVaguely speaking, MH adjustments can be viewed as inflating the calculated p-values and hence decreasing the probability that we reject any given hypothesis. In other words, we are accounting for using the data multiple times by making the definition of statistical significance more stringent."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#notation",
    "href": "blog/flavors-multiple-testing.html#notation",
    "title": "Multiple Testing: Methods Overview",
    "section": "Notation",
    "text": "Notation\nWe are interested in testing a bunch of m null hypotheses about a population parameter \\(\\beta\\).\nAs a running example, you can think of \\(\\beta\\) as the causal impact of a new product feature on user engagement and \\(m\\) as indexing some geographical regions such as cities. We are interested in whether the new feature is more impactful in some cities than others. We will denote these hypotheses with \\(H_1, H_2, \\dots, H_m\\) and refer to their associated p-values with \\(p_1, p_2, \\dots, p_m\\).\nWe use \\(\\alpha\\) to denote the probability of a Type 1 error – rejecting a true null (i.e., a false positive). In technical jargon, we refer to \\(\\alpha\\) as test size. We often choose \\(\\alpha=.05\\), meaning that we are allowing a 5% chance that we will make such an error. This corresponds to the 95% confidence intervals that we often see.\nNext, statistical power is the probability of correctly rejecting a false null hypothesis (i.e., a true positive). This is a desirable property – the higher it is, the better. While this terminology does not describe the entire hypothesis testing framework, it does cover what is necessary to continue reading the article."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#a-closer-look",
    "href": "blog/flavors-multiple-testing.html#a-closer-look",
    "title": "Multiple Testing: Methods Overview",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhen is MH Control NOT Necessary\nMultiple hypothesis adjustments are not required when:\n\nwe are interested in a purely predictive model, such as in machine learning settings,\nwe have an independent dataset for each correlated hypothesis,\nthe dependent variables we are testing are not correlated. An example is testing the impact of a new feature or intervention on a set of outcomes unrelated to each other – e.g., health and education variables. In such cases, it is common to adjust for MH separately within the health- and education-related hypotheses.\n\nBefore we dive into the methods, let’s establish some basic terminology and notation.\n\n\nFWER vs FDR\nThere are two distinct types of targets that data scientists are after. The first one was introduced in the 1950s by the famous statistician John Tukey and is called Familywise Error Rate (FWER). FWER is the probability of making at least one type 1 error:\n\\[ FWER = \\mathbb{P} (\\text{At Least 1 Type 1 Error}). \\]\nIn the example from the introduction, \\(FWER = 0.3\\).\nControlling the FWER is good, but sometimes it is not great. It does what it is supposed to do – limit the number of false positives, but it is often too conservative. In practice, too few variables remain significant after such FWER adjustments, especially when testing many (as opposed to just a few) hypotheses.\nTo correct this, in the 1990s, Yoav Benjamini and Yosef Hochberg, conceptualized controlling the so-called False Discovery Rate (FDR). FDR is the expected proportion of false positives:\n\\[ FDR = \\mathbb{E} \\left[ \\frac{\\text{\\# False Positives}}{\\text{\\# False Positives + \\# True Positives} } \\right]. \\]\nTake a minute to notice the significant difference between these two approaches. FWER controls the probability of having at least one false positive at \\(\\alpha\\). At the same time, FDR is okay with having several false positives as long as they are (on average) less than \\(\\alpha\\) of all hypotheses.\nFWER is more conservative than FDR, resulting in fewer significant variables. In our example, applying an FWER adjustment as opposed to an FDR one will lead to fewer statistically significant differences in the impact of the new feature across various cities.\nLet’s go over the most popular ways to control the FWER.\n\n\nFWER Control\n\nBonferroni’s Procedure\nYou might have heard of this one. The procedure is extremely simple – it goes like this:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nreject \\(H_i\\) if \\(p_i \\times m  \\leq \\alpha\\) for all \\(i\\).\n\n\n\nIn words, we multiply all \\(p\\)-values by m and reject the hypotheses with adjusted \\(p\\)-values that are smaller than or equal to \\(\\alpha\\).\nThe Bonferroni adjustment is often considered to control the FWER, but it actually controls an even more conservative target – the per-family error rate (PFER). PFER is the sum of probabilities of Type 1 error for all the hypotheses.\nYou correctly guessed that PFER is considered too strict. Although it is common in practice, there are better alternatives. While the Bonferroni correction is simple and elegant, it is always dominated by the Holm-Bonferroni procedure, so there is really no reason ever to use it.\n\n\nHolm & Bonferroni’s Procedure\nThis adjustment offers greater statistical power than the Bonferroni method regardless of the test size without significantly compromising on simplicity. In algorithmic language:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(p\\)-values in ascending order – \\(p_1, p_2, \\dots, p_m.\\)\nReject the null hypothesis \\(H_1,\\dots H_{k-1}\\), where \\(k\\) is the smallest index for which \\(p_i \\times (m+1-k) \\leq \\alpha\\).\n\n\n\nMuch like the Bonferroni method with a multiplication factor of (\\(m+1-k\\)) instead of \\(m\\).\nThese two procedures control the FWER by assuming a kind of a ‘worst-case’ scenario in which the \\(p\\)-values are close to being independent of each other. To see why this is the case, imagine the extreme opposite scenario in which there is perfect positive dependence. Then, there is effectively one test statistic, and FWER adjustment is unnecessary.\nIn practice, however, the \\(p\\)-values are often correlated, and exploiting this dependence can result in even more powerful FWER corrections. This can be achieved with various resampling methods, such as the bootstrap or permutation procedures.\n\n\nWestfall & Young‘s Procedure\nThe Westfall & Young method, developed in the 1990s, was the first resampling procedure controlling the FWER. It uses the bootstrap to account for the dependence structure among the \\(p\\)-values and improves on Holm-Bonferroni’s method.\nIt goes roughly like this:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(m\\) \\(p\\)-values in ascending order – \\(p_1, \\dots, p_M\\).\nBegin with a \\(COUNTER_i = 0\\) for each hypothesis \\(i\\).\nTake a bootstrap sample and compute the \\(m\\) \\(p\\)-values (\\(p^*_1, \\dots, p^*_m\\))\nDefine the successive minima by starting with the largest \\(p\\)-value (\\(\\tilde{p}_M=p^*_M\\)) and for each subsequent hypothesis \\(i\\) taking the minimum between \\(\\tilde{p}_{i+1}\\) and \\(p^*_i\\). This gives a list of \\(p\\)-values \\(\\tilde{p}_1, \\dots, \\tilde{p}_m\\).\nIf \\(\\tilde{p}_i \\leq p_i\\) add 1 unit to \\(COUNTER_i\\) for each \\(i\\).\nRepeat steps 3-5 above \\(B\\) times and compute the fraction \\(\\hat{p}_i = COUNT_i/B\\). This gives a list of \\(p\\)-values \\(\\hat{p}_1, \\dots, \\hat{p}_m\\).\nEnforce monotonicity by starting with \\(\\hat{p}_1\\) and for each subsequent hypothesis \\(i\\) taking the maximum between \\(\\hat{p}_{i-1}\\) and \\(\\hat{p}_i\\). This final vector presents the FWER-adjusted \\(p\\)-values.\nReject all null hypotheses \\(i\\) for which \\(\\hat{p}\\leq \\alpha\\).\n\n\n\nYes, this is a complicated procedure. But it does offer substantial improvements in power, especially if the hypotheses are positively correlated.\nImportantly, Westfall and Young’s method rests on a critical assumption called “subset pivotality.” Roughly speaking, this condition requires that the joint distribution of any subset of test statistics does not depend on whether the remaining hypotheses are true or not.\nSoftware Package: multtest.\n\n\nRomano & Wolf‘s Procedure(s)\nThis procedure improves on Westfall and Young’s method by not requiring a pivotality assumption. Here I am describing the method from Romano and Wolf’s 2005 Econometrica paper, although the authors have developed a few closely related FWER adjustments. This is the rough algorithm for it:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the test statistics in descending order, \\(t_1 \\geq t_2 \\geq, \\dots, t_m\\).\nTake B bootstrap samples, and for each hypothesis i compute the empirical distribution function of the successive maximum test statistic random variable. That is, the random variable of the maximum test statistic between \\(t_i, t_{i+1}, \\dots, t_m\\). Call this distribution \\(c(i)\\).\nReject \\(H_i\\) if \\(t_i\\) is greater than the 1-quantile of \\(c(1)\\).\nDenote by h the number of rejected hypotheses. If \\(h=0\\) stop. Otherwise, for \\(H_{h+1},\\dots H_s\\):\nReject \\(H_i\\) if \\(t_i &gt; c(h+1)\\)\nIterate until no further hypotheses are rejected.\n\n\n\nAnd you thought the Westfall and Young correction was tricky. The good news is that there are software packages for all these adjustments, so you won’t have to program them yourself. Arguably, this one of the best ways to control FWER; does not rest on pivotality assumptions. However, it can be difficult to explain to non-technical audiences; difficult to explain to anyone, really.\nSoftware Package: hdm.\nThis wraps up the descriptions of the methods controlling the FWER.\n\n\n\nFDR Control\nAs a reminder, FDR procedures have greater power at the cost of increased rates of type 1 errors compared to FWER adjustments. This is the dominant framework in the modern literature on MH adjustment.\n\nBenjamini & Hochberg‘s Procedure\nThe Benjamini-Hochberg (BH) method is simple:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(p\\)-values in ascending order – \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m}{k} \\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\n\n\nBH is valid when the tests are independent and in some, but not all, dependence scenarios. In general, FDR control is tricky with arbitrary dependence.\nThis adjustment has an interesting geometric interpretation:\n\nplot \\(p\\) versus \\(k\\),\ndraw a line through the origin with a slope equal to \\(frac{\\alpha}{m}\\),\nreject all hypotheses associated with the \\(p\\)-values on the left up to the last point below this line.\n\n\n\nBenjamini & Yekutieli‘s Procedure\nBenjamini and Yekutieli (BY) developed a refinement of the BH procedure, which adds a magical constant \\(c(m)\\):\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(p\\)-values in ascending order – \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m.c(m)}{k}\\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\n\n\nThe obvious question is in choosing the optimal value for \\(c(m)\\). You can view BH as a special case of BY with \\(c(m)=1\\). The authors show that the following choice works under any arbitrary dependence assumption:\n\\[ c(m) = \\sum_{i=1}^m \\frac{1}{i} \\approx log(m) + 0.57721 + \\frac{1}{2m}. \\]\nFascinating. For most applications, this simple procedure provides a credible way of taming the false positives rate. There are improvements to this method, but it feels like we are entering a world in which we get theoretical gains with only limited practical implications.\nThe main way to improve on BH/BY is to know (or estimate) the share of false hypotheses among the ones we are testing. This, however, goes beyond the scope of the article.\n\n\nKnockoffs\nWhat if we would like to combine variable selection and control the FDR? BH and BY do not really tell us how to do that. Enter knockoffs. The knockoffs approach is the kid on the block, and it does more than just FDR control.\nThe idea behind the knockoffs is novel and unrelated to any of the methods I discussed above. Knockoffs are copies of the variables with specific properties. In particular, for each feature/covariate variable \\(X\\), we create a knockoff copy \\(\\tilde{X}\\) such that the correlation structure between any two knockoff variables is the same as between their original counterparts. These knockoffs are constructed without looking at the outcome variable; hence, they are unrelated to it by design. Consequently, we can gauge whether each variable is statistically significant by comparing its test statistic to that of the one for its knockoff copy.\nKnockoffs are among the most active fields of research in statistics, with new papers coming out daily (ok, maybe weekly). These methods come in two flavors.\nThe fixed-design knockoff filter controls FDR for low-dimensional linear models regardless of the dependency structure of the features. This is great, but it does not work in high dimensions where variable selection is usually most needed.\nThe model-X knockoff filter solves this issue but requires exact knowledge of the joint distribution of the features. This is often infeasible in practice; even estimating that distribution in high dimensions can be really challenging as the curse of dimensionality lays its hammer on the data scientist.\nThe knockoff idea presents a big step forward because it solves two statistical problems at the same time – selecting among a wide set of predictors and controlling the FDR. I am really excited to see where the current research will take us next.\nSoftware Package: knockoff.\n\n\nOther Modern Methods\nThere is certainly no shortage of statistical methods in this field. Each week, there is at least one new paper dealing with some aspects of FDR or FWER control. Knockoffs, in particular, have gotten a lot of attention recently, with researchers extending the idea to all kinds of settings and models.\nI will briefly mention some of the newer methods that have caught my eye in recent months. Many of these control the FDR asymptotically, and some are valid under general dependency structures among the test statistics.\nOne recent variation of the knockoffs idea uses Gaussian Mirrors; while others rely on subsampling approaches – Data Splitting (based on estimating two independent regression coefficients after splitting the dataset in half) and Data Aggregation. Clever ideas to improve power include using covariate information to optimally weigh hypotheses in the BH framework or incorporating them into FDR regressions.\nTime will show whether any of these newcomers will come to dominate in practice."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#an-example",
    "href": "blog/flavors-multiple-testing.html#an-example",
    "title": "Multiple Testing: Methods Overview",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Check the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\n\n\n\nNumber of Statistically Significant Variables\n\n\nHere is a table of the \\(p\\)-values for each feature and various MH adjustments.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaw\nBonferroni\nHolm\nRom.-Wolf\nBenj.-Hoch.\nBenj.-Yek.\n\n\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nage\n0.00\n0.02\n0.01\n0.01\n0.00\n0.01\n\n\nsibsp\n0.02\n0.34\n0.18\n0.16\n0.04\n0.14\n\n\nparch\n0.32\n1.00\n1.00\n0.77\n0.40\n1.00\n\n\nfare\n0.22\n1.00\n1.00\n0.66\n0.30\n0.98\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nembarkedC\n0.01\n0.23\n0.15\n0.14\n0.04\n0.13\n\n\nembarkedQ\n0.05\n0.76\n0.35\n0.29\n0.09\n0.28\n\n\ncabinA\n0.39\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinB\n0.37\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinC\n0.92\n1.00\n1.00\n0.92\n0.92\n1.00\n\n\ncabinD\n0.06\n0.89\n0.36\n0.29\n0.09\n0.30\n\n\ncabinE\n0.01\n0.07\n0.05\n0.05\n0.01\n0.05\n\n\ncabinF\n0.02\n0.31\n0.18\n0.16\n0.04\n0.14\n\n\n\nEight variables were statistically significant without any MH adjustment. As expected, the FWER adjustments lead to fewer significant variables than the FDR ones. Interestingly, there is a noticeable difference between the two FDR methods – BH and BY, with the latter being more conservative.\nYou can find the code for this analysis in this GitHub repository."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#bottom-line",
    "href": "blog/flavors-multiple-testing.html#bottom-line",
    "title": "Multiple Testing: Methods Overview",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMultiple testing corrections are likely necessary when you are using a single dataset multiple times (i.e., testing multiple hypotheses).\nTwo major frameworks exist – FWER and FDR control. The former is often considered too conservative, while the latter is the dominant way researchers and practitioners think about correcting for MH testing.\nIn most settings, the Benjamini-Yekuiteli approach offers a great balance between statistical power and technical simplicity.\nKnockoffs are a novel and exciting approach to FDR control."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#where-to-learn-more",
    "href": "blog/flavors-multiple-testing.html#where-to-learn-more",
    "title": "Multiple Testing: Methods Overview",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great resource on the classic approaches to tackling MH issues (both for FDR and FWER control), but it lacks material on modern methodologies. Emmanuel Candès’ website features an accessible introduction to the world of knockoffs. Clarke et al. (2020) strip down many technicalities and provide accessible descriptions of both Westfall and Young’s, as well as Romano and Wolf’s methods. Korthauer et al. (2019) compare some of the more recent approaches to controlling the FDR, which are beyond the scope of this blog post."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#references",
    "href": "blog/flavors-multiple-testing.html#references",
    "title": "Multiple Testing: Methods Overview",
    "section": "References",
    "text": "References\nBarber, R. F., & Candès, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics, 43(5), 2055-2085.\nBenjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of Statistics, 1165-1188.\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300.\nCandes, E., Fan, Y., Janson, L., & Lv, J. (2018). Panning for gold:’model‐X’knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3), 551-577.\nClarke, D., Romano, J. P., & Wolf, M. (2020). The Romano–Wolf multiple-hypothesis correction in Stata. The Stata Journal, 20(4), 812-843.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2022). False discovery rate control via data splitting. Journal of the American Statistical Association, 1-38.\nKorthauer, K., Kimes, P. K., Duvallet, C., Reyes, A., Subramanian, A., Teng, M., … & Hicks, S. C. (2019). A practical guide to methods controlling false discoveries in computational biology. Genome biology, 20(1), 1-21.\nRomano, J. P., & Wolf, M. (2005). Stepwise multiple testing as formalized data snooping. Econometrica, 73(4), 1237-1282.\nRomano, J. P., & Wolf, M. (2005). Exact and approximate stepdown methods for multiple hypothesis testing. Journal of the American Statistical Association, 100(469), 94-108.\nWestfall, P. H., & Young, S. S. (1993). Resampling-based multiple testing: Examples and methods for p-value adjustment (Vol. 279). John Wiley & Sons.\nXing, X., Zhao, Z., & Liu, J. S. (2021). Controlling false discovery rate using gaussian mirrors. Journal of the American Statistical Association, 1-20."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#background",
    "href": "blog/lasso-heterogeneous-effects.html#background",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Background",
    "text": "Background\nLasso is one of my favorite machine learning algorithms. It is so simple, elegant, and powerful. My feelings aside, Lasso indeed has a lot to offer. While, admittedly, it is outperformed by more complex, black-box type methods (e.g., boosting or neural networks), it has several advantages: interpretability, computational efficiency, and flexibility. Even when it comes to accuracy, theory tells us that under appropriate assumptions, Lasso can uncover the true submodel and we can even derive bounds on its prediction loss.\nIn this article I will briefly describe two ways researchers use Lasso to detect heterogeneous treatment effects. The underlying idea is to throw a bunch of covariates in the model and let the \\(L1\\) regularization do the difficult job of identifying which ones are important for treatment effect heterogeneity."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#notation",
    "href": "blog/lasso-heterogeneous-effects.html#notation",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Notation",
    "text": "Notation\nAs always, let’s start with some notation. Let \\(T\\) denote a binary treatment indicator, \\(Y(0), Y(1)\\) be the potential outcomes under each treatment state (\\(Y\\) is the observed one), and \\(X\\) be a covariate vector. Lastly, \\(p\\) is the share of units in the treatment group, \\(p=\\frac{1}{N}\\sum T\\), where \\(N\\) is the sample size.\nThe Lasso coefficient vector is commonly expressed as the solution to the following problem:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-X\\beta||_2^2 + \\lambda||\\beta||_1 \\},\\]\nwhere \\(\\lambda\\) is the regularization parameter governing the variance-bias trade-off.\nWe are interested in the heterogeneous treatment effect given \\(X (HTE(X))\\):\n\\[HTE(X) = E[Y(1)-Y(0)|X=x].\\]\nThat is, \\(HTE(X)\\) is the average treatment effect for units with covariate levels \\(X=x\\).\nMore precisely, our goal is identifying which variables in \\(X\\) divide the population of interest such that there are meaningful treatment effect differences across these groups. For instance, in the case of estimating the impact of school quality on test scores, \\(X\\) might be students’ gender (e.g., girls benefit more than boys), or in the context of online A/B testing, \\(X\\) might denote previous product engagement (e.g., tenured users benefit more from a new feature than inexperienced users).\nBroadly speaking, there are two main approaches to using Lasso to solve this problem — (i) a linear model with interactions between \\(T\\) and \\(X\\), and (ii) directly regressing the imputed unit-level treatment effects on \\(X\\)."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#a-closer-look",
    "href": "blog/lasso-heterogeneous-effects.html#a-closer-look",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLasso with Treatment Variable Interactions\nIn a low-dimensional world where regularization is not necessary and researchers are interested in \\(HTE\\)s, they often use a linear model in which the treatment variable is interacted with the covariates. Statistically significant interaction coefficients identify the \\(X\\) variables for which the treatment has a differential impact.\nMathematically, when all variables are properly interacted, this is analogous to splitting the sample into subgroups based on \\(X\\) and running OLS on each group separately. This is feasible and convenient when \\(X\\) is binary or categorical, but not when it is continuous. The advantage of this approach is that linear regression produces \\(p\\)-values which can be (mis)used to determine statistical significance of these interaction variables.\nThe OLS model is then:\n\\[Y = \\beta_1 T + \\beta_2 X + \\beta_3 X \\times T + \\epsilon,\\]\nwhere \\(\\epsilon\\) is the error term. The attention here falls on the coefficient vector \\(\\beta_3\\) which identifies whether the treatment has had a differential impact on units with a particular characteristic \\(X\\).\nIn high-dimensional settings with a wide \\(X\\), this is not feasible. Instead, we can use an algorithm to pick out the variables in \\(X\\) that are important for treatment effect heterogeneity.\nImai and Ratkovic (2013) show how to adapt the Lasso to this setting. It turns out we should not simply throw this model into the Lasso loss function. Can you guess why? Some variables might be predictive of the baseline outcome while others only of the treatment effect heterogeneity. The trick is to have two separate Lasso constraints — \\(\\lambda_1\\) and \\(\\lambda_2\\).\nSo, the loss function looks something like this:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-f(X,T)\\beta_1 - X\\times T \\beta_2 ||_2^2 + \\lambda_1||\\beta_1||_1 + \\lambda_2||\\beta_2||_1 \\}.\\]\nA simplified version of the algorithm is as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nGenerate interaction variables, \\(\\tilde{X}(T)\\).\nRun Lasso of \\(Y\\) on \\(T\\), \\(X\\) and \\(\\tilde{X}(T)\\).\nIdentify statistically significant variables determining treatment effect heterogeneity.\n\n\n\nTwo notes. First, this requires some assurance that we are not overfitting, so that some form of sample splitting or cross validation is necessary. On top of this, Athey and Imbens (2017) suggest comparing these results with post-lasso OLS estimates to further guard against overfitting. Second, multiple testing is an issue as is the case with ML algorithms more generally. (You can check my earlier post on multiple hypothesis testing in linear machine learning models.) Options to take care of this include sample splitting and bootstrap, among others.\nSoftware Package: FindIt.\n\n\nLasso with Knockoffs\nAn alternative approach directly regresses the unit-level treatment effects on \\(X\\). To get there, we first model the outcome function and impute the missing potential outcome for each unit. See my previous post on using Machine Learning tools for causal inference for more information on how we might do that.\nThis approach was developed by Xie et al. (2018). They recognized the multiple hypothesis issue and suggested using knockoffs to control the False Discovery Rate. This is still not completely kosher, as it does not account for the fact that the outcome variable is estimated in an earlier step. Oh well. My guess and hope are that this is more of a theoretical concern, and empirically the inference we get is still “correct.”\nHere is a simplified version of their algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nTransform the outcome variable \\(\\tilde{Y}=Y\\times\\frac{(T-p)}{p(1-p)}\\).\nCalculate unit-level treatment effects, \\(\\hat{Y}(1)-\\hat{Y}(0)\\).\nGenerate the difference \\(Y^*=\\tilde{Y}-\\frac{1}{N}\\sum \\hat{Y}(1)-\\hat{Y}(0)\\).\nRun Lasso of \\(Y^*\\) on \\(X\\) and \\(X^*\\) (the knockoff counterparts of \\(X\\)).\nFollow the knockoff method to obtain the set of significant variables.\n\n\n\nRemember that \\(\\tilde{Y}\\) has the special property that \\(E[\\tilde{Y} \\mid X=x]=HTE(X)\\) under the unconfoundedness assumption.\nInterestingly, in the special case when \\(p=1/2\\) the algebra reduces further which provides computation scaling advantages. Tian et al. (2014) showed this result first."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#an-example",
    "href": "blog/lasso-heterogeneous-effects.html#an-example",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate the latter method. As such, it is a mere depiction of the approach, and its results should certainly not be taken seriously.\nThe outcome variable was survived, and the treatment variable was male. As is well known, women were much more likely to survive than men (\\(74\\%\\) vs \\(19\\%\\) in this sample). So, I analyzed whether the gender difference in survival was impacted by other factors. I included the following covariates – pclass (ticket class), age, sibsp (number of siblings aboard), parch (number of parents aboard), fare, embarked (port of Embarkation), and cabin. Some of these were categorical in which case I converted them to a bunch of binary variables.\nThe knockoff filter identified a single variable as having a significant impact on the treatment effect – pclass. For the more affluent passengers (i.e., those in the higher ticket classes), this gender difference in survival was much smaller.\nYou can find the code in this GitHub repo."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#bottom-line",
    "href": "blog/lasso-heterogeneous-effects.html#bottom-line",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe core idea behind using Lasso in HTE estimation is to leverage \\(L_1\\) regularization to select which covariates explain differences in treatment responses.\nThere are two main ways researchers use Lasso to estimate HTEs. Use a linear model with all covariates interacted with the treatment indicator, and apply Lasso with two separate regularization constraints. Directly regress unit-level treatment effects on the covariates.\nI am not aware of simulation studies comparing both approaches.\nWhile Lasso is among the simplest and most popular machine learning algorithms, more suitable methods may exist for estimating HTEs."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#where-to-learn-more",
    "href": "blog/lasso-heterogeneous-effects.html#where-to-learn-more",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nHu (2022) is an excellent summary of the several ways researchers use Machine Learning to uncover heterogeneity in treatment effect estimation."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#references",
    "href": "blog/lasso-heterogeneous-effects.html#references",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "References",
    "text": "References\nBarber, R. F., & Candès, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics: 2055-2085.\nBarber, R. F., & Candès, E. J. (2019). A knockoff filter for high-dimensional selective inference. The Annals of Statistics, 47(5), 2504-2537.\nChatterjee, A., & Lahiri, S. N. (2011). Bootstrapping lasso estimators. Journal of the American Statistical Association, 106(494), 608-625.\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. The Annals of Applied Statistics (2013): 443-470.\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\nMeinshausen, N., Meier, L., & Bühlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nXie, Y., Chen, N., & Shi, X. (2018). False discovery rate controlled heterogeneous treatment effect detection for online controlled experiments. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 876-885)."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#background",
    "href": "blog/overlapping-conf-intervals.html#background",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Background",
    "text": "Background\nThis is a mistake I’ve made myself—more times than I’d like to admit. Even seasoned professors and expert data scientists sometimes fall into the same trap.\nIt typically begins with a bar graph showing two sample means side by side, each accompanied by error bars representing 95% confidence intervals. The side-by-side placement suggests a comparison is imminent. Naturally, we check whether the confidence intervals overlap. If they don’t, we may quickly (and incorrectly) conclude that the difference between the means is statistically significant—and therefore meaningful.\nThis intuitive but flawed approach to evaluating significance is surprisingly common. Here’s why it doesn’t hold up."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#diving-deeper",
    "href": "blog/overlapping-conf-intervals.html#diving-deeper",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nThe Basics of Confidence Intervals\nLet’s use a simplified example adapted from Schenker and Gentleman (2001). Suppose we are comparing two quantities—\\(Y_1\\) and \\(Y_2\\)—such as average user engagement on Android vs. iOS or sales in two different regions. We assume ideal conditions: large, random samples; well-behaved distributions; and reliable estimators.\nWe’re testing the null hypothesis:\n\\[H_0: Y_1 = Y_2.\\]\nWe denote our sample estimates as \\(\\hat{Y}_1\\) and \\(\\hat{Y}_2\\), with corresponding standard errors \\(\\hat{SE}(Y_1)\\) and \\(\\hat{SE}(Y_2)\\). The \\(95\\%\\) confidence intervals for these estimates are:\n\\[ \\hat{Y_1} \\pm 1.96 \\times \\hat{SE}(Y_1) \\]\nand \\[ \\hat{Y_2} \\pm 1.96 \\times \\hat{SE}(Y_2). \\]\nCrucially, we can also construct a confidence interval for the difference:\n\\[ (\\hat{Y_1} - \\hat{Y_2}) \\pm 1.96 \\times \\sqrt{ \\hat{SE}(Y_1)^2+ \\hat{SE}(Y_2)^2}. \\]\nThis is the interval we should be analyzing when testing whether \\(Y_1\\) and \\(Y_2\\) differ significantly. Gelman and Stern (2006) make the same point from a slightly different angle.\n\n\nTwo Approaches, One Mistake\nThe Naïve Approach:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nLook at whether the confidence intervals for \\(Y_1\\) and \\(Y_2\\) overlap.\nReject \\(H_0\\) if they do not overlap; otherwise, do not reject.\n\n\n\nThe Correct Approach:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nCompute the confidence interval for the difference \\(Y_1 - Y_2\\).\nReject \\(H_0\\) if this interval does not contain 0; otherwise, do not reject.\n\n\n\n\n\nWhy the Naïve Method Fails\nTo understand the error, consider the following: under the naïve method, we’re implicitly relying on the interval:\n\\[ (\\hat{Y_1} - \\hat{Y_2}) \\pm 1.96 \\times (\\hat{SE}(Y_1) + \\hat{SE}(Y_2)). \\]\nCompare this to the statistically correct confidence interval for the difference:\n\\[ \\frac{\\hat{SE}(Y_1)+ \\hat{SE}(Y_2)}{\\sqrt{\\hat{SE}(Y_1)^2 + \\hat{SE}(Y_2)^2}} \\]\nThe ratio of the widths of these intervals is:\n\\[ \\frac{\\hat{SE}(Y_1)+ \\hat{SE}(Y_2)}{\\sqrt{\\hat{SE}(Y_1)^2 + \\hat{SE}(Y_2)^2}} \\]\nThis ratio is always greater than 1, meaning the naïve method uses a wider interval. It is more conservative when the null hypothesis is true (i.e., less likely to reject it), and less conservative when the null is false (i.e., more prone to false positives).\nThe discrepancy is largest when the standard errors are similar, and smallest when one standard error dominates."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#an-example",
    "href": "blog/overlapping-conf-intervals.html#an-example",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "An Example",
    "text": "An Example\nSchenker and Gentleman (2001) offer a helpful illustration using proportions:\n\n\\(\\hat{Y}_1 = 0.56\\), \\(\\hat{Y}_2 = 0.44\\)\n\\(\\hat{SE}(Y_1) = \\hat{SE}(Y_2) = 0.0351\\)\n\nThe individual \\(95\\%\\) confidence intervals are:\n\nFor \\(Y_1\\): \\([0.49, 0.63]\\)\nFor \\(Y_2\\): \\([0.37, 0.51]\\)\n\nThese intervals do overlap. Under the naïve method, we would not reject the null hypothesis.\nHowever, the confidence interval for the difference is:\n\\[[0.02,0.22]\\]\nThis interval does not contain \\(0\\), meaning we would reject the null hypothesis using the correct method. The difference is statistically significant."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#bottom-line",
    "href": "blog/overlapping-conf-intervals.html#bottom-line",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nVisual overlap of confidence intervals is an intuitive—but unreliable—method for assessing statistical significance.\nThis rule of thumb often misleads, particularly when standard errors are similar.\nAlways test for significance by examining the confidence interval for the difference between two estimates."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#where-to-learn-more",
    "href": "blog/overlapping-conf-intervals.html#where-to-learn-more",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper exploration of this topic, including simulation results and discussion of error rates, see the two papers cited below."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#references",
    "href": "blog/overlapping-conf-intervals.html#references",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "References",
    "text": "References\nCole, S. R., & Blair, R. C. (1999). Overlapping confidence intervals. Journal of the American Academy of Dermatology, 41(6), 1051-1052.\nGelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60(4), 328-331.\nSchenker, N., & Gentleman, J. F. (2001). On judging the significance of differences by examining the overlap between confidence intervals. The American Statistician, 55(3), 182-186."
  },
  {
    "objectID": "blog/roles-covariates-rtc.html#background",
    "href": "blog/roles-covariates-rtc.html#background",
    "title": "The Roles of Covariates in Randomized Experiments",
    "section": "Background",
    "text": "Background\nProperly implemented randomized experiments—such as randomized controlled trials (RCTs) and A/B tests—guarantee unbiased estimates of the causal effect of a treatment \\(T\\) on an outcome \\(Y\\). A natural question that arises is whether information on covariates \\(X\\) is still useful in these settings. This is a question I often get from data scientists.\nThe short answer is: yes, covariates are often essential—not for identifying causal effects per se, but for enhancing precision, robustness, and interpretability.\nThis short blog post outlines the most common and important roles covariates play in randomized experiments."
  },
  {
    "objectID": "blog/roles-covariates-rtc.html#a-closer-look",
    "href": "blog/roles-covariates-rtc.html#a-closer-look",
    "title": "The Roles of Covariates in Randomized Experiments",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nVariance Reduction. Adjusting for covariates often leads to more precise estimates and higher statistical power, especially when \\(X\\) is strongly correlated with the outcome \\(Y\\). In some cases, this gain in efficiency can be the difference between detecting a statistically significant effect or not. This is the logic behind methods like ANCOVA or regression adjustment in randomized settings.\nHeterogeneous Treatment Effects. Covariates are indispensable when exploring treatment effect heterogeneity—how the effect of the treatment varies across subpopulations. This can uncover insights like “the intervention works better for older users” or “only high-engagement customers benefit.” Understanding these patterns is often as important as estimating the average effect.\nBalance Checks and Covariate Adjustment. Randomization balances covariates on average, but imbalances can occur in finite samples—especially small ones. Covariate data lets you check whether treatment and control groups look similar at baseline, and it allows you to adjust for any discrepancies that might influence outcomes. This helps bolster the credibility of your findings.\nStratification or Blocking. Covariates can be used before treatment assignment to form strata or blocks (e.g., by age group or location). Randomization is then done within each block, leading to better balance across treatment groups and often greater statistical efficiency.\nExternal Validity. Covariates help describe your experimental sample and compare it to a broader population of interest. For example, are your participants younger or wealthier than your target market? This matters when considering how generalizable your results are beyond the experimental setting."
  },
  {
    "objectID": "blog/roles-covariates-rtc.html#bottom-line",
    "href": "blog/roles-covariates-rtc.html#bottom-line",
    "title": "The Roles of Covariates in Randomized Experiments",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nRandomized experiments don’t require covariates to estimate causal effects unbiasedly.\nBut covariates are often indispensable for achieving credible, precise, and policy-relevant results."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Advanced Topics in Statistical Data Science",
    "section": "",
    "text": "The Kolmogorov–Smirnov Test as a Goodness-of-fit\n\n\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nJackknife vs. Bootstrap: A Tale of Two Resamplers\n\n\n\nbootstrap\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nMay 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Roles of Covariates in Randomized Experiments\n\n\n\nrandomized experiments\n\ncausal inference\n\n\n\n\n\n\n\n\n\nMay 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference with Residualized Regressions\n\n\n\ncausal inference\n\nlinear models\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCausal vs. Predictive Modeling: Subtle, but Crucial Differences\n\n\n\ncausal inference\n\nmachine learning\n\n\n\n\n\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Two Types of Weights in Causal Inference\n\n\n\nweights\n\ncausal inference\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBinscatter: A New Visual Tool for Data Analysis\n\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFilling in Missing Data with MCMC\n\n\n\nmissing data\n\n\n\n\n\n\n\n\n\nJan 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Limits of Semiparametric Models: The Efficiency Bound\n\n\n\nstatistical inference\n\nsemiparametric models\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Limits of Nonparametric Models\n\n\n\nstatistical inference\n\nnonparametric models\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Limits of Parametric Models: The Cramér-Rao Bound\n\n\n\nstatistical inference\n\nparametric models\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Three Classes of Statistical Models\n\n\n\nstatistical models\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Delta Method: Simplifying Confidence Intervals for Complex Estimators\n\n\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nJan 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStein’s Paradox: A Simple Illustration\n\n\n\nstatistical inference\n\nparadox\n\n\n\n\n\n\n\n\n\nJan 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMutual Information: What, Why, How, and When\n\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Variables with Predefined Correlation\n\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nDec 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStratified Sampling with Continuous Variables\n\n\n\nrandomized experiments\n\ncausal inference\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nColumn-Sampling Bootstrap?\n\n\n\nbootstrap\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bootstrap and its Limitations\n\n\n\nbootstrap\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSimpson’s Paradox: A Simple Illustration\n\n\n\nparadox\n\ncausal inference\n\n\n\n\n\n\n\n\n\nDec 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCausation without Correlation\n\n\n\ncausal inference\n\ncorrelation\n\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting Methods: A Brief Overview\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Analysis of Randomized Experiments: A Modern Approach\n\n\n\nbayesian methods\n\nrandomized experiments\n\n\n\n\n\n\n\n\n\nOct 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeights in Statistical Analyses\n\n\n\nweights\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCausality without Experiments, Unconfoundedness, or Instruments\n\n\n\ncausal inference\n\ninstrumental variables\n\n\n\n\n\n\n\n\n\nAug 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFOCI: A New Variable Selection Method\n\n\n\nvariable selection\n\nmachine learning\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNonlinear Correlations and Chatterjee’s Coefficient\n\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Brief Introduction to Conformal Inference\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Conformal Inference for Variable Importance in Machine Learning\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNew Developments in False Discovery Rate\n\n\n\nmultiple testing\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nML-Based Regression Adjustments in Randomized Experiments\n\n\n\nmachine learning\n\nrandomized experiments\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Alphabet of Learners for Heterogeneous Treatment Effects\n\n\n\nmachine learning\n\nrandomized experiments\n\nheterogeneous treatment effects\n\n\n\n\n\n\n\n\n\nJul 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLasso for Heterogeneous Treatment Effects Estimation\n\n\n\nheterogeneous treatment effects\n\ncausal inference\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of Machine Learning Methods in Causal Inference\n\n\n\nmachine learning\n\ncausal inference\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Variance of Propensity Score Matching Estimators\n\n\n\npropensity score\n\ncausal inference\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation is a Cosine\n\n\n\ncorrelation\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation is Not (Always) Transitive\n\n\n\ncorrelation\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLord’s Paradox: A Simple Illustration\n\n\n\ncorrelation\n\nparadox\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing in Linear Machine Learning Models\n\n\n\nhypothesis testing\n\nmachine learning\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Testing: Methods Overview\n\n\n\nmultiple testing\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing with Population Data\n\n\n\nhypothesis testing\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapping Confidence Intervals and Statistical (In)Significance\n\n\n\nstatistical inference\n\nhypothesis testing\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/correlation-is-cosine.html#background",
    "href": "blog/correlation-is-cosine.html#background",
    "title": "Correlation is a Cosine",
    "section": "Background",
    "text": "Background\nYou might have come across the statement, “correlation is a cosine,” but never taken the time to explore its precise meaning. It certainly sounds intriguing—how can the simplest bivariate summary statistic be connected to a trigonometric function you first encountered in sixth grade? What exactly is the relationship between correlation and cosines?"
  },
  {
    "objectID": "blog/correlation-is-cosine.html#a-closer-look",
    "href": "blog/correlation-is-cosine.html#a-closer-look",
    "title": "Correlation is a Cosine",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Law of Cosines\nThe law of cosines states that in any triangle with sides \\(x\\), \\(y\\), and \\(z\\) and an angle (between \\(x\\) and \\(y\\)) \\(\\theta\\), we have:\n\\[ z^2 = x^2 + y^2 - 2 x y cos(\\theta), \\]\nIn the special case when \\(\\theta=\\frac{\\pi}{2}\\), the term on the right-hand side equals \\(0\\) and the equation reduces to the well-known Pythagorean Theorem.\n\n\nThe Variance of the \\(A+B\\)\nLet’s imagine two random variables \\(A\\), \\(B\\). The variance of their sum is given by:\n\\[ var(A+B) = var(A)+var(B)+2 cov(A,B), \\]\nwhere \\(cov(\\cdot)\\), denotes covariance. We can substitute the last term with its definition as follows:\n\\[ var(A+B) = var(A)+var(B)+2 corr(A,B) sd(A) sd(B). \\]\nNext, we know that \\(var(\\cdot)=sd^2(\\cdot)\\). Substituting, we get:\n\\[ sd^2(A+B) = sd^2 (A)+ sd^2 (B)+2 corr(A,B) sd(A) sd(B).\\]\n\n\nPutting the Two Together\nSetting \\(x=sd(A)\\), \\(y=sd(B)\\), and \\(z=sd(A+B)\\) in the first equation gives the desired result. With one small caveat – the negative sign on the cosine term. To get around this we can simply look at the complementary angle \\(\\delta = \\pi - \\theta\\).\nThat is, we imagine a triangle with sides equal to \\(sd(A)\\), \\(sd(B)\\) and \\(sd(A+B)\\), where \\(\\theta\\) is the angle between \\(sd(A)\\), \\(sd(B)\\). When this angle is small (\\(\\theta &lt; \\frac{\\pi}{2}\\)), the two sides point in the same direction and A and B are positively correlated. The opposite is true for \\(\\theta &gt; \\frac{\\pi}{2}\\). As mentioned above, \\(\\theta = \\frac{\\pi}{2}\\) kills the correlation term, consistent with \\(A\\) and \\(B\\) being independent.\n\n\nCorrelation as a Dot Product\nThere’s another way to see this connection that makes it even clearer.\nIf you think of \\(A\\) and \\(B\\) as vectors in \\(n\\)-dimensional space (e.g., \\(A = (a_1, a_2, \\ldots, a_n)\\)), the cosine of the angle between them is given by:\n\\[\n\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|},\n\\]\nwhere \\(A \\cdot B\\) is the dot product, and \\(\\|\\cdot\\|\\) denotes the Euclidean norm. When \\(A\\) and \\(B\\) are standardized (i.e., mean zero and unit variance), this cosine becomes the Pearson correlation coefficient:\n\\[\n\\text{corr}(A, B) = \\frac{1}{n} \\sum_{i=1}^n A_i^* B_i^* \\approx \\cos(\\theta),\n\\]\nwhere \\(A^*\\) and \\(B^*\\) are the standardized versions of \\(A\\) and \\(B\\).\n\n\nCosine Similarity vs. Correlation\n\n\n\n\n\n\nCosine similarity vs. correlation\n\n\n\nCosine similarity and Pearson correlation are closely related, but not always the same:\n\nCosine similarity considers only the angle between vectors. It’s scale-invariant but not shift-invariant.\nCorrelation removes both mean and scale, making it invariant to affine transformations.\n\nSo, while “correlation is a cosine,” the statement is strictly true when you’re working with standardized vectors."
  },
  {
    "objectID": "blog/correlation-is-cosine.html#an-example",
    "href": "blog/correlation-is-cosine.html#an-example",
    "title": "Correlation is a Cosine",
    "section": "An Example",
    "text": "An Example\nLet’s generate two random vectors, standardize them, compute their correlation and angle, and plot them as vectors. We’ll also see how correlation equals the cosine of the angle between the vectors.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\n# Generate two random vectors\nA &lt;- rnorm(100)\nB &lt;- 0.8 * A + sqrt(1 - 0.8^2) * rnorm(100)  # Correlated with A\n\n# Standardize\nA_std &lt;- scale(A)\nB_std &lt;- scale(B)\n\n# Correlation and cosine\ncorrelation &lt;- cor(A, B)\ncosine &lt;- sum(A_std * B_std) / (sqrt(sum(A_std^2)) * sqrt(sum(B_std^2)))\nangle_deg &lt;- acos(cosine) * 180 / pi\n\n# Print results\ncat(\"Correlation:\", round(correlation, 3), \"\\n\")\n&gt; Correlation: 0.825 \ncat(\"Angle (degrees):\", round(angle_deg, 1), \"\\n\")\n&gt; Angle (degrees): 34.4 \n\n# Plot vectors\nplot(c(0, A_std[1]), c(0, B_std[1]), type = \"n\", xlab = \"A (standardized)\", ylab = \"B (standardized)\",\n     main = \"First Vectors from A and B\")\narrows(0, 0, A_std[1], 0, col = \"blue\", lwd = 2)\narrows(0, 0, A_std[1], B_std[1], col = \"red\", lwd = 2)\nlegend(\"topright\", legend = c(\"A\", \"B\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1988)\n\n# Generate two correlated vectors\nA = np.random.randn(100)\nB = 0.8 * A + np.sqrt(1 - 0.8**2) * np.random.randn(100)\n\n# Standardize\nA_std = (A - np.mean(A)) / np.std(A)\nB_std = (B - np.mean(B)) / np.std(B)\n\n# Correlation and cosine\ncorrelation = np.corrcoef(A, B)[0, 1]\ncosine = np.dot(A_std, B_std) / (np.linalg.norm(A_std) * np.linalg.norm(B_std))\nangle = np.arccos(np.clip(cosine, -1, 1)) * 180 / np.pi\n\nprint(f\"Correlation: {correlation:.3f}\")\nprint(f\"Angle (degrees): {angle:.1f}\")\n\n# Plot first two vectors\nplt.figure(figsize=(5, 5))\nplt.quiver(0, 0, A_std[0], 0, angles='xy', scale_units='xy', scale=1, color='blue', label='A')\nplt.quiver(0, 0, A_std[0], B_std[0], angles='xy', scale_units='xy', scale=1, color='red', label='B')\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nplt.xlabel(\"A (standardized)\")\nplt.ylabel(\"B (standardized)\")\nplt.title(\"First Vectors from A and B\")\nplt.legend()\nplt.grid(True)\nplt.gca().set_aspect('equal')\nplt.show()\n\n\n\nIn this example, \\(cor(A,B)=0.825\\) and the angle between the two vectors is \\(34.4^\\circ\\), and we have \\(cos(34.4^\\circ) = cor(A,B)=0.825\\). You can also visualize these vectors, but I am not showing that graph here."
  },
  {
    "objectID": "blog/correlation-is-cosine.html#where-to-learn-more",
    "href": "blog/correlation-is-cosine.html#where-to-learn-more",
    "title": "Correlation is a Cosine",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAs with anything else, a Google search is your friend here, with multiple Stack Overflow posts explaining this connection from all sorts of angles. However, I do find John D. Cook’s blog post most helpful, and I am following his exposition closely."
  },
  {
    "objectID": "blog/correlation-is-cosine.html#bottom-line",
    "href": "blog/correlation-is-cosine.html#bottom-line",
    "title": "Correlation is a Cosine",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe variance formula mirrors the law of cosines.\nStandardizing the variables makes correlation equal the cosine of the angle.\nSo: “Correlation is a cosine” — literally!"
  },
  {
    "objectID": "blog/correlation-is-cosine.html#references",
    "href": "blog/correlation-is-cosine.html#references",
    "title": "Correlation is a Cosine",
    "section": "References",
    "text": "References\nCosines and correlation, Cook 2010, blog post"
  },
  {
    "objectID": "blog/paradox-stein.html#background",
    "href": "blog/paradox-stein.html#background",
    "title": "Stein’s Paradox: A Simple Illustration",
    "section": "Background",
    "text": "Background\nIn the realm of statistics, few findings are as counterintuitive and fascinating as Stein’s paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein’s paradox is not merely about memorizing an oddity—it’s about recognizing the delicate balance inherent in statistical decision-making.\nIn essence, Stein’s paradox asserts that when estimating the means of multiple variables simultaneously, it’s possible to achieve better results compared to relying solely on the sample averages.\nLet’s now delve deeper into this statement and unravel the underlying mechanisms."
  },
  {
    "objectID": "blog/paradox-stein.html#a-closer-look",
    "href": "blog/paradox-stein.html#a-closer-look",
    "title": "Stein’s Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Mean Squared Error (MSE)\nTo understand the paradox, let’s begin by quantify what we mean by “better” estimation. The MSE of an estimator \\(\\hat{\\mu}\\) is defined as its expected squared error (or loss):\n\\[\\text{MSE}(\\hat{\\mu}) = \\mathbb{E}\\left[ ( \\hat{\\mu} - \\mu )^2 \\right],\\]\nwhere \\(\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_p)\\) is the true vector of means. All else equal, the lower MSE the better.\n\n\nMathematical Formulation\nStein’s paradox arises in the context of estimating multiple parameters simultaneously. Suppose you’re estimating the mean \\(\\mu_i\\) of several independent normal distributions:\n\\[X_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, p,\\]\nwhere \\(X_i\\) are observed values, \\(\\mu_i\\) are the unknown means, and \\(\\sigma^2\\) is known. We assume non-zero covariance between the variables.\nA natural approach is to estimate each _i using its corresponding sample mean \\(X_i\\), which is the maximum likelihood estimator (MLE). However, in dimensions \\(p \\geq 3\\), this seemingly reasonable approach is dominated by an alternative method.\nThe surprise? Shrinking the individual estimates toward a common value—such as the overall mean—produces an estimator with uniformly lower expected squared error.\nThe MSE of the MLE is equal to:\n\\[R(\\hat{\\mu}_\\text{MLE}) = p\\sigma^2.\\]\nNow consider the biased(!) James-Stein (JS) estimator:\n\\[\\hat{\\mu}_\\text{JS} = \\left( 1 - \\frac{(p - 2)\\sigma^2}{\\lVert X \\rVert^2} \\right) X,\\]\nwhere \\(\\lVert X \\rVert^2 = \\sum_i X_i^2\\) is the squared norm of the observed data. The shrinkage factor \\(1 - \\frac{(p - 2)\\sigma^2}{|X|^2}\\) pulls the estimates toward zero (or any other pre-specified point).\nRemarkably, the James-Stein estimator has lower MSE than the MLE for \\(p \\geq 3\\):\n\\[\\text{MSE}(\\hat{\\mu}_{\\text{JS}}) &lt; \\text{MSE}(\\hat{\\mu}_{\\text{MLE}}).\\]\nThis is the mystery at its core.\n\n\nExplanation\nThis result holds because the James-Stein estimator balances variance reduction and bias introduction in a way that minimizes overall MSE. The MLE, in contrast, does not account for the possibility of shared structure among the parameters. The counterintuitive nature of this result stems from the independence of the \\(X_i\\)’s. Intuition suggests that pooling information across independent observations should not improve estimation, yet the James-Stein estimator demonstrates otherwise."
  },
  {
    "objectID": "blog/paradox-stein.html#an-example",
    "href": "blog/paradox-stein.html#an-example",
    "title": "Stein’s Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nLet’s emulate this paradox in R and python in a setting with \\(p=5\\).\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\np &lt;- 5  # Number of means\nn &lt;- 1000  # Number of simulations\nsigma &lt;- 1\nmu &lt;- rnorm(p, mean = 5, sd = 2)  # True means\n\n# results storage\nmse_mle &lt;- numeric(n)  \nmse_js &lt;- numeric(n)\n\n# create a fake dataset and compute the MSEs of both the MLE and JS estimator. \n# repeat this 1,000 times and take the average loss for each estimator.\nfor (sim in 1:n) {\n  # Simulate observations\n  X &lt;- rnorm(p, mean = mu, sd = sigma)\n\n  # MLE estimator and its MSE\n  mle &lt;- X\n  mse_mle[sim] &lt;- sum((mle - mu)^2)\n\n  # James-Stein estimator and its MSE\n  shrinkage &lt;- max(0, 1 - ((p - 2) * sigma^2) / sum(X^2))\n  js &lt;- shrinkage * X\n  mse_js[sim] &lt;- sum((js - mu)^2)\n}\n\n# print the results.\ncat(\"Average MSE of MLE:\", mean(mse_mle), \"\\n\")\n&gt; Average MSE of MLE: 5.125081 \ncat(\"Average MSE of James-Stein:\", mean(mse_js), \"\\n\")\n&gt; Average MSE of James-Stein: 5.055019 \n\n\nimport numpy as np\nnp.random.seed(1988)\n\n# Parameters\np = 5  # Number of means\nn = 1000  # Number of simulations\nsigma = 1\nmu = np.random.normal(loc=5, scale=2, size=p)  # True means\n\n# Results storage\nmse_mle = np.zeros(n)\nmse_js = np.zeros(n)\n\n# Simulate data and compute MSEs for MLE and James-Stein estimator\nfor sim in range(n):\n    # Simulate observations\n    X = np.random.normal(loc=mu, scale=sigma, size=p)\n\n    # MLE estimator and its MSE\n    mle = X\n    mse_mle[sim] = np.sum((mle - mu) ** 2)\n\n    # James-Stein estimator and its MSE\n    shrinkage = max(0, 1 - ((p - 2) * sigma**2) / np.sum(X**2))\n    js = shrinkage * X\n    mse_js[sim] = np.sum((js - mu) ** 2)\n\n# Print the results\nprint(\"Average MSE of MLE:\", np.mean(mse_mle).round(3))\n&gt; Average MSE of MLE: 4.998\nprint(\"Average MSE of James-Stein:\", np.mean(mse_js).round(3))\n&gt; Average MSE of James-Stein: 4.951\n\n\n\nIn this example, average MSE of the James-Stein estimator (\\(5.06\\)) is consistently lower than that of the MLE (\\(5.13\\)), illustrating the paradox in action."
  },
  {
    "objectID": "blog/paradox-stein.html#bottom-line",
    "href": "blog/paradox-stein.html#bottom-line",
    "title": "Stein’s Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nStein’s paradox shows that shrinkage estimators can outperform the MLE in dimensions \\(p \\geq 3\\), even when the underlying variables are independent.\nThe James-Stein estimator achieves lower MSE by balancing variance reduction and bias introduction.\nUnderstanding this result highlights the power of shrinkage techniques in high-dimensional statistics."
  },
  {
    "objectID": "blog/paradox-stein.html#references",
    "href": "blog/paradox-stein.html#references",
    "title": "Stein’s Paradox: A Simple Illustration",
    "section": "References",
    "text": "References\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#background",
    "href": "blog/stratified-sampling-cont-var.html#background",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Background",
    "text": "Background\nStratified sampling is a foundational technique in survey design, ensuring that observations capture key characteristics of a population. By dividing the data into distinct strata and sampling from each, stratified sampling often results in more efficient estimates than simple random sampling. Strata are typically defined by categorical variables such as classrooms, villages, or user types. This method is particularly advantageous when some strata are rare but carry critical information, as it ensures their representation in the sample. It is also often employed to tackle spillover effects or manage survey costs more effectively.\nWhile straightforward for categorical variables (like geographic region), continuous variables—such as income or churn score—pose greater challenges for stratified sampling. The primary issue lies in the curse of dimensionality: attempting to create strata across multiple continuous variables results in an explosion of possible combinations, making effective sampling impractical. For example, stratifying a population based on income at every possible dollar amount is absurd.\nIn this article, I present two solutions to the problem of stratified sampling with continuous variables."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#a-closer-look",
    "href": "blog/stratified-sampling-cont-var.html#a-closer-look",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Traditional Method: Equal-Sized Binning\nThis approach involves dividing the continuous variable(s) into intervals or bins. For example, churn score, a single continuous variable \\(X\\), can be divided into quantiles (e.g., quartiles or deciles), ensuring each bin contains approximately the same number of observations/users.\nLet’s focus on the case of building ten equally-sized strata. Mathematically, for a continuous variable \\(X\\), the decile-based binning can be defined as:\n\\[\\text{Bin}_i = \\{x \\in X : Q_{10\\times (i-1)+1} \\leq x &lt; Q_{10\\times i}\\} \\text{ for } i \\in \\{1,\\dots,10\\}, \\]\nwhere \\(Q_k\\) represents the \\(k\\)-th quantile of \\(X\\). This approach splits in the first ten percentiles (i.e., minimum value to the \\(10\\)th percentile) into a single stratum, the next ten percentiles (\\(10\\)th to \\(20\\)th) into another stratum, and so on.\nWhen dealing with multiple variables, this method extends to either marginally stratify each variable or jointly stratify them. However, joint stratification across multiple variables can also fall prey to the curse of dimensionality.\n\n\nThe Modern Method: Unsupervised Clustering\nAn alternative approach uses unsupervised clustering algorithms, such as \\(k\\)-means or hierarchical clustering, to group observations into clusters, treating these clusters as strata. Unlike binning, clustering leverages the distribution of the data to form natural groupings.\nFormally, let \\(X\\) be a matrix of n observations across \\(p\\) continuous variables. One class of clustering algorithms aims to assign each observation \\(i\\) to one of \\(k\\) clusters:\n\\[\\text{minimize} \\quad \\sum_{j=1}^k \\sum_{i \\in \\mathcal{C}_j} \\text{distance}(X_i - \\mu_j), \\]\nwhere \\(\\mu_j=\\frac{1}{|S_j|}\\sum_{i\\in \\mathcal{C}_j} X\\)​ is the centroid of cluster \\(\\mathcal{C}_j\\) of \\(size |S_j|\\).\nCommonly, \\(\\text{distance}(X_i, \\mu_j)=\\|X_i - \\mu_j\\|^2\\) which leads to \\(k\\)-means clustering. Unlike in the binning approach, here we are not restricting each strata to have the same number of observations."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#pros-and-cons",
    "href": "blog/stratified-sampling-cont-var.html#pros-and-cons",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nUnsurprisingly, each method comes with its trade-offs. Traditional binning is simple and interpretable but can struggle with multivariate dependencies. Clustering accounts for multivariate relationships between variables, avoids imposing arbitrary bin thresholds, and may results in more natural groupings. However, it can be computationally expensive and sensitive to the choice of algorithm and hyperparameters (e.g., \\(k\\) in \\(k\\)-means).\nOne can also imagine a hybrid approach. Begin with a dimensionality reduction method like PCA and then perform binning on the first few principal components."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#an-example",
    "href": "blog/stratified-sampling-cont-var.html#an-example",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "An Example",
    "text": "An Example\nHere is R and python code illustrating both types of approaches on the popular iris dataset. We are interested in creating strata based on the SepalLenght variable. We begin with the traditional binning approach.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\ndata(iris)\n\n#Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris$SepalLengthBin &lt;- cut(iris$Sepal.Length, \n                           breaks = quantile(iris$Sepal.Length, probs = seq(0, 1, 0.25)), include.lowest = TRUE)\n\n#Inspect the resulting strata\ntable(iris$SepalLengthBin)\n&gt; [4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] \n&gt;       41        39        35        35\n\n#Perform k-means clustering on two continuous variables\niris_cluster &lt;- kmeans(iris[, c(\"Sepal.Length\", \"Petal.Length\")], centers = 4)\n\n#Assign clusters as strata\niris$Cluster &lt;- as.factor(iris_cluster$cluster)\n\n#Inspect the resulting strata\ntable(iris$Cluster) \n&gt; 1  2  3  4 \n&gt; 50 15 54 31 \n\n\n# Load libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nnp.random.seed(1988)\n\n# Load the iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris['SepalLengthBin'] = pd.qcut(iris['Sepal.Length'], q=4, labels=False)\n\n# Inspect the resulting strata\nprint(\"Quantile Bins (Sepal.Length):\")\nprint(iris['SepalLengthBin'].value_counts())\n&gt; SepalLengthBin\n&gt; 0    41\n&gt; 1    39\n&gt; 3    35\n&gt; 2    35\n\n# Perform k-means clustering on two continuous variables\nkmeans = KMeans(n_clusters=4, random_state=1988)\niris['Cluster'] = kmeans.fit_predict(iris[['Sepal.Length', 'Petal.Length']])\n\n# Assign clusters as strata\niris['Cluster'] = iris['Cluster'].astype('category')\n\n# Inspect the resulting strata\nprint(\"\\nCluster Sizes:\")\nprint(iris['Cluster'].value_counts())\n&gt; Cluster\n&gt; 2    50\n&gt; 3    50\n&gt; 0    28\n&gt; 1    22\n\n\n\nHere we also have four clusters, but their size ranges from \\(25\\) to \\(50\\) observations each."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#bottom-line",
    "href": "blog/stratified-sampling-cont-var.html#bottom-line",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nStratified sampling with continuous variables requires balancing simplicity and sophistication.\nTraditional binning remains a practical choice for single continuous variables or very few categorical ones.\nClustering provides a robust alternative, enabling stratification with multiple continuous variables at the cost of adding complexity and tuning parameters."
  },
  {
    "objectID": "blog/mutual-information.html#introduction",
    "href": "blog/mutual-information.html#introduction",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Introduction",
    "text": "Introduction\nWhen exploring dependencies between variables, the data scientist’s toolbox often relies on correlation measures to reveal relationships and potential patterns. But what if we’re looking to capture relationships beyond linear correlations? Mutual Information (MI) quantifies the “amount of information” shared between variables in a more general sense. It measures how much we know about \\(Y\\) by observing \\(X\\). This approach goes beyond linear patterns and can help us uncover more complex relationships within data.\nMI can be especially helpful for applications such as feature selection and unsupervised learning. For readers familiar with various types of correlation metrics, Mutual Information provides an additional lens to interpret relationships within data.\nIn this article, I’ll guide you through the concept of Mutual Information, its definition, properties, and usage, as well as comparisons with other dependency measures. We’ll explore MI’s mathematical foundations, its advantages and limitations, and its applications, concluding with an example demonstrating MI’s behavior alongside more traditional measures like Pearson’s correlation."
  },
  {
    "objectID": "blog/mutual-information.html#notation",
    "href": "blog/mutual-information.html#notation",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Notation",
    "text": "Notation\nBefore diving deeper, let’s establish our notation:\n\nRandom variables will be denoted by capital letters (\\(X\\), \\(Y\\)).\nLowercase letters (\\(x\\),\\(y\\)) represent specific values of these variables.\n\\(p(x)\\) denotes the probability mass/density function of \\(X\\).\n\\(p(x,y)\\) represents the joint probability mass/density function of \\(X\\) and \\(Y\\).\n\\(p(x\\mid y)\\) is the conditional probability of \\(X\\) given \\(Y\\).\n\\(H(X)\\) represents the entropy of random variable \\(X\\).\n\nThese should not surprise anyone, as they are standard conventions in most statistics textbooks."
  },
  {
    "objectID": "blog/mutual-information.html#a-closer-look",
    "href": "blog/mutual-information.html#a-closer-look",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Entropy\nMutual Information is related to the notion of entropy, a measure of uncertainty or randomness in a random variable. In less formal terms, entropy quantifies how “surprising” or “unpredictable” the outcomes of \\(X\\) are.\nFormally, for a discrete random variable \\(X\\) with possible outcomes \\(x_1, x_2, \\dots, x_n\\) and associated probabilities \\(p(x_1), p(x_2), \\dots, p(x_n)\\), entropy \\(H(X)\\) is defined as:\n\\[H(X) = -\\sum_{i=1}^n p(x_i) \\log p(x_i).\\]\nFor continuous variables we switch the summation with an integral.\nEntropy equals \\(0\\) when there’s no uncertainty, such as when \\(X\\) always takes a single outcome. High entropy means greater uncertainty (many possible outcomes, all equally likely), while low entropy indicates less uncertainty, as some outcomes are much more likely than others. In essence, entropy tells us how much “information” is gained on average when observing the variable’s realization.\n\n\nMathematical Definitions of MI\nMI can be defined in multiple ways. Perhaps the most intuitive definition of MI between two random variables \\(X\\) and \\(Y\\) is that it measures the difference between the joint distribution and the product of their marginals. If two random variables are independent, their joint distribution is the product of their marginals \\(p(x,y)=p(x)p(y)\\). In some sense, the discrepancy between these two objects (i.e., the two sides of the equality) measures the strength of association between \\(X\\) and \\(Y\\) (i.e., their “independence”).\nFormally, MI is the Kullback-Leibler divergence between the joint distribution and the product of the two marginals:\n\\[ MI(X,Y) = D_{KL} (p(X,Y) || p(X)p(Y)). \\]\nLet’s now examine MI from a different angle. We can express mutual information as follows:\n\\[ MI(X,Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\left(\\frac{p(x,y)}{p(x)p(y)}\\right)\\]\nAgain, for continuous variables, the sums become integrals.\nAlternatively, MI can also be expressed in terms of entropy. It equals the sum of the entropy of \\(X\\) and \\(Y\\) taking away their joint entropy:\n\\[ \\begin{align*}MI(X,Y) & = H(X) - H(X|Y) \\\\ & = H(Y) - H(Y|X) \\\\ & = H(X) + H(Y) - H(X,Y).\\end{align*} \\]\nYou can compute MI in R with the infotheo package.\nSoftware Package: infotheo.\n\n\nProperties\nMI has several intriguing properties:\n\nNon-negativity: \\(MI(X;Y) \\geq 0\\). Mutual Information is always non-negative, as it measures the amount of information one variable provides about the other. Higher values correspond to stronger association.\nSymmetry: \\(MI(X,Y) = MI(Y,X)\\). This symmetry implies that the information \\(X\\) provides about \\(Y\\) is the same as what Y provides about X.\nIndependence: Similarly to Chatterjee’s correlation coefficient, \\(MI(X,Y) = 0\\) if and only if \\(X\\) and \\(Y\\) are independent.\nScale-invarance: Mutual information is scale invariant. If you apply a scaling transformation to the variables, their MI will not be affected.\n\n\n\nConditional MI\nConditional Mutual Information (CMI) extends the concept of MI to measure dependency between \\(X\\) and \\(Y\\) given a third variable \\(Z\\). CMI is useful for investigating how much information \\(X\\) and \\(Y\\) share independently of \\(Z\\). It is defined as:\n\\[MI(X,Y|Z) = \\sum_{x,y,z} p(x,y,z) \\log \\left(\\frac{p(x,y|z)}{p(x|z)p(y|z)}\\right). \\]\nThis can be valuable in causal inference, where understanding dependencies conditioned on specific variables aids in interpreting relationships within complex models. CMI is also particularly useful for feature selection when accounting for redundancy among already included features. Let’s explore this idea in greater detail.\n\n\nFeature Selection with MI\nIn machine learning, MI serves as a useful metric for feature selection (Brown et al. 2012, Vergara and Estévez 2014). Consider an outcome variable \\(Y\\) and a set of features \\(X\\in\\mathbb{R}^p\\) with n i.i.d. observations. By evaluating the MI between each feature and the target variable, one can retain features with the highest information content, passing a certain threshold. This is somewhat primitive since it assumes independence across features. More sophisticated approaches take feature dependency into account.\nFor instance, methods like Minimum Redundancy Maximum Relevance (mRMR, Peng et al. 2005) aim to maximize the relevance of features to the target while minimizing redundancy among features. Here is a concise version of the mRMR algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nCalculate MI between each feature and the target (relevance).\nCalculate MI between features (redundancy).\nSelect features that maximize relevance while minimizing redundancy: \\[ \\text{mRMR} = \\max_{X_i} \\left[MI(X_i;Y) - \\frac{1}{|S|} \\sum_{X_j \\in S} MI(X_i;X_j)\\right], \\]\n\nwhere \\(S\\) is the set of already selected features.\n\n\n\n\nPros and Cons\nLike any other statistical tool, mutual Information has several advantages and limitations. On the positive side, it captures both linear and nonlinear relationships, is scale-invariant, and works with both continuous and discrete variables, making it a theoretically well-founded measure. However, it requires density estimation for continuous variables, can be computationally intensive for large datasets, and its results can be sensitive to binning choices for continuous variables. Additionally, there is no standard normalization for MI."
  },
  {
    "objectID": "blog/mutual-information.html#an-example",
    "href": "blog/mutual-information.html#an-example",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "An Example",
    "text": "An Example\nLet’s implement MI calculation in R and python and compare it with traditional correlation measures using the iris dataset.\n\nRPython\n\n\nrm(list=ls())\nlibrary(infotheo)\nlibrary(corrplot)\nlibrary(dplyr)\ndata(iris)\n\n# Calculate mutual information matrix\nmi_matrix &lt;- mutinformation(discretize(iris[,1:4]))\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(iris[,1:4])\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value &lt;- mi_matrix[1,3]\ncor_value &lt;- cor_matrix[1,3]\n\n# Print results\nprint(paste(\"Mutual Information:\", round(mi_value, 3)))\nprint(paste(\"Pearson Correlation:\", round(cor_value, 3)))\n\n&gt;[1] \"Mutual Information: 0.585\"\n&gt;[1] \"Pearson Correlation: 0.872\"\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.metrics import mutual_info_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load dataset\niris = sns.load_dataset('iris')\n\n# Discretize the dataset (except the target variable)\nX = iris.iloc[:, :-1]\nest = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\nX_discretized = est.fit_transform(X)\n\n# Calculate mutual information matrix\nmi_matrix = np.zeros((X.shape[1], X.shape[1]))\nfor i in range(X.shape[1]):\n    for j in range(X.shape[1]):\n        mi_matrix[i, j] = mutual_info_score(X_discretized[:, i], X_discretized[:, j])\n\n# Calculate correlation matrix\ncor_matrix = X.corr()\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value = mi_matrix[0, 2]  # Sepal.Length vs Petal.Length\ncor_value = cor_matrix.iloc[0, 2]  # Sepal.Length vs Petal.Length\n\n# Print results\nprint(f\"Mutual Information: {mi_value:.3f}\")\nprint(f\"Pearson Correlation: {cor_value:.3f}\")\n\n&gt; Mutual Information: 0.905\n&gt; Pearson Correlation: 0.872\n\n\n\n\nThe left matrix displays the MI results and the right one shows the standard (Pearson) correlation values. The default scales are different, so one should compare the values and not the colors. Indeed, the variable pairs with negative linear correlation also have the lowest MI values.\nThis example demonstrates how MI can capture nonlinear relationships that might be missed by traditional correlation measures."
  },
  {
    "objectID": "blog/mutual-information.html#bottom-line",
    "href": "blog/mutual-information.html#bottom-line",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMutual Information provides a comprehensive measure of statistical dependence, capturing both linear and nonlinear relationships.\nUnlike correlation coefficients, MI works naturally with both continuous and categorical variables.\nMI serves as the foundation for sophisticated feature selection algorithms like mRMR."
  },
  {
    "objectID": "blog/mutual-information.html#where-to-learn-more",
    "href": "blog/mutual-information.html#where-to-learn-more",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great place to start and learning the basics. Brown et al. (2012) and Vergara and Estévez (2014) are the go-to resources for conditional MI and using MI for feature selection."
  },
  {
    "objectID": "blog/mutual-information.html#references",
    "href": "blog/mutual-information.html#references",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "References",
    "text": "References\nBrown, G., Pocock, A., Zhao, M. J., & Luján, M. (2012). Conditional likelihood maximisation: a unifying framework for information theoretic feature selection. JMLR.\nCover, T. M., & Thomas, J. A. (2006). Elements of information theory (2nd ed.). Wiley-Interscience.\nKraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical Review E, 69(6).\nPeng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE TPAMI.\nRoss, B. C. (2014). Mutual information between discrete and continuous data sets. PloS one, 9(2).\nVergara, J. R., & Estévez, P. A. (2014). A review of feature selection methods based on mutual information. Neural Computing and Applications, 24(1)."
  },
  {
    "objectID": "blog/three-classes-stat-models.html#background",
    "href": "blog/three-classes-stat-models.html#background",
    "title": "The Three Classes of Statistical Models",
    "section": "Background",
    "text": "Background\nStatistical modeling is among the most exciting elements of working with data. When mentoring junior data scientists, I never fail to see the spark in their eyes when our projects have overcome the data gathering and cleaning stages and have reached that precious Rubicon.\nA major goal in the craft of statistical modeling is extracting the most information from the data at hand. Statisticians and econometricians call this efficiency or precision. In simple terms, this means achieving the lowest possible variance from a class of available estimators. As an example, imagine two findings—both centered at \\(3\\%\\) (e.g., the treatment effect of an intervention of interest)—but one with a \\(95\\%\\) confidence interval of \\([2\\%, 4\\%]\\), and the other with \\([-7\\%, 13\\%]\\). The former is clearly more informative and useful than the latter.\nIn a series of four articles, I will dive deeper into the topic of efficiency with a varying degree of detail and complexity. As a first step in this exploration, I want to demystify the distinction between the three classes of statistical models: parametric, semiparametric, and nonparametric. The goal of this article is to explore in depth the concepts of parameterizing a model and the distinctions between these three groups."
  },
  {
    "objectID": "blog/three-classes-stat-models.html#a-closer-look",
    "href": "blog/three-classes-stat-models.html#a-closer-look",
    "title": "The Three Classes of Statistical Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nParametric Models\nParametric models impose strong assumptions about the underlying data-generating process, making them less flexible but highly interpretable and data-efficient. They require fewer data points to estimate the target accurately compared to semiparametric or nonparametric models. Consequently, they are often the default choice in practice.\nA classic example is density estimation. Suppose you observe that your data appears symmetric around its mean. You might assume the data follows a normal (Gaussian) distribution. In this case, specifying the mean \\(\\mu\\) and variance \\(\\sigma^2\\) fully characterizes the distribution, allowing you to estimate the density parametrically. And then, you can make all kinds of calculations related to your variable, such as what is the probability that it will take values greater than a given number \\(c\\).\nFormally, a parametric model describes the data-generating process entirely using a finite-dimensional parameter vector. As yet another example, in the omnipresent linear model:\n\\[Y = X\\beta + \\epsilon,\\]\nthe parameter specifies the entire relationship between \\(X\\) and \\(Y\\). This imposes a strong assumption: the relationship is linear, meaning a unit change in \\(X\\) consistently results in a \\(\\beta\\) change in \\(Y\\), regardless of \\(X\\)’s magnitude (small or large). While convenient, this assumption may not hold in all practical scenarios, as real-world relationships often are complex and deviate from linearity.\n\n\nSemiparametric Models\nSemiparametric models combine the structure of parametric models with the flexibility of nonparametric methods. They specify certain aspects of the data-generating process parametrically while leaving other aspects unspecified or modeled flexibly.\nConsider the partially linear model:\n\\[Y = X \\beta + g(Z) + \\epsilon,\\]\nwhere \\(\\beta\\) is a parametric component describing the linear effect of \\(X\\), while \\(g(Z)\\) is an unknown and potentially complex function capturing the nonparametric effect of a covariate matrix \\(Z\\). Here, the model imposes linearity on \\(X\\)’s effect but allows \\(Z\\)’s effect to be fully flexible.\nSemiparametric models are highly versatile, balancing the interpretability of parametric models with the adaptability of nonparametric ones. However, estimating \\(\\beta\\) efficiently while accounting for the unknown \\(g(Z)\\) poses challenges, often requiring further assumptions.\nIn practice, semiparametric models play a crucial role in causal inference, particularly when working with observational cross-sectional data (see Imbens and Rubin 2015). A common strategy involves estimating propensity scores using parametric models (like logistic regression) and then employing nonparametric techniques such as kernel weighting, matching, or stratification to estimate treatment effects. This hybrid approach helps address confounding while maintaining computational tractability and interpretability.\n\n\nNonparametric Models\nNonparametric models make minimal assumptions about the underlying data-generating process, allowing the data to “speak for itself”. Unlike parametric and semiparametric models, these ones do not specify a finite-dimensional parameter vector or impose rigid structural assumptions. This flexibility makes them highly robust to model misspecification but often requires larger datasets to achieve accurate estimates.\nLet’s get back to the density estimation example. Histograms are a form of nonparametric models as they do not assume specific underlying functional form. Instead of fitting a parametric distribution (e.g., normal), you might use a kernel density estimator:\n\\[\\hat{f}(x)=\\frac{1}{nh}\\sum_i K \\left( \\frac{x-X_i}{h} \\right),\\]\nwhere \\(K(\\cdot)\\) is a kernel function, and \\(h\\) is a bandwidth parameter controlling the smoothness of the estimate. А kernel function assigns varying weight to observations around a data point. They are non-negative, symmetric around zero, and sum to one. Commonly employed kernel functions include:\n\nGaussian \\(K(\\cdot)=\\frac{1}{\\sqrt{2 \\pi}}e^{-0.5x^2}\\),\nEpanechnikov \\(K(\\cdot)=\\frac{3}{4}(1-x^2)\\),\nRectangular: \\(K(\\cdot)=0.5\\).\n\nThis approach does not rely on assumptions about the data’s shape, allowing it to adapt to various distributions.\nNonparametric regression provides another illustration. In it, the relationship between \\(X\\) and \\(Y\\) is modeled as:\n\\[Y=m(X)+\\epsilon,\\]\nwhere \\(m(X)\\) is an unknown function estimated directly from the data using methods like local polynomial regression, splines. Unlike the linearity assumption in parametric models, nonparametric regression allows \\(m(X)\\) to capture complex, nonlinear relationships. A commonly used variant of this is LOESS regression often overlayed in bivariate scatterplots.\nMany popular machine learning (ML) methods are also nonparametric. In this context it is important to distinguish between parameters which impose assumptions on the data and hyperparameters which serve to tune the algorithm. Tree-based techniques exemplify the nonparametric nature of ML: gradient boosting machines, random forests, and individual decision trees can all grow more complex as they encounter more data, creating flexible models that capture intricate patterns. Think even of unsupervised clustering models including \\(k\\)-means and hierarchical clustering.\nWhile nonparametric models are powerful, their flexibility comes at a cost: they can overfit small datasets and often require careful tuning (e.g., choosing the kernel bandwidth) to balance bias and variance. Nonetheless, they are invaluable for exploratory analysis and applications where minimal assumptions are desired.\nDid I also mention the curse of dimensionality?! You will quickly fall into its trap with even moderate number of variables. Yes, there is just so much you can do without adding some structure to your models."
  },
  {
    "objectID": "blog/three-classes-stat-models.html#an-example",
    "href": "blog/three-classes-stat-models.html#an-example",
    "title": "The Three Classes of Statistical Models",
    "section": "An Example",
    "text": "An Example\nWe illustrate the distinctions between parametric, semiparametric, and nonparametric models using a toy example. We generate \\(1,000\\) observations of\n\\[Y=sin(X)+\\epsilon,\\]\nwhere \\(X\\) is uniformly distributed and is normally distributed noise. We then model the relationship between \\(Y\\) and \\(X\\) using three approaches.\n\nRPython\n\n\n# clear workspace and load libraries\nrm(list=ls())\nset.seed(1988)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nx &lt;- runif(1000, 0, 10)  # Uniformly distributed x\ny &lt;- sin(x) + rnorm(1000, mean = 0, sd = 0.3)  # Non-linear relationship with noise\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit models\nlinear_model &lt;- lm(y ~ x, data = data)\nloess_model &lt;- loess(y ~ x, data = data, span = 0.3)\n\n# Piecewise linear model\nsplit_point &lt;- median(data$x)\ndata &lt;- data %&gt;%\n  mutate(split_group = ifelse(x &lt;= split_point, \"first_half\", \"second_half\"))\n\nlinear1 &lt;- lm(y ~ x, data = filter(data, split_group == \"first_half\"))\nlinear2 &lt;- lm(y ~ x, data = filter(data, split_group == \"second_half\"))\n\n# Predictions\ndata &lt;- data %&gt;%\n  mutate(pred_linear = predict(linear_model, newdata = data),\n         pred_loess = predict(loess_model, newdata = data))\n\npiecewise_preds &lt;- bind_rows(\n  data.frame(x = filter(data, split_group == \"first_half\")$x,\n             y = predict(linear1, newdata = filter(data, split_group == \"first_half\"))),\n  data.frame(x = filter(data, split_group == \"second_half\")$x,\n             y = predict(linear2, newdata = filter(data, split_group == \"second_half\")))\n) %&gt;%\n  arrange(x)\n\n# Plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  geom_line(aes(y = pred_linear, color = \"Parametric (Linear)\"), linewidth = 1) +\n  geom_line(aes(y = pred_loess, color = \"Nonparametric (Loess)\"), linewidth = 1) +\n  geom_line(data = piecewise_preds, aes(x = x, y = y, color = \"Semiparametric (Piecewise Linear)\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Parametric (Linear)\" = \"red\",\n                                \"Nonparametric (Loess)\" = \"blue\",\n                                \"Semiparametric (Piecewise Linear)\" = \"green\")) +\n  labs(title = \"Bivariate Scatterplot with Regression Fits\",\n       x = \"X\", y = \"Y\", color = \"Model Type\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = c(0.8, 0.2))\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nnp.random.seed(1988)\n\nx = np.random.uniform(0, 10, 1000)\ny = np.sin(x) + np.random.normal(0, 0.3, 1000)\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Linear model\nlinear_model = LinearRegression()\nlinear_model.fit(data[['x']], data['y'])\ndata['pred_linear'] = linear_model.predict(data[['x']])\n\n# Loess model\nloess_result = lowess(data['y'], data['x'], frac=0.3, return_sorted=True)\ndata['pred_loess'] = np.interp(data['x'], loess_result[:, 0], loess_result[:, 1])\n\n# Piecewise linear\nsplit_point = np.median(data['x'])\ndata['split_group'] = np.where(data['x'] &lt;= split_point, 'first_half', 'second_half')\n\nfirst_half = data[data['split_group'] == 'first_half'].copy()\nsecond_half = data[data['split_group'] == 'second_half'].copy()\n\nlinear1 = LinearRegression()\nlinear1.fit(first_half[['x']], first_half['y'])\nfirst_half['pred_piecewise'] = linear1.predict(first_half[['x']])\n\nlinear2 = LinearRegression()\nlinear2.fit(second_half[['x']], second_half['y'])\nsecond_half['pred_piecewise'] = linear2.predict(second_half[['x']])\n\npiecewise_preds = pd.concat([first_half[['x', 'pred_piecewise']],\n                             second_half[['x', 'pred_piecewise']]]).sort_values('x')\n\n# Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='x', y='y', data=data, alpha=0.6, label='Data')\nplt.plot(data.sort_values('x')['x'], data.sort_values('x')['pred_linear'], color='red', label='Parametric (Linear)', linewidth=1)\nplt.plot(data.sort_values('x')['x'], data.sort_values('x')['pred_loess'], color='blue', label='Nonparametric (Loess)', linewidth=1)\nplt.plot(piecewise_preds['x'], piecewise_preds['pred_piecewise'], color='green', label='Semiparametric (Piecewise Linear)', linewidth=1)\n\nplt.title('Bivariate Scatterplot with Regression Fits', fontsize=14)\nplt.xlabel('X', fontsize=12)\nplt.ylabel('Y', fontsize=12)\nplt.legend(title='Model Type', loc='lower right')\nplt.grid(True)\nplt.show()\n\n\n\nA parametric model, such as linear regression (although it could also be quadratic or a higher order polynomial), provides a simple, interpretable approximation but may miss crucial aspects of the true relationship. A “semiparametric” model, like piecewise linear regression, offers greater flexibility by allowing for changes in slope, capturing some curvature while maintaining a degree of interpretability. (One can cast this piecewise linear model as a parametric one, but for simplicity’s sake let’s go with this uncommon and imprecise definition of semiparametric.) Finally, a nonparametric model, such as LOESS, provides the most flexible representation, closely following the underlying sinusoidal pattern but potentially leading to overfitting.\n\nYou should not be surprised. This example demonstrates how the choice of model class significantly impacts the flexibility and interpretability of the fitted relationship. Do not take this example too seriously, it merely serves to illustrate the varying degree of complexity of statistical models."
  },
  {
    "objectID": "blog/three-classes-stat-models.html#bottom-line",
    "href": "blog/three-classes-stat-models.html#bottom-line",
    "title": "The Three Classes of Statistical Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nParametric models impose the strongest assumptions and require the least amount of data. These are most models employed in practice. Think of linear regression.\nSemiparametric models strike balance between flexibility and interpretation while allowing for flexible relationships in the data. Think of (non-Augmented) Inverse Propensity Score Weighting.\nNonparametric models are flexible and data-hungry. They allow for flexible associations between your variables. Think of a histrogram or kernel density."
  },
  {
    "objectID": "blog/jackknife-vs-bootstrap.html#background",
    "href": "blog/jackknife-vs-bootstrap.html#background",
    "title": "Jackknife vs. Bootstrap: A Tale of Two Resamplers",
    "section": "Background",
    "text": "Background\nIf you’ve ever dived into resampling methods, you’ve likely come across the jackknife and the bootstrap. They both aim to help us estimate uncertainty or bias without relying on strong parametric assumptions. And they sound similar: both create new datasets from the original one, and both involve recalculating the estimator across these datasets. So, what made the bootstrap, developed decades after the jackknife, such a big deal? Aren’t they too similar to be considered separate methods and have distinct properties?\nThis article walks through the key intuition behind these methods and explains why the bootstrap was a genuine innovation."
  },
  {
    "objectID": "blog/jackknife-vs-bootstrap.html#notation",
    "href": "blog/jackknife-vs-bootstrap.html#notation",
    "title": "Jackknife vs. Bootstrap: A Tale of Two Resamplers",
    "section": "Notation",
    "text": "Notation\nWe observe a sample of data points \\(X_1, X_2, \\dots, X_n\\), and we’re interested in an estimator \\(\\hat{\\theta} = \\hat{\\theta}(X_1, \\dots, X_n)\\) such as the mean, median, or a regression coefficient. Our goal is to understand and quantify the variability or bias of \\(\\hat{\\theta}\\)."
  },
  {
    "objectID": "blog/jackknife-vs-bootstrap.html#a-closer-look",
    "href": "blog/jackknife-vs-bootstrap.html#a-closer-look",
    "title": "Jackknife vs. Bootstrap: A Tale of Two Resamplers",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nSmooth and Non-smooth Data Quantities\nWhat does it mean for a quantity to be smooth? A smooth quantity is one that responds gradually to small changes in the data—technically, it’s a statistic that is continuous or even differentiable as a function of the data points. The mean is a classic example: if you nudge any individual value slightly, the overall mean shifts just a little. In contrast, non-smooth quantities like the median can change abruptly; removing or altering a single observation may cause the median to jump, especially in small samples. This matters because the jackknife estimates bias and variance by systematically leaving out one observation at a time, and it performs best when the estimator varies smoothly with the data. When that assumption breaks, as it does with non-smooth statistics, the jackknife can give misleading results—whereas the bootstrap often still performs well.\n\n\nThe Jackknife: Systematic Deletion\nThe jackknife, introduced by Quenouille and popularized by Tukey in the 1950s, works by systematically leaving out one observation at a time. For each \\(i\\), we compute \\(\\hat{\\theta}^{(-i)}\\), the estimator based on the dataset with the \\(i\\)th observation removed.\nThis gives us \\(n\\) estimates to work with: \\[\n\\hat{\\theta}^{(-1)}, \\hat{\\theta}^{(-2)}, \\dots, \\hat{\\theta}^{(-n)}\n\\]\nThese can be used to estimate the bias or variance of \\(\\hat{\\theta}\\). But the method assumes that the estimator behaves “smoothly” as data points change—a property that fails for medians, quantiles, and many modern estimators.\n\n\nThe Bootstrap: Simulating the Sampling Distribution\nThe bootstrap, invented by Bradley Efron in 1979, made a conceptual leap: instead of deleting one observation at a time, draw samples of size \\(n\\) from the data, with replacement. Each resample is like a new dataset: \\[\nX_1^*, X_2^*, \\dots, X_n^* \\sim \\text{Empirical distribution of } \\{X_1, \\dots, X_n\\}\n\\]\nCompute \\(\\hat{\\theta}^*\\) for each resample. Repeat this many times—hundreds or thousands—and you get a full approximation of the sampling distribution of \\(\\hat{\\theta}\\).\nThis approach doesn’t rely on linearity or smoothness. It works for medians, maximums, machine learning models—you name it. It does have limitations though. In an earlier article I discussed some of the situations in which the bootstrap fails. Fun fact: Bradley Efron is still a professor of statistics at Stanford and can often be spotted in the front row at department seminars.\n\n\nThe Conceptual Difference\n\nJackknife: Analytic, leaves out one data point at a time.\nBootstrap: Simulation-based, creates synthetic datasets by resampling with replacement.\n\nThe bootstrap became feasible thanks to improvements in computing. It provided a practical, general-purpose way to get confidence intervals and bias corrections for almost any estimator—not just the well-behaved ones."
  },
  {
    "objectID": "blog/jackknife-vs-bootstrap.html#bottom-line",
    "href": "blog/jackknife-vs-bootstrap.html#bottom-line",
    "title": "Jackknife vs. Bootstrap: A Tale of Two Resamplers",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe jackknife is elegant but limited; it works best for linear, smooth estimators.\nThe bootstrap is flexible and simulation-based, requiring no assumptions about estimator smoothness.\nBootstrap’s strength lies in approximating the full sampling distribution—even for complex estimators.\nEfron’s insight: use the data to simulate the data-generating process."
  },
  {
    "objectID": "blog/jackknife-vs-bootstrap.html#where-to-learn-more",
    "href": "blog/jackknife-vs-bootstrap.html#where-to-learn-more",
    "title": "Jackknife vs. Bootstrap: A Tale of Two Resamplers",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nA great place to start is Efron and Tibshirani’s classic An Introduction to the Bootstrap. For those seeking for a technical challenge, look at advanced resampling chapters in texts like Wasserman’s All of Statistics or Hastie and Efron’s Computer Age Statistical Inference. You can never really go wrong with the latter two books."
  },
  {
    "objectID": "blog/jackknife-vs-bootstrap.html#references",
    "href": "blog/jackknife-vs-bootstrap.html#references",
    "title": "Jackknife vs. Bootstrap: A Tale of Two Resamplers",
    "section": "References",
    "text": "References\nEfron, B. (1992). Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics: Methodology and distribution (pp. 569-593). New York, NY: Springer New York.\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press.\nQuenouille, M. H. (1956). Notes on bias in estimation. Biometrika, 43(3/4), 353-360.\nTukey, J. (1958). Bias and confidence in not quite large samples. Ann. Math. Statist., 29, 614.\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media."
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html#background",
    "href": "blog/column-sampling-bootstrap.html#background",
    "title": "Column-Sampling Bootstrap?",
    "section": "Background",
    "text": "Background\nThe bootstrap is a versatile resampling technique traditionally focused on rows. Let’s add a twist to the plain vanilla bootstrap. Imagine you have a wide dataset—many variables but few rows—and want to test the statistical significance of a correlation between two variables. An example is a genetic dataset with thousands of columns (genetic information and outcomes) but a limited number of rows (patients). Can you use the bootstrap to determine if the correlation between a specific gene-outcome pair is statistically significant?\nOne creative approach is resampling columns instead of rows, generating a distribution of correlation coefficients to assess the significance of your observed correlation."
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html#a-closer-look",
    "href": "blog/column-sampling-bootstrap.html#a-closer-look",
    "title": "Column-Sampling Bootstrap?",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nHere’s the basic algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRandomly sample columns from your dataset with replacement to create fake dataset.\nCompute their correlation coefficient.\nRepeat this many times.\nCompare your observed correlation to the distribution of these synthetic correlations.\nDeclare statistical significance if the observed correlation appears as an “outlier” in this synthetic distribution.\n\n\n\nThis approach allows you to explore a large number of possible correlations in a computationally efficient way. But does it actually work? Let’s unpack the key considerations.\nThe column-sampling bootstrap is most valuable when a dataset has many columns but too few rows for traditional bootstrap methods. The abundance of columns provides a rich sampling landscape. The goal is determining whether a correlation is significantly stronger or weaker than what might occur by chance. Let’s simplify the problem and ignore any issues stemming from being unable to estimate the correlation coefficients well enough.\n\n\nProblems\nHowever, several critical challenges exist. The method assumes columns are independent and identically distributed (i.i.d.), which rarely holds in practice. Columns often represent related variables—like gene measurements or interconnected phenomena—and these dependencies can bias resampled correlations. Moreover, by resampling columns, you ignore row-level relationships, such as connections in time series or grouped data (like patients from the same household).\nInterpreting the null distribution presents another significant challenge. Synthetic correlation coefficients generated through column resampling might not represent a meaningful null hypothesis. If your dataset contains highly correlated features, the null distribution could shift, potentially leading to misleading conclusions. Unlike traditional bootstrapping—where samples reflect a subpopulation—this method lacks that fundamental connection.\n\n\nThe Verdict\nWhile the column-sampling bootstrap is an intriguing concept, it will likely prove useful only in very specific, carefully constrained settings."
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html#an-example",
    "href": "blog/column-sampling-bootstrap.html#an-example",
    "title": "Column-Sampling Bootstrap?",
    "section": "An Example",
    "text": "An Example\nWhile we should be skeptical of the column-sampling bootstrap in practical applications, it can be instructive to see how we might implement it.\nBelow is a sample R and python code illustrating the main concept. We begin with setting up a synthetic dataset.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\ndata &lt;- as.data.frame(matrix(rnorm(1000), nrow = 50, ncol = 20))\nobserved_correlation &lt;- cor(data[[1]], data[[2]])\n\n# Perform the resampling\nn_bootstrap &lt;- 1000  # Number of bootstrap iterations\nn_columns &lt;- ncol(data)  # Total number of columns in the dataset\nbootstrap_correlations &lt;- numeric(n_bootstrap)\n\nfor (i in 1:n_bootstrap) {\n  resampled_columns &lt;- sample(1:n_columns, size = n_columns, replace = TRUE)\n  resampled_data &lt;- data[, resampled_columns]\n  bootstrap_correlations[i] &lt;- cor(resampled_data[[1]], resampled_data[[2]])\n}\n\n# Test the significance of the observed correlation\np_value &lt;- mean(abs(bootstrap_correlations) &gt;= abs(observed_correlation))\n\n# Print the results\ncat(\"Observed Correlation:\", observed_correlation, \"\\n\")\n&gt; Observed Correlation: 0.05758855 \ncat(\"P-value:\", p_value, \"\\n\")\n&gt; P-value: 0.676 \n\n\nimport numpy as np\nnp.random.seed(1988)\n\n# Generate synthetic dataset\ndata = np.random.normal(size=(50, 20))  # 50 rows, 20 columns\nobserved_correlation = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n\n# Perform the resampling\nn_bootstrap = 1000  # Number of bootstrap iterations\nn_columns = data.shape[1]  # Total number of columns in the dataset\nbootstrap_correlations = []\n\nfor _ in range(n_bootstrap):\n    # Resample columns with replacement\n    resampled_columns = np.random.choice(n_columns, size=n_columns, replace=True)\n    resampled_data = data[:, resampled_columns]\n    # Compute correlation between the first two columns of the resampled data\n    bootstrap_correlations.append(np.corrcoef(resampled_data[:, 0], resampled_data[:, 1])[0, 1])\n\n# Test the significance of the observed correlation\nbootstrap_correlations = np.array(bootstrap_correlations)\np_value = np.mean(np.abs(bootstrap_correlations) &gt;= np.abs(observed_correlation))\n\n# Print the results\nprint(\"Observed Correlation:\", observed_correlation)\nprint(\"P-value:\", p_value)\n\n\n\nThe observed correlation is quite low and equal to \\(0.58\\). Its associated p-value is \\(0.676\\), consistent with the value not being statistically significant."
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html#bottom-line",
    "href": "blog/column-sampling-bootstrap.html#bottom-line",
    "title": "Column-Sampling Bootstrap?",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe column-sampling bootstrap is a thought-provoking twist on traditional resampling techniques that leverages the width of your dataset.\nWhile it offers computational efficiency and flexibility, its reliance on the i.i.d. assumption and potential to overlook row-level dependencies highlight the need for careful application.\nThe column-sampling bootrap should not be your go-to method to assess statistical significance."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#background",
    "href": "blog/bayesian-ab-tests.html#background",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Background",
    "text": "Background\nImagine you’re a data scientist evaluating an A/B test of a new recommendation algorithm. The results show a modest but promising 0.5% lift in conversion rate—up from \\(8\\%\\) to \\(8.5\\%\\) across \\(20,000\\) users—along with a \\(p\\)-value of \\(0.06\\). Given historical data, the improvement seems realistic, the cost to implement is low, and the projected revenue impact is substantial. Yet, the conventional analysis compels you to “fail to reject the null hypothesis” simply because the p-value doesn’t meet an arbitrary \\(0.05\\) threshold.\nTraditional A/B testing approaches (randomized controlled trials or RCTs) rely primarily on frequentist methods like \\(t\\)-tests or chi-squared tests to detect differences between treatment and control groups. For more precision, analysts often adjust for covariates in linear models. After weeks or months of testing, however, these methods often boil down to a single outcome: the \\(p\\)-value. This binary interpretation—significant or not—can obscure valuable insights, as seen in the example above.\nBayesian statistics, by contrast, offers a more nuanced and potentially more informative alternative. In this article, we’ll walk through a practical example, discuss the implementation details, and dive into the Bayesian framework’s mathematical foundations for experimentation. One of the core advantages of the Bayesian approach is its ability to produce a full posterior distribution, offering a richer view of the experiment’s outcomes beyond just point estimates.\nThis article assumes a familiarity with Bayesian fundamentals like priors, posteriors, and conjugate distributions. If you’d like a refresher, see the References section below."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#notation",
    "href": "blog/bayesian-ab-tests.html#notation",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Notation",
    "text": "Notation\nLet’s establish our notation for a binary intervention randomized experiment:\n\n\\(T \\in {0,1}\\): Treatment indicator\n\\(N_T\\): Number of units in treatment group\n\\(N_C\\): Number of units in control group\n\\(N = N_T + N_C\\): Total sample size\n\\(X_T\\): Number of “successes” in treatment group\n\\(X_C\\): Number of “successes” in control group\n\\(Y\\): Success rate (e.g., conversion rate, employment status)."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#a-closer-look",
    "href": "blog/bayesian-ab-tests.html#a-closer-look",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "A Closer Look",
    "text": "A Closer Look\nWe are interested in making inferences about the treatment effect, \\(\\tau\\), of the intervention \\(T\\) on the success rate \\(Y\\). In Bayesian terms, this means we seek the posterior distribution of \\(\\tau\\). Once we obtain this distribution or can draw observations from it, we can calculate various statistics to summarize the impact of the intervention, potentially providing a richer understanding of its effects.\nThe Bayesian methodology of analyzing experiments proceeds in four steps.\n\nStep 1: Specify the Prior Distribution\nWe begin by choosing the prior distributions for Y. The prior distribution represents our beliefs about the parameter before seeing the data. It is usually informed by previous A/B testing experience combined with deep context knowledge. For example, we might believe that the treatment effect is around two percentage points (\\(\\hat{\\tau}=0.02\\)) with some uncertainty around it.\nGiven our binary outcome, the Beta distribution serves as a natural conjugate prior: \\[ Y_i \\sim \\text{Beta} (\\alpha_i, \\beta_i), \\hspace{.5cm} i \\in \\{T,C\\}.  \\]\n\n\nStep 2: Specify the Likelihood Function\nFor binary outcomes, we model the data using a Binomial distribution:\n\\[X_i|Y_i \\sim \\text{Binomial}(N_i, Y_i), \\hspace{.5cm} i\\in\\{T,C\\}.  \\]\nThis choice reflects the inherent structure of our data: counting successes in a fixed number of trials. We are now ready to use the Bayes rule to arrive at the posterior distributions.\n\n\nStep 3: Posterior Distributions Derivation\nThanks to conjugacy between Beta and Binomial distributions, we obtain closed-form posterior distributions:\n\\[ Y_i | X_i \\sim \\text{Beta}(\\alpha_i+X_i, \\beta_i+N_i-X_i), \\hspace{.5cm} i\\in\\{T,C\\}  .\\]\nThis mathematical convenience allows us to avoid more complex numerical methods like MCMC sampling. We can now even plot the two posterior distributions and visually asses their differences.\n\n\nStep 4: Inference and Decision Making\nThe Bayesian framework enables rich analysis beyond traditional frequentist-style hypothesis testing. Here are some examples:\nProbability of Positive Impact\nWe can calculate the probability that the outcomes for the treatment group are higher than those of the control group. In closed form, we have:\n\\[ P(Y_T&gt;Y_C|X_T,X_C) = \\frac{1}{N}\\sum_i\\mathbf{1}(Y_{T,i}&gt;Y_{C,i}),\\]\nwhere \\(\\mathbf{1}(\\cdot)\\) is the indicator function. This is simply share of observations for which \\(Y_T &gt;Y_C\\).\nAverage Treatment Effect\nAnother potential example of an object of interest is the Average Treatment Effect (ATE):\n\\[ ATE = \\frac{1}{N_T}\\sum_{i \\in T}Y_{T,i} -  \\frac{1}{N_C}\\sum_{i \\in C}Y_{C,i} \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\n\\[ ATE \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\nTo summarize, here is the high-level algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nSpecify the Prior Distribution\nSpecify the Likelihood Function\nPosterior Distributions Derivation\nInference and Decision Making\n\n\n\nAnd that’s it. You see how with the cost of additional assumptions we get much more than a single \\(p\\)-value, and that’s where much of the appeal of this approach lies.\n\n\nAdvantages\nOverall, Bayesian thinking entails some compelling advantages:\n\nIncorporation of Prior Information. It allows you to incorporate prior knowledge or expert opinion into the analysis. This is particularly useful when historical data or domain expertise is available.\nProbabilistic Interpretation: It provides direct probabilistic interpretations of the results, such as the probability that one variant is better than another. This may be more intuitive than frequentist p-values, which are often misinterpreted.\nHandling of Small Sample Sizes: It tends to perform better with small sample sizes because they incorporate prior distributions, which can regularize estimates and prevent overfitting.\nContinuous Learning: As new data comes in, Bayesian methods provide a natural way to update the posterior distribution, leading to continuous learning and adaptation.\n\nThe downsides are mostly related to the choice of prior distribution. In settings where there is a lack of expert knowledge, this Bayesian approach to experimentation might not be very attractive. Computation can also be an issue in more complex settings."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#an-example",
    "href": "blog/bayesian-ab-tests.html#an-example",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "An Example",
    "text": "An Example\nLet’s code an example in R and python. We start with generating some fake data and selecting parameters for the prior distributions.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Data: Number of successes and total observations for T and C\nn_T &lt;- 1000\nx_T &lt;- 70\nn_C &lt;- 900\nx_C &lt;- 50\n\n# Prior parameters for the Beta distribution\nalpha_T &lt;- 1\nbeta_T &lt;- 1\nalpha_C &lt;- 1\nbeta_C &lt;- 1\n\n# Posterior parameters\nposterior_alpha_T &lt;- alpha_T + x_T\nposterior_beta_T &lt;- beta_T + n_T - x_T\nposterior_alpha_C &lt;- alpha_C + x_C\nposterior_beta_C &lt;- beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T &lt;- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C &lt;- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Sample from the posterior distributions\nposterior_obs_T &lt;- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C &lt;- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Estimate the probability that T is better than C\nprob_T_better &lt;- mean(posterior_obs_T &gt; posterior_obs_C)\nprint(paste(\"Probability that T is better than C:\", round(prob_T_better, digit=3)))\n\n# Estimate the average treatment effect\ntreatment_effect &lt;- mean(posterior_obs_T - posterior_obs_C)\nprint(paste(\"Average change in Y b/w T and C:\", round(treatment_effect, digit=3)))\n\ntreatment_effect_limit &lt;- (alpha_T + x_T)/(alpha_T + beta_T + n_T) - (alpha_C + x_C)/(alpha_C + beta_C + n_C)\nprint(treatment_effect_limit)\n\n# we can even plot both posteriors distributions.\ndf &lt;- data.frame(\n  y = c(posterior_obs_T, posterior_obs_C),\n  group = factor(rep(c(\"T\", \"C\"), each = 10000))\n)\n\nggplot(df, aes(x = y, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Posterior Distributions of Outcomes\",\n       x = \"Y = 1\", y = \"Density\") +\n  theme_minimal()\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.random.seed(1988)\n\n# Data: Number of successes and total observations for T and C\nn_T = 1000\nx_T = 70\nn_C = 900\nx_C = 50\n\n# Prior parameters for the Beta distribution\nalpha_T = 1\nbeta_T = 1\nalpha_C = 1\nbeta_C = 1\n\n# Posterior parameters\nposterior_alpha_T = alpha_T + x_T\nposterior_beta_T = beta_T + n_T - x_T\nposterior_alpha_C = alpha_C + x_C\nposterior_beta_C = beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T = np.random.beta(posterior_alpha_T, posterior_beta_T, 10000)\nposterior_obs_C = np.random.beta(posterior_alpha_C, posterior_beta_C, 10000)\n\n# Estimate the probability that T is better than C\nprob_T_better = np.mean(posterior_obs_T &gt; posterior_obs_C)\nprint(f\"Probability that T is better than C: {prob_T_better:.3f}\")\n\n# Estimate the average treatment effect\ntreatment_effect = np.mean(posterior_obs_T - posterior_obs_C)\nprint(f\"Average change in Y b/w T and C: {treatment_effect:.3f}\")\n\n# Plot posterior distributions\nplt.figure(figsize=(8, 6))\nsns.kdeplot(posterior_obs_T, fill=True, label=\"T\", alpha=0.5)\nsns.kdeplot(posterior_obs_C, fill=True, label=\"C\", alpha=0.5)\nplt.title(\"Posterior Distributions of Outcomes\")\nplt.xlabel(\"Y = 1\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n\n\n\nHere are the two posterior distributions.\n\nThere is also a specialized bayesAB package in R. It produces some cool charts, so I definitely recommend giving it a try.\nSoftware Package: bayesAB."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#bottom-line",
    "href": "blog/bayesian-ab-tests.html#bottom-line",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBayesian inference offers a compelling alternative to the traditional methods based on frequentist statistics.\nThe main idea rests on incorporating prior information on the success rates in the treatment and control group as a starting point.\nAdvantages of bayesian methods include incorporating prior information, providing probabilistic interpretations, handling small sample sizes better, and enabling continuous learning.\nThe main challenge is the choice of prior distribution, which can be difficult without expert knowledge."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#where-to-learn-more",
    "href": "blog/bayesian-ab-tests.html#where-to-learn-more",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAccessible introductions to the world of Bayesian inference are common. An example is Will Kurt’s book “Bayesian Statistics the Fun Way“. See also the papers I cite below. As almost everything else, Google is also a great starting point."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#references",
    "href": "blog/bayesian-ab-tests.html#references",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "References",
    "text": "References\nDeng, A. (2015, May). Objective bayesian two sample hypothesis testing for online controlled experiments. In Proceedings of the 24th International Conference on World Wide Web (pp. 923-928).\nKamalbasha, S., & Eugster, M. J. (2021). Bayesian A/B testing for business decisions. In Data Science–Analytics and Applications: Proceedings of the 3rd International Data Science Conference–iDSC2020 (pp. 50-57). Springer Fachmedien Wiesbaden.\nKurt, Will. Bayesian statistics the fun way: understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. No Starch Press, 2019.\nStevens, N. T., & Hagar, L. (2022). Comparative probability metrics: using posterior probabilities to account for practical equivalence in A/B tests. The American Statistician, 76(3), 224-237."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#background",
    "href": "blog/flavors-het-treatment-effects.html#background",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Background",
    "text": "Background\nNumerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods – the so-called \\(S\\)-, \\(T\\)-, \\(X\\)- and \\(R\\)-learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as the lasso, gradient boosting, or even neural networks. In some clever sense, \\(S-T-X-R\\) is the \\(A-B-C\\) of heterogeneous treatment effect estimation.\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The CausalML Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#notation",
    "href": "blog/flavors-het-treatment-effects.html#notation",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Notation",
    "text": "Notation\nAs usual, let’s begin by setting some mathematical notation. I use D to denote a binary treatment indicator, \\(Y\\) is the observed outcome and \\(X\\) is a covariate of interest. The potential outcomes under each treatment state are \\(Y(0)\\) and \\(Y(1)\\), and \\(p\\) is the (conditional) probability of assignment into the treatment (i.e., the propensity score).\nThe average treatment effect is then the mean difference in potential outcomes across all units:\n\\[ATE = E[Y(1)-E(0)].\\]\nInterest is, instead, in the ATE for units with values \\(X=x\\) which I refer to as the heterogeneous treatment effect, \\(HTE(X)\\):\n\\[HTE(X) = E[Y(1)-E(0)|X].\\]\nIt is also helpful to define the conditional outcome functions under each treatment state:\n\\[\\mu(X,d) = E[Y(d)|X].\\]\nIt then follows that \\(HTE(X)\\) can also be expressed as:\n\\[HTE(X) = \\mu(X,1) - \\mu(X,0).\\]"
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#a-closer-look",
    "href": "blog/flavors-het-treatment-effects.html#a-closer-look",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\nI will now briefly describe the four learners for heterogeneous treatment effects in ascending order of complexity.\n\n\\(S\\)(ingle) Learner\nThe idea behind the \\(S\\)-learner is to estimate a single outcome function \\(\\mu(X,D)\\) and then calculate \\(HTE(X)\\) by taking the difference in the predicted values between the units in the treatment and control groups.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nUse the entire sample to estimate \\(\\hat{\\mu}(X,D)\\).\nCompute \\(\\hat{HTE}(X)=\\hat{\\mu}(X,1)-\\hat{\\mu}(X,0)\\).\n\n\n\nThis is intuitive and fine. But the problem is that the treatment variable D might be excluded in step 1 when it is not highly correlated with the outcome. (Note that in observational data this does not necessarily imply a null treatment effect.) In this case, we cannot even move to step 2.\n\n\n\\(T\\)(wo) Learner\nThe \\(T\\)-learner solves the above problem by forcing the response models to include \\(D\\). The idea is to first estimate two separate (conditional) outcome functions – one for the treatment and one for the control and proceed similarly.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for \\(\\hat{\\mu}(X,1)\\).\nThen \\(\\hat{HTE}(X)=\\hat{\\mu}(X,1)- \\hat{\\mu}(X,0)\\)\n\n\n\nThis is better, but still not great. A potential problem is when there are different number of observations in the treatment and control groups. For instance, in the common case where the treatment group is much smaller, their \\(HTE(X)\\) will be estimated much less precisely than the that of the control group. When combining the two to arrive at a final estimate of \\(HTE(X)\\) the former should get a smaller weight than the latter. This is because the ML algorithms optimize for learning the \\(\\mu(\\cdot)\\) functions, and not the \\(HTE(X)\\) function directly.\n\n\n\\(X\\) Learner\nThe \\(X\\)-learner is designed to overcome the above concern. The procedure starts similarly to the \\(T\\)-learner but then weighs differently the \\(HTE(X)\\)’s for the treatment and control groups.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for \\(\\hat{\\mu}(X,1)\\).\nEstimate the unit-level treatment effect for the observations in the control group, \\(\\hat{\\mu}(X,1)-Y\\), and for the treatment group, \\(Y-\\hat{\\mu}(X,0)\\).\nCombine both estimates by weighing them using the predicted propensity score, \\(\\hat{p}\\): \\[HTE(X)=\\hat{p} \\times HTE(X|D=1) + (1-\\hat{p})\\times HTE(X|D=0).\\]\n\n\n\nHere \\(\\hat{p}\\) balances the uncertainty associated with the \\(HTE(X)\\)’s in each group and hence, this approach is particularly effective when there is a significant difference in the number of units between the two groups.\n\n\n\\(R\\)(obinson) Learner\nTo avoid getting too deep into technical details, I will not be describing the entire algorithm. The \\(R\\)-learner models both the outcome, and the propensity score and begins by computing unit-level predicted outcomes and treatment probabilities using cross-fitting and leave-one-out estimation. The key innovation is plugging these into a novel loss function featuring squared deviations of these predictions as well as a regularization term. This ensures “optimality” in learning the treatment effect heterogeneity."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#bottom-line",
    "href": "blog/flavors-het-treatment-effects.html#bottom-line",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nML methods offer a promising way of determining which groups of units experience differential response to treatments.\nI summarized four such model-agnostic methods – the \\(S\\)-, \\(T\\)-, \\(X\\)-, and \\(R\\)-learners.\nCompared to the simpler \\(S\\)- and \\(T\\)- learners, the \\(X\\)- and \\(R\\)-learners solve some common issues and are more attractive options in most settings."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#where-to-learn-more",
    "href": "blog/flavors-het-treatment-effects.html#where-to-learn-more",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nI have previously written on how ML can be useful in causal inference, more generally and how we can use the Lasso to estimate heterogeneous treatment effects. Hu (2022) offers a detailed summary of a bunch of ML methods for HTE estimation. A Statistical Odds and Ends blog post describes the \\(S\\)-, \\(T\\)- and \\(X\\)-learners and contains useful advice on when each of them is preferable. Chapter 21 of Causal Inference for the Brave and True also discusses this material and provides useful examples."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#references",
    "href": "blog/flavors-het-treatment-effects.html#references",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "References",
    "text": "References\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\nKünzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165. Nie,\nXie, N., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I develop and apply causal inference and machine learning tools to solve complex and ambiguous problems with data.\n\nI’m currently a Staff Data Scientist at Adobe.\nPreviously, I held postdoctoral positions at UC Berkeley and Stanford. My PhD dissertation (2017) combined natural experiments with innovative statistical methods to study the wage/employment effects of immigration and the consequences of double-shift schooling systems.\nThroughout my career, I’ve been fortunate to work closely with mentors who were students of Nobel Prize laureates. As a way to give back, I maintain a blog that makes cutting-edge academic research in data science accessible to a broader audience.\n\nMy academic research has been published in leading peer-reviewed journals in labor economics, political science, medicine, and general science and has been featured by most major media outlets such as The New York Times, The Washington Post, and The Wall Street Journal, among many others.\nMy children’s book on causal inference reached \\(\\#2\\) on Amazon’s New Releases in Probability and Statistics list.\n\nOutside of work, I’m an avid mushroom picker and mountain biker—but most of all, I love spending time with my family away from concrete. Originally from Asenovgrad, Bulgaria, I now live in the San Francisco Bay Area with my wife and our son, Luca.\n\nOh, and my Erdős number is \\(8\\)."
  }
]