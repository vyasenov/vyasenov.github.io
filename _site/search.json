[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "‚ÄúThe Labor Market Effects of Mexican Repatriations: Longitudinal Evidence from the 1930s‚Äù (w/ JongKwan Lee and Giovanni Peri)\n\nJournal of Public Economics (2022) 25, 104558; latest version; replication package\nMedia: The Washington Post,The New York Times, The Economist, Bloomberg, This American Life, The Guardian, Forbes\nSummary: A large-scale program of repatriating Mexicans and Mexican Americans during the Great Depression did not achieve its goal ‚Äì more jobs for native workers.\n\n‚ÄúThe Role of Labor Market Institutions in the Impact of Immigration on Wages and Employment‚Äù (w/ Mette Foged and Linea Hasager)\n\nScandinavian Journal of Economics (2022) 124(1), 164-213; latest version; replication package\nSummary: Labor market institutions may reduce the wage inequality effect of immigration while weakening the associated economic benefits.\n\n‚ÄúDoes Halting Refugee Resettlement Reduce Crime? Evidence from the US Refugee Ban‚Äù (2020) (w/ Daniel Masterson)\n\nAmerican Political Science Review (2021) 115(3), 1066-1073; latest version; replication package; appendix\nMedia: Bloomberg, The World Bank Blog\nSummary: The refugee ban introduced by Trump in early 2017 did not lower local crime rates.\n\n‚ÄúAssociation between Health Care Utilization and Immigration Enforcement Events in San Francisco‚Äù (w/ J. Hainmueller, M. Hotard, D. Lawrence, L. Gottlieb, and J. Torres)\n\nJAMA Network Open (2020) 3(11), e2025065-e2025065; replication code; appendix; pre-analysis plan\nSummary: Healthcare utilization among likely undocumented immigrants in San Francisco did not respond to local ICE raids or anti-immigration rhetoric and policies.\n\n‚ÄúPublic Health Insurance Expansion for Immigrant Children and Interstate Migration of Low-income Immigrants‚Äù (w/ D. Lawrence, F. Mendoza, and J. Hainmueller)\n\nJAMA Pediatrics (2020) Vol 174(1), pp22-28.; replication package\nMedia: Vox, Reuters, Yahoo! News\nSummary: Public health insurance expansion for recent immigrants did not lead to an increased interstate in-migration among eligible foreign-born.\n\n‚ÄúStandardizing the Fee-Waiver Application Increased Naturalization Rates of Low-Income Immigrants‚Äù (w/ M. Hotard, D. Lawrence, J. Hainmueller, and D. Laitin)\n\nPNAS Vol 116(34), pp16768-16772 (2019); latest version; replication package\nMedia: The Washington Post, Fortune\nSummary: Standardizing the fee waiver for citizenship applications raised naturalization rates among low-income immigrants.\n\n‚ÄúDoes Schedule Irregularity Affect Productivity? Evidence from Random Assignment into College Classes‚Äù (w/ Lester Lusher and Phuc Luong)\n\nLabour Economics Vol 60, pp115-128 (2019); latest version; replication package\nSummary: More volatile school start schedules throughout the week do not lead to lower college test scores.\n\n‚ÄúThe Labor Market Effects of a Refugee Wave: Synthetic Control Method Meets the Mariel Boatlift‚Äù (w/ Giovanni Peri)\n\nJournal of Human Resources\nMedia: The Wall Street Journal, The Economist, Vox, The Atlantic, Bloomberg (#1, #2){target=‚Äú_blank‚Äù}, The Chicago Tribune, CBS News, Newsweek (#1, #2){target=‚Äú_blank‚Äù}, Business Insider, Yahoo! Finance\nSummary: The Mariel Boatlift of 1980 did not result in large, statistically detectable wage or employment changes among low-skilled Miamians.\n\n‚ÄúGender Performance Gaps: Quasi-Experimental Evidence on the Role of Gender Differences in Sleep Cycles‚Äù (w/ Lester Lusher)\n\nEconomic Inquiry Vol 56(1), pp252-262 (2018); latest version; replication package\nMedia: The Washington Post, The Independent\nSummary: Girls benefit from earlier school start times relative to boys, partially contributing to the observed gender performance gap.\n\n‚ÄúDouble-Shift Schooling and Student Success: Quasi-experimental Evidence from Europe‚Äù (w/ Lester Lusher)\n\nEconomics Letters Vol.139, pp36-39 (2016); latest version; replication package\nSummary: Students achieve slightly higher grades when taking classes in the morning compared to the afternoon."
  },
  {
    "objectID": "research.html#projects",
    "href": "research.html#projects",
    "title": "Research",
    "section": "",
    "text": "Cool Study (2023): Short description.\nAnother One (2022): Short description."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I develop and apply causal inference and machine learning tools to solve complex and ambiguous problems with data.\n\nI‚Äôm currently a Staff Data Scientist at Adobe.\nPreviously, I held postdoctoral positions at UC Berkeley and Stanford. My PhD dissertation (2017) combined natural experiments with innovative statistical methods to study the wage/employment effects of immigration and the consequences of double-shift schooling systems.\nThroughout my career, I‚Äôve been fortunate to work closely with mentors who were students of Nobel Prize laureates. As a way to give back, I maintain a blog that makes cutting-edge academic research in data science accessible to a broader audience.\n\nMy academic research has been published in leading peer-reviewed journals in labor economics, political science, medicine, and general science and has been featured by most major media outlets such as The New York Times, The Washington Post, and The Wall Street Journal, among many others.\nMy children‚Äôs book on causal inference reached \\(\\#2\\) on Amazon‚Äôs New Releases in Probability and Statistics list.\n\nOutside of work, I‚Äôm an avid mushroom picker and mountain biker‚Äîbut most of all, I love spending time with my family away from concrete. Originally from Asenovgrad, Bulgaria, I now live in the San Francisco Bay Area with my wife and our son, Luca.\n\nOh, and my Erd≈ës number is \\(8\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi there!",
    "section": "",
    "text": "While my real name is Vasil, most people call me Vasco. I am an applied statistician."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Staff Data Scientist, Adobe (2021 ‚Äì )\nData Scientist, Immigration Policy Lab, Stanford University (2020 ‚Äì )\nConsultant, Ministry of Employment, Copenhagen, Denmark (2017)\nResearch Intern, Ministry of Labor and Social Policy, Sofia, Bulgaria (2014)"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Research",
    "section": "",
    "text": "High School\nBachelor"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Research",
    "section": "Experience",
    "text": "Experience\n\nMinistry of X"
  },
  {
    "objectID": "cv.html#awards",
    "href": "cv.html#awards",
    "title": "CV",
    "section": "Awards",
    "text": "Awards\n\nUS 2050 Initiative Grant, Peter G. Peterson Foundation, and Ford Foundation (2018)\nEarly Career Research Grant, W.E. Upjohn Institute for Employment Research (2018)\nRay Beaumont Memorial Award, Economics Department, UC Davis (2017)\nExcellence in Joint Mathematics-Economics Major Award Nominee, UC San Diego (2012)"
  },
  {
    "objectID": "blog/second.html",
    "href": "blog/second.html",
    "title": "My Second Post",
    "section": "",
    "text": "This is my first blog post using Quarto. üéâ"
  },
  {
    "objectID": "blog/first.html",
    "href": "blog/first.html",
    "title": "My First Post",
    "section": "",
    "text": "This is my first blog post using Quarto. üéâ"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Advanced Topics in Statistical Data Science\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Two Types of Weights in Causal Inference\n\n\n\nweights\n\ncausal inference\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBinscatter: A New Visual Tool for Data Analysis\n\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFilling in Missing Data with MCMC\n\n\n\nmissing data\n\nMCMC\n\n\n\n\n\n\n\n\n\nJan 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Limits of Semiparametric Models: The Efficiency Bound\n\n\n\nstatistical inference\n\nsemiparametric models\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Limits of Nonparametric Models\n\n\n\nstatistical inference\n\nnonparametric models\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Limits of Parametric Models: The Cram√©r-Rao Bound\n\n\n\nstatistical inference\n\nparametric models\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Three Classes of Statistical Models\n\n\n\nstatistical models\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Delta Method: Simplifying Confidence Intervals for Complex Estimators\n\n\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nJan 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStein‚Äôs Paradox: A Simple Illustration\n\n\n\nstatistical inference\n\nparadox\n\n\n\n\n\n\n\n\n\nJan 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMutual Information: What, Why, How, and When\n\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Variables with Predefined Correlation\n\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nDec 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStratified Sampling with Continuous Variables\n\n\n\nrandomized experiments\n\ncausal inference\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nColumn-Sampling Bootstrap?\n\n\n\nbootstrap\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bootstrap and its Limitations\n\n\n\nbootstrap\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSimpson‚Äôs Paradox: A Simple Illustration\n\n\n\nparadox\n\ncausal inference\n\n\n\n\n\n\n\n\n\nDec 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCausation without Correlation\n\n\n\ncausal inference\n\ncorrelation\n\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting Methods: A Brief Overview\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Analysis of Randomized Experiments: A Modern Approach\n\n\n\nbayesian methods\n\nexperimentation\n\n\n\n\n\n\n\n\n\nOct 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeights in Statistical Analyses\n\n\n\nweights\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCausality without Experiments, Unconfoundedness, or Instruments\n\n\n\ncausal inference\n\ninstrumental variables\n\n\n\n\n\n\n\n\n\nAug 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFOCI: A New Variable Selection Method\n\n\n\nvariable selection\n\nmachine learning\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNonlinear Correlations and Chatterjee‚Äôs Coefficient\n\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Brief Introduction to Conformal Inference\n\n\n\nconformal inference\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Conformal Inference for Variable Importance in Machine Learning\n\n\n\nconformal inference\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNew Developments in False Discovery Rate\n\n\n\nmultiple testing\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nML-Based Regression Adjustments in Randomized Experiments\n\n\n\nmachine learning\n\nrandomized experiments\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Alphabet of Learners for Heterogeneous Treatment Effects\n\n\n\nmachine learning\n\nrandomized experiments\n\nheterogeneous treatment effects\n\n\n\n\n\n\n\n\n\nJul 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLasso for Heterogeneous Treatment Effects Estimation\n\n\n\nlasso\n\nheterogeneous treatment effects\n\ncausal inference\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of Machine Learning Methods in Causal Inference\n\n\n\nmachine learning\n\ncausal inference\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Variance of Propensity Score Matching Estimators\n\n\n\npropensity score\n\ncausal inference\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation is a Cosine\n\n\n\ncorrelation\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation is Not (Always) Transitive\n\n\n\ncorrelation\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLord‚Äôs Paradox: A Simple Illustration\n\n\n\ncorrelation\n\nparadox\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing in Linear Machine Learning Models\n\n\n\nhypothesis testing\n\nmachine learning\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Testing: Methods Overview (Part 1)\n\n\n\nmultiple testing\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing with Population Data\n\n\n\nhypothesis testing\n\nstatistical inference\n\n\n\n\n\n\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapping Confidence Intervals and Statistical (In)Significance\n\n\n\nstatistical inference\n\nhypothesis testing\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/third.html",
    "href": "blog/third.html",
    "title": "My Third Post",
    "section": "",
    "text": "This is my first blog post using Quarto. üéâ"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "",
    "text": "‚ÄúThe Labor Market Effects of Mexican Repatriations: Longitudinal Evidence from the 1930s‚Äù (w/ JongKwan Lee and Giovanni Peri)\n\nJournal of Public Economics (2022) 25, 104558; latest version; replication package\nMedia: The Washington Post,The New York Times, The Economist, Bloomberg, This American Life, The Guardian, Forbes\nSummary: A large-scale program of repatriating Mexicans and Mexican Americans during the Great Depression did not achieve its goal ‚Äì more jobs for native workers.\n\n‚ÄúThe Role of Labor Market Institutions in the Impact of Immigration on Wages and Employment‚Äù (w/ Mette Foged and Linea Hasager)\n\nScandinavian Journal of Economics (2022) 124(1), 164-213; latest version; replication package\nSummary: Labor market institutions may reduce the wage inequality effect of immigration while weakening the associated economic benefits.\n\n‚ÄúDoes Halting Refugee Resettlement Reduce Crime? Evidence from the US Refugee Ban‚Äù (2020) (w/ Daniel Masterson)\n\nAmerican Political Science Review (2021) 115(3), 1066-1073; latest version; replication package; appendix\nMedia: Bloomberg, The World Bank Blog\nSummary: The refugee ban introduced by Trump in early 2017 did not lower local crime rates.\n\n‚ÄúAssociation between Health Care Utilization and Immigration Enforcement Events in San Francisco‚Äù (w/ J. Hainmueller, M. Hotard, D. Lawrence, L. Gottlieb, and J. Torres)\n\nJAMA Network Open (2020) 3(11), e2025065-e2025065; replication code; appendix; pre-analysis plan\nSummary: Healthcare utilization among likely undocumented immigrants in San Francisco did not respond to local ICE raids or anti-immigration rhetoric and policies.\n\n‚ÄúPublic Health Insurance Expansion for Immigrant Children and Interstate Migration of Low-income Immigrants‚Äù (w/ D. Lawrence, F. Mendoza, and J. Hainmueller)\n\nJAMA Pediatrics (2020) Vol 174(1), pp22-28.; replication package\nMedia: Vox, Reuters, Yahoo! News\nSummary: Public health insurance expansion for recent immigrants did not lead to an increased interstate in-migration among eligible foreign-born.\n\n‚ÄúStandardizing the Fee-Waiver Application Increased Naturalization Rates of Low-Income Immigrants‚Äù (w/ M. Hotard, D. Lawrence, J. Hainmueller, and D. Laitin)\n\nPNAS Vol 116(34), pp16768-16772 (2019); latest version; replication package\nMedia: The Washington Post, Fortune\nSummary: Standardizing the fee waiver for citizenship applications raised naturalization rates among low-income immigrants.\n\n‚ÄúDoes Schedule Irregularity Affect Productivity? Evidence from Random Assignment into College Classes‚Äù (w/ Lester Lusher and Phuc Luong)\n\nLabour Economics Vol 60, pp115-128 (2019); latest version; replication package\nSummary: More volatile school start schedules throughout the week do not lead to lower college test scores.\n\n‚ÄúThe Labor Market Effects of a Refugee Wave: Synthetic Control Method Meets the Mariel Boatlift‚Äù (w/ Giovanni Peri)\n\nJournal of Human Resources\nMedia: The Wall Street Journal, The Economist, Vox, The Atlantic, Bloomberg (#1, #2){target=‚Äú_blank‚Äù}, The Chicago Tribune, CBS News, Newsweek (#1, #2){target=‚Äú_blank‚Äù}, Business Insider, Yahoo! Finance\nSummary: The Mariel Boatlift of 1980 did not result in large, statistically detectable wage or employment changes among low-skilled Miamians.\n\n‚ÄúGender Performance Gaps: Quasi-Experimental Evidence on the Role of Gender Differences in Sleep Cycles‚Äù (w/ Lester Lusher)\n\nEconomic Inquiry Vol 56(1), pp252-262 (2018); latest version; replication package\nMedia: The Washington Post, The Independent\nSummary: Girls benefit from earlier school start times relative to boys, partially contributing to the observed gender performance gap.\n\n‚ÄúDouble-Shift Schooling and Student Success: Quasi-experimental Evidence from Europe‚Äù (w/ Lester Lusher)\n\nEconomics Letters Vol.139, pp36-39 (2016); latest version; replication package\nSummary: Students achieve slightly higher grades when taking classes in the morning compared to the afternoon."
  },
  {
    "objectID": "cv.html#employment",
    "href": "cv.html#employment",
    "title": "CV",
    "section": "",
    "text": "Staff Data Scientist, Adobe (2021 ‚Äì )\nData Scientist, Immigration Policy Lab, Stanford University (2020 ‚Äì )\nConsultant, Ministry of Employment, Copenhagen, Denmark (2017)\nResearch Intern, Ministry of Labor and Social Policy, Sofia, Bulgaria (2014)"
  },
  {
    "objectID": "cv.html#education-training",
    "href": "cv.html#education-training",
    "title": "CV",
    "section": "Education / Training",
    "text": "Education / Training\n\nPostdoctoral Fellow, Immigration Policy Lab, Stanford University (2018 ‚Äì 2020)\nPostdoctoral Fellow, Goldman School of Public Policy, UC Berkeley (2017 ‚Äì 2018)\nPh.D.¬†Economics, UC Davis (2017)\nM.A.¬†Economics, UC Davis (2013)\nB.S. Mathematics-Economics, UC San Diego (magna cum laude) (2012)\nH.S. Diploma, ‚Äú–°–≤. –ö–Ω—è–∑ –ë–æ—Ä–∏—Å I‚Äù, Asenovgrad, Bulgaria (2007)"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "CV",
    "section": "Teaching",
    "text": "Teaching\nI have taught lectures on applications of quasi-experimental methods at:\n\nYale University (2021)\nStanford University (2020, 2021)\nUC Berkeley (2018)\nUC Davis (2017)"
  },
  {
    "objectID": "childrenbook.html",
    "href": "childrenbook.html",
    "title": "Causal Inference for Toddlers",
    "section": "",
    "text": "English (pdf)   –ë—ä–ª–≥–∞—Ä—Å–∫–∏ (pdf)   Amazon (paperback)"
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Research",
    "section": "Working Papers",
    "text": "Working Papers\n\n‚ÄúWho Can Work from Home?‚Äô (2020)\nIZA WP No.¬†13197; replication package."
  },
  {
    "objectID": "research.html#non-academic-publications",
    "href": "research.html#non-academic-publications",
    "title": "Research",
    "section": "Non-Academic Publications",
    "text": "Non-Academic Publications\n\n‚ÄúImmigration in Local US Economies was Associated with Strong Native Wage Growth for 40 Years‚Äù (w/ Giovanni Peri) Global Migration Center (UC Davis) 03/2020\n‚ÄúNew Evidence on Immigration and Jobs‚Äù (w/ Giovanni Peri) The Wall Street Journal, 01/2016"
  },
  {
    "objectID": "index.html#my-name-is-vasco-and-i-am-an-applied-statistician.",
    "href": "index.html#my-name-is-vasco-and-i-am-an-applied-statistician.",
    "title": "Hi there!",
    "section": "",
    "text": "While my real name is Vasil, most people call me Vasco."
  },
  {
    "objectID": "blog/correlation-is-cosine.html",
    "href": "blog/correlation-is-cosine.html",
    "title": "Correlation is a Cosine",
    "section": "",
    "text": "You might have come across the statement, ‚Äúcorrelation is a cosine,‚Äù but never taken the time to explore its precise meaning. It certainly sounds intriguing‚Äîhow can the simplest bivariate summary statistic be connected to a trigonometric function you first encountered in sixth grade? What exactly is the relationship between correlation and cosines?"
  },
  {
    "objectID": "blog/correlation-is-cosine.html#background",
    "href": "blog/correlation-is-cosine.html#background",
    "title": "Correlation is a Cosine",
    "section": "",
    "text": "You might have come across the statement, ‚Äúcorrelation is a cosine,‚Äù but never taken the time to explore its precise meaning. It certainly sounds intriguing‚Äîhow can the simplest bivariate summary statistic be connected to a trigonometric function you first encountered in sixth grade? What exactly is the relationship between correlation and cosines?"
  },
  {
    "objectID": "blog/correlation-is-cosine.html#diving-deeper",
    "href": "blog/correlation-is-cosine.html#diving-deeper",
    "title": "Correlation is a Cosine",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nThe Law of Cosines\nThe law of cosines states that in any triangle with sides \\(x\\), \\(y\\), and \\(z\\) and an angle (between \\(x\\) and \\(y\\)) \\(\\theta\\), we have:\n\\[ z^2 = x^2 + y^2 - 2 x y cos(\\theta), \\]\nIn the special case when \\(\\theta=\\frac{\\pi}{2}\\), the term on the right-hand side equals 0 and the equation reduces to the well-known Pythagorean Theorem.\n\n\nThe Variance of the Sum of Two Random Variables\nLet‚Äôs imagine two random variables \\(A\\), \\(B\\). The variance of their sum is given by:\n\\[ var(A+B) = var(A)+var(B)+2 cov(A,B), \\]\nwhere \\(cov(\\cdot)\\), denotes covariance. We can substitute the last term with its definition as follows:\n\\[ var(A+B) = var(A)+var(B)+2 corr(A,B) sd(A) sd(B). \\]\nNext, we know that var()=sd^2(). Substituting, we get:\n\\[ sd^2(A+B) = sd^2 (A)+ sd^2 (B)+2 corr(A,B) sd(A) sd(B).\\]\n\n\nPutting the Two Equations Together\nSetting \\(x=sd(A)\\), \\(y=sd(B)\\), and \\(z=sd(A+B)\\) in the first equation gives the desired result. With one small caveat ‚Äì the negative sign on the cosine term. To get around this we can simply look at the complementary angle \\(\\delta = \\pi - \\theta\\).\nThat is, we imagine a triangle with sides equal to \\(sd(A)\\), \\(sd(B)\\) and \\(sd(A+B)\\), where \\(\\theta\\) is the angle between \\(sd(A)\\), \\(sd(B)\\). When this angle is small (\\(\\theta &lt; \\frac{\\pi}{2}\\)), the two sides point in the same direction and A and B are positively correlated. The opposite is true for \\(\\theta &gt; \\frac{\\pi}{2}\\). As mentioned above, \\(\\theta = \\frac{\\pi}{2}\\) kills the correlation term, consistent with \\(A\\) and \\(B\\) being independent."
  },
  {
    "objectID": "blog/correlation-is-cosine.html#where-to-learn-more",
    "href": "blog/correlation-is-cosine.html#where-to-learn-more",
    "title": "Correlation is a Cosine",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAs with anything else, a Google search is your friend here, with multiple Stack Overflow posts explaining this connection from all sorts of angles. However, I do find John D. Cook‚Äôs blog post most helpful, and I am following his exposition closely."
  },
  {
    "objectID": "blog/correlation-is-cosine.html#bottom-line",
    "href": "blog/correlation-is-cosine.html#bottom-line",
    "title": "Correlation is a Cosine",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe variance formula mirrors the law of cosines.\nStandardizing the variables makes correlation equal the cosine of the angle.\nSo: ‚ÄúCorrelation is a cosine‚Äù ‚Äî literally!"
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html",
    "href": "blog/hypothesis-testing-all-data.html",
    "title": "Hypothesis Testing with Population Data",
    "section": "",
    "text": "Classical statistical theory is built on the idea of working with a sample of data from a given population of interest. Our software packages compute confidence intervals to reflect precisely this ‚Äì we observe only a small part of that population.\nIn modern times, however, we often work with all data points, and not just random samples. Examples abound, especially in the tech industry. Companies store all sales and website activity, the FBI records all homicides, and school records contain information on all students.\nHow can we interpret confidence intervals when we work with such datasets? More generally, how do we think about uncertainty in these settings? We know exactly how many items are sold or how many homicides occur each year; nothing is uncertain about that.\nThe short answer is that the confidence intervals in this setting have a fundamentally different interpretation ‚Äì one reflecting parameters of an underlying metaphorical population."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#background",
    "href": "blog/hypothesis-testing-all-data.html#background",
    "title": "Hypothesis Testing with Population Data",
    "section": "",
    "text": "Classical statistical theory is built on the idea of working with a sample of data from a given population of interest. Our software packages compute confidence intervals to reflect precisely this ‚Äì we observe only a small part of that population.\nIn modern times, however, we often work with all data points, and not just random samples. Examples abound, especially in the tech industry. Companies store all sales and website activity, the FBI records all homicides, and school records contain information on all students.\nHow can we interpret confidence intervals when we work with such datasets? More generally, how do we think about uncertainty in these settings? We know exactly how many items are sold or how many homicides occur each year; nothing is uncertain about that.\nThe short answer is that the confidence intervals in this setting have a fundamentally different interpretation ‚Äì one reflecting parameters of an underlying metaphorical population."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#diving-deeper",
    "href": "blog/hypothesis-testing-all-data.html#diving-deeper",
    "title": "Hypothesis Testing When We Have All Data",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nLet‚Äôs focus on a specific example ‚Äì homicides in the US. According to the Crime in the US report published by the FBI, in 2018, there were \\(14,123\\) homicides, and for 2019 this number was \\(13,927\\). This is a decrease of 196 cases, or roughly equivalent to a \\(1.4\\%\\) drop.\nMother nature and the world around us are incredibly complex, so numbers around us can go up and down for no obvious reason. So, does this drop reflect a real change in the underlying crime rate?\nTo answer this question, it is helpful to model annual homicides as coming from a Poisson distribution from a figurative population of alternative US histories. This distribution has a mean \\(\\lambda\\) equal to the hypothetical true underlying homicide rate. We want to know whether \\(\\lambda\\) changed from 2018 to 2019. (See my earlier post on determining statistical significance between two quantities.)\nIt turns out that the confidence interval for the change in this underlying homicide rate is:\n\\[ (14,123-13,927) \\pm 1.96 \\times \\sqrt{14,123+13,927}=(-132.26, 524.26). \\]\nThis interval clearly contains 0, so we cannot conclude that there was a real drop in the crime rate between 2018 and 2019. In other words, the 1.4% drop in homicides between 2018 and 2019 was within the range consistent with the noise in our world. It should not be confused with increased underlying safety in the US."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#one-more-thing",
    "href": "blog/hypothesis-testing-all-data.html#one-more-thing",
    "title": "Hypothesis Testing When We Have All Data",
    "section": "One More Thing",
    "text": "One More Thing\nThis is not all. This type of thinking is also helpful in a slightly different context. Let‚Äôs focus on 2018, when there were \\(14,123\\) homicides in the US, corresponding to an average daily rate of about 38.7 cases.\nImagine someone asked us to calculate the probability that there would be less than 25 cases on a given day, but no such day took place in 2018. It would still be na√Øve to conclude that the probability of this event was zero.\nWe can look at the left tail of the Poisson distribution with mean \\(\\lambda = 38.7\\) to answer this question:\nppois(25, lambda=38.7)\nThis gives us a \\(1.27%\\) probability of such an event, suggesting that, on average, there should be about \\(4.6\\) such days per year. Data would not help answer this question."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#bottom-line",
    "href": "blog/hypothesis-testing-all-data.html#bottom-line",
    "title": "Hypothesis Testing with Population Data",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWorking with all data eliminates the uncertainty that usually arises in random samples.\nConfidence intervals in such settings are still meaningful ‚Äì they represent uncertainty associated with the underlying parameters of a metaphorical population."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#where-to-learn-more",
    "href": "blog/hypothesis-testing-all-data.html#where-to-learn-more",
    "title": "Hypothesis Testing with Population Data",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThis article was inspired by ‚ÄúThe Art of Statistics‚Äù (2019) which beautifully explains an impressively wide range of statistical topics in an engaging way. It is a non-technical read accessible to everyone interested in combining statistics and data to make inferences about the world."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#references",
    "href": "blog/hypothesis-testing-all-data.html#references",
    "title": "Hypothesis Testing with Population Data",
    "section": "References",
    "text": "References\nThe FBI (2018) Crime in the US.\nThe FBI (2019) Crime in the US.\nSpiegelhalter, D. (2019). The art of statistics: Learning from data. Penguin UK."
  },
  {
    "objectID": "blog/correlation-transitive.html",
    "href": "blog/correlation-transitive.html",
    "title": "Correlation is Not (Always) Transitive",
    "section": "",
    "text": "At first, I found this really puzzling. \\(X\\) is correlated (Pearson) with Y, and Y is correlated with \\(Z\\). Does this mean X is necessarily correlated with \\(Z\\)? Intuitively, this totally makes sense. The answer, however, is ‚Äúno.‚Äù\nPerhaps the strangest thing is how easy it is to rationalize this ‚Äúpuzzle.‚Äù I drink more beer (\\(X\\)) and read more books (\\(Z\\)) when I am on a vacation (Y). That is, both pairs ‚Äì \\(X\\) and \\(Y\\) and \\(Z\\) and \\(Y\\) ‚Äì are positively correlated. But I do not drink more beer when I read more books ‚Äì \\(X\\) and \\(Z\\) are not correlated. It is now obvious that correlation is not (always) transitive, but a second ago, this sounded bizarre."
  },
  {
    "objectID": "blog/correlation-transitive.html#background",
    "href": "blog/correlation-transitive.html#background",
    "title": "Correlation is Not (Always) Transitive",
    "section": "",
    "text": "At first, I found this really puzzling. \\(X\\) is correlated (Pearson) with Y, and Y is correlated with \\(Z\\). Does this mean X is necessarily correlated with \\(Z\\)? Intuitively, this totally makes sense. The answer, however, is ‚Äúno.‚Äù\nPerhaps the strangest thing is how easy it is to rationalize this ‚Äúpuzzle.‚Äù I drink more beer (\\(X\\)) and read more books (\\(Z\\)) when I am on a vacation (Y). That is, both pairs ‚Äì \\(X\\) and \\(Y\\) and \\(Z\\) and \\(Y\\) ‚Äì are positively correlated. But I do not drink more beer when I read more books ‚Äì \\(X\\) and \\(Z\\) are not correlated. It is now obvious that correlation is not (always) transitive, but a second ago, this sounded bizarre."
  },
  {
    "objectID": "blog/correlation-transitive.html#diving-deeper",
    "href": "blog/correlation-transitive.html#diving-deeper",
    "title": "Correlation is Not (Always) Transitive",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nLet‚Äôs denote the respective correlations between \\(X\\), \\(Y\\) and \\(Z\\) by \\(cor(X,Y)\\), \\(cor(X,Z)\\), and \\(cor(Y,Z)\\). For simplicity (and without loss of generality), let‚Äôs work with standardized versions of these variables ‚Äì that is, means of \\(0\\) and variances of \\(1\\). This implies, \\(cov(X,Y) = cor(X,Y)\\) for any pair.\nWe can write the linear projections of \\(X\\) and \\(Z\\) on \\(Y\\) as follows:\n\\[ X = cor(X,Y)Y + \\epsilon^{X,Y}, \\]\n\\[ Z = cor(Z,Y)Y + \\epsilon^{Z,Y}. \\]\nThen, we have:\n\\[ cor(X,Z)=cor(X,Y)cor(Z,Y)+cor(\\epsilon^{X,Y},\\epsilon^{Z,Y}).\\]\nWe can use the Cauchy-Schwarz inequality to bound the last term, which gives the final range of possible values for cor(X,Z):\n\\[cor(X,Y)cor(Z,Y) - \\sqrt{(1-cor(X,Y)^2) (1-cor(Z,Y)^2)}\\]\n\\[\\leq cor(X,Z) \\leq  \\]\n\\[cor(X,Y)cor(Z,Y) + \\sqrt{(1-cor(X,Y)^2) (1-cor(Z,Y)^2)}\\]\nFor instance, if we set \\(cor(X,Y)=cor(Z,Y)=0.6\\), then we get:\n\\[-.28 \\leq cor(X,Z) \\leq 1.\\]\nThat is, \\(cor(X,Z)\\) can be negative.\n\nAn (Extremely Simple) Example\nPerhaps the simplest example to illustrate this is:\n\\(X\\) and \\(Z\\) are independent random variables, \\(Y=X+Z\\). The result follows.\nThe following code sets up this example in R.\nset.seed(68493) x &lt;- runif(n=1000) z &lt;- runif(n=1000) y &lt;- x + z\nBelow is a table with correlation coefficients and p-values associated with the null hypotheses that they are equal to zero.\nYou can find the code for this exercise in this GitHub repository."
  },
  {
    "objectID": "blog/correlation-transitive.html#when-is-correlation-transitive",
    "href": "blog/correlation-transitive.html#when-is-correlation-transitive",
    "title": "Correlation is Not (Always) Transitive",
    "section": "When Is Correlation Transitive",
    "text": "When Is Correlation Transitive\nFrom the equation above it follows that when both \\(cor(X,Y)\\) and \\(cor(Z,Y)\\) are sufficiently large, then \\(cor(X,Z)\\) is sure to be positive (i.e., bounded below by \\(0\\)).\nIn the example above, if we fix \\(cor(X,Y)=.6\\), then we need \\(cor(Z,Y)&gt;.8\\) to guarantee that \\(cor(X,Z)&gt;0\\)."
  },
  {
    "objectID": "blog/correlation-transitive.html#where-to-learn-more",
    "href": "blog/correlation-transitive.html#where-to-learn-more",
    "title": "Correlation is Not (Always) Transitive",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMultiple Stack Overflow threads explain this phenomenon from various angles. Olkin (1981) derives some further mathematical results related to transitivity in higher dimensions."
  },
  {
    "objectID": "blog/correlation-transitive.html#bottom-line",
    "href": "blog/correlation-transitive.html#bottom-line",
    "title": "Correlation is Not (Always) Transitive",
    "section": "Bottom Line",
    "text": "Bottom Line\n\n\\(X\\) and \\(Z\\) both being correlated with \\(Y\\) does not guarantee that \\(X\\) and \\(Z\\) are correlated with each other.\nThis is the case when the former two correlations are ‚Äúlarge enough.‚Äù"
  },
  {
    "objectID": "blog/correlation-transitive.html#references",
    "href": "blog/correlation-transitive.html#references",
    "title": "Correlation is Not (Always) Transitive",
    "section": "References",
    "text": "References\nOlkin, I. (1981). Range restrictions for product-moment correlation matrices. Psychometrika, 46, 469-472. doi:10.1007/BF02293804"
  },
  {
    "objectID": "blog/causation-without-correlation.html",
    "href": "blog/causation-without-correlation.html",
    "title": "Causation without Correlation",
    "section": "",
    "text": "While most people understand that correlation doesn‚Äôt imply causation, it might surprise many to learn that causation doesn‚Äôt always result in correlation. In the absence of randomization, causal relationships do not require observable correlation. This counterintuitive concept challenges our natural tendency to expect that when one variable causes change in another, we should see a clear (linear) relationship between them. The core idea is that confounding variables or other statistical phenomena can obscure the causal link. Let‚Äôs explore this concept through a few examples."
  },
  {
    "objectID": "blog/causation-without-correlation.html#background",
    "href": "blog/causation-without-correlation.html#background",
    "title": "Causation without Correlation",
    "section": "",
    "text": "While most people understand that correlation doesn‚Äôt imply causation, it might surprise many to learn that causation doesn‚Äôt always result in correlation. In the absence of randomization, causal relationships do not require observable correlation. This counterintuitive concept challenges our natural tendency to expect that when one variable causes change in another, we should see a clear (linear) relationship between them. The core idea is that confounding variables or other statistical phenomena can obscure the causal link. Let‚Äôs explore this concept through a few examples."
  },
  {
    "objectID": "blog/causation-without-correlation.html#diving-deeper",
    "href": "blog/causation-without-correlation.html#diving-deeper",
    "title": "Causation without Correlation",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nIn the popular book Causal Inference: The Mixtape, Scott Cunningham gives an example of a sailor steering a boat in stormy waters. The wind may be so strong as to offset the boat‚Äôs natural moving direction. For instance, the sailor might steer (treatment, \\(T\\)) the boat north, while a southward wind (confounder, \\(Z\\)) causes the boat to move east (outcome, \\(Y\\)). An onlooker would not observe any direct relationship between \\(T\\) and \\(Y\\), even though \\(T\\) causes \\(Y\\).\nAt first, this sounds counterintuitive. On second thought, such patterns are everywhere. Consider the following.\n\nExample 1: Parenting Styles and Children‚Äôs Behavior\nA parent might adopt a stricter parenting style (\\(T\\)) in response to a child‚Äôs behavioral issues (\\(Y\\)). However, other influences, like peer pressure or school environment (\\(Z\\)), may also shape the child‚Äôs behavior, sometimes overriding the parent‚Äôs efforts. The net observable outcome could show no correlation between stricter parenting and improved behavior, even though the stricter parenting is causally effective in certain contexts.\nThis idea can be taken one step further. An observable relationship might even appear positive when the causal relationship is negative.\n\n\nExample 2: Ice Cream Sales and Shark Attacks\nImagine two beaches with vastly different safety protocols (\\(Z\\)): one has lifeguards trained to prevent shark attacks, while the other does not. On the safer beach, higher ice cream sales (\\(T\\)) correlate positively with shark attacks (\\(Y\\)), because more people visit the beach when safety protocols are in place. This hides the fact that proper safety protocols causally reduce shark attacks. The observed positive correlation between \\(T\\) and \\(Y\\) masks the negative causal relationship.\n\n\nOne More Thing\nA more trivial scenario leading to the lack of correlation in causal relationships is non-linearity. I do not find this scenario too insightful simply because it can be avoided by using more sophisticated measures of correlation. See my earlier post on the Chatterjee correlation coefficient.\nExamples of such relationships abound. Consider a parabolic relationship, where increasing a drug‚Äôs dosage initially improves patient outcomes but becomes harmful at higher doses. Despite a clear causal relationship, the correlation coefficient might be close to zero because the relationship is not linear.\n\n\nExample 3: Threshold Effects and Phase Transitions\nTake the classic example of temperature and water‚Äôs state. Increasing temperature causes water to change state at exactly 100¬∞C. Below and above this point, temperature changes cause minimal effects on the water‚Äôs state. Aggregating these observations leads to a weak correlation, despite the temperature being the direct cause of the phase transition.\nOther fascinating scenarios include Lord‚Äôs Paradox, and Simpson‚Äôs Paradox, where a causal relationship can appear to reverse or disappear when data is aggregated.\n\n\nExample 4: Hospital Mortality Rates\nSuppose two hospitals treat patients with different levels of severity. Hospital \\(A\\) specializes in high-risk patients, while Hospital \\(B\\) treats mostly low-risk cases. When comparing raw mortality rates (\\(Y\\)), Hospital \\(A\\) might appear worse, even though it provides superior care (\\(T\\)). Disaggregating the data by risk level reveals the causal effect of Hospital \\(A\\)‚Äôs superior treatment within each group."
  },
  {
    "objectID": "blog/causation-without-correlation.html#bottom-line",
    "href": "blog/causation-without-correlation.html#bottom-line",
    "title": "Causation without Correlation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nIn observational data causation does not require correlation.\nCorrelation‚Äîor the lack thereof‚Äîcan obscure our understanding of causal relationships.\nWith the right tools and frameworks, we can disentangle the true causal effects, even when correlation gives us a wrong answer."
  },
  {
    "objectID": "blog/causation-without-correlation.html#references",
    "href": "blog/causation-without-correlation.html#references",
    "title": "Causation without Correlation",
    "section": "References",
    "text": "References\nCunningham, S. (2021). Causal inference: The mixtape. Yale university press."
  },
  {
    "objectID": "blog/weights-statistics.html",
    "href": "blog/weights-statistics.html",
    "title": "Weights in Statistical Analyses",
    "section": "",
    "text": "Weights in statistical analyses offer a way to assign varying importance to observations in a dataset. Although powerful, they can be quite confusing due to the various types of weights available. In this article, I will unpack the details behind the most common types of weights used in data science.\nMost types of statistical analyses can be performed with weights. These include calculating summary statistics, regression models, bootstrap, etc. Even maximum likelihood estimation is minimally affected. In this article, I will keep it simple and only discuss mean and variance estimation.\nThe two primary types of weights encountered in practice are sampling weights and frequency weights. I will explore each in turn and conclude with a comparative example.\nTo begin, let‚Äôs imagine a well-behaved random variable X of which we have an iid sample of size n.¬†I will use w to denote the relevant weighting variable."
  },
  {
    "objectID": "blog/weights-statistics.html#background",
    "href": "blog/weights-statistics.html#background",
    "title": "Weights in Statistical Analyses",
    "section": "",
    "text": "Weights in statistical analyses offer a way to assign varying importance to observations in a dataset. Although powerful, they can be quite confusing due to the various types of weights available. In this article, I will unpack the details behind the most common types of weights used in data science.\nMost types of statistical analyses can be performed with weights. These include calculating summary statistics, regression models, bootstrap, etc. Even maximum likelihood estimation is minimally affected. In this article, I will keep it simple and only discuss mean and variance estimation.\nThe two primary types of weights encountered in practice are sampling weights and frequency weights. I will explore each in turn and conclude with a comparative example.\nTo begin, let‚Äôs imagine a well-behaved random variable X of which we have an iid sample of size n.¬†I will use w to denote the relevant weighting variable."
  },
  {
    "objectID": "blog/weights-statistics.html#diving-deeper",
    "href": "blog/weights-statistics.html#diving-deeper",
    "title": "Weights in Statistical Analyses",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nMean estimation does not depend on the type of weights. We have:\n\\[ \\bar{X} = \\frac{\\sum_i wX}{\\sum_i w}. \\]\nVariance estimation depends on the weight type.\nRemember that we estimate the standard error of \\(\\bar{X}\\) as:\n\\[ SE(\\bar{X})=\\frac{s}{\\sqrt{n}}, \\hspace{.3cm} \\text{where} \\hspace{.3cm} s=\\sqrt{\\frac{1}{N-1}\\sum_i(X-\\bar{X})^2} \\]\nis an estimate of the standard deviation of \\(X\\). I will explain below how estimation of \\(SE(\\bar{X})\\) differs for sampling and frequency weights. Imporantly, s_w will denote the weighted version of this standard error.\n\nSampling Weights\nSampling weights measure the inverse probability of an observation entering the sample. They are particularly relevant in surveys when some units in the population are sampled more frequently than others. These weights adjust for sampling discrepancies to ensure that survey estimates are representative of the entire population. Sampling weights are also sometimes called probability weights.\nIntuitively, if we want to use the survey to accurately represent the population, we should assign higher importance to observations that are less likely to be sampled and lower importance to those more likely to appear in our data. This is exactly what sampling weights achieve. Sampling weights are, thus, inversely proportional to the probability of selection. Therefore, a smaller weight indicates a higher probability of being sampled, and vice versa.\nFor example, households in rural areas might be less likely to enter a survey due to the increased resources required to reach remote locations. Conversely, urban households are typically easier to sample and thus more likely to appear in the data.\nThis weighting approach is also common in causal inference analyses using propensity score methods. Similarly, in machine learning, weighting is often employed to adjust for unbalanced samples in the context of rare events (e.g., fraud detection, cancer diagnosis).\nLet‚Äôs now get back to the discussion on variance estimation. With sampling weights, we have:\n\\[ SE^{\\text{sampling}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{S}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\frac{(\\sum_i w)^2}{\\sum_i w^2}}}\\]\nThe numerator is the weighted version of the sum of squared deviations from the mean, emphasizing observations with higher weights. The denominator normalizes the standard error based on the total sum of the weights. When the weights differ greatly‚Äîhigh weights make some observations much more influential, increasing uncertainty about the population mean and so the sampling-weighted standard error is often larger.\nI will move on to describing frequency weights.\nSoftware Package: survey.\n\n\nFrequency Weights\nFrequency weights measure the number of times an observation should be counted in the analysis. They are most relevant when there are multiple identical observations or when duplicating observations is possible. Naturally, higher weights correspond to observations that appear more frequently. Frequency weights are common in aggregated datasets. For instance, with market- or city-level data, we might want to weight the rows by market size, thus assigning more importance to larger units.\nWe can gain intuition from a linear algebra perspective. In a regression context, the design matrix X must be of full rank (to ensure invertibility), which implies no two rows can be identical. We thus need to collapse X to keep only distinct rows and record the number of times each row appears in the original data. This record constitutes the frequency weights.\nThe formula for estimating the standard error of X with frequency weights is:\n\\[ SE^{\\text{frequency}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{F}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\sum_i w}}.\\]\nHere \\(n_{\\text{eff}}\\) is the effective sample size (i.e., the sum of all weights) and \\(s_w\\) is the weighted standard deviation of \\(X\\).\nThe frequency-weighted standard error is typically the smaller because it increases the effective sample size without introducing variability in the contribution of different observations. In contrast, the sampling-weighted standard error could be larger if some observations are given much higher weights, increasing the variability and lowering the precision.\nSoftware Package: survey\n\n\nOne More Thing\nThere is actually one more type of weights which are not so commonly used in practice, precision weights. They represent the precision (\\(1/\\text{variance}\\)) of observations. A weight of 5, for example, reflects 5 times the precision of a weight of 1, originally based on averaging 5 replicate observations. Precision weights often come up in statistical theory in places such as Generalized Least Squares where they promise efficiency gains (i.e., lower variance) relative to traditional Ordinary Least Squares estimation. In practice, precision weights are in fact frequency weights normalized to sum to n.¬†Lastly, Stata‚Äôs user guide refers to them as analytic weights."
  },
  {
    "objectID": "blog/weights-statistics.html#an-example",
    "href": "blog/weights-statistics.html#an-example",
    "title": "Weights in Statistical Analyses",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs see all of this in practice. We begin by creating a fake dataset of a variable \\(X\\) with both sampling and frequency weights. The weights are randomly created and hence have no underlying meaning, so think of this example as a tutorial, without a strong focus on the results.\n\nRPython\n\n\n# clear workspace and load libraries\nlibrary(survey)\nrm(list=ls())\nset.seed(681)\n\n# generate fake data\nn &lt;- 100000\ndata &lt;- data.frame(\n  x = rnorm(n),\n  prob_selection = runif(n, .1, .9),\n  freq_weight = rpois(n, 3)\n)\ndata$samp_weight &lt;- 1 / data$prob_selection\n\n# calculate the average value of $X$ using both types of weights.\ndesign_unweight &lt;- svydesign(ids = ~1, data = data, weights = ~1)\ndesign_samp &lt;- svydesign(ids = ~1, data = data, weights = ~samp_weight)\ndesign_freq &lt;- svydesign(ids = ~1, data = data, weights = ~freq_weight)\n\nmean_unweight &lt;- svymean(~x, design_unweight)\nmean_samp &lt;- svymean(~x, design_samp)\nmean_freq &lt;- svymean(~x, design_freq)\n\n# print results\nprint(round(mean_unweight, digits=3))\n&gt;    mean     SE\n&gt; x -0.002 0.0032\nprint(round(mean_samp, digits=3))\n&gt;  mean     SE\n&gt; x    0 0.0038\nprint(round(mean_freq, digits=3))\n&gt;    mean     SE\n&gt; x -0.002 0.0037\n\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.weightstats import DescrStatsW\n\n# Set seed for reproducibility\nnp.random.seed(681)\n\n# Generate fake data\nn = 100000\ndata = pd.DataFrame({\n    \"x\": np.random.normal(size=n),\n    \"prob_selection\": np.random.uniform(0.1, 0.9, size=n),\n    \"freq_weight\": np.random.poisson(3, size=n)\n})\ndata[\"samp_weight\"] = 1 / data[\"prob_selection\"]\n\n# Calculate the average value of X using both types of weights\n# Unweighted mean\nmean_unweight = DescrStatsW(data[\"x\"]).mean, DescrStatsW(data[\"x\"]).std_mean\n\n# Sampling-weighted mean\nmean_samp = DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).std_mean\n\n# Frequency-weighted mean\nmean_freq = DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).std_mean\n\n# Print results\nprint(\"Unweighted Mean and SE:\", np.round(mean_unweight, 3))\n&gt; Unweighted Mean and SE: [-0.004  0.003]\nprint(\"Sampling-Weighted Mean and SE:\", np.round(mean_samp, 3))\n&gt; Sampling-Weighted Mean and SE: [-0.002  0.002]\nprint(\"Frequency-Weighted Mean and SE:\", np.round(mean_freq, 3))\n&gt; Frequency-Weighted Mean and SE: [-0.004  0.002]\n\n\n\nThe unweighted and frequency-weighted means match exactly, (I am not sure why the sampling-weighted mean is slightly lower.) while the variances are different. The variance with frequency weights is lower than that of sampling weights."
  },
  {
    "objectID": "blog/weights-statistics.html#bottom-line",
    "href": "blog/weights-statistics.html#bottom-line",
    "title": "Weights in Statistical Analyses",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWeights are one of the most confusing aspects of working with data.\nSampling and frequency weights are the most common types of weights found in practice.\nThe former measure the inverse probability of being sampled, while the latter represent the number of times an observation enters the sample.\nWhile weighting usually does not impact point estimates (e.g., regression coefficients, means), incorrect usage of weights can lead to inaccurate confidence intervals and p-values.\nThis is relevant only if (i) your dataset contains weights, and (ii) you are interested in population-level statistics."
  },
  {
    "objectID": "blog/weights-statistics.html#where-to-learn-more",
    "href": "blog/weights-statistics.html#where-to-learn-more",
    "title": "Weights in Statistical Analyses",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nGoogle is a great starting place. Lumley‚Äôs blog post titled Weights in Statistics was incredibly helpful in preparing this article. Stata‚Äôs manuals which are publically available contain more detailed information on various types of weighting schemes. See also Solon et al.¬†(2015) for using weights in causal inference."
  },
  {
    "objectID": "blog/weights-statistics.html#references",
    "href": "blog/weights-statistics.html#references",
    "title": "Weights in Statistical Analyses",
    "section": "References",
    "text": "References\nDupraz, Y. (2013). Using Weights in Stata. Memo, 54(2)\nLumley, T. (2020), Weights in Statistics, Blog Post\nSolon, G., Haider, S. J., & Wooldridge, J. M. (2015). What are we weighting for?. Journal of Human resources, 50(2), 301-316.\nStata User‚Äôs Guide (2023)"
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html",
    "href": "blog/column-sampling-bootstrap.html",
    "title": "Column-Sampling Bootstrap?",
    "section": "",
    "text": "The bootstrap is a versatile resampling technique traditionally focused on rows. Let‚Äôs add a twist to the plain vanilla bootstrap. Imagine you have a wide dataset‚Äîmany variables but few rows‚Äîand want to test the statistical significance of a correlation between two variables. An example is a genetic dataset with thousands of columns (genetic information and outcomes) but a limited number of rows (patients). Can you use the bootstrap to determine if the correlation between a specific gene-outcome pair is statistically significant?\nOne creative approach is resampling columns instead of rows, generating a distribution of correlation coefficients to assess the significance of your observed correlation."
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html#background",
    "href": "blog/column-sampling-bootstrap.html#background",
    "title": "Column-Sampling Bootstrap?",
    "section": "",
    "text": "The bootstrap is a versatile resampling technique traditionally focused on rows. Let‚Äôs add a twist to the plain vanilla bootstrap. Imagine you have a wide dataset‚Äîmany variables but few rows‚Äîand want to test the statistical significance of a correlation between two variables. An example is a genetic dataset with thousands of columns (genetic information and outcomes) but a limited number of rows (patients). Can you use the bootstrap to determine if the correlation between a specific gene-outcome pair is statistically significant?\nOne creative approach is resampling columns instead of rows, generating a distribution of correlation coefficients to assess the significance of your observed correlation."
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html#diving-deeper",
    "href": "blog/column-sampling-bootstrap.html#diving-deeper",
    "title": "Column-Sampling Bootstrap?",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nDefinition\nHere‚Äôs the basic algorithm:\n\nRandomly sample columns from your dataset with replacement to create fake dataset.\nCompute their correlation coefficient.\nRepeat this many times.\nCompare your observed correlation to the distribution of these synthetic correlations.\nDeclare statistical significance if the observed correlation appears as an ‚Äúoutlier‚Äù in this synthetic distribution.\n\nThis approach allows you to explore a large number of possible correlations in a computationally efficient way. But does it actually work? Let‚Äôs unpack the key considerations.\nThe column-sampling bootstrap is most valuable when a dataset has many columns but too few rows for traditional bootstrap methods. The abundance of columns provides a rich sampling landscape. The goal is determining whether a correlation is significantly stronger or weaker than what might occur by chance. Let‚Äôs simplify the problem and ignore any issues stemming from being unable to estimate the correlation coefficients well enough.\n\n\nChallenges\nHowever, several critical challenges exist. The method assumes columns are independent and identically distributed (i.i.d.), which rarely holds in practice. Columns often represent related variables‚Äîlike gene measurements or interconnected phenomena‚Äîand these dependencies can bias resampled correlations. Moreover, by resampling columns, you ignore row-level relationships, such as connections in time series or grouped data (like patients from the same household).\nInterpreting the null distribution presents another significant challenge. Synthetic correlation coefficients generated through column resampling might not represent a meaningful null hypothesis. If your dataset contains highly correlated features, the null distribution could shift, potentially leading to misleading conclusions. Unlike traditional bootstrapping‚Äîwhere samples reflect a subpopulation‚Äîthis method lacks that fundamental connection.\n\n\nThe Verdict\nWhile the column-sampling bootstrap is an intriguing concept, it will likely prove useful only in very specific, carefully constrained settings."
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html#an-example",
    "href": "blog/column-sampling-bootstrap.html#an-example",
    "title": "Column-Sampling Bootstrap?",
    "section": "An Example",
    "text": "An Example\nWhile we should be skeptical of the column-sampling bootstrap in practical applications, it can be instructive to see how we might implement it.\nBelow is a sample R and python code illustrating the main concept. We begin with setting up a synthetic dataset.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\ndata &lt;- as.data.frame(matrix(rnorm(1000), nrow = 50, ncol = 20))\nobserved_correlation &lt;- cor(data[[1]], data[[2]])\n\n# Perform the resampling\nn_bootstrap &lt;- 1000  # Number of bootstrap iterations\nn_columns &lt;- ncol(data)  # Total number of columns in the dataset\nbootstrap_correlations &lt;- numeric(n_bootstrap)\n\nfor (i in 1:n_bootstrap) {\n  resampled_columns &lt;- sample(1:n_columns, size = n_columns, replace = TRUE)\n  resampled_data &lt;- data[, resampled_columns]\n  bootstrap_correlations[i] &lt;- cor(resampled_data[[1]], resampled_data[[2]])\n}\n\n# Test the significance of the observed correlation\np_value &lt;- mean(abs(bootstrap_correlations) &gt;= abs(observed_correlation))\n\n# Print the results\ncat(\"Observed Correlation:\", observed_correlation, \"\\n\")\n&gt; Observed Correlation: 0.05758855 \ncat(\"P-value:\", p_value, \"\\n\")\n&gt; P-value: 0.676 \n\n\nimport numpy as np\nnp.random.seed(1988)\n\n# Generate synthetic dataset\ndata = np.random.normal(size=(50, 20))  # 50 rows, 20 columns\nobserved_correlation = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n\n# Perform the resampling\nn_bootstrap = 1000  # Number of bootstrap iterations\nn_columns = data.shape[1]  # Total number of columns in the dataset\nbootstrap_correlations = []\n\nfor _ in range(n_bootstrap):\n    # Resample columns with replacement\n    resampled_columns = np.random.choice(n_columns, size=n_columns, replace=True)\n    resampled_data = data[:, resampled_columns]\n    # Compute correlation between the first two columns of the resampled data\n    bootstrap_correlations.append(np.corrcoef(resampled_data[:, 0], resampled_data[:, 1])[0, 1])\n\n# Test the significance of the observed correlation\nbootstrap_correlations = np.array(bootstrap_correlations)\np_value = np.mean(np.abs(bootstrap_correlations) &gt;= np.abs(observed_correlation))\n\n# Print the results\nprint(\"Observed Correlation:\", observed_correlation)\nprint(\"P-value:\", p_value)\n\n\n\nThe observed correlation is quite low and equal to \\(0.58\\). Its associated p-value is \\(0.676\\), consistent with the value not being statistically significant."
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html#bottom-line",
    "href": "blog/column-sampling-bootstrap.html#bottom-line",
    "title": "Column-Sampling Bootstrap?",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe column-sampling bootstrap is a thought-proviking twist on traditional resampling techniques that leverages the width of your dataset.\nWhile it offers computational efficiency and flexibility, its reliance on the i.i.d. assumption and potential to overlook row-level dependencies highlight the need for careful application.\nThe column-sampling bootrap should not be your go-to method to assess statistical significance."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html",
    "href": "blog/stratified-sampling-cont-var.html",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "",
    "text": "Stratified sampling is a foundational technique in survey design, ensuring that observations capture key characteristics of a population. By dividing the data into distinct strata and sampling from each, stratified sampling often results in more efficient estimates than simple random sampling. Strata are typically defined by categorical variables such as classrooms, villages, or user types. This method is particularly advantageous when some strata are rare but carry critical information, as it ensures their representation in the sample. It is also often employed to tackle spillover effects or manage survey costs more effectively.\nWhile straightforward for categorical variables (like geographic region), continuous variables‚Äîsuch as income or churn score‚Äîpose greater challenges for stratified sampling. The primary issue lies in the curse of dimensionality: attempting to create strata across multiple continuous variables results in an explosion of possible combinations, making effective sampling impractical. For example, stratifying a population based on income at every possible dollar amount is absurd.\nIn this article, I present two solutions to the problem of stratified sampling with continuous variables."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#background",
    "href": "blog/stratified-sampling-cont-var.html#background",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "",
    "text": "Stratified sampling is a foundational technique in survey design, ensuring that observations capture key characteristics of a population. By dividing the data into distinct strata and sampling from each, stratified sampling often results in more efficient estimates than simple random sampling. Strata are typically defined by categorical variables such as classrooms, villages, or user types. This method is particularly advantageous when some strata are rare but carry critical information, as it ensures their representation in the sample. It is also often employed to tackle spillover effects or manage survey costs more effectively.\nWhile straightforward for categorical variables (like geographic region), continuous variables‚Äîsuch as income or churn score‚Äîpose greater challenges for stratified sampling. The primary issue lies in the curse of dimensionality: attempting to create strata across multiple continuous variables results in an explosion of possible combinations, making effective sampling impractical. For example, stratifying a population based on income at every possible dollar amount is absurd.\nIn this article, I present two solutions to the problem of stratified sampling with continuous variables."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#diving-deeper",
    "href": "blog/stratified-sampling-cont-var.html#diving-deeper",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nThe Traditional Method: Equal-Sized Binning\nThis approach involves dividing the continuous variable(s) into intervals or bins. For example, churn score, a single continuous variable \\(X\\), can be divided into quantiles (e.g., quartiles or deciles), ensuring each bin contains approximately the same number of observations/users.\nLet‚Äôs focus on the case of building ten equally-sized strata. Mathematically, for a continuous variable \\(X\\), the decile-based binning can be defined as:\n\\[\\text{Bin}_i = \\{x \\in X : Q_{10\\times (i-1)+1} \\leq x &lt; Q_{10\\times i}\\} \\text{ for } i \\in \\{1,\\dots,10\\}, \\]\nwhere \\(Q_k\\) represents the \\(k\\)-th quantile of \\(X\\). This approach splits in the first ten percentiles (i.e., minimum value to the \\(10\\)th percentile) into a single stratum, the next ten percentiles (\\(10\\)th to \\(20\\)th) into another stratum, and so on.\nWhen dealing with multiple variables, this method extends to either marginally stratify each variable or jointly stratify them. However, joint stratification across multiple variables can also fall prey to the curse of dimensionality.\n\n\nThe Modern Method: Unsupervised Clustering\nAn alternative approach uses unsupervised clustering algorithms, such as \\(k\\)-means or hierarchical clustering, to group observations into clusters, treating these clusters as strata. Unlike binning, clustering leverages the distribution of the data to form natural groupings.\nFormally, let \\(X\\) be a matrix of n observations across \\(p\\) continuous variables. One class of clustering algorithms aims to assign each observation \\(i\\) to one of \\(k\\) clusters:\n\\[\\text{minimize} \\quad \\sum_{j=1}^k \\sum_{i \\in \\mathcal{C}_j} \\text{distance}(X_i - \\mu_j), \\]\nwhere \\(\\mu_j=\\frac{1}{|S_j|}\\sum_{i\\in \\mathcal{C}_j} X\\)‚Äã is the centroid of cluster \\(\\mathcal{C}_j\\) of \\(size |S_j|\\).\nCommonly, \\(\\text{distance}(X_i, \\mu_j)=\\|X_i - \\mu_j\\|^2\\) which leads to \\(k\\)-means clustering. Unlike in the binning approach, here we are not restricting each strata to have the same number of observations."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#pros-and-cons",
    "href": "blog/stratified-sampling-cont-var.html#pros-and-cons",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nUnsurprisingly, each method comes with its trade-offs. Traditional binning is simple and interpretable but can struggle with multivariate dependencies. Clustering accounts for multivariate relationships between variables, avoids imposing arbitrary bin thresholds, and may results in more natural groupings. However, it can be computationally expensive and sensitive to the choice of algorithm and hyperparameters (e.g., \\(k\\) in \\(k\\)-means).\nOne can also imagine a hybrid approach. Begin with a dimensionality reduction method like PCA and then perform binning on the first few principal components."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#an-example",
    "href": "blog/stratified-sampling-cont-var.html#an-example",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "An Example",
    "text": "An Example\nHere is R and python code illustrating both types of approaches on the popular iris dataset. We are interested in creating strata based on the SepalLenght variable. We begin with the traditional binning approach.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\ndata(iris)\n\n#Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris$SepalLengthBin &lt;- cut(iris$Sepal.Length, \n                           breaks = quantile(iris$Sepal.Length, probs = seq(0, 1, 0.25)), include.lowest = TRUE)\n\n#Inspect the resulting strata\ntable(iris$SepalLengthBin)\n&gt; [4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] \n&gt;       41        39        35        35\n\n#Perform k-means clustering on two continuous variables\niris_cluster &lt;- kmeans(iris[, c(\"Sepal.Length\", \"Petal.Length\")], centers = 4)\n\n#Assign clusters as strata\niris$Cluster &lt;- as.factor(iris_cluster$cluster)\n\n#Inspect the resulting strata\ntable(iris$Cluster) \n&gt; 1  2  3  4 \n&gt; 50 15 54 31 \n\n\n# Load libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nnp.random.seed(1988)\n\n# Load the iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris['SepalLengthBin'] = pd.qcut(iris['Sepal.Length'], q=4, labels=False)\n\n# Inspect the resulting strata\nprint(\"Quantile Bins (Sepal.Length):\")\nprint(iris['SepalLengthBin'].value_counts())\n&gt; SepalLengthBin\n&gt; 0    41\n&gt; 1    39\n&gt; 3    35\n&gt; 2    35\n\n# Perform k-means clustering on two continuous variables\nkmeans = KMeans(n_clusters=4, random_state=1988)\niris['Cluster'] = kmeans.fit_predict(iris[['Sepal.Length', 'Petal.Length']])\n\n# Assign clusters as strata\niris['Cluster'] = iris['Cluster'].astype('category')\n\n# Inspect the resulting strata\nprint(\"\\nCluster Sizes:\")\nprint(iris['Cluster'].value_counts())\n&gt; Cluster\n&gt; 2    50\n&gt; 3    50\n&gt; 0    28\n&gt; 1    22\n\n\n\nHere we also have four clusters, but their size ranges from \\(25\\) to \\(50\\) observations each."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#bottom-line",
    "href": "blog/stratified-sampling-cont-var.html#bottom-line",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nStratified sampling with continuous variables requires balancing simplicity and sophistication.\nTraditional binning remains a practical choice for single continuous variables or very few categorical ones.\nClustering provides a robust alternative, enabling stratification with multiple continuous variables at the cost of adding complexity and tuning parameters."
  },
  {
    "objectID": "blog/three-classes-stat-models.html",
    "href": "blog/three-classes-stat-models.html",
    "title": "The Three Classes of Statistical Models",
    "section": "",
    "text": "Statistical modeling is among the most exciting elements of working with data. When mentoring junior data scientists, I never fail to see the spark in their eyes when our projects have overcome the data gathering and cleaning stages and have reached that precious Rubicon.\nA major goal in the craft of statistical modeling is extracting the most information from the data at hand. Statisticians and econometricians call this efficiency or precision. In simple terms, this means achieving the lowest possible variance from a class of available estimators. As an example, imagine two findings‚Äîboth centered at \\(3\\%\\) (e.g., the treatment effect of an intervention of interest)‚Äîbut one with a \\(95\\%\\) confidence interval of \\([2\\%, 4\\%]\\), and the other with \\([-7\\%, 13\\%]\\). The former is clearly more informative and useful than the latter.\nIn a series of four articles, I will dive deeper into the topic of efficiency with a varying degree of detail and complexity. As a first step in this exploration, I want to demystify the distinction between the three classes of statistcal models: parametric, semiparametric, and nonparametric. The goal of this article is to explore in depth the concepts of parameterizing a model and the distinctions between these three groups."
  },
  {
    "objectID": "blog/three-classes-stat-models.html#background",
    "href": "blog/three-classes-stat-models.html#background",
    "title": "The Three Classes of Statistical Models",
    "section": "",
    "text": "Statistical modeling is among the most exciting elements of working with data. When mentoring junior data scientists, I never fail to see the spark in their eyes when our projects have overcome the data gathering and cleaning stages and have reached that precious Rubicon.\nA major goal in the craft of statistical modeling is extracting the most information from the data at hand. Statisticians and econometricians call this efficiency or precision. In simple terms, this means achieving the lowest possible variance from a class of available estimators. As an example, imagine two findings‚Äîboth centered at \\(3\\%\\) (e.g., the treatment effect of an intervention of interest)‚Äîbut one with a \\(95\\%\\) confidence interval of \\([2\\%, 4\\%]\\), and the other with \\([-7\\%, 13\\%]\\). The former is clearly more informative and useful than the latter.\nIn a series of four articles, I will dive deeper into the topic of efficiency with a varying degree of detail and complexity. As a first step in this exploration, I want to demystify the distinction between the three classes of statistcal models: parametric, semiparametric, and nonparametric. The goal of this article is to explore in depth the concepts of parameterizing a model and the distinctions between these three groups."
  },
  {
    "objectID": "blog/three-classes-stat-models.html#diving-deeper",
    "href": "blog/three-classes-stat-models.html#diving-deeper",
    "title": "The Three Classes of Statistical Models",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nParametric Models\nParametric models impose strong assumptions about the underlying data-generating process, making them less flexible but highly interpretable and data-efficient. They require fewer data points to estimate the target accurately compared to semiparametric or nonparametric models. Consequently, they are often the default choice in practice.\nA classic example is density estimation. Suppose you observe that your data appears symmetric around its mean. You might assume the data follows a normal (Gaussian) distribution. In this case, specifying the mean \\(\\mu\\) and variance \\(\\sigma^2\\) fully characterizes the distribution, allowing you to estimate the density parametrically. And then, you can make all kinds of calculations related to your variable, such as what is the probability that it will take values greater than c.\nFormally, a parametric model describes the data-generating process entirely using a finite-dimensional parameter vector. As yet another example, in the omnipresent linear model:\n\\[Y = X\\beta + \\epsilon,\\]\nthe parameter specifies the entire relationship between \\(X\\) and \\(Y\\). This imposes a strong assumption: the relationship is linear, meaning a unit change in \\(X\\) consistently results in a \\(\\beta\\) change in \\(Y\\), regardless of \\(X\\)‚Äôs magnitude (small or large). While convenient, this assumption may not hold in all practical scenarios, as real-world relationships often are complex and deviate from linearity.\n\n\nSemiparametric Models\nSemiparametric models combine the structure of parametric models with the flexibility of nonparametric methods. They specify certain aspects of the data-generating process parametrically while leaving other aspects unspecified or modeled flexibly.\nConsider the partially linear model:\n\\[Y = X \\beta + g(Z) + \\epsilon,\\]\nwhere \\(\\beta\\) is a parametric component describing the linear effect of \\(X\\), while \\(g(Z)\\) is an unknown and potentially complex function capturing the nonparametric effect of a covariate matrix \\(Z\\). Here, the model imposes linearity on \\(X\\)‚Äôs effect but allows \\(Z\\)‚Äôs effect to be fully flexible.\nSemiparametric models are highly versatile, balancing the interpretability of parametric models with the adaptability of nonparametric ones. However, estimating \\(\\beta\\) efficiently while accounting for the unknown \\(g(Z)\\) poses challenges, often requiring further assumptions.\nIn practice, semiparametric models play a crucial role in causal inference, particularly when working with observational cross-sectional data (see Imbens and Rubin 2015). A common strategy involves estimating propensity scores using parametric models (like logistic regression) and then employing nonparametric techniques such as kernel weighting, matching, or stratification to estimate treatment effects. This hybrid approach helps address confounding while maintaining computational tractability and interpretability.\n\n\nNonparametric Models\nNonparametric models make minimal assumptions about the underlying data-generating process, allowing the data to ‚Äúspeak for itself‚Äù. Unlike parametric and semiparametric models, these ones do not specify a finite-dimensional parameter vector or impose rigid structural assumptions. This flexibility makes them highly robust to model misspecification but often requires larger datasets to achieve accurate estimates.\nLet‚Äôs get back to the density estimation example. Histograms are a form of nonparametric models as they do not assume specific underlying functional form. Instead of fitting a parametric distribution (e.g., normal), you might use a kernel density estimator:\n\\[\\hat{f}(x)=\\frac{1}{nh}\\sum_i K \\left( \\frac{x-X_i}{h} \\right),\\]\nwhere \\(K(\\cdot)\\) is a kernel function, and h is a bandwidth parameter controlling the smoothness of the estimate. –ê kernel function assigns varying weight to observations around a data point. They are non-negative, symmetric around zero, and sum to one. Commonly employed kernel functions include:\n\nGaussian K()=e{-0.5x2},\nEpanechnikov K()=(1-x^2),\nRectangular: K()=0.5.\n\nThis approach does not rely on assumptions about the data‚Äôs shape, allowing it to adapt to various distributions.\nNonparametric regression provides another illustration. In it, the relationship between \\(X\\) and \\(Y\\) is modeled as:\n\\[Y=m(X)+\\epsilon,\\]\nwhere \\(m(X)\\) is an unknown function estimated directly from the data using methods like local polynomial regression, splines. Unlike the linearity assumption in parametric models, nonparametric regression allows \\(m(X)\\) to capture complex, nonlinear relationships. A commonly used variant of this is LOESS regression often overlayed in bivariate scatterplots.\nMany popular machine learning (ML) methods are also nonparametric. In this context it is important to distinguis between parameters which impose assumptions on the data and hyperparameters which serve to tune the algorithm. Tree-based techniques exemplify the nonparametric nature of ML: gradient boosting machines, random forests, and individual decision trees can all grow more complex as they encounter more data, creating flexible models that capture intricate patterns. Think even of unsupervised clustering models including k-means and hierarchical clustering.\nWhile nonparametric models are powerful, their flexibility comes at a cost: they can overfit small datasets and often require careful tuning (e.g., choosing the kernel bandwidth) to balance bias and variance. Nonetheless, they are invaluable for exploratory analysis and applications where minimal assumptions are desired.\nDid I also mention the curse of dimensionality?! You will quickly fall into its trap with even moderate number of variables. Yes, there is just so much you can do without adding some structure to your models."
  },
  {
    "objectID": "blog/three-classes-stat-models.html#an-example",
    "href": "blog/three-classes-stat-models.html#an-example",
    "title": "The Three Classes of Statistical Models",
    "section": "An Example",
    "text": "An Example\nWe illustrate the distinctions between parametric, semiparametric, and nonparametric models using a toy example. We generate \\(1,000\\) observations of\n\\[Y=sin(X)+\\epsilon,\\]\nwhere \\(X\\) is uniformly distributed and is normally distributed noise. We then model the relationship between \\(Y\\) and \\(X\\) using three approaches.\n\nRPython\n\n\n# clear workspace and load libraries\nrm(list=ls())\nset.seed(1988)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nx &lt;- runif(1000, 0, 10)  # Uniformly distributed x\ny &lt;- sin(x) + rnorm(1000, mean = 0, sd = 0.3)  # Non-linear relationship with noise\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit models\nlinear_model &lt;- lm(y ~ x, data = data)\nloess_model &lt;- loess(y ~ x, data = data, span = 0.3)\n\n# Piecewise linear model\nsplit_point &lt;- median(data$x)\ndata &lt;- data %&gt;%\n  mutate(split_group = ifelse(x &lt;= split_point, \"first_half\", \"second_half\"))\n\nlinear1 &lt;- lm(y ~ x, data = filter(data, split_group == \"first_half\"))\nlinear2 &lt;- lm(y ~ x, data = filter(data, split_group == \"second_half\"))\n\n# Predictions\ndata &lt;- data %&gt;%\n  mutate(pred_linear = predict(linear_model, newdata = data),\n         pred_loess = predict(loess_model, newdata = data))\n\npiecewise_preds &lt;- bind_rows(\n  data.frame(x = filter(data, split_group == \"first_half\")$x,\n             y = predict(linear1, newdata = filter(data, split_group == \"first_half\"))),\n  data.frame(x = filter(data, split_group == \"second_half\")$x,\n             y = predict(linear2, newdata = filter(data, split_group == \"second_half\")))\n) %&gt;%\n  arrange(x)\n\n# Plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  geom_line(aes(y = pred_linear, color = \"Parametric (Linear)\"), linewidth = 1) +\n  geom_line(aes(y = pred_loess, color = \"Nonparametric (Loess)\"), linewidth = 1) +\n  geom_line(data = piecewise_preds, aes(x = x, y = y, color = \"Semiparametric (Piecewise Linear)\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Parametric (Linear)\" = \"red\",\n                                \"Nonparametric (Loess)\" = \"blue\",\n                                \"Semiparametric (Piecewise Linear)\" = \"green\")) +\n  labs(title = \"Bivariate Scatterplot with Regression Fits\",\n       x = \"X\", y = \"Y\", color = \"Model Type\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = c(0.8, 0.2))\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nnp.random.seed(1988)\n\nx = np.random.uniform(0, 10, 1000)\ny = np.sin(x) + np.random.normal(0, 0.3, 1000)\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# Linear model\nlinear_model = LinearRegression()\nlinear_model.fit(data[['x']], data['y'])\ndata['pred_linear'] = linear_model.predict(data[['x']])\n\n# Loess model\nloess_result = lowess(data['y'], data['x'], frac=0.3, return_sorted=True)\ndata['pred_loess'] = np.interp(data['x'], loess_result[:, 0], loess_result[:, 1])\n\n# Piecewise linear\nsplit_point = np.median(data['x'])\ndata['split_group'] = np.where(data['x'] &lt;= split_point, 'first_half', 'second_half')\n\nfirst_half = data[data['split_group'] == 'first_half'].copy()\nsecond_half = data[data['split_group'] == 'second_half'].copy()\n\nlinear1 = LinearRegression()\nlinear1.fit(first_half[['x']], first_half['y'])\nfirst_half['pred_piecewise'] = linear1.predict(first_half[['x']])\n\nlinear2 = LinearRegression()\nlinear2.fit(second_half[['x']], second_half['y'])\nsecond_half['pred_piecewise'] = linear2.predict(second_half[['x']])\n\npiecewise_preds = pd.concat([first_half[['x', 'pred_piecewise']],\n                             second_half[['x', 'pred_piecewise']]]).sort_values('x')\n\n# Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='x', y='y', data=data, alpha=0.6, label='Data')\nplt.plot(data.sort_values('x')['x'], data.sort_values('x')['pred_linear'], color='red', label='Parametric (Linear)', linewidth=1)\nplt.plot(data.sort_values('x')['x'], data.sort_values('x')['pred_loess'], color='blue', label='Nonparametric (Loess)', linewidth=1)\nplt.plot(piecewise_preds['x'], piecewise_preds['pred_piecewise'], color='green', label='Semiparametric (Piecewise Linear)', linewidth=1)\n\nplt.title('Bivariate Scatterplot with Regression Fits', fontsize=14)\nplt.xlabel('X', fontsize=12)\nplt.ylabel('Y', fontsize=12)\nplt.legend(title='Model Type', loc='lower right')\nplt.grid(True)\nplt.show()\n\n\n\nA parametric model, such as linear regression (although it could also be quadratic or a higher order polynomial), provides a simple, interpretable approximation but may miss crucial aspects of the true relationship. A ‚Äúsemiparametric‚Äù model, like piecewise linear regression, offers greater flexibility by allowing for changes in slope, capturing some curvature while maintaining a degree of interpretability. (One can cast this piecewise linear model as a parametric one, but for simplicity‚Äôs sake let‚Äôs go with this uncommon and imprecise definition of semiparametric.) Finally, a nonparametric model, such as LOESS, provides the most flexible representation, closely following the underlying sinusoidal pattern but potentially leading to overfitting.\n\nYou should not be surprised. This example demonstrates how the choice of model class significantly impacts the flexibility and interpretability of the fitted relationship. Do not take this example too seriously, it merely serves to illustrate the varying degree of complexity of statistical models."
  },
  {
    "objectID": "blog/three-classes-stat-models.html#bottom-line",
    "href": "blog/three-classes-stat-models.html#bottom-line",
    "title": "The Three Classes of Statistical Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nParametric models impose the strongest assumptions and require the least amount of data. These are most models employed in practice. Think of linear regression.\nSemiparametric models strike balance between flexibility and interpretation while allowing for flexible relationships in the data. Think of (non-Augmented) Inverse Propensity Score Weighting.\nNonparametric models are flexible and data-hungry. They allow for flexible associations between your variables. Think of a histrogram or kernel density."
  },
  {
    "objectID": "blog/two-types-weights-causality.html",
    "href": "blog/two-types-weights-causality.html",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "",
    "text": "Causal inference fundamentally seeks to answer: What is the effect of a treatment or intervention? The challenge lies in ensuring that the comparison groups‚Äîtreated versus untreated‚Äîare balanced in terms of their characteristics, so differences in outcomes can be attributed solely to the intervention. This idea, often referred to as ‚Äúlike-with-like‚Äù or ‚Äúapples-to-apples‚Äù comparison, is as simple as it is intuitive.\nBalance is thus fundamental to causal inference. Weighting methods, central to achieving this balance, have evolved to include two primary types: inverse propensity score weights and covariate balancing weights. This article briefly describes these weights, their mathematical foundations, and the intuition behind them, with an example in R to bring it all together."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#background",
    "href": "blog/two-types-weights-causality.html#background",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "",
    "text": "Causal inference fundamentally seeks to answer: What is the effect of a treatment or intervention? The challenge lies in ensuring that the comparison groups‚Äîtreated versus untreated‚Äîare balanced in terms of their characteristics, so differences in outcomes can be attributed solely to the intervention. This idea, often referred to as ‚Äúlike-with-like‚Äù or ‚Äúapples-to-apples‚Äù comparison, is as simple as it is intuitive.\nBalance is thus fundamental to causal inference. Weighting methods, central to achieving this balance, have evolved to include two primary types: inverse propensity score weights and covariate balancing weights. This article briefly describes these weights, their mathematical foundations, and the intuition behind them, with an example in R to bring it all together."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#notation",
    "href": "blog/two-types-weights-causality.html#notation",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Notation",
    "text": "Notation\nTo set the stage, we begin by establishing the potential outcome framework and laying out the notation for the discussion:\n\n\\(Y(0), Y(1)\\): Potential outcomes under treatment and control.\n\\(W\\): Treatment indicator (\\(1\\) for treated, \\(0\\) for untreated).\n\\(X\\): Vector of covariates (i.e., control variables).\n\\(e(X)=P(W=1 \\mid X)\\): Propensity score, the probability of receiving treatment given covariates.\n\\(\\mu_1‚Äã=E[Y(1)]\\): Mean outcome under treatment.\n\\(\\tau=\\mu_1 - \\mu_0\\): Average treatment effect (ATE), the main object of interest.\n\\(n\\): number of observations in the sample.\n\\(n_T\\): number of observations in the treatment group.\n\nWe observe a random sample of size \\(n\\), of \\(\\{Y_i, W_i, X_i \\}\\), where \\(i\\) indexes units (e.g., individuals, firms, schools etc.). Under the assumptions of strong ignorability‚Äîunconfoundedness \\(W \\perp (Y(0), Y(1)) \\mid X\\) and overlap \\((0&lt;e(X)&lt;1)\\) ‚Äî the ATE can be identified and estimated."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#diving-deeper",
    "href": "blog/two-types-weights-causality.html#diving-deeper",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nInverse Propensity Score Weights\nInverse propensity score (IPS) weights rely on the estimated propensity score (X). The weights for treated and untreated groups are defined as:\n\\[ \\gamma_{\\text{IPS}}(X) = \\begin{cases} 1 / \\hat{e}(X) & \\text{if } W = 1, \\\\ 1 / (1 - \\hat{e}(X)) & \\text{if } W = 0. \\end{cases} \\]\nIPS weights intuitively correct for the unequal probability of treatment assignment. If an individual has a low probability of treatment \\(e(X)\\approx 0\\) but is treated, their weight is large, amplifying their influence in the analysis. Conversely, individuals with high treatment probabilities are downweighted to prevent overrepresentation.\nA few notes. First, these weights adjust the observed data such that the distribution of covariates in the weighted treated and control groups resembles that of the overall population. This helps mitigate selection bias, ensuring that comparisons between treated and control groups reflect a treatment effect rather than confounded differences in covariates (e.g., the control group is older).\nA second key property is that the weighted average of the outcomes for the treated group is an unbiased estimator for the mean outcome under treatment, \\(\\mu_1\\). This can be expressed mathematically as:\n\\[ \\hat{\\mu}_1(X)=\\frac{\\sum_i W_i Y_i/ \\hat{e}(X)}{ \\sum_i W_i / \\hat{e}(X)}=\\frac{1}{n_T}\\sum_i  \\gamma_{\\text{IPS}}(X_i) W_i Y_i. \\]\nThis equality is particularly useful in the next step, when estimating the treatment effect \\(\\tau\\).\nThird, these IPS weights are generally unknown and have to be estimated from the data. A leading exception is controlled randomized experiments where the researcher determines the probability of treatment. This is, however, uncommon. Traditionally, practitioners rely on methods like logistic regression to first obtain \\(\\hat{e}(X)\\) and then take its reciprocal to construct the weights.\nUsing IPS weights entails two primary challenges. When \\(\\hat{e}(X)\\) is close to \\(0\\) or \\(1\\), weights can become excessively large, leading to high variance in estimates. Practitioners then turn to trim observations with ‚Äútoo large‚Äù or ‚Äútoo small‚Äù \\(\\hat{e}(X)\\) values. Moreover, model misspecification in \\(\\hat{e}(X)\\) can lead to poor covariate balance, introducing bias. More flexible methods such as machin learning models can alleviate this problem.\nSoftware Packages: MatchIt, Matching.\n\n\nCovariate Balancing Weights\nAn alternative approach seeks weights that directly balance the dataset at hand. Typically this is framed as a constrained optimization problem with placing restrictions on the maximum imbalance in X between the treatment and control groups.\nThe weights \\(\\gamma_{\\text{CB}}(X)\\) are obtained by solving:\n\\[ \\min_{\\gamma} \\text{Imbalance}(\\gamma) + \\lambda \\text{Penalty}(\\gamma),\\]\nwhere ‚ÄúImbalance‚Äù measures covariate discrepancies between groups, and ‚ÄúPenalty‚Äù controls for extreme weights. A common formulation balances means:\n\\[ \\frac{1}{n} \\sum_{i=1}^n W_i \\gamma(X_i) f(X_i) \\approx \\frac{1}{n} \\sum_{i=1}^n f(X_i), \\]\nwith \\(f(x_i)=x\\). The same idea can be applied to higher moments of \\(X\\) like variance or skewness. Examples methods relying on this type of weights include include Hainmueller (2012), Chan et al.¬†(2016), and Athey et al.¬†(2018), among many others.\nCovariate balancing weights bypass the need to explicitly estimate \\(e(X)\\). Instead, they solve for weights that ensure the treated and control groups are balanced across predefined covariates or their transformations. This method aligns closely with the goal of causal inference: achieving balance. The main challenge is selecting the right set of covariates to be balanced. Variance estimation can also be more involved, although modern statistical packages take care of it.\nSoftware Packages: MatchIt, Matching."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#hybrid-approach",
    "href": "blog/two-types-weights-causality.html#hybrid-approach",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Hybrid Approach",
    "text": "Hybrid Approach\nImai and Ratkovic (2013) developed a hybrid method that combines elements of covariate balancing and inverse propensity score methods. Their method, known as Covariate Balancing Propensity Score (CBPS), aims to estimate propensity scores while simultaneously achieving covariate balance. Instead of relying on a tuning parameter, CBPS ensures balance by solving the following moment conditions:\n\\[ \\sum_{i}\\left[ W_i - e(X_i; \\gamma) \\right] X_i = 0. \\]\nAdditionally, CBPS estimates by solving the standard likelihood score equation:\n\\[ \\sum_{i} \\left[ W_i - e(X_i; \\gamma) \\right] \\frac{\\partial e(X_i; \\gamma)}{\\partial \\gamma} = 0. \\]\nThe CBPS is operationalized in the Genralized Method of Moments (GMM) framework from the econmetrics literature. This approach ensures that the estimated propensity scores lead to balanced covariates, potentially improving the robustness of causal inference. CBPS can be implemented using available GMM software packages and is particularly useful when traditional propensity score models fail to achieve adequate balance.\nSoftware Packages: CBPS."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#an-example",
    "href": "blog/two-types-weights-causality.html#an-example",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate these methods in practice with R. Consider a dataset with treatment \\(W\\), outcome \\(Y\\), and covariates \\(X_1, X_2‚Äã\\). We estimate the ATE using both IPS, entropy balancing weights and CBPS. The exercise starts with generating some synthetic data.\nrm(list=ls())\nlibrary(MASS)\nlibrary(WeightIt)\nset.seed(1988)\n\n# generate fake data\nn &lt;- 1000\nX1 &lt;- rnorm(n)\nX2 &lt;- rnorm(n)\nW &lt;- rbinom(n, 1, plogis(0.5 * X1 - 0.25 * X2))\nY &lt;- 3 + 2 * W + X1 + X2 + rnorm(n)\ndata = data.frame(Y, W, X1, X2)\n\n# define functions that will calculate the weights and the associated Average Treatment Effects.\n\ncompute_weights &lt;- function(method) {\n    weightit(W ~ X1 + X2, method = method, data = data)$weights\n}\n\ncompute_ate &lt;- function(weights) {\n     weighted.mean(Y[W == 1], weights = weights[W == 1]) - \n        weighted.mean(Y[W == 0], weights = weights[W == 0])\n}\n\n# calcualte the three types of estimates.\nips_weights &lt;- compute_weights(\"glm\")\nebal_weights &lt;- compute_weights(\"ebal\")\ncbps_weights &lt;- compute_weights(\"cbps\")\n\n# we estimate the average treatment effect and print the results.\nips_ate &lt;- compute_ate(ips_weights)\nebal_ate &lt;- compute_ate(ebal_weights)\ncbps_ate &lt;- compute_ate(cbps_weights)\n\ncat(\"ATE (IPS Weights):\", ips_ate, \"\\n\")\n&gt;ATE (IPS Weights): 2.287048 \ncat(\"ATE (Entropy Balance Weights):\", ebal_ate, \"\\n\")\n&gt;ATE (Entropy Balance Weights): 2.287048 \n\ncat(\"ATE (CBPS Weights):\", cbps_ate, \"\\n\")\n&gt;ATE (CBPS Weights): 2.287048 \nThe weights are all very highly correlated with each other (not shown above), so they yield nearly identical results. For simplicity, I have ignored variance estimation and confidence intervals."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#where-to-learn-more",
    "href": "blog/two-types-weights-causality.html#where-to-learn-more",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThis article was inspired by Ben-Michael et al.¬†(2021) and Chattopadhyay et al.¬†(2020). Both references are great starting points. There are plenty of accessible materials on the topic online. My favorite is Imbens (2015). For more in-depth content turn to Imbens and Rubin (2015)‚Äôs seminal textbook."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#bottom-line",
    "href": "blog/two-types-weights-causality.html#bottom-line",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nCovariate balance between the treatment and control groups is at the core of causal inference.\nThere are two broad classes of weights that achieve such balance.\nIPS weights adjust for treatment probability but can be unstable.\nCovariate balancing weights directly target balance X, bypassing propensity score estimation."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#references",
    "href": "blog/two-types-weights-causality.html#references",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "References",
    "text": "References\nAthey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing: debiased inference of average treatment effects in high dimensions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 80(4), 597-623.\nBen-Michael, E., Feller, A., Hirshberg, D. A., & Zubizarreta, J. R. (2021). The balancing act in causal inference. arXiv preprint arXiv:2110.14831.\nChan, K. C. G., Yam, S. C. P., & Zhang, Z. (2016). Globally efficient non-parametric inference of average treatment effects by empirical balancing calibration weighting. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(3), 673-700.\nChattopadhyay, A., Hase, C. H., & Zubizarreta, J. R. (2020). Balancing vs modeling approaches to weighting in practice. Statistics in Medicine, 39(24), 3227-3254.\nHainmueller, J. (2012). Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20(1), 25-46.\nHirshberg, D. A., & Wager, S. (2018). Augmented minimax linear estimation for treatment and policy evaluation.\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society Series B: Statistical Methodology, 76(1), 243-263.\nImbens, G. W., & Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical sciences. Cambridge university press.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nLi, F., Morgan, K. L., & Zaslavsky, A. M. (2018). Balancing covariates via propensity score weighting. Journal of the American Statistical Association, 113(521), 390-400.\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41-55."
  },
  {
    "objectID": "blog/stein-paradox.html",
    "href": "blog/stein-paradox.html",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "In the realm of statistics, few findings are as counterintuitive and fascinating as Stein‚Äôs paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein‚Äôs paradox is not merely about memorizing an oddity‚Äîit‚Äôs about recognizing the delicate balance inherent in statistical decision-making.\nIn essence, Stein‚Äôs paradox asserts that when estimating the means of multiple variables simultaneously, it‚Äôs possible to achieve better results compared to relying solely on the sample averages.\nLet‚Äôs now delve deeper into this statement and unravel the underlying mechanisms."
  },
  {
    "objectID": "blog/stein-paradox.html#background",
    "href": "blog/stein-paradox.html#background",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "In the realm of statistics, few findings are as counterintuitive and fascinating as Stein‚Äôs paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein‚Äôs paradox is not merely about memorizing an oddity‚Äîit‚Äôs about recognizing the delicate balance inherent in statistical decision-making.\nIn essence, Stein‚Äôs paradox asserts that when estimating the means of multiple variables simultaneously, it‚Äôs possible to achieve better results compared to relying solely on the sample averages.\nLet‚Äôs now delve deeper into this statement and unravel the underlying mechanisms."
  },
  {
    "objectID": "blog/stein-paradox.html#diving-deeper",
    "href": "blog/stein-paradox.html#diving-deeper",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "Diving Deeper",
    "text": "Diving Deeper"
  },
  {
    "objectID": "blog/stein-paradox.html#refresher-on-mean-squared-error-mse",
    "href": "blog/stein-paradox.html#refresher-on-mean-squared-error-mse",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "Refresher on Mean Squared Error (MSE)",
    "text": "Refresher on Mean Squared Error (MSE)\nTo understand the paradox, let‚Äôs begin by quantify what we mean by ‚Äúbetter‚Äù estimation. The MSE of an estimator \\(\\hat{\\mu}\\) is defined as its expected squared error (or loss):\n\\[\\text{MSE}(\\hat{\\mu}) = \\mathbb{E}\\left[ ( \\hat{\\mu} - \\mu )^2 \\right],\\]\nwhere \\(\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_p)\\) is the true vector of means. All else equal, the lower MSE the better.\n\nMathematical Formulation\nStein‚Äôs paradox arises in the context of estimating multiple parameters simultaneously. Suppose you‚Äôre estimating the mean \\(\\mu_i\\) of several independent normal distributions:\n\\[X_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, p,\\]\nwhere \\(X_i\\) are observed values, \\(\\mu_i\\) are the unknown means, and \\(\\sigma^2\\) is known. We assume non-zero covariance between the variables.\nA natural approach is to estimate each _i using its corresponding sample mean \\(X_i\\), which is the maximum likelihood estimator (MLE). However, in dimensions \\(p \\geq 3\\), this seemingly reasonable approach is dominated by an alternative method.\nThe surprise? Shrinking the individual estimates toward a common value‚Äîsuch as the overall mean‚Äîproduces an estimator with uniformly lower expected squared error.\nThe MSE of the MLE is equal to:\n\\[R(\\hat{\\mu}_\\text{MLE}) = p\\sigma^2.\\]\nNow consider the biased(!) James-Stein (JS) estimator:\n\\[\\hat{\\mu}_\\text{JS} = \\left( 1 - \\frac{(p - 2)\\sigma^2}{\\lVert X \\rVert^2} \\right) X,\\]\nwhere \\(\\lVert X \\rVert^2 = \\sum_i X_i^2\\) is the squared norm of the observed data. The shrinkage factor \\(1 - \\frac{(p - 2)\\sigma^2}{|X|^2}\\) pulls the estimates toward zero (or any other pre-specified point).\nRemarkably, the James-Stein estimator has lower MSE than the MLE for \\(p \\geq 3\\):\n\\[\\text{MSE}(\\hat{\\mu}_{\\text{JS}}) &lt; \\text{MSE}(\\hat{\\mu}_{\\text{MLE}}).\\]\nThis is the mystery at its core.\n\n\nAn Explanation\nThis result holds because the James-Stein estimator balances variance reduction and bias introduction in a way that minimizes overall MSE. The MLE, in contrast, does not account for the possibility of shared structure among the parameters. The counterintuitive nature of this result stems from the independence of the \\(X_i\\)‚Äôs. Intuition suggests that pooling information across independent observations should not improve estimation, yet the James-Stein estimator demonstrates otherwise."
  },
  {
    "objectID": "blog/stein-paradox.html#an-example",
    "href": "blog/stein-paradox.html#an-example",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs emulate this paradox in R and python in a setting with \\(p=5\\).\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\np &lt;- 5  # Number of means\nn &lt;- 1000  # Number of simulations\nsigma &lt;- 1\nmu &lt;- rnorm(p, mean = 5, sd = 2)  # True means\n\n# results storage\nmse_mle &lt;- numeric(n)  \nmse_js &lt;- numeric(n)\n\n# create a fake dataset and compute the MSEs of both the MLE and JS estimator. \n# repeat this 1,000 times and take the average loss for each estimator.\nfor (sim in 1:n) {\n  # Simulate observations\n  X &lt;- rnorm(p, mean = mu, sd = sigma)\n\n  # MLE estimator and its MSE\n  mle &lt;- X\n  mse_mle[sim] &lt;- sum((mle - mu)^2)\n\n  # James-Stein estimator and its MSE\n  shrinkage &lt;- max(0, 1 - ((p - 2) * sigma^2) / sum(X^2))\n  js &lt;- shrinkage * X\n  mse_js[sim] &lt;- sum((js - mu)^2)\n}\n\n# print the results.\ncat(\"Average MSE of MLE:\", mean(mse_mle), \"\\n\")\n&gt; Average MSE of MLE: 5.125081 \ncat(\"Average MSE of James-Stein:\", mean(mse_js), \"\\n\")\n&gt; Average MSE of James-Stein: 5.055019 \n\n\nimport numpy as np\nnp.random.seed(1988)\n\n# Parameters\np = 5  # Number of means\nn = 1000  # Number of simulations\nsigma = 1\nmu = np.random.normal(loc=5, scale=2, size=p)  # True means\n\n# Results storage\nmse_mle = np.zeros(n)\nmse_js = np.zeros(n)\n\n# Simulate data and compute MSEs for MLE and James-Stein estimator\nfor sim in range(n):\n    # Simulate observations\n    X = np.random.normal(loc=mu, scale=sigma, size=p)\n\n    # MLE estimator and its MSE\n    mle = X\n    mse_mle[sim] = np.sum((mle - mu) ** 2)\n\n    # James-Stein estimator and its MSE\n    shrinkage = max(0, 1 - ((p - 2) * sigma**2) / np.sum(X**2))\n    js = shrinkage * X\n    mse_js[sim] = np.sum((js - mu) ** 2)\n\n# Print the results\nprint(\"Average MSE of MLE:\", np.mean(mse_mle).round(3))\n&gt; Average MSE of MLE: 4.998\nprint(\"Average MSE of James-Stein:\", np.mean(mse_js).round(3))\n&gt; Average MSE of James-Stein: 4.951\n\n\n\nIn this example, average MSE of the James-Stein estimator (\\(5.06\\)) is consistently lower than that of the MLE (\\(5.13\\)), illustrating the paradox in action."
  },
  {
    "objectID": "blog/stein-paradox.html#bottom-line",
    "href": "blog/stein-paradox.html#bottom-line",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nStein‚Äôs paradox shows that shrinkage estimators can outperform the MLE in dimensions \\(p \\geq 3\\), even when the underlying variables are independent.\nThe James-Stein estimator achieves lower MSE by balancing variance reduction and bias introduction.\nUnderstanding this result highlights the power of shrinkage techniques in high-dimensional statistics."
  },
  {
    "objectID": "blog/stein-paradox.html#references",
    "href": "blog/stein-paradox.html#references",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "References",
    "text": "References\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html",
    "href": "blog/filling-missing-data-mcmc.html",
    "title": "Filling in Missing Data with MCMC",
    "section": "",
    "text": "Every dataset inevitably contains missing or incomplete values. Practitioners then face the dilemma of how to address these missing observations. A common approach, though potentially problematic, is to simply ignore them. I am guilty of doing this all too often. While convenient, ignoring missing data can introduce bias into analyses, particularly if the missingness is not entirely random. Moreover, throwing away data usually results in loss of statistical precision. Traditional methods for handling missing data, such as mean or median imputation, usually oversimplify the underlying data-generating process. Regression-based adjustments offer some improvement, but they rely on the linearity assumption.\nThis article introduces Markov Chain Monte Carlo (MCMC) as a robust and theoretically sound methodology for addressing missing data. Unlike arbitrary imputation methods, MCMC leverages the inherent information within the dataset to generate plausible values for the missing observations. The core principle of MCMC involves treating missing data as random variables and employing the algorithm to sample from their posterior distribution, thereby capturing the uncertainty and built-in structure within the data. The use of MCMC to draw observations repeatedly in the context of missing data is often referred to as Multiple Imputation (MI).\nLet‚Äôs break this down step by step, explore the underlying intuition, as well as an illustrative example, assuming a basic understanding of probability theory and Bayesian methods."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#background",
    "href": "blog/filling-missing-data-mcmc.html#background",
    "title": "Filling in Missing Data with MCMC",
    "section": "",
    "text": "Every dataset inevitably contains missing or incomplete values. Practitioners then face the dilemma of how to address these missing observations. A common approach, though potentially problematic, is to simply ignore them. I am guilty of doing this all too often. While convenient, ignoring missing data can introduce bias into analyses, particularly if the missingness is not entirely random. Moreover, throwing away data usually results in loss of statistical precision. Traditional methods for handling missing data, such as mean or median imputation, usually oversimplify the underlying data-generating process. Regression-based adjustments offer some improvement, but they rely on the linearity assumption.\nThis article introduces Markov Chain Monte Carlo (MCMC) as a robust and theoretically sound methodology for addressing missing data. Unlike arbitrary imputation methods, MCMC leverages the inherent information within the dataset to generate plausible values for the missing observations. The core principle of MCMC involves treating missing data as random variables and employing the algorithm to sample from their posterior distribution, thereby capturing the uncertainty and built-in structure within the data. The use of MCMC to draw observations repeatedly in the context of missing data is often referred to as Multiple Imputation (MI).\nLet‚Äôs break this down step by step, explore the underlying intuition, as well as an illustrative example, assuming a basic understanding of probability theory and Bayesian methods."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#notation",
    "href": "blog/filling-missing-data-mcmc.html#notation",
    "title": "Filling in Missing Data with MCMC",
    "section": "Notation",
    "text": "Notation\nTo keep things precise, let‚Äôs set up some notation. Let Y be the complete dataset, which we wish we had. We observe some, but not all observations of Y. Let‚Äôs split it into observed data \\(Y_{\\text{obs}}\\), and missing (or incomplete) data \\(Y_{\\text{miss}}\\), so that \\(Y = \\left( Y_{\\text{obs}}, Y_{\\text{miss}} \\right)\\).\nAssume a model for the data parameterized by \\(\\theta\\), that is \\(Y \\sim f(Y \\mid \\theta)\\). A simple example would be that a univaraite \\(Y\\) is Gaussian with some unspecified mean and variance. Our goal is to estimate and fill in \\(Y_{\\text{miss}}\\) by sampling from the posterior distribution\n\\[P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta).\\]\nThe results and intuition hold also conditional on some covariates \\(X\\), but for simplicity‚Äôs sake, I will keep that out of the notation for now."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#diving-deeper",
    "href": "blog/filling-missing-data-mcmc.html#diving-deeper",
    "title": "Filling in Missing Data with MCMC",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nDefinition\nMarkov Chain Monte Carlo is a powerful computational technique designed to draw observations from complex probability distributions that are difficult to directly sample from. This might happen because they do not have a nice closed-form analytical expression, or they do, but it‚Äôs too messy. Such distributions often arise in Bayesian statistics, where we aim to estimate the posterior distribution of parameters given observed data.\nThe ‚Äúmagic‚Äù of MCMC lies in its iterative nature. It begins with an initial guess for the parameter \\(\\theta\\). Then, a sophisticated sampling algorithm, such as the Metropolis-Hastings or Gibbs sampler, is employed to generate a sequence of observations. These observations are not independent but are related to each other in a specific way, forming a Markov chain. Crucially, under certain conditions, this Markov chain will eventually converge to the true target distribution.\nIn the context of missing data, MCMC iteratively alternates between the \\(I\\)- and the \\(P\\)-steps. At the \\(t\\)-th iteration with current guess for \\(\\theta\\) denoted \\(\\theta^t\\), these steps are:\n\nThe \\(I\\)-step (imputation): Draw \\(Y_{\\text{miss}}\\) from \\(P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta^t)\\). That is, from its conditional distribution given the observed data and current parameter estimates.\nThe \\(P\\)-step (posterior): Draw \\(\\theta^{t+1}\\) from \\(P(\\theta \\mid Y_{\\text{obs}}, Y_{\\text{miss}}^{t+1})\\). This is its posterior distribution given the observed data and the newly imputed \\(Y_{\\text{miss}}\\).\n\nThis back-and-forth dance ensures that the imputed values reflect the uncertainty and structure of the data. And with enough iterations (large t) the chain will converge to our target, \\(P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta)\\).\nSoftware Packages: mcmc, MCMCPack, mice.\n\n\nPractical Considerations\nConvergence diagnostics are crucial to ensure the MCMC chains have reached a stable equilibrium, as the initial values can significantly influence the results. In simple words, the chain should run long enough so that the posterior distribution does not change significantly after each additional iteration. It is also common to discard (or ‚Äúburn‚Äù) an initial batch of values since they do not come from the final, stable posterior distribution. Additionally, computational costs can be a significant factor, especially for large datasets or complex models but efficient algorithms and parallel processing can help. Lastly, model specification is critical, as the choice of imputation model directly impacts the quality of the imputed values."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#an-example",
    "href": "blog/filling-missing-data-mcmc.html#an-example",
    "title": "Filling in Missing Data with MCMC",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs walk through an example using a simple dataset with missing values. Suppose you have a dataset with two variables, \\(X\\) and \\(Y\\), where \\(Y \\sim N(\\beta_0 + \\beta_1 X, \\sigma^2)\\), and some values of \\(Y\\) are missing. We assume the following relationship:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\nwhere \\(\\epsilon\\) is an error term. We impose priors on \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) (which collectively comprise \\(\\theta\\) in this exampe).\nWe begin with generating some fake data and introduce missingness in \\(Y\\).\nrm(list=ls())\nset.seed(1988)\nlibrary(mice)\n\n# generate fake data\nn &lt;- 100                  \nc &lt;- 0.2                 \nX &lt;- rnorm(n, mean = 5, sd = 2)\nbeta0 &lt;- 2                 \nbeta1 &lt;- 1.5              \nepsilon &lt;- rnorm(n, mean = 0, sd = 1)  \nY &lt;- beta0 + beta1 * X + epsilon      \n\n# introduce missingness in Y\nmissing_indices &lt;- sample(1:n, size = n * c, replace = FALSE)\nY[missing_indices] &lt;- NA  \n\n# combine data into a data frame\ndata &lt;- data.frame(X = X, Y = Y)\nhead(data)\n\n# perform imputation\nimputed_data &lt;- mice(data, \n                    m = 5, \n                    method = \"norm\", \n                    seed = 1988)\nmodels &lt;- with(imputed_data, lm(Y ~ X))\n\n# print results\nsummary(pool(models))\n\n         term estimate std.error statistic       df      p.value\n1 (Intercept) 1.730583 0.3008046  5.753179 49.51038 5.435802e-07\n2           X 1.546689 0.0544164 28.423207 52.02756 2.313802e-33\nThis is clearly a silly example since \\(Y\\) is missing at random, suggesting that missing data does not result in bias. Anyway, for illustration purposes we run the Bayesian Regression algorithm to fill in the missing \\(Y\\) and proceed with a linear regression of \\(Y\\) on \\(X\\).\nSpecifically, the code below uses normal (linear regression) imputation to fill in the missing values. For each missing point the algorithm fits a linear regression model predicting \\(Y\\) from \\(X\\) using the complete data and then use this model to predict or impute the missing \\(Y\\). This process is repeated \\(m\\) times (hence the name multiple imputation), creating \\(m\\) different versions of the dataset with the missing values filled in.\nBoth coefficients fall in the expected respective regions."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#where-to-learn-more",
    "href": "blog/filling-missing-data-mcmc.html#where-to-learn-more",
    "title": "Filling in Missing Data with MCMC",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFollowing some computational innovations, Bayesian methods have experienced somewhat of a revival in the last fiveteen years. Consequently, there are plenty of high-quality materials online. Takahashi (2017) is an accessible resource on MCMC and Multiple Imputation which I used extensively."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#bottom-line",
    "href": "blog/filling-missing-data-mcmc.html#bottom-line",
    "title": "Filling in Missing Data with MCMC",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMissing data is an ever-present issue in practice.\nStandard approaches to dealing with missing information include ignoring it or imputing it with mean or predicted values.\nMCMC leverages the full joint distribution of the data, making it a robust imputation method.\nBy alternating between imputing missing values and updating parameters, MCMC aligns imputations with the observed data‚Äôs structure."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#references",
    "href": "blog/filling-missing-data-mcmc.html#references",
    "title": "Filling in Missing Data with MCMC",
    "section": "References",
    "text": "References\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman and Hall/CRC.\nRubin, D B 1987 Multiple Imputation for Nonresponse in Surveys. New York, NY: John Wiley & Sons. DOI: https://doi.org/10.1002/9780470316696\nSchafer, J L 1997 Analysis of Incomplete Multivariate Data. Boca Raton, FL: Chapman & Hall/CRC. DOI: https://doi.org/10.1201/9781439821862\nScheuren, F 2005 Multiple imputation: How it began and continues. The American Statistician, 59(4): 315‚Äì319.\nTakahashi, M. (2017). Statistical inference in missing data by MCMC and non-MCMC multiple imputation algorithms: Assessing the effects of between-imputation iterations. Data Science Journal, 16, 37-37."
  },
  {
    "objectID": "blog/binscatter.html",
    "href": "blog/binscatter.html",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "",
    "text": "In the realm of data visualization, the classical scatter plot has long been a staple for exploring bivariate relationships. However, as datasets grow larger and more complex, traditional scatter plots can become cluttered and less informative. Privacy concerns may also limit the ability to plot raw data, and simple bivariate plots often fail to reveal causal relationships. This is where binscatter, or binned scatter plots, come into play.\nBinscatter offers a cleaner, more interpretable way to visualize the relationship between two variables, especially when dealing with large datasets. By aggregating data points into bins and plotting the average outcome within each bin, binscatter simplifies the visualization, making it easier to discern patterns and trends. It‚Äôs particularly useful for:\n\nIntuitive visualization for large datasets by grouping data into bins.\nHighlighting trends and relationship between variables effectively.\nExtending these ideas to control for covariates.\n\nIn this article, I will introduce binscatter, explore its mathematical foundation, and demonstrate its utility with an example in R and python."
  },
  {
    "objectID": "blog/binscatter.html#background",
    "href": "blog/binscatter.html#background",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "",
    "text": "In the realm of data visualization, the classical scatter plot has long been a staple for exploring bivariate relationships. However, as datasets grow larger and more complex, traditional scatter plots can become cluttered and less informative. Privacy concerns may also limit the ability to plot raw data, and simple bivariate plots often fail to reveal causal relationships. This is where binscatter, or binned scatter plots, come into play.\nBinscatter offers a cleaner, more interpretable way to visualize the relationship between two variables, especially when dealing with large datasets. By aggregating data points into bins and plotting the average outcome within each bin, binscatter simplifies the visualization, making it easier to discern patterns and trends. It‚Äôs particularly useful for:\n\nIntuitive visualization for large datasets by grouping data into bins.\nHighlighting trends and relationship between variables effectively.\nExtending these ideas to control for covariates.\n\nIn this article, I will introduce binscatter, explore its mathematical foundation, and demonstrate its utility with an example in R and python."
  },
  {
    "objectID": "blog/binscatter.html#notation",
    "href": "blog/binscatter.html#notation",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Notation",
    "text": "Notation\nTo formalize binscatter, let‚Äôs define the following:\n\n\\(X\\): The independent/predictor variable.\n\\(Y\\): The dependent/outcome/response variable.\n\\(n\\): The number of observations in the dataset.\n\\(K\\): The number of bins into which \\(X\\) is divided.\n\\(\\bar{Y}_k\\): The mean of \\(Y\\) for observations falling in the \\(k\\)-th bin of \\(X\\). Similarly for \\(\\bar{X}_k\\).\n\\(B_k\\)‚Äã: The observations falling in the \\(k\\)-th bin.\n\\(W\\): The covariate to be controlled. This can be a vector too."
  },
  {
    "objectID": "blog/binscatter.html#diving-deeper",
    "href": "blog/binscatter.html#diving-deeper",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nFormal Definition\nA binscatter plot is constructed by partitioning the range of the independent variable X into a fixed number of \\(K\\) bins, \\(B_1,\\dots,B_K\\) typically using empirical quantiles. This ensures each bin is of roughly the same size. Within each bin, the average value of the dependent variable \\(Y\\) is calculated. These averages are then plotted against the midpoint of each bin, \\(\\bar{X}\\), resulting in a series of points that represent an estimate of conditional mean of \\(Y\\) given \\(X\\), $ E[Y|X]$.\nIn technical jargon binscatter provides a nonparametric estimate of the conditional mean function, offering a visual summary of the relationship between the two variables. The resulting graph allows assessment of linearity, monotonicity, convexity, etc.\nHere is the step-by-step recipe for construcing a binscatter plot.\n\n\nAlgorithm:\nBin construction: Divide the range of X into K equal-width bins, or use quantile-based bins for equal sample sizes within bins. For example, with \\(K=10\\), the observations in B_1 would be those between the minimimum value of \\(X\\) and that of its \\(10\\)th percentile. Mean calculation: Compute the mean of Y within each bin:\n\\[\\bar{Y}_k= \\frac{1}{|B_k|} \\sum_{i \\in B_k} Y_i,\\]\nwhere \\(\\midB_k\\mid\\) is the number of observations in bin \\(B_k\\)‚Äã. Plotting: Plot {Y}_k against the midpoints of each bin, \\(\\bar{X}_k\\). Software Package: binsreg.\nQuite simple, right? Let‚Äôs explore certain useful extensions of this idea.\n\n\nAdjusting for Covariates: The Wrong Way\nIn many applications, it is essential to control for additional covariates \\(W\\) to isolate the relationship between the primary variables of interest. The object of interest then becomes the conditional mean \\(E[Y\\mid W,X]\\). An example would be focusing on the relationship between income (\\(Y\\)) and education level (\\(X\\)) when controling for parental education (W).\nA common but flawed approach to incorporating covariates in binscatter is residualized binscatter. This method involves first regressing separately both \\(Y\\) and \\(X\\) on the covariates \\(W\\) to obtain residuals \\(\\hat{u}_Y\\)‚Äã and \\(\\hat{u}_X\\)‚Äã, and then applying the binscatter method to these residuals:\n\\[\\bar{\\hat{u}}_{Y,k} = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\hat{u}_{X,i}.\\]\nWhile this approach is motivated by the Frisch-Waugh-Lovell theorem in linear regression, it can lead to incorrect conclusions in more general settings. The residualized binscatter may not accurately reflect the true conditional mean function, especially if the underlying relationship is nonlinear. Therefore, it is generally not recommended for empirical work.\n\n\nAdjusting for Covariates: The Right Way\nInstead, this should be done using a semi-parametric partially linear regression model. This is achieved by modeling the conditional mean function as\n\\[Y = \\mu_0(X) + W \\gamma_0 + \\varepsilon,\\]\nwhere \\(\\mu_0(X)\\) captures the main effect of \\(X\\), and \\(W' \\gamma_0\\) adjusts for the influence of additional covariates. Rather than residualizing, we estimate \\(\\mu_0(X)\\) using the least-squares approach:\n\\[(\\hat{\\beta}, \\hat{\\gamma}) = \\arg\\min_{\\beta, \\gamma} \\sum (Y- b(X)' \\beta - W' \\gamma)^2,\\]\nwhere \\(b(X)\\) represents the binning basis functions. The final binscatter plot displays the estimated conditional mean function\n\\[\\hat{\\mu}(X_k) = b(X_k)' \\hat{\\beta}\\]\nagainst \\(\\bar{X}_k\\), ensuring a correct visualization of the relationship between \\(X\\) and \\(Y\\) after accounting for the covariates \\(W\\)."
  },
  {
    "objectID": "blog/binscatter.html#practical-considerations",
    "href": "blog/binscatter.html#practical-considerations",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Practical Considerations",
    "text": "Practical Considerations\nA key decision is the choice of the number of bins \\(K\\). Too few bins can oversmooth the data, masking important features, while too many bins can lead to undersmoothing, resulting in a noisy and less interpretable plot. An optimal choice of K balances bias and variance, often determined using data-driven methods. To address this, Cattaneo et al.¬†(2024) propose an adaptive, Integrated Mean Squared Error (IMSE)-optimal choice of K for which get a plug-in formula.\nThoughtful data scientist always have variance in their mind. If, for instance, we see some linear relationship between \\(Y\\) and \\(X\\), how can we determine whether it is statistically significant? Quantifying the uncertainty around binscatter estimates is crucial. The authors also discuss constructing confidence bands, which can be added to the plot to visually represent estimation uncertainty, enhancing both interpretability and reliability."
  },
  {
    "objectID": "blog/binscatter.html#an-example",
    "href": "blog/binscatter.html#an-example",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "An Example",
    "text": "An Example\nAs an example let‚Äôs examine the relationship between the variables Sepal.Length and Petal.Length in the popular iris dataset. We will use a fixed number of ten bins. Alternatively, the package binsreg will automatically calculate the optimal \\(K\\).\n\nRPython\n\n\n# clear the workspace and load libraries\nrm(list=ls())\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(binsreg)\ndata(iris)\n\n# define the number of bins\nbins &lt;- 10\n\n# create binned data\niris_binned &lt;- iris %&gt;%\n  mutate(bin = cut(Sepal.Length, breaks = bins, include.lowest = TRUE)) %&gt;%\n  group_by(bin) %&gt;%\n  summarize(\n    bin_mid = mean(as.numeric(as.character(bin))),\n    mean_petal_length = mean(Petal.Length)\n  )\n\n# Add a panel label for the raw scatter plot\niris_raw &lt;- iris %&gt;% \n    mutate(panel = \"1. Raw Scatter Plot\")\n\n# Add a panel label for the binned scatter plot\niris_binned &lt;- iris_binned %&gt;%\n  mutate(panel = \"2. Binned Scatter Plot\")\n\n# Combine raw and binned data into a single dataset for plotting\nplot_data &lt;- bind_rows(\niris_raw %&gt;% rename(x = Sepal.Length, y = Petal.Length),\n  iris_binned %&gt;% rename(x = bin_mid, y = mean_petal_length)\n)\n\n# Create the plot\nggplot(plot_data, aes(x = x, y = y)) +\n  geom_point() +\n  facet_wrap(~ panel, scales = \"free_x\", ncol = 2) +\n  labs(title = \"Comparison of Raw and Binned Scatter Plots\",\n  x = \"Sepal Length\",\n  y = \"Petal Length\") +\n  theme_minimal()\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Number of bins\nbins = 10\n\n# Create binned data\niris['bin'] = pd.cut(iris['Sepal.Length'], bins=bins, include_lowest=True)\niris_binned = iris.groupby('bin').agg(\n    bin_mid=('Sepal.Length', lambda x: (x.min() + x.max()) / 2),\n    mean_petal_length=('Petal.Length', 'mean')\n).reset_index()\n\n# Add panel labels\niris_raw = iris[['Sepal.Length', 'Petal.Length']].copy()\niris_raw['panel'] = \"1. Raw Scatter Plot\"\n\niris_binned = iris_binned.rename(columns={'bin_mid': 'Sepal.Length', 'mean_petal_length': 'Petal.Length'})\niris_binned['panel'] = \"2. Binned Scatter Plot\"\n\n# Combine raw and binned data\nplot_data = pd.concat([iris_raw, iris_binned], ignore_index=True)\n\n# Plot\nsns.set_theme(style=\"whitegrid\")\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Raw scatter plot\nsns.scatterplot(\n    data=plot_data[plot_data['panel'] == \"1. Raw Scatter Plot\"],\n    x='Sepal.Length', y='Petal.Length', ax=axes[0]\n)\naxes[0].set_title(\"1. Raw Scatter Plot\")\naxes[0].set_xlabel(\"Sepal Length\")\naxes[0].set_ylabel(\"Petal Length\")\n\n# Binned scatter plot\nsns.scatterplot(\n    data=plot_data[plot_data['panel'] == \"2. Binned Scatter Plot\"],\n    x='Sepal.Length', y='Petal.Length', ax=axes[1]\n)\naxes[1].set_title(\"2. Binned Scatter Plot\")\naxes[1].set_xlabel(\"Sepal Length\")\n\n# Adjust layout\nplt.suptitle(\"Comparison of Raw and Binned Scatter Plots\")\nplt.tight_layout()\nplt.show()\n\n\n\nHere is the resulting image. The left scatter plot displays the raw data and the right one shows the binscatter. Binscatter removes some of the clutter and highlights the linear relationship more directly."
  },
  {
    "objectID": "blog/binscatter.html#bottom-line",
    "href": "blog/binscatter.html#bottom-line",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBinscatter simplifies scatterplots by aggregating data into bins and plotting means.\nIt is a powerful tool for visualizing relationships in large or noisy datasets.\nConditional and residualized binscatter extend its utility to controlling for covariates.\nWhile intuitive, binscatter is sensitive to binning choices and may obscure nuances."
  },
  {
    "objectID": "blog/binscatter.html#where-to-learn-more",
    "href": "blog/binscatter.html#where-to-learn-more",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nBoth papers cited below are relatively accessible and will answer your questions. Start with Starr and Goldfarb (2020)."
  },
  {
    "objectID": "blog/binscatter.html#references",
    "href": "blog/binscatter.html#references",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "References",
    "text": "References\nCattaneo, M. D., Crump, R. K., Farrell, M. H., & Feng, Y. (2024). On Binscatter Regressions. American Economic Review, 111(3), 718‚Äì748.\nStarr, E., & Goldfarb, B. (2020). Binned scatterplots: A simple tool to make research easier and better. Strategic Management Journal, 41(12), 2261-2274."
  },
  {
    "objectID": "blog/mutual-information.html",
    "href": "blog/mutual-information.html",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "",
    "text": "When exploring dependencies between variables, the data scientist‚Äôs toolbox often relies on correlation measures to reveal relationships and potential patterns. But what if we‚Äôre looking to capture relationships beyond linear correlations? Mutual Information (MI) quantifies the ‚Äúamount of information‚Äù shared between variables in a more general sense. It measures how much we know about \\(Y\\) by observing \\(X\\). This approach goes beyond linear patterns and can help us uncover more complex relationships within data.\nMI can be especially helpful for applications such as feature selection and unsupervised learning. For readers familiar with various types of correlation metrics, Mutual Information provides an additional lens to interpret relationships within data.\nIn this article, I‚Äôll guide you through the concept of Mutual Information, its definition, properties, and usage, as well as comparisons with other dependency measures. We‚Äôll explore MI‚Äôs mathematical foundations, its advantages and limitations, and its applications, concluding with an example demonstrating MI‚Äôs behavior alongside more traditional measures like Pearson‚Äôs correlation."
  },
  {
    "objectID": "blog/mutual-information.html#introduction",
    "href": "blog/mutual-information.html#introduction",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "",
    "text": "When exploring dependencies between variables, the data scientist‚Äôs toolbox often relies on correlation measures to reveal relationships and potential patterns. But what if we‚Äôre looking to capture relationships beyond linear correlations? Mutual Information (MI) quantifies the ‚Äúamount of information‚Äù shared between variables in a more general sense. It measures how much we know about \\(Y\\) by observing \\(X\\). This approach goes beyond linear patterns and can help us uncover more complex relationships within data.\nMI can be especially helpful for applications such as feature selection and unsupervised learning. For readers familiar with various types of correlation metrics, Mutual Information provides an additional lens to interpret relationships within data.\nIn this article, I‚Äôll guide you through the concept of Mutual Information, its definition, properties, and usage, as well as comparisons with other dependency measures. We‚Äôll explore MI‚Äôs mathematical foundations, its advantages and limitations, and its applications, concluding with an example demonstrating MI‚Äôs behavior alongside more traditional measures like Pearson‚Äôs correlation."
  },
  {
    "objectID": "blog/mutual-information.html#notation",
    "href": "blog/mutual-information.html#notation",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Notation",
    "text": "Notation\nBefore diving deeper, let‚Äôs establish our notation:\n\nRandom variables will be denoted by capital letters (\\(X\\), \\(Y\\)).\nLowercase letters (\\(x\\),\\(y\\)) represent specific values of these variables.\n\\(p(x)\\) denotes the probability mass/density function of \\(X\\).\n\\(p(x,y)\\) represents the joint probability mass/density function of \\(X\\) and \\(Y\\).\n\\(p(x\\mid y)\\) is the conditional probability of \\(X\\) given \\(Y\\).\n\\(H(X)\\) represents the entropy of random variable \\(X\\).\n\nThese should not surprise anyone, as they are standard conventions in most statistics textbooks."
  },
  {
    "objectID": "blog/mutual-information.html#diving-deeper",
    "href": "blog/mutual-information.html#diving-deeper",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nRefresher on Entropy\nMutual Information is related to the notion of entropy, a measure of uncertainty or randomness in a random variable. In less formal terms, entropy quantifies how ‚Äúsurprising‚Äù or ‚Äúunpredictable‚Äù the outcomes of \\(X\\) are.\nFormally, for a discrete random variable X with possible outcomes \\(x_1, x_2, \\dots, x_n\\) and associated probabilities \\(p(x_1), p(x_2), \\dots, p(x_n)\\), entropy \\(H(X)\\) is defined as:\n[H(X) = -_{i=1}^n p(x_i) p(x_i).]\nFor continuous variables we switch the summation with an integral.\nEntropy equals \\(0\\) when there‚Äôs no uncertainty, such as when \\(X\\) always takes a single outcome. High entropy means greater uncertainty (many possible outcomes, all equally likely), while low entropy indicates less uncertainty, as some outcomes are much more likely than others. In essence, entropy tells us how much ‚Äúinformation‚Äù is gained on average when observing the variable‚Äôs realization.\n\n\nMathematical Definitions of MI\nMI can be defined in mulitple ways. Perhaps the most intiuitive definition of MI between two random variables \\(X\\) and $ Y$ is that it measures the difference between the joint distribution and the product of their marginals. If two random variables are independent, their joint distribution is the product of their marginals \\(p(x,y)=p(x)p(y)\\). In some sense, the discrepancy between these two objects (i.e., the two sides of the equality) measures the strength of association between \\(X\\) and \\(Y\\) (i.e., their ‚Äúindependence‚Äù).\nFormally, MI is the Kullback-Leibler divergence between the joint distribution and the product of the two marginals:\n\\[ MI(X,Y) = D_{KL}(p(X,Y) || p(X)p(Y). \\]\nLet‚Äôs now examine MI from a different angle. We can express mutual information as follows:\n\\[ MI(X,Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\left(\\frac{p(x,y)}{p(x)p(y)}\\right)\\]\nAgain, for continuous variables, the sums become integrals. This is a more standard definition since it does not rely on\nAlternatively, MI can also be expressed in terms of entropy. It equals the sum of the entropy of \\(X\\) and \\(Y\\) taking away their joint entropy:\n\\[ \\begin{align*}MI(X,Y) & = H(X) - H(X|Y) \\\\ & = H(Y) - H(Y|X) \\\\ & = H(X) + H(Y) - H(X,Y)\\end{align*} \\]\n. You can compute MI in R with the infotheo package.\nSoftware Package: infotheo.\n\n\nProperties\nMI has several intriguing properties:\n\nNon-negativity: \\(MI(X;Y) \\geq 0\\). Mutual Information is always non-negative, as it measures the amount of information one variable provides about the other. Higher values correspond to stronger association.\nSymmetry: \\(MI(X,Y) = MI(Y,X)\\). This symmetry implies that the information \\(X\\) provides about \\(Y\\) is the same as what Y provides about X.\nIndependence: Similarly to Chatterjee‚Äôs correlation coefficient, \\(MI(X,Y) = 0\\) if and only if \\(X\\) and \\(Y\\) are independent.\nScale-invarance: Mutual information is scale invariant. If you apply a scaling transformation to the variables, their MI will not be affected.\n\n\n\nConditional MI\nConditional Mutual Information (CMI) extends the concept of MI to measure dependency between \\(X\\) and \\(Y\\) given a third variable \\(Z\\). CMI is useful for investigating how much information \\(X\\) and \\(Y\\) share independently of \\(Z\\). It is defined as:\n\\[MI(X,Y|Z) = \\sum_{x,y,z} p(x,y,z) \\log \\left(\\frac{p(x,y|z)}{p(x|z)p(y|z)}\\right). \\]\nThis can be valuable in causal inference, where understanding dependencies conditioned on specific variables aids in interpreting relationships within complex models. CMI is also particularly useful for feature selection when accounting for redundancy among already included features. Let‚Äôs explore this idea in greater detail.\n\n\nFeature Selection with MI\nIn machine learning, MI serves as a useful metric for feature selection (Brown et al.¬†2012, Vergara and Est√©vez 2014). Consider an outcome variable \\(Y\\) and a set of features \\(X\\in\\mathbb{R}^p\\) with n i.i.d. observations. By evaluating the MI between each feature and the target variable, one can retain features with the highest information content, passing a certain threshold. This is somewhat primitive since it assumes independence across features. More sophisticated approaches take feature dependency into acount.\nFor instance, methods like Minimum Redundancy Maximum Relevance (mRMR, Peng et al.¬†2005) aim to maximize the relevance of features to the target while minimizing redundancy among features. Here is a concise version of the mRMR algorithm:\n\nCalculate MI between each feature and the target (relevance).\nCalculate MI between features (redundancy).\nSelect features that maximize relevance while minimizing redundancy: \\[ \\text{mRMR} = \\max_{X_i} \\left[MI(X_i;Y) - \\frac{1}{|S|} \\sum_{X_j \\in S} MI(X_i;X_j)\\right], \\]\n\nwhere \\(S\\) is the set of already selected features."
  },
  {
    "objectID": "blog/mutual-information.html#pros-and-cons",
    "href": "blog/mutual-information.html#pros-and-cons",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nLike any other statistical tool, mutual Information has several advantages and limitations. On the positive side, it captures both linear and nonlinear relationships, is scale-invariant, and works with both continuous and discrete variables, making it a theoretically well-founded measure. However, it requires density estimation for continuous variables, can be computationally intensive for large datasets, and its results can be sensitive to binning choices for continuous variables. Additionally, there is no standard normalization for MI."
  },
  {
    "objectID": "blog/mutual-information.html#an-example",
    "href": "blog/mutual-information.html#an-example",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs implement MI calculation in R and python and compare it with traditional correlation measures using the iris dataset.\n\nRPython\n\n\nrm(list=ls())\nlibrary(infotheo)\nlibrary(corrplot)\nlibrary(dplyr)\ndata(iris)\n\n# Calculate mutual information matrix\nmi_matrix &lt;- mutinformation(discretize(iris[,1:4]))\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(iris[,1:4])\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value &lt;- mi_matrix[1,3]\ncor_value &lt;- cor_matrix[1,3]\n\n# Print results\nprint(paste(\"Mutual Information:\", round(mi_value, 3)))\nprint(paste(\"Pearson Correlation:\", round(cor_value, 3)))\n\n&gt;[1] \"Mutual Information: 0.585\"\n&gt;[1] \"Pearson Correlation: 0.872\"\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.metrics import mutual_info_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load dataset\niris = sns.load_dataset('iris')\n\n# Discretize the dataset (except the target variable)\nX = iris.iloc[:, :-1]\nest = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\nX_discretized = est.fit_transform(X)\n\n# Calculate mutual information matrix\nmi_matrix = np.zeros((X.shape[1], X.shape[1]))\nfor i in range(X.shape[1]):\n    for j in range(X.shape[1]):\n        mi_matrix[i, j] = mutual_info_score(X_discretized[:, i], X_discretized[:, j])\n\n# Calculate correlation matrix\ncor_matrix = X.corr()\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value = mi_matrix[0, 2]  # Sepal.Length vs Petal.Length\ncor_value = cor_matrix.iloc[0, 2]  # Sepal.Length vs Petal.Length\n\n# Print results\nprint(f\"Mutual Information: {mi_value:.3f}\")\nprint(f\"Pearson Correlation: {cor_value:.3f}\")\n\n&gt; Mutual Information: 0.905\n&gt; Pearson Correlation: 0.872\n\n\n\n\nThe left matrix displays the MI results and the right one shows the standard (Pearson) correlation values. The default scales are different, so one should compare the values and not the colors. Indeed, the variable pairs with negative linear correlation also have the lowest MI values.\nThis example demonstrates how MI can capture nonlinear relationships that might be missed by traditional correlation measures."
  },
  {
    "objectID": "blog/mutual-information.html#bottom-line",
    "href": "blog/mutual-information.html#bottom-line",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMutual Information provides a comprehensive measure of statistical dependence, capturing both linear and nonlinear relationships.\nUnlike correlation coefficients, MI works naturally with both continuous and categorical variables.\nMI serves as the foundation for sophisticated feature selection algorithms like mRMR."
  },
  {
    "objectID": "blog/mutual-information.html#where-to-learn-more",
    "href": "blog/mutual-information.html#where-to-learn-more",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great place to start and learng the basics. Brown et al.¬†(2012) and Vergara and Est√©vez (2014) are the go-to resources for conditional MI and using MI for feature selection."
  },
  {
    "objectID": "blog/mutual-information.html#references",
    "href": "blog/mutual-information.html#references",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "References",
    "text": "References\nBrown, G., Pocock, A., Zhao, M. J., & Luj√°n, M. (2012). Conditional likelihood maximisation: a unifying framework for information theoretic feature selection. JMLR.\nCover, T. M., & Thomas, J. A. (2006). Elements of information theory (2nd ed.). Wiley-Interscience.\nKraskov, A., St√∂gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical Review E, 69(6).\nPeng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE TPAMI.\nRoss, B. C. (2014). Mutual information between discrete and continuous data sets. PloS one, 9(2).\nVergara, J. R., & Est√©vez, P. A. (2014). A review of feature selection methods based on mutual information. Neural Computing and Applications, 24(1)."
  },
  {
    "objectID": "blog/delta-method.html",
    "href": "blog/delta-method.html",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "",
    "text": "You‚Äôve likely encountered this scenario: you‚Äôve calculated an estimate for a particular parameter, and now you require a confidence interval. Seems straightforward, doesn‚Äôt it? However, the task becomes considerably more challenging if your estimator is a nonlinear function of other random variables. Whether you‚Äôre dealing with ratios, transformations, or intricate functional relationships, directly deriving the variance for your estimator can feel incredibly daunting. In some instances, the bootstrap might offer a solution, but it can also be computationally demanding.\nEnter the Delta Method, a technique that harnesses the power of Taylor series approximations to assist in calculating confidence intervals within complex scenarios. By linearizing a function of random variables around their mean, the Delta Method provides a way to approximate their variance (and consequently, confidence intervals). This effectively transforms a convoluted problem into a more manageable one. Let‚Äôs delve deeper together, assuming you already have a foundational understanding of hypothesis testing."
  },
  {
    "objectID": "blog/delta-method.html#background",
    "href": "blog/delta-method.html#background",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "",
    "text": "You‚Äôve likely encountered this scenario: you‚Äôve calculated an estimate for a particular parameter, and now you require a confidence interval. Seems straightforward, doesn‚Äôt it? However, the task becomes considerably more challenging if your estimator is a nonlinear function of other random variables. Whether you‚Äôre dealing with ratios, transformations, or intricate functional relationships, directly deriving the variance for your estimator can feel incredibly daunting. In some instances, the bootstrap might offer a solution, but it can also be computationally demanding.\nEnter the Delta Method, a technique that harnesses the power of Taylor series approximations to assist in calculating confidence intervals within complex scenarios. By linearizing a function of random variables around their mean, the Delta Method provides a way to approximate their variance (and consequently, confidence intervals). This effectively transforms a convoluted problem into a more manageable one. Let‚Äôs delve deeper together, assuming you already have a foundational understanding of hypothesis testing."
  },
  {
    "objectID": "blog/delta-method.html#notation",
    "href": "blog/delta-method.html#notation",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "Notation",
    "text": "Notation\nBefore diving into the technical weeds, let‚Äôs set up some notation to keep things grounded. Let \\(X=(x_1, \\dots, x_k)\\) be a random vector of dimension \\(k\\), with mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\) (or simply a scalar \\(\\sigma^2\\) when \\(k=1\\)). Suppose you have a continuous, differentiable function \\(g(\\cdot)\\), and you‚Äôre interested in approximating the variance of \\(g(X)\\), denoted as \\(\\text{Var}(g(X))\\)."
  },
  {
    "objectID": "blog/delta-method.html#diving-deeper",
    "href": "blog/delta-method.html#diving-deeper",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nThe Delta Method builds on a simple premise: for a smooth function \\(g(\\cdot)\\), we can approximate \\(g(X)\\) around its mean \\(\\mu\\) using a first-order Taylor expansion:\n\\[g(X) \\approx g(\\mu) + \\nabla g(\\mu)^T (X - \\mu),\\]\nwhere \\(\\nabla g(\\mu)\\) is the gradient of \\(g(\\cdot)\\) evaluated at \\(\\mu\\), i.e., a $ k $ vector of partial derivatives:\n\\[\\nabla g(\\mu) = \\left[ \\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2}, \\dots, \\frac{\\partial g}{\\partial x_k} \\right]^T.\\]\nBy substituting this into the approximation, the variance of \\(g(X)\\) becomes:\n\\[\\begin{align*} \\text{Var}(g(X)) & = \\text{Var}(g(\\mu) + \\nabla g(\\mu)^T (X - \\mu)) \\\\ & = \\text{Var}(g(\\mu) + \\nabla g(\\mu)^T X -  \\nabla g(\\mu)^T  \\mu) \\\\  &= \\text{Var}(g(\\mu)^T X)  \\\\ &=  \\nabla g(\\mu)^T \\Sigma \\nabla g(\\mu).  \\end{align*}\\]\nIn the univariate \\(k=1\\) case, we have:\n\\[\\text{Var}(g(X)) = \\sigma^2 [g(\\cdot)']^2.\\]\nIf \\(X\\) is a sample-based estimator (e.g., sample mean, regression coefficients), then \\(\\Sigma\\) would be its estimated covariance matrix, and the Delta Method gives us an approximate standard error for \\(g(X)\\). This approximation works well for large samples but may break down when variances are high or sample sizes are small."
  },
  {
    "objectID": "blog/delta-method.html#an-example",
    "href": "blog/delta-method.html#an-example",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs walk through an example to make this concrete. Suppose you‚Äôre studying the ratio of two independent random variables: \\(R = \\frac{X_1}{X_2}\\), where \\(X_1 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim N(\\mu_2, \\sigma_2^2)\\). I know some of you want specific numbers, so we can set \\(\\mu_1 = 5\\), \\(\\mu_2 = 10\\), \\(\\sigma_1 = 2\\), and \\(\\sigma_2=1\\).\nWe want to approximate the variance of \\(R\\) using the Delta Method. Here is the step-by-step procedure to get there.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nDefine \\(g(X)\\) and obtain its gradient. Here, \\(g(X) = \\frac{X_1}{X_2}\\) and the gradient is: \\[\\nabla g(\\mu) = \\left[ \\frac{1}{\\mu_2}, -\\frac{\\mu_1}{\\mu_2^2} \\right]^T.\\]\nEvaluate \\(g(\\mu)\\) at _1 and _2. In our example \\[\\nabla g(\\mu) = [0.1, -0.5]^T.\\]\nCompute the variance approximation. We have \\[\\Sigma = \\begin{bmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_2^2 \\end{bmatrix} = \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix}.\\] Thus, the approximate variance of \\(R\\) is: \\[\\text{Var}(R) \\approx \\nabla g(\\mu)^T \\Sigma \\nabla g(\\mu) = \\frac{\\sigma_1^2}{\\mu_2^2} + \\frac{\\mu_1^2 \\sigma_2^2}{\\mu_2^4}=\\frac{4}{100}+\\frac{25}{625}=0.08.\\]\n\n\n\nAnd that‚Äôs it. We used the Delta Method to compute the approximate variance of \\(R = \\frac{X_1}{X_2}\\)."
  },
  {
    "objectID": "blog/delta-method.html#bottom-line",
    "href": "blog/delta-method.html#bottom-line",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe Delta Method is a generic way of computing confidence intervals in non-standard situations.\nIt works by linearizing nonlinear functions to approximate variances and standard errors.\nThis technique works for any smooth function, making it a go-to tool in econometrics, biostatistics, and machine learning."
  },
  {
    "objectID": "blog/delta-method.html#references",
    "href": "blog/delta-method.html#references",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "References",
    "text": "References\nCasella, G., & Berger, R. L. (2002). Statistical Inference.\nGreene, W. H. (2018). Econometric Analysis."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html",
    "href": "blog/limits-nonparametric-models.html",
    "title": "The Limits of Nonparametric Models",
    "section": "",
    "text": "Nonparametric statistics offers a powerful toolkit for data analysis when the underlying data-generating process is too complex or unknown to be captured by parametric models. Unlike parametric methods, which assume a specific form for the underlying distribution or functional relationship, nonparametric approaches let the data ‚Äúspeak for itself.‚Äù\nIn earlier articles, we explored parametric and semiparametric models, where the focus was on attaining the lowest possible variance. However, in nonparametric statistics, this goal is unsuitable, as prioritizing minimal variance often introduces significant bias. The spotlight falls instead on selecting the optimal tuning parameter: the bandwidth. This article explains the various considerations and mathematical details associated with selecting the optimal bandwidth value in Kernel Density (KD) and Kernel Regression (KR) estimation."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#background",
    "href": "blog/limits-nonparametric-models.html#background",
    "title": "The Limits of Nonparametric Models",
    "section": "",
    "text": "Nonparametric statistics offers a powerful toolkit for data analysis when the underlying data-generating process is too complex or unknown to be captured by parametric models. Unlike parametric methods, which assume a specific form for the underlying distribution or functional relationship, nonparametric approaches let the data ‚Äúspeak for itself.‚Äù\nIn earlier articles, we explored parametric and semiparametric models, where the focus was on attaining the lowest possible variance. However, in nonparametric statistics, this goal is unsuitable, as prioritizing minimal variance often introduces significant bias. The spotlight falls instead on selecting the optimal tuning parameter: the bandwidth. This article explains the various considerations and mathematical details associated with selecting the optimal bandwidth value in Kernel Density (KD) and Kernel Regression (KR) estimation."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#notation",
    "href": "blog/limits-nonparametric-models.html#notation",
    "title": "The Limits of Nonparametric Models",
    "section": "Notation",
    "text": "Notation\nWe begin by establishing the mathematical terminology and notations essential for our analysis.\n\nKernels\nA kernel function \\(K(\\cdot)\\) is a non-negative, symmetric function that integrates to \\(1\\) over its domain. It acts as a weighting function that determines how observations contribute to the estimate at any given point, effectively spreading the mass of each data point over a local neighborhood.\n\n\nKernel Density\nKD is sort of like creating a smooth histogram. The kernel density estimator for a sample of n observations \\(X_1, X_2, \\dots, X_n \\in \\mathbb{R}\\) is given by:\n\\[\\hat{f}_h (x) = \\frac{1}{n h} \\sum_i K\\left(\\frac{x - X_i}{h}\\right),\\]\nwhere \\(h &gt; 0\\) is the bandwidth, a positive scalar controlling the smoothness of the estimate. Its function is to determine the window length around x in which the observations get positive weight.\nThe value \\(\\hat{f}_h (x)\\) will be large when there are many data points around \\(x\\), and small otherwise. Some popular kernel functions like Epanechnikov, Gaussian or triangular assign higher weights to observations closer to \\(x\\) and decaying importance to data further away.\nDensity estimation can also be performed in higher dimensions where the curse of dimensionality lurks in the background. In two dimensions, these estimates typically take the form of heat map-type figures.\n\n\nKernel Regression\nKernel regression, also known as local regression, is much like fitting a flexible smooth curve through data points. In this context, we have access to \\(n\\) observations of an outcome variable \\(Y\\). The objective is to estimate the conditional mean function at some point X=x:\n\\[m(x) = \\mathbb{E}[Y \\mid X = x].\\]\nAmong various kernel regression methods, the Nadaraya-Watson (NW) estimator is particularly popular. It is defined as:\n\\[\\hat{m}_h (x) = \\frac{\\sum_i K\\left(\\frac{x - X_i}{h}\\right) Y_i}{\\sum_i K\\left(\\frac{x - X_i}{h}\\right)}.\\]\nThe NW method fits a local constant around \\(x\\) equal to the average \\(Y\\) in that region (determined by the bandwidth). This approach is also known as local constant regression. More sophisticated variants include local linear and quadratic methods, which extend this fundamental idea but require more data. Across all these approaches, the bandwidth \\(h\\) serves as the key parameter that determines the trade-off between bias and variance, a relationship we will explore in detail below."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#diving-deeper",
    "href": "blog/limits-nonparametric-models.html#diving-deeper",
    "title": "The Limits of Nonparametric Models",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nIn practice, the choice of kernel is not as consequential. The primary focus of nonparametric statistics is selecting the optimal bandwidth \\(h\\).\n\nThe Bias-Variance Tradeoff\nAt the heart of bandwidth selection is the bias-variance tradeoff. Intuitively:\n\nSmall bandwidth (‚Äúoverfitting‚Äù): Captures fine details but also noise, leading to high variance and low bias.\nLarge bandwidth (‚Äúunderfitting‚Äù): Smoothes over the noise but can miss important structure, leading to high bias and low variance.\nFor nonparmetric curve estimation, the mean integrated squared error (MISE) is a common loss function for evaluating performance:\n\n\\[\\text{MISE}(h) = \\mathbb{E}\\left[ \\int \\left( \\hat{f}_h(x) - f(x) \\right)^2 dx \\right].\\]\nThis decomposes into bias and variance terms:\n\\[\\text{MISE}(h) =  \\int \\text{Bias}^2(\\hat{f}_h(x)) dx + \\int \\text{Var}(\\hat{f}_h(x)) dx.\\]\nMISE is analogous to the mean squared error (MSE) commonly used in machine learning, but integrated, or summed, across the support of the data. Minimizing the MISE yields the optimal bandwidth by balancing the bias-variance tradeoff, a fundamental concept in statistical estimation theory.\n\n\nOptimal \\(h\\) for Kernel Density\nSilverman‚Äôs rule of thumb provides a practical, closed-form approximation for \\(h\\) in KDE, assuming the data is roughly Gaussian. The expression is:\n\\[h^* = \\left( \\frac{4\\sigma^5}{3n} \\right) ^{\\frac{1}{5}}  = 1.06 \\sigma n^{-1/5},\\]\nwhere \\(\\sigma\\) is the standard deviation of the data. This formula emerges from theoretical analysis of the bias-variance tradeoff and serves as an effective initial bandwidth choice. While it performs well for many datasets, particularly those with approximately normal distributions, it may not be optimal for multimodal or heavily skewed distributions.\n\n\nOptimal \\(h\\) for Kernel Regression\nFor Nadaraya-Watson regression, several methods exist for selecting the bandwidth h. Cross-validation provides reliable results but can be computationally demanding. Theoretical analysis based on MISE minimization for common kernel functions suggests that the optimal bandwidth follows the relationship:\n\\[h^* \\propto n^{-1/5}.\\]\nThis expressiom highlights a fundamental characteristic of nonparametric methods: their convergence rate is slower than that of parametric methods. This represents one manifestation of the curse of dimensionality in nonparametric estimation. Furthermore, this relationship only specifies the rate at which the optimal bandwidth should decrease with sample size (proportionality to \\(n^{-1/5}\\)), rather than providing an exact value. Without the proportionality constant, its direct practical application is limited."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#an-example",
    "href": "blog/limits-nonparametric-models.html#an-example",
    "title": "The Limits of Nonparametric Models",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate bandwidth selection with a simple example.\nrequire(graphics)\nwith(cars, {\n    plot(speed, dist)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 2), col = 2)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 5), col = 3)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 10), col = 4)\n})\nThis yields a simplified version of the following figure:\n\nThe green line corresponds to the Nadaraya-Watson regression with an optimal bandwidth and features some degree of curveture. The red line has a smaller bandwidth (higher variance, lower bias), and the blue line is undersmoothed (higher bandwidth)."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#bottom-line",
    "href": "blog/limits-nonparametric-models.html#bottom-line",
    "title": "The Limits of Nonparametric Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBandwidth selection is critical for nonparametric methods to balance bias and variance.\nSilverman‚Äôs rule of thumb offers a simple yet effective starting point for KD.\nFor commonly used second-order kernels, the optimal bandwidth in KR scales as \\(n^{-1/5}\\).\nPractical implementation often relies on cross-validation or plug-in methods and is limited by the curse of dimensionality."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#references",
    "href": "blog/limits-nonparametric-models.html#references",
    "title": "The Limits of Nonparametric Models",
    "section": "References",
    "text": "References\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.\nLi, Q., & Racine, J. S. (2023). Nonparametric econometrics: theory and practice. Princeton University Press.\nSilverman, B. W. (1986). Density Estimation for Statistics and Data Analysis. Wand, M. P., & Jones, M. C. (1995). Kernel Smoothing.\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media."
  },
  {
    "objectID": "blog/limits-semiparam-models.html",
    "href": "blog/limits-semiparam-models.html",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "",
    "text": "The efficiency bound is a cornerstone of the academic literature on semiparametric models, and it‚Äôs easy to see why. This bound quantifies the potential loss in efficiency (i.e., increase in variance) that arises when opting for a semiparametric model over a fully parametric one. In doing so, it offers a rigorous benchmark for evaluating the asymptotic variance of any estimator. By providing insights into the trade-offs between model flexibility and statistical precision, the efficiency bound occupies a critical role in understanding the theoretical limits of estimation. Despite its importance, this concept and the broader class of semiparametric models remain underappreciated within much of the data science community.\nThis article aims to demystify the notion of the semiparametric efficiency bound and its relevance to practical applications. It unpacks the mathematical foundations underlying this concept, shedding light on its relationship with the Cram√©r-Rao lower bound (CRLB). I will also touch on the bound‚Äôs implications for real-world data analysis, where balancing flexibility and efficiency is often a key concern."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#background",
    "href": "blog/limits-semiparam-models.html#background",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "",
    "text": "The efficiency bound is a cornerstone of the academic literature on semiparametric models, and it‚Äôs easy to see why. This bound quantifies the potential loss in efficiency (i.e., increase in variance) that arises when opting for a semiparametric model over a fully parametric one. In doing so, it offers a rigorous benchmark for evaluating the asymptotic variance of any estimator. By providing insights into the trade-offs between model flexibility and statistical precision, the efficiency bound occupies a critical role in understanding the theoretical limits of estimation. Despite its importance, this concept and the broader class of semiparametric models remain underappreciated within much of the data science community.\nThis article aims to demystify the notion of the semiparametric efficiency bound and its relevance to practical applications. It unpacks the mathematical foundations underlying this concept, shedding light on its relationship with the Cram√©r-Rao lower bound (CRLB). I will also touch on the bound‚Äôs implications for real-world data analysis, where balancing flexibility and efficiency is often a key concern."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#notation",
    "href": "blog/limits-semiparam-models.html#notation",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "Notation",
    "text": "Notation\nBefore diving in, let‚Äôs establish the necessary notation to guide our technical discussion. The model governing the data is characterized by parameters \\(\\theta\\) and \\(\\eta\\) with likelihood \\(f(X; \\theta, \\eta)\\). Moreover:\n\n\\(\\theta \\in \\mathbb{R}^d\\) is the parameter of interest, a finite-dimensional vector we want to estimate. Often \\(\\theta\\) is a scalar. It represents the parametric component of the model.\n\\(\\eta\\) is a nuisance parameter, which is infinite-dimensional (e.g., a nonparametric density or function). It is a nuisance in the sense that it is part of the model, but we are not interested in it for its own sake. It represents the nonparametric component of the model.\n\nA leading example is the partially linear model:\n\\[Y= \\theta X+g(Z)+\\epsilon,\\]\nwhere \\(Y\\) is the outcome variable, \\(Z\\) represents a vector of covariates, \\(g(\\cdot)\\) is a function characterized by \\(\\eta\\), while \\(\\epsilon\\) is an error term. To fit this model in the likelihood notation above, think of \\(Z\\) as a component of \\(X\\). We assume we have a random i.i.d. sample of all necessary variables."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#diving-deeper",
    "href": "blog/limits-semiparam-models.html#diving-deeper",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nIn semiparametric models, the presence of \\(\\eta\\) complicates the estimation in that it can obscure the relationship between \\(\\theta\\) and the observed data. The semiparametric efficiency bound generalizes the CRLB by accounting for the nuisance parameter and isolating the information relevant to \\(\\theta\\).\n\nParametric Submodels\nLet‚Äôs take a sidestep for a minute. A parametric submodel, say \\(f(\\theta)\\), that contains \\(\\theta\\) alone, represents a subset of distributions that satisfy semiparametric assumptions and contains the true distribution f(X; , ). For any semiparametric estimator that is consistent and asymptotically normal, its asymptotic variance can be compared to the CRLB of the parametric submodel. Since this relationship holds for all possible parametric submodels, the semiparametric estimator‚Äôs variance cannot be smaller than any submodel‚Äôs bound. In other words, the asymptotic variance of any semiparametric estimator is at least as large as the largest CRLB across all parametric submodels.\nInformally,\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\max_{\\text{{param. submodel}}} \\text{CRLB}.\\]\nThis is our first insight on the semiparametric efficiency bound, which admittedly is more of theoretical than practical significance.\n\n\nEfficient Influence Functions\nThe semiparametric efficiency bound depends on the interplay between \\(\\theta\\) and \\(\\eta\\), captured through the something called the Efficient Influence Function (EIF). Remember that the score function for \\(\\theta\\),\n\\[S_\\theta(X)=\\frac{\\partial}{\\partial \\theta} \\log f(X; \\theta, \\eta),\\]\nmeasures the sensitivity of the log-likelihood to changes in \\(\\theta\\). We can similarly define the score with respect to \\(\\eta\\):\n\\[S_\\eta(X)=\\frac{\\partial}{\\partial \\eta} \\log f(X; \\theta, \\eta).\\]\nNow enter the EIF \\(\\psi^*(X)\\) which captures the variation in \\(\\theta\\) while adjusting for the nuisance parameter \\(\\eta\\). It satisfies the orthogonality condition:\n\\[\\mathbb{E}\\left[ \\psi^*(X) \\cdot S_\\eta(X) \\right] = 0,\\]\nensuring that the influence of \\(\\eta\\) is removed from \\(\\psi(X)\\). In other words, \\(\\psi^*(X)\\) captures only information about \\(\\theta\\), uncontaminated by nuisance parameters. It is the influence function with the lowest possible variance.\n\n\nEfficient Score\nThe next piece of the puzzle is the Efficient Score \\(S^*_{\\theta}\\), the projection of \\(S_\\theta(X)\\) onto the space orthogonal to the nuisance tangent space \\(\\mathcal{T}_\\eta\\):\n\\[S^*_{\\theta}(X) = S_\\theta(X)  - \\Pi( S_\\theta(X) \\mid \\mathcal{T}_\\eta),\\]\nwhere \\(\\Pi(\\cdot)\\) is the projection operator. Here \\(\\mathcal{T}_\\eta\\) is simply the linear subspace spanned by \\(S_\\eta(X)\\). The Efficient Score is the part of the score vector that is ‚Äúfree‚Äù from the influence of nuisance parameters. It represents the best possible score function for estimating \\(\\theta\\) in the presence of \\(\\eta\\). (A similar technique underlies the so-called Neyman orthogonality principle in dobule/debiased machine learning.) We can construct the efficient influence function by appropriately scaling the efficient score to get to the optimal EIF.\n\n\nThe Semiparametric Efficiency Bound\nWe are, at last, ready to state the main result. The semiparametric efficiency bound is determined by the variance of the Efficient Score:\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{\\mathbb{E}[S_\\theta^*(X)^2]}.\\]\n‚Äã This generalizes the Cram√©r-Rao lower bound for semiparametric models by incorporating the complexity introduced by the nuisance parameter \\(\\eta\\). In parametric models, this bound collapses to and is determined by the Fisher Information, while in here, it is governed by the efficient score.\nTo achieve the bound in practice, nuisance parameters are often removed through methods like regression residuals, inverse probability weighting, or targeted maximum likelihood estimation (TMLE). These techniques isolate the information about \\(\\theta\\) from \\(\\eta\\), enabling efficient estimation."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#bottom-line",
    "href": "blog/limits-semiparam-models.html#bottom-line",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nSemiparametric models blend parametric assumptions (related to a parameter of interest) with nonparametric flexibility (related to nuisance parameters).\nThe efficient influence function isolates the information about the parameter of interest, removing the impact of nuisance parameters.\nThe semiparametric efficiency bound generalizes CRLB to this class of models. It is determined by the variance of the efficient score vector.\nPractical estimation achieving this bound often involves removing nuisance effects through residualization or other adjustment techniques."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#references",
    "href": "blog/limits-semiparam-models.html#references",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "References",
    "text": "References\nBickel, P. J., Klaassen, C. A., Ritov, Y., & Wellner, J. A. (1993)\nEfficient and Adaptive Estimation for Semiparametric Models. Johns Hopkins University Press.\nGreene, William H. ‚ÄúEconometric analysis‚Äù. New Jersey: Prentice Hall (2000): 201-215.\nHines, O., Dukes, O., Diaz-Ordaz, K., & Vansteelandt, S. (2022). Demystifying statistical learning based on efficient influence functions. The American Statistician, 76(3), 292-304.\nIchimura, H., & Todd, P. (2007) Implementing Nonparametric and Semiparametric Estimators. In Heckman, J. & Leamer, E. (Eds.), Handbook of Econometrics (Vol. 6B).\nNewey, W. K. (1990) Semiparametric Efficiency Bounds. Journal of Applied Econometrics, 5(2), 99‚Äì135.\nTsiatis, A. (2007). Semiparametric theory and missing data. Springer Science & Business Media\nVan der Vaart, A. W. (2000) Asymptotic Statistics. Cambridge University Press."
  },
  {
    "objectID": "blog/limits-parametric-models.html",
    "href": "blog/limits-parametric-models.html",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "",
    "text": "Obtaining the lowest possible variance is a primary goal for anyone working with statistical models. Efficiency (or precision), as is the jargon, is a cornerstone of statistics and econometrics, guiding us toward estimators that extract the maximum possible information from the data. It can make or break a data project.\nThe Cram√©r-Rao lower bound (CRLB) plays a pivotal role in this context by establishing a theoretical limit on the variance of unbiased estimators. Unbiased estimators are those that yield the true answer (on average), rendering them a highly attractive class of methods. The CRLB highlights the best achievable precision for parameter estimation based on the Fisher information in the data. This article explores the theoretical foundation of the CRLB, its computation, and its implications for practical estimation.\nIn what follows, I am concerned with unbiased estimators, a common practice that should not be taken for granted. As a counterexample, consider the James-Stein estimator ‚Äîa biased but attractive technique."
  },
  {
    "objectID": "blog/limits-parametric-models.html#background",
    "href": "blog/limits-parametric-models.html#background",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "",
    "text": "Obtaining the lowest possible variance is a primary goal for anyone working with statistical models. Efficiency (or precision), as is the jargon, is a cornerstone of statistics and econometrics, guiding us toward estimators that extract the maximum possible information from the data. It can make or break a data project.\nThe Cram√©r-Rao lower bound (CRLB) plays a pivotal role in this context by establishing a theoretical limit on the variance of unbiased estimators. Unbiased estimators are those that yield the true answer (on average), rendering them a highly attractive class of methods. The CRLB highlights the best achievable precision for parameter estimation based on the Fisher information in the data. This article explores the theoretical foundation of the CRLB, its computation, and its implications for practical estimation.\nIn what follows, I am concerned with unbiased estimators, a common practice that should not be taken for granted. As a counterexample, consider the James-Stein estimator ‚Äîa biased but attractive technique."
  },
  {
    "objectID": "blog/limits-parametric-models.html#notation",
    "href": "blog/limits-parametric-models.html#notation",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Notation",
    "text": "Notation\nBefore diving in, let‚Äôs establish a unified notation to structure the mathematical discussion:\n\nLet X denote the observed data, with \\(X_1, X_2, \\dots, X_n\\) being n independent and identically distributed (i.i.d.) observations.\nThe model governing the data is characterized by a (finite-dimensional) parameter \\(\\theta \\in \\mathbb{R}^d\\) which we aim to estimate.\nThe likelihood of the data is \\(f(x; \\theta)\\), fully specified by the parameter \\(\\theta\\)."
  },
  {
    "objectID": "blog/limits-parametric-models.html#diving-deeper",
    "href": "blog/limits-parametric-models.html#diving-deeper",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nThe Cram√©r-Rao lower bound provides a theoretical benchmark for how precise an unbiased estimator can be. It sets the minimum variance that any unbiased estimator of a parameter \\(\\theta\\) can achieve, given a specific data-generating process.\n\nThe CRLB Formula\nFor a parameter \\(\\theta\\) in a parametric model with likelihood \\(f(x; \\theta)\\), the CRLB is expressed as:\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{I(\\theta)},\\]\nwhere \\(I(\\theta)\\) is the Fisher information (FI), defined as:\n\\[I(\\theta) = \\mathbb{E}\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 \\right].\\]\n\n\nIntuition\nTo understand the CRLB, we must delve into the concept of Fisher information named after one of the modern fathers of statistics R.A. Fisher. Intuitively, FI quantifies how much information the observed data carries about the parameter \\(\\theta\\).\nThink of the likelihood function \\(f(x; \\theta)\\) as describing the probability of observing a given dataset \\(x\\) for a particular value of \\(\\theta\\). If the likelihood changes sharply with \\(\\theta\\) (i.e., \\(\\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta)\\) is large), small changes in \\(\\theta\\) lead to noticeable differences in the likelihood. This variability reflects high information: the data can ‚Äúpinpoint‚Äù \\(\\theta\\) with greater precision. Conversely, if the likelihood changes slowly with \\(\\theta\\), the data offers less information about its true value.\nMathematically, the Fisher information \\(I(\\theta)\\) is the variance of the the partial derivative\n\\[\\frac{\\partial}{\\partial \\theta} logf(x;\\theta),\\]\nwhich we refer to as the score function. This score measures how sensitive the likelihood function is to changes in \\(\\theta\\). Higher variance in the score corresponds to more precise information about \\(\\theta\\).\n\n\nPractical Application\nThe CRLB provides a benchmark for evaluating the performance of estimators. For example, if you propose an unbiased estimator \\(\\hat{\\theta}\\), you can compare its variance to the CRLB. If \\(\\text{Var}(\\hat{\\theta}) = \\frac{1}{I(\\theta)}\\), we say the estimator is efficient. However, if the variance is higher, there may be room to improve the estimation method.\nMoreover, the CRLB also offers insight into the difficulty of estimating a parameter. If \\(I(\\theta)\\) is ‚Äúsmall‚Äù, so that the bound on the variance is high, then no unbiased estimator can achieve high precision with the available data. It is possible to develop a biased estimator for \\(\\theta\\) with lower variance, but it is not clear why you would do that."
  },
  {
    "objectID": "blog/limits-parametric-models.html#an-example",
    "href": "blog/limits-parametric-models.html#an-example",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "An Example",
    "text": "An Example\nImagine you are estimating the mean of a normal distribution, where \\(X \\sim N(\\mu, \\sigma^2)\\), and \\(\\sigma^2\\) is known. The likelihood for a single observation \\(x_i\\) is:\n\\[f(x_i;\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2 \\sigma^2}}.\\]\nUsing the Fisher information defintion given above, taking the derivative and simplifying, we find:\n\\[I(\\mu)=  \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 = \\frac{1}{\\sigma^2}.\\]\nFor n independent observations, this expression becomes:\n\\[I(\\mu)=\\frac{n}{\\sigma^2}.\\]\nThe CRLB for the variance of any unbiased estimator of is:\n\\[\\text{Var}(\\hat{\\mu})\\geq \\frac{\\sigma^2}{n}\\]\nThis result aligns with our intuition: as n increases, the precision of our estimate improves. In other words, more data leads to more informative results."
  },
  {
    "objectID": "blog/limits-parametric-models.html#where-to-learn-more",
    "href": "blog/limits-parametric-models.html#where-to-learn-more",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAny graduate econometrics textbook will do. Personally, my grad school nightmares were induced by Greene‚Äôs textbook (cited below). It can be dry but certainly contains what you need to know."
  },
  {
    "objectID": "blog/limits-parametric-models.html#bottom-line",
    "href": "blog/limits-parametric-models.html#bottom-line",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe CRLB establishes a theoretical lower limit on the variance of unbiased estimators, serving as a benchmark for efficiency.\nFisher information measures the sensitivity of the likelihood to changes in the parameter \\(\\theta\\), linking the amount of information in the data to the precision of estimation.\nEfficient estimators achieve the CRLB and are optimal under the given model assumptions."
  },
  {
    "objectID": "blog/limits-parametric-models.html#references",
    "href": "blog/limits-parametric-models.html#references",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "References",
    "text": "References\nGreene, William H. ‚ÄúEconometric analysis‚Äù. New Jersey: Prentice Hall (2000): 201-215."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html",
    "href": "blog/bayesian-ab-tests.html",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "",
    "text": "Imagine you‚Äôre a data scientist evaluating an A/B test of a new recommendation algorithm. The results show a modest but promising 0.5% lift in conversion rate‚Äîup from \\(8\\%\\) to \\(8.5\\%\\) across \\(20,000\\) users‚Äîalong with a \\(p\\)-value of \\(0.06\\). Given historical data, the improvement seems realistic, the cost to implement is low, and the projected revenue impact is substantial. Yet, the conventional analysis compels you to ‚Äúfail to reject the null hypothesis‚Äù simply because the p-value doesn‚Äôt meet an arbitrary \\(0.05\\) threshold.\nTraditional A/B testing approaches (randomized controlled trials or RCTs) rely primarily on frequentist methods like \\(t\\)-tests or chi-squared tests to detect differences between treatment and control groups. For more precision, analysts often adjust for covariates in linear models. After weeks or months of testing, however, these methods often boil down to a single outcome: the \\(p\\)-value. This binary interpretation‚Äîsignificant or not‚Äîcan obscure valuable insights, as seen in the example above.\nBayesian statistics, by contrast, offers a more nuanced and potentially more informative alternative. In this article, we‚Äôll walk through a practical example, discuss the implementation details, and dive into the Bayesian framework‚Äôs mathematical foundations for experimentation. One of the core advantages of the Bayesian approach is its ability to produce a full posterior distribution, offering a richer view of the experiment‚Äôs outcomes beyond just point estimates.\nThis article assumes a familiarity with Bayesian fundamentals like priors, posteriors, and conjugate distributions. If you‚Äôd like a refresher, see the References section below."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#background",
    "href": "blog/bayesian-ab-tests.html#background",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "",
    "text": "Imagine you‚Äôre a data scientist evaluating an A/B test of a new recommendation algorithm. The results show a modest but promising 0.5% lift in conversion rate‚Äîup from \\(8\\%\\) to \\(8.5\\%\\) across \\(20,000\\) users‚Äîalong with a \\(p\\)-value of \\(0.06\\). Given historical data, the improvement seems realistic, the cost to implement is low, and the projected revenue impact is substantial. Yet, the conventional analysis compels you to ‚Äúfail to reject the null hypothesis‚Äù simply because the p-value doesn‚Äôt meet an arbitrary \\(0.05\\) threshold.\nTraditional A/B testing approaches (randomized controlled trials or RCTs) rely primarily on frequentist methods like \\(t\\)-tests or chi-squared tests to detect differences between treatment and control groups. For more precision, analysts often adjust for covariates in linear models. After weeks or months of testing, however, these methods often boil down to a single outcome: the \\(p\\)-value. This binary interpretation‚Äîsignificant or not‚Äîcan obscure valuable insights, as seen in the example above.\nBayesian statistics, by contrast, offers a more nuanced and potentially more informative alternative. In this article, we‚Äôll walk through a practical example, discuss the implementation details, and dive into the Bayesian framework‚Äôs mathematical foundations for experimentation. One of the core advantages of the Bayesian approach is its ability to produce a full posterior distribution, offering a richer view of the experiment‚Äôs outcomes beyond just point estimates.\nThis article assumes a familiarity with Bayesian fundamentals like priors, posteriors, and conjugate distributions. If you‚Äôd like a refresher, see the References section below."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#notation",
    "href": "blog/bayesian-ab-tests.html#notation",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs establish our notation for a binary intervention randomized experiment:\n\n\\(T \\in {0,1}\\): Treatment indicator\n\\(N_T\\): Number of units in treatment group\n\\(N_C\\): Number of units in control group\n\\(N = N_T + N_C\\): Total sample size\n\\(X_T\\): Number of ‚Äúsuccesses‚Äù in treatment group\n\\(X_C\\): Number of ‚Äúsuccesses‚Äù in control group\n\\(Y\\): Success rate (e.g., conversion rate, employment status)."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#diving-deeper",
    "href": "blog/bayesian-ab-tests.html#diving-deeper",
    "title": "Bayesian Analysis of A/B Tests: A Modern Approach",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nWe are interested in making inferences about the treatment effect,$ $, of the intervention \\(T\\) on the success rate \\(Y\\). In Bayesian terms, this means we seek the posterior distribution of \\(\\tau\\). Once we obtain this distribution or can draw observations from it, we can calculate various statistics to summarize the impact of the intervention, potentially providing a richer understanding of its effects.\nThe Bayesian methodology of analyzing experiments proceeds in four steps.\n\nStep 1: Specify the Prior Distribution\nWe begin by choosing the prior distributions for Y. The prior distribution represents our beliefs about the parameter before seeing the data. It is usually informed by previous A/B testing experience combined with deep context knowledge. For example, we might believe that the treatment effect is around two percentage points (\\(\\hat{\\tau}=0.02\\)) with some uncertainty around it.\nGiven our binary outcome, the Beta distribution serves as a natural conjugate prior:\n\\[ Y_i \\sim \\text{Beta} (\\alpha_i, \\beta_i), \\hspace{.5cm} i \\in \\{T,C\\}.  \\]\n\n\nStep 2: Specify the Likelihood Function\nFor binary outcomes, we model the data using a Binomial distribution:\n\\[X_i|Y_i \\sim \\text{Binomial}(N_i, Y_i), \\hspace{.5cm} i\\in\\{T,C\\}.  \\]\nThis choice reflects the inherent structure of our data: counting successes in a fixed number of trials. We are now ready to use the Bayes rule to arrive at the posterior distributions.\n\n\nStep 3: Posterior Distributions Derivation\nThanks to conjugacy between Beta and Binomial distributions, we obtain closed-form posterior distributions:\n\\[ Y_i | X_i \\sim \\text{Beta}(\\alpha_i+X_i, \\beta_i+N_i-X_i), \\hspace{.5cm} i\\in\\{T,C\\}  .\\]\nThis mathematical convenience allows us to avoid more complex numerical methods like MCMC sampling. We can now even plot the two posterior distributions and visually asses their differences.\n\n\nStep 4: Inference and Decision Making\nThe Bayesian framework enables rich analysis beyond traditional frequentist-style hypothesis testing. Here are some examples:\n\nExample 1: Probability of Positive Impact\nWe can calculate the probability that the outcomes for the treatment group are higher than those of the control group. In closed form, we have:\n\\[ P(Y_T&gt;Y_C|X_T,X_C) = \\frac{1}{N}\\sum_i\\mathbf{1}(Y_{T,i}&gt;Y_{C,i}),\\]\nwhere \\(\\mathbf{1}(\\cdot)\\) is the indicator function. This is simply share of observations for which \\(Y_T &gt;Y_C\\).\n\n\nExample 2: Average Treatment Effect\nAnother potential example of an object of interest is the Average Treatment Effect (ATE):\n\\[ ATE = \\frac{1}{N_T}\\sum_{i \\in T}Y_{T,i} -  \\frac{1}{N_C}\\sum_{i \\in C}Y_{C,i} \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\n\\[ ATE \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\nAnd that‚Äôs it. You see how with the cost of additional assumptions we get much more than a single \\(p\\)-value, and that‚Äôs where much of the appeal of this approach lies.\n\n\n\nAdvantages\nOverall, Bayesian thinking entails some compelling advantages:\n\nIncorporation of Prior Information. It allows you to incorporate prior knowledge or expert opinion into the analysis. This is particularly useful when historical data or domain expertise is available.\nProbabilistic Interpretation: It provides direct probabilistic interpretations of the results, such as the probability that one variant is better than another. This may be more intuitive than frequentist p-values, which are often misinterpreted.\nHandling of Small Sample Sizes: It tends to perform better with small sample sizes because they incorporate prior distributions, which can regularize estimates and prevent overfitting.\nContinuous Learning: As new data comes in, Bayesian methods provide a natural way to update the posterior distribution, leading to continuous learning and adaptation.\n\nThe downsides are mostly related to the choice of prior distribution. In settings where there is a lack of expert knowledge, this Bayesian approach to experimentation might not be very attractive. Computation can also be an issue in more complex settings."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#an-example",
    "href": "blog/bayesian-ab-tests.html#an-example",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs code an example in R and python. We start with generating some fake data and selecting parameters for the prior distributions.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Data: Number of successes and total observations for T and C\nn_T &lt;- 1000\nx_T &lt;- 70\nn_C &lt;- 900\nx_C &lt;- 50\n\n# Prior parameters for the Beta distribution\nalpha_T &lt;- 1\nbeta_T &lt;- 1\nalpha_C &lt;- 1\nbeta_C &lt;- 1\n\n# Posterior parameters\nposterior_alpha_T &lt;- alpha_T + x_T\nposterior_beta_T &lt;- beta_T + n_T - x_T\nposterior_alpha_C &lt;- alpha_C + x_C\nposterior_beta_C &lt;- beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T &lt;- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C &lt;- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Sample from the posterior distributions\nposterior_obs_T &lt;- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C &lt;- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Estimate the probability that T is better than C\nprob_T_better &lt;- mean(posterior_obs_T &gt; posterior_obs_C)\nprint(paste(\"Probability that T is better than C:\", round(prob_T_better, digit=3)))\n\n# Estimate the average treatment effect\ntreatment_effect &lt;- mean(posterior_obs_T - posterior_obs_C)\nprint(paste(\"Average change in Y b/w T and C:\", round(treatment_effect, digit=3)))\n\ntreatment_effect_limit &lt;- (alpha_T + x_T)/(alpha_T + beta_T + n_T) - (alpha_C + x_C)/(alpha_C + beta_C + n_C)\nprint(treatment_effect_limit)\n\n# we can even plot both posteriors distributions.\ndf &lt;- data.frame(\n  y = c(posterior_obs_T, posterior_obs_C),\n  group = factor(rep(c(\"T\", \"C\"), each = 10000))\n)\n\nggplot(df, aes(x = y, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Posterior Distributions of Outcomes\",\n       x = \"Y = 1\", y = \"Density\") +\n  theme_minimal()\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.random.seed(1988)\n\n# Data: Number of successes and total observations for T and C\nn_T = 1000\nx_T = 70\nn_C = 900\nx_C = 50\n\n# Prior parameters for the Beta distribution\nalpha_T = 1\nbeta_T = 1\nalpha_C = 1\nbeta_C = 1\n\n# Posterior parameters\nposterior_alpha_T = alpha_T + x_T\nposterior_beta_T = beta_T + n_T - x_T\nposterior_alpha_C = alpha_C + x_C\nposterior_beta_C = beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T = np.random.beta(posterior_alpha_T, posterior_beta_T, 10000)\nposterior_obs_C = np.random.beta(posterior_alpha_C, posterior_beta_C, 10000)\n\n# Estimate the probability that T is better than C\nprob_T_better = np.mean(posterior_obs_T &gt; posterior_obs_C)\nprint(f\"Probability that T is better than C: {prob_T_better:.3f}\")\n\n# Estimate the average treatment effect\ntreatment_effect = np.mean(posterior_obs_T - posterior_obs_C)\nprint(f\"Average change in Y b/w T and C: {treatment_effect:.3f}\")\n\n# Plot posterior distributions\nplt.figure(figsize=(8, 6))\nsns.kdeplot(posterior_obs_T, fill=True, label=\"T\", alpha=0.5)\nsns.kdeplot(posterior_obs_C, fill=True, label=\"C\", alpha=0.5)\nplt.title(\"Posterior Distributions of Outcomes\")\nplt.xlabel(\"Y = 1\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n\n\n\nHere are the two posterior distributions.\n\nThere is also a specalized bayesAB package in R. It produces some cool charts, so I definitely recommend giving it a try.\nSoftware Package: bayesAB."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#bottom-line",
    "href": "blog/bayesian-ab-tests.html#bottom-line",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBayesian inference offers a compelling alternative to the traditional methods based on frequentist statistics.\nThe main idea rests on incorporating prior information on the success rates in the treatment and control group as a starting point.\nAdvantages of bayesian methods include incorporating prior information, providing probabilistic interpretations, handling small sample sizes better, and enabling continuous learning.\nThe main challenge is the choice of prior distribution, which can be difficult without expert knowledge."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#where-to-learn-more",
    "href": "blog/bayesian-ab-tests.html#where-to-learn-more",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAccessible introductions to the world of Bayesian inference are common. An example is Will Kurt‚Äôs book ‚ÄúBayesian Statistics the Fun Way‚Äú. See also the papers I cite below. As almost everything else, Google is also a great starting point."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#references",
    "href": "blog/bayesian-ab-tests.html#references",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "References",
    "text": "References\nDeng, A. (2015, May). Objective bayesian two sample hypothesis testing for online controlled experiments. In Proceedings of the 24th International Conference on World Wide Web (pp.¬†923-928).\nKamalbasha, S., & Eugster, M. J. (2021). Bayesian A/B testing for business decisions. In Data Science‚ÄìAnalytics and Applications: Proceedings of the 3rd International Data Science Conference‚ÄìiDSC2020 (pp.¬†50-57). Springer Fachmedien Wiesbaden.\nKurt, Will. Bayesian statistics the fun way: understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. No Starch Press, 2019.\nStevens, N. T., & Hagar, L. (2022). Comparative probability metrics: using posterior probabilities to account for practical equivalence in A/B tests. The American Statistician, 76(3), 224-237."
  },
  {
    "objectID": "blog/overview-ml-methods-ci.html",
    "href": "blog/overview-ml-methods-ci.html",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "",
    "text": "The most exciting trend in causal inference over the last decade has been the infusion of machine learning (ML) techniques. Supervised machine learning is designed to find complex patterns in data and as such, it is merely occupied with prediction. Causal inference, on the other hand, pays close attention to statistical precision and inference based on asymptotic properties like consistency and normality. The two worlds are, thus, fundamentally different.\nIt should be no surprise then that machine learning and causal inference do not naturally speak to each other, and some modifications are required to marry them. The good news is recent innovations have led to a bunch of ways in which ML models can be used in isolating causal effects especially in settings with many covariates (also called ‚Äúhigh dimensional‚Äù settings).\nIn this article, I will briefly describe the ways in which we can use ML when looking for causal relationships. I will indeed be concise, and I will avoid diving deeper into technical details. This blog post will look more like a laundry list with references to papers and software packages than a tutorial."
  },
  {
    "objectID": "blog/overview-ml-methods-ci.html#background",
    "href": "blog/overview-ml-methods-ci.html#background",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "",
    "text": "The most exciting trend in causal inference over the last decade has been the infusion of machine learning (ML) techniques. Supervised machine learning is designed to find complex patterns in data and as such, it is merely occupied with prediction. Causal inference, on the other hand, pays close attention to statistical precision and inference based on asymptotic properties like consistency and normality. The two worlds are, thus, fundamentally different.\nIt should be no surprise then that machine learning and causal inference do not naturally speak to each other, and some modifications are required to marry them. The good news is recent innovations have led to a bunch of ways in which ML models can be used in isolating causal effects especially in settings with many covariates (also called ‚Äúhigh dimensional‚Äù settings).\nIn this article, I will briefly describe the ways in which we can use ML when looking for causal relationships. I will indeed be concise, and I will avoid diving deeper into technical details. This blog post will look more like a laundry list with references to papers and software packages than a tutorial."
  },
  {
    "objectID": "blog/overview-ml-methods-ci.html#notation",
    "href": "blog/overview-ml-methods-ci.html#notation",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Notation",
    "text": "Notation\nIt is helpful to quickly summarize some features of the potential outcome framework. Imagine we have a i.i.d. random sample of a binary treatment indicator \\(D\\), outcome variable \\(Y\\) and a vector of covariates \\(X\\). Assume the potential outcomes \\(Y(0)\\) and \\(Y(1)\\) are unrelated to the binary treatment status \\(D\\) which is often referred to as the unconfoundedness or ignorability.\nA common estimand of interest is the Average Treatment Effect (ATE)\n\\[ATE = E[Y(1) - Y(0)],\\]\nwhere \\(Y(d)\\) is the potential outcome under treatment regime \\(D=d\\). Another popular estimand is the Conditional ATE (CATE),\n\\[CATE(X) = E[Y(1) - Y(0) | X],\\]\nwhich is the ATE for a particular group of units with a fixed covariates level (e.g., women, men, new users, etc.).\nThe ATE can be expressed in at least three useful ways:\n\\[\\begin{align*} ATE & = \\mathbf{E} \\left[ \\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(outcome model only)} \\\\ & = \\mathbf{E}\\left[ \\frac{YD}{e(X)} - \\frac{Y(1-D)}{1-e(X)} \\right] \\hspace{1cm} \\text{(prop. score model only)} \\\\ & = \\mathbf{E} \\left[ \\frac{[Y-\\mu(1,X)D]}{e(X)} - \\frac{[Y-\\mu(0,X)](1-D)}{1-e(X)} \\right] \\\\ & + \\mathbf{E} \\left[\\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(both models)} \\end{align*}\\]\nwhere\n\\[\\mu(D,X) = \\mathbf{E}[Y|D,X]\\]\nis the outcome model and\n\\[e(x)=\\mathbf{E}[D|X]\\]\nis the propensity score.\nThis formulation is helpful because it naturally splits the types of treatment effect estimation methods into three separate categories ‚Äì (i) those that require only estimation of \\(\\mu(D,X)\\), (ii) those that use only \\(e(X)\\), and (iii) those that need both.\nOne can think of the propensity score (PS) and the outcome models as nuisance functions ‚Äì ones that are not of direct interest but play a part in treatment effect estimation. ML methods are attractive candidates for estimating these nuisance functions flexibly."
  },
  {
    "objectID": "blog/overview-ml-methods-ci.html#diving-deeper",
    "href": "blog/overview-ml-methods-ci.html#diving-deeper",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nCovariate Balancing Methods\nUnder the ignorability assumption, all confounding bias comes from differences in the covariates \\(X\\) between the treatment and the control groups. Intuitively, balancing these is enough to guarantee unbiasedness. One line of research develops methods to do exactly that ‚Äì directly equate covariates between the two groups of interest. These approaches circumvent estimation of the two nuisance functions mentioned above.\nThese are inspired by the ML view of data analysis framed as an optimization problem. Examples include Entropy Balancing, Genetic Matching, Stable Weights, and Residual Balancing. The last approach combines balancing with a regression adjustment to reduce extrapolation when estimating the counterfactuals for the treatment group. Some of these methods were designed with a low dimensional setting in mind, but they still carry the spirit of ML type of thinking.\nSoftware Packages: MatchIt, Ebal, BalanceHD.\n\n\nML Methods for the Propensity Score Model\nPropensity score methods rely on correctly specifying the PS model. In low-dimensional settings, it is possible to estimate it nonparametrically. In practice, however, this is unrealistic when data scientists have access to continuous or even bunch of discrete covariates. Can ML methods come to the rescue?\nIn principle, yes. A major challenge in this context, however, is the choice of a loss function. In the ML world loss functions target measures of fit (e.g., Root Mean Squared Error, log likelihood, etc.) but these would be problematic here as they do not aim at balancing covariates important to reduce bias. Thus, these methods do not perform very well unless used with much caution.\nImai and Ratkovic (2014) propose a PS method that directly balances covariates. Another choice is the Boosted CART implementation. As its name suggests, it iteratively forms a bunch of tree models and averages them, but with an appropriately chosen loss function. A series of simulation studies analyze the performance of various ML algorithms used to estimate the PS, but overall, these methods are nowadays dominated by some of the doubly robust approaches described below.\nSoftware Packages: TWANG, CBPS.\n\n\nML Methods for the Outcome Model\nWe can also estimate treatment effects directly by modelling the outcome variable. This also requires correct model specification, and even then, it is prone to extrapolation in finite samples. Examples include Bayesian Additive Regression Trees (BART) and other ensemble methods.\nBelloni et al.¬†(2014) show that the set of features optimal when estimating the outcome model, is not necessarily optimal for estimating treatment effects. The issue is omitting a variable that is correlated with the treatment even if its correlation with the outcome is only modest, can introduce considerable bias. Moreover, typically, the rate of convergence in this context when using ML models will be slower than , meaning that you will need much more data to get a good treatment effect estimate.\nOverall, there is no statistical theory of why ML methods should work well here, but some methods tend to perform well empirically. This brings us to the doubly robust approach.\nSoftware Packages: BART, rBART, BayesTree.\n\n\nML Methods for Both Models & Doubly Robust Methods\nMethods combining models for both the propensity score and the outcome have long been advocated. Intuitively, the propensity score can be seen as a balancing step after which regression adjustment can remove any remaining bias. Imbens (2015), for instance, promotes this type of thinking in matching methods specifically.\nDoubly robust (DR) estimators use both nuisance models and have the amazing property of being consistent even if only one of the two models is correctly specified. You can think of the bias term as a product of the biases in the two nuisance models ‚Äì if one of them is equal to zero, the entire term vanishes. Additionally, if both models are correctly specified, some methods are semiparametrically efficient, (i.e., ‚Äúthe best‚Äù in a large class of flexible models). A simple DR method is the Augmented Inverse Probability Weighting (AIPW) estimator which in linear models comes down to running weighted OLS regression of Y on D and X with the estimated (inverse) propensity score as weights.\nThere is more good news. Amazingly, DR methods can still converge at a rate \\(\\sqrt{N}\\) even if the underlying nuisance models converge at slower rates. The formal requirement is that the nuisance models must belong to something called a Donsker class. In simple words, they should not be too complex and prone to overfit.\nOne line of research has developed the Double ML framework. This work has been so influential that it deserves a blog post of its own. Without going into technical details, the authors of the original paper show that na√Øve application of ML methods when estimating both nuisance functions results in two types of biases ‚Äì regularization and overfitting. DoubleML makes use of something called Neyman orthogonalization (think of the Frisch‚ÄìWaugh‚ÄìLovell theorem) to remove the former, and sample splitting to avoid the latter. In simple settings, this method combines the residuals from regressions of \\(Y\\) on \\(X\\) and \\(Y\\) on \\(D\\), but in general it can take more complicated forms.\nAnother line of research has developed the Double Post Lasso approach. The idea here is simpler ‚Äì use Lasso to select covariates relevant to the outcome regression and then again to select ones relevant to the propensity score. Lastly, use OLS to regress \\(Y\\) on the union of the covariates selected previously. This procedure removes confounding or regularization bias that might be present in methods using ML models to estimate only one of the nuisance models.\nSoftware Packages: DoubleML, hdm, dlsr.\n\n\nHeterogeneous Treatment Effect Estimation\nMining for heterogeneous treatment effects has been a particularly fruitful field for ML methods. A leading example is the causal tree method developed by Athey and Imbens (2016). It resembles the traditional CART algorithm, but it uses a different criterion for splitting the data: instead of focusing on Mean Squared Error (MSE) for the outcome, it uses MSE for treatment effect. The result is a decision tree, in which units in each leaf have similar treatment effects. The method also features ‚Äúhonest‚Äù sample splitting for obtaining variance estimates ‚Äì one half of the data is used to determine the optimal tree, and the other half to estimate the treatment effects.\nBuilding on this idea, Wager and Athey (2018) propose a random forest-based method which generates a bunch of causal trees and averages the results to induce smoothness in the treatment effect‚Äôs function. Magically, the authors show the predictions are asymptotically normal and centered around the true value for each unit! This is exciting as it allows for standard methods for statistical inference.\nOther methods include BART, a Bayesian version of random forests mentioned above, Imai and Ratkovic (2013) who propose adding treatment indicators interacted with covariates in a LASSO regression to determine which variables are important for treatment effect heterogeneity. Similarly, Tian et al.¬†(2014) suggest modifying the covariates in a straightforward way and running a regression of the outcome on the modified variables without an intercept. Other methods include the R-learner of Nie and Wager (2021) which relies on estimating the two nuisance models described above and using a special loss function. K√ºnzel et al.¬†(2019) propose a X-learner metaalogrithm.\nSoftware Packages: FindIt, rlearner, grf, causalToolbox.\n\n\nOthers\nA by-product of estimating treatment effect heterogeneity is that we can determine which units should be treated. Intuitively, if the treatment effect is close to zero (or even negative) for some users, there is not much to be gained from the exposure. Kitagawa and Tetenov (2018) analyze a setting with limited complexity, and Athey and Wager (2021) develop the DoubleML framework discussed above for choosing whom to treat. ML has also been used for variance reduction in randomized experiments via regression adjustments. See, for instance, Wager et al.¬†(2016), Bloniarz et al.¬†(2016), and List et al.¬†(2022)."
  },
  {
    "objectID": "blog/overview-ml-methods-ci.html#bottom-line",
    "href": "blog/overview-ml-methods-ci.html#bottom-line",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods are slowly becoming an indispensable part of data scientists‚Äô toolkit for estimating causal relationships. There is an abundance of methods aiding practitioners in both ATE and CATE estimation.\nDoubly robust approaches offer better theoretical guarantees than methods relying on estimating either the outcome or the propensity score models.\nThe leading approaches for estimating ATEs are Double ML and Double Post Lasso.\nThe leading approach for estimating CATEs is the causal forest method."
  },
  {
    "objectID": "blog/overview-ml-methods-ci.html#where-to-learn-more",
    "href": "blog/overview-ml-methods-ci.html#where-to-learn-more",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMore technical data scientists will find the following review papers useful:\n\nAthey and Imbens (2019)\nAthey and Imbens (2017)\nVarian (2014)\nKreif and DiazOrdaz (2019)\nMullainathan and Spiess (2017)\nHu (2023)\n\nThere are a few major Python frameworks for using ML in causal inference estimation. More practically-oriented folks might like their documentation:\n\nCausalML\nEconML\nDoubleML"
  },
  {
    "objectID": "blog/overview-ml-methods-ci.html#references",
    "href": "blog/overview-ml-methods-ci.html#references",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "References",
    "text": "References\nAthey, S., & Imbens, G. (2016). Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27), 7353-7360.\nAthey, S., & Imbens, G. W. (2017). The state of applied econometrics: Causality and policy evaluation. Journal of Economic perspectives, 31(2), 3-32.\nAthey, S., & Imbens, G. W. (2019). Machine learning methods that economists should know about. Annual Review of Economics, 11, 685-725.\nAthey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 80(4), 597-623.\nAthey, S., & Wager, S. (2021). Policy learning with observational data. Econometrica, 89(1), 133-161.\nAustin, P. C. (2012). Using ensemble-based methods for directly estimating causal effects: an investigation of tree-based G-computation. Multivariate behavioral research, 47(1), 115-135.\nBelloni, A., Chernozhukov, V., & Hansen, C. (2014). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2), 608-650.\nBloniarz, A., Liu, H., Zhang, C. H., Sekhon, J. S., & Yu, B. (2016). Lasso adjustments of treatment effect estimates in randomized experiments. Proceedings of the National Academy of Sciences, 113(27), 7383-7390.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal.\nDiamond, A., & Sekhon, J. S. (2013). Genetic matching for estimating causal effects: A general multivariate matching method for achieving balance in observational studies. Review of Economics and Statistics, 95(3), 932-945.\nHahn, P. R., Murray, J. S., & Carvalho, C. M. (2020). Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion). Bayesian Analysis, 15(3), 965-1056.\nHainmueller, J. (2012). Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20(1), 25-46.\nHill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240.\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. Annals of Applied Statistics\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B: Statistical Methodology, 243-263.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nKitagawa, T., & Tetenov, A. (2018). Who should be treated? empirical welfare maximization methods for treatment choice. Econometrica, 86(2), 591-616.\nKreif, N., & DiazOrdaz, K. (2019). Machine learning in policy evaluation: new tools for causal inference. arXiv preprint arXiv:1903.00402.\nK√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165.\nLee, B. K., Lessler, J., & Stuart, E. A. (2010). Improving propensity score weighting using machine learning. Statistics in medicine, 29(3), 337-346.\nList, J. A., Muir, I., & Sun, G. K. (2022). Using Machine Learning for Efficient Flexible Regression Adjustment in Economic Experiments (No.¬†w30756). National Bureau of Economic Research.\nMcCaffrey, D. F., Ridgeway, G., & Morral, A. R. (2004). Propensity score estimation with boosted regression for evaluating causal effects in observational studies. Psychological methods, 9(4), 403.\nMullainathan, S., & Spiess, J. (2017). Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2), 87-106.\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\nRobins, J. M., Rotnitzky, A., & Zhao, L. P. (1994). Estimation of regression coefficients when some regressors are not always observed. Journal of the American statistical Association, 89(427), 846-866.\nSetoguchi, S., Schneeweiss, S., Brookhart, M. A., Glynn, R. J., & Cook, E. F. (2008). Evaluating uses of data mining techniques in propensity score estimation: a simulation study. Pharmacoepidemiology and drug safety, 17(6), 546-555.\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\nVarian, H. R. (2014). Big data: New tricks for econometrics. Journal of Economic Perspectives, 28(2), 3-28.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nWager, S., Du, W., Taylor, J., & Tibshirani, R. J. (2016). High-dimensional regression adjustments in randomized experiments. Proceedings of the National Academy of Sciences, 113(45), 12673-12678.\nWestreich, D., Lessler, J., & Funk, M. J. (2010). Propensity score estimation: neural networks, support vector machines, decision trees (CART), and meta-classifiers as alternatives to logistic regression. Journal of clinical epidemiology, 63(8), 826-833.\nWyss, R., Ellis, A. R., Brookhart, M. A., Girman, C. J., Jonsson Funk, M., LoCasale, R., & St√ºrmer, T. (2014). The role of prediction modeling in propensity score estimation: an evaluation of logistic regression, bCART, and the covariate-balancing propensity score. American journal of epidemiology, 180(6), 645-655.\nZivich, P. N., & Breskin, A. (2021). Machine learning for causal inference: on the use of cross-fit estimators. Epidemiology (Cambridge, Mass.), 32(3), 393.\nZubizarreta, J. R. (2015). Stable weights that balance covariates for estimation with incomplete outcome data. Journal of the American Statistical Association, 110(511), 910-922."
  },
  {
    "objectID": "blog/ml-based-adjustments.html",
    "href": "blog/ml-based-adjustments.html",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "",
    "text": "Randomized experiments are the gold standard when interested in measuring causal relationships with data. In settings with small treatment effects or underpowered designs, a major focus falls on decreasing the variance. In simple low-dimensional settings a common attempt to do that is to include a bunch of covariates and their interaction with the treatment variable in an OLS regression. Under standard assumptions, the coefficient on the treatment variable is still asymptotically unbiased (albeit not in finite samples) and including the interactions guarantees that this estimator does not have higher asymptotic variance than the simple difference-in-means.\nIn high-dimensional settings, however, this can easily lead to overfitting and new tools for variance reduction are needed. In this article, I will focus on two ways Machine Learning (ML) can be helpful with this problem when we have access to a bunch of covariates. In the first set of methods, we use a ML algorithm (such as the lasso) to directly estimate the treatment effect. Alternatively, we can first use ML to predict the outcome and then feed that prediction in an OLS regression.\nA helpful benchmark with which to compare these methods is the simple (non-parametric) difference-in-means estimator. Under certain conditions, both approaches guarantee smaller or equal variance."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#background",
    "href": "blog/ml-based-adjustments.html#background",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "",
    "text": "Randomized experiments are the gold standard when interested in measuring causal relationships with data. In settings with small treatment effects or underpowered designs, a major focus falls on decreasing the variance. In simple low-dimensional settings a common attempt to do that is to include a bunch of covariates and their interaction with the treatment variable in an OLS regression. Under standard assumptions, the coefficient on the treatment variable is still asymptotically unbiased (albeit not in finite samples) and including the interactions guarantees that this estimator does not have higher asymptotic variance than the simple difference-in-means.\nIn high-dimensional settings, however, this can easily lead to overfitting and new tools for variance reduction are needed. In this article, I will focus on two ways Machine Learning (ML) can be helpful with this problem when we have access to a bunch of covariates. In the first set of methods, we use a ML algorithm (such as the lasso) to directly estimate the treatment effect. Alternatively, we can first use ML to predict the outcome and then feed that prediction in an OLS regression.\nA helpful benchmark with which to compare these methods is the simple (non-parametric) difference-in-means estimator. Under certain conditions, both approaches guarantee smaller or equal variance."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#notation",
    "href": "blog/ml-based-adjustments.html#notation",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "Notation",
    "text": "Notation\nI use \\(\\bar{Y}^T\\) and \\(\\bar{Y}^C\\) to denote the sample average outcomes for the treatment and control groups respectively. \\(X\\) is the covariate vector and its deviations from the average are \\(\\tilde{X}\\). The benchmark estimator can be expressed as:\n\\[\\hat{ATE}^{simple} = \\bar{Y}^T - \\bar{Y}^C.\\]"
  },
  {
    "objectID": "blog/ml-based-adjustments.html#diving-deeper",
    "href": "blog/ml-based-adjustments.html#diving-deeper",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nBroadly speaking, there are two ML Methods for Variance Reduction.\n\nUsing ML Regression Directly\nThe simplest and most natural way to incorporate covariates is to add them to a linear model (along with the treatment variable and their interactions with the treatment variable). Bloniarz et al.¬†(2015) show we can directly use Lasso regression instead of OLS.\nTo guarantee that the lasso does not omit the treatment variable, we can run two separate regressions, one for each (treatment) group. Then the estimator can be formulated as:\n\\[\\hat{ATE}^{lasso} = (\\bar{Y}^T-\\tilde{X}^T\\beta^{T}_{lasso}) - (\\bar{Y}^C-\\tilde{X}^C\\beta^{C}_{lasso}),\\]\nwhere \\(\\beta^{i}_{lasso}\\) is the coefficient vector from the lasso regressions on observations in group \\(i\\in\\{T,C\\}\\). The authors also give a conservative formula for computing the variance of \\(\\hat{ATE}^{lasso}\\). When the two lasso regressions select different sets of covariates (which is probably common in practice), this is no longer guaranteed to yield equal or lower asymptotic variance compared to the benchmark.\nAlgorithm:\n\nFor the treatment and control groups separately, run lasso regression of \\(Y\\) on \\(\\tilde{X}\\) go get \\(\\hat{\\beta}^T_{lasso}\\) and \\(\\hat{\\beta}^C_{lasso}\\).\nCalculate the treatment effect estimate \\(\\hat{ATE}^{lasso}\\) using the above formula.\nCalculate the estimate of the variance of \\(\\hat{ATE}^{lasso}\\) using the formula in Blonarz et al.¬†(2015).\n\nThe authors also propose the lasso+OLS estimator which first uses \\(L1\\) regularization as above to select the covariates and then plugs those in OLS to get the treatment effect estimate.\nA similar idea has also been studied by Wager et al (2016). They show that when additionally, assuming Gaussian data (along with a bunch of regularity assumptions), we can use any ‚Äúrisk consistent‚Äù ML estimator such as ridge, elastic net, etc. ‚ÄúRisk consistent‚Äù here means as we give the algorithm more data, it gets closer to the truth. The lower the risk the higher the variance reduction gains compared to the simple difference-in-means estimator. The authors also propose a simple cross-fitting approach to calculate confidence intervals.\nAlgorithm:\n\nSplit the data into \\(k\\) equal sized folds.\nFor each fold \\(k\\):\n\n\ncalculate \\(\\bar{Y}^k, \\tilde{X}^k\\).\nget the coefficients \\(\\hat{\\beta}_{lasso}^{-k}\\) based on regressions on all other \\(k-1\\) folds.\ncombine both quantities and calculate \\(\\hat{ATE}^{lasso}\\).\ncalculate its standard error.\n\n\nGet the final estimates \\(\\hat{ATE}^{lasso}\\) and its standard error by taking weighted averages across all \\(k\\) folds.\n\nThis concludes the discussion of using a ML-type linear regression model to reduce the variance in A/B tests. Let‚Äôs now move on to the second method.\n\n\nUsing ML Regression Indirectly\nAn alternative approach first uses ML to predict Y and then plugs that prediction into an OLS regression of the outcome on the treatment variable. One can then use cross-fitting to do the prediction which ensures the ‚Äúna√Øve‚Äù OLS confidence intervals remain valid. The authors call this procedure MLRATE (machine learning regression-adjusted treatment effect estimator).\nHere is a rough version of the algorithm:\nAlgorithm:\n\nSplit the data in \\(k\\) equal-sized folds.\nFor each fold \\(k\\):\n\n\nPredict Y by applying a ML algorithm to all other \\(k-1\\) folds. Call this prediction \\(\\bar{Y}_k\\).\n\n\nGet a final prediction \\(\\bar{Y}=\\sum_k\\bar{Y}_k\\).\nRun OLS of \\(Y\\) on \\(T\\),$ {Y}_k$ and \\((\\bar{Y}_k-\\bar{Y})\\timesT\\) and use the associated standard errors and \\(p\\)-values."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#bottom-line",
    "href": "blog/ml-based-adjustments.html#bottom-line",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nRegression adjustments are a commonly used tool to reduce variance in A/B tests.\nThe machine learning toolbox offers possibly more powerful algorithms in this space.\nThere are two main ML approaches. Both can be shown under certain conditions to be at least as good as the simple difference-in-means estimator.\nThe first approach uses ML regression algorithms directly.\nThe second method, instead, uses ML to predict the outcome and adds that in an OLS regression."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#where-to-learn-more",
    "href": "blog/ml-based-adjustments.html#where-to-learn-more",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nUnfortunately, I do not know of any papers that summarize or compare these methods. If you are interested in learning more, look at the original references and any papers mentioned therein."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#references",
    "href": "blog/ml-based-adjustments.html#references",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "References",
    "text": "References\nBelloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse models. Bernoulli 19(2): 521-547\nBloniarz, A., Liu, H., Zhang, C. H., Sekhon, J. S., & Yu, B. (2016). Lasso adjustments of treatment effect estimates in randomized experiments. Proceedings of the National Academy of Sciences, 113(27), 7383-7390.\nGuo, Y., Coey, D., Konutgan, M., Li, W., Schoener, C., & Goldman, M. (2021). Machine learning for variance reduction in online experiments. Advances in Neural Information Processing Systems, 34, 8637-8648.\nLin, W. (2013). Agnostic notes on regression adjustments to experimental data: Reexamining Freedman‚Äôs critique. Ann. Appl. Stat. 7(1): 295-318\nList, J. A., Muir, I., & Sun, G. K. (2022). Using Machine Learning for Efficient Flexible Regression Adjustment in Economic Experiments (No.¬†w30756). National Bureau of Economic Research.\nNegi, A., & Wooldridge, J. M. (2021). Revisiting regression adjustment in experiments with heterogeneous treatment effects. Econometric Reviews, 40(5), 504-534.\nPoyarkov, A., Drutsa, A., Khalyavin, A., Gusev, G., & Serdyukov, P. (2016, August). Boosted decision tree regression adjustment for variance reduction in online controlled experiments. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.¬†235-244).\nWager, S., Du, W., Taylor, J., & Tibshirani, R. J. (2016). High-dimensional regression adjustments in randomized experiments. Proceedings of the National Academy of Sciences, 113(45), 12673-12678."
  },
  {
    "objectID": "blog/lord-paradox.html",
    "href": "blog/lord-paradox.html",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Lord‚Äôs paradox presents a fascinating challenge in causal inference and statistics. It highlights how different statistical methods applied to the same data can lead to contradictory conclusions. The paradox typically arises when comparing changes over time between two groups in the context of adjusting for baseline characteristics. Despite its theoretical nature, the phenomena is deeply relevant to practical data analysis, especially in observational studies where causation is challenging to establish. Let‚Äôs look at an example."
  },
  {
    "objectID": "blog/lord-paradox.html#background",
    "href": "blog/lord-paradox.html#background",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Lord‚Äôs paradox presents a fascinating challenge in causal inference and statistics. It highlights how different statistical methods applied to the same data can lead to contradictory conclusions. The paradox typically arises when comparing changes over time between two groups in the context of adjusting for baseline characteristics. Despite its theoretical nature, the phenomena is deeply relevant to practical data analysis, especially in observational studies where causation is challenging to establish. Let‚Äôs look at an example."
  },
  {
    "objectID": "blog/lord-paradox.html#diving-deeper",
    "href": "blog/lord-paradox.html#diving-deeper",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nMean Differences Over Time\nTo explore Lord‚Äôs paradox, consider the following scenario: Suppose we have two groups of individuals‚Äî\\(A\\) and $\\(B\\)‚Äîwith their body weights measured at two time points: before and after a specific intervention. Let the weight at the initial time point be denoted as \\(W_{\\text{pre}}\\), and the weight at the final time point be \\(W_{\\text{post}}\\). We are interested in whether the intervention caused a change in weight between the two groups.\nOne approach to analyze this is to compute the (unadjusted) mean weight change for each group over time:\n\\[\\Delta = \\Delta^A - \\Delta^B.\\]\nIf this quantity is statistically significant, we might conclude that the intervention had a differential effect.\nControlling for Baseline Characteristics An alternative approach involves adjusting for baseline weight W_{} using, for example, a regression model:\n\\[W_{\\text{post}}=\\beta_1 + \\beta_2 G_A + \\beta_3 W_{\\text{pre}} + \\epsilon,\\]\nwhere G is a binary indicator for group A membership and \\(\\epsilon\\) is an error term. Here, \\(\\beta_2\\) captures the group difference in \\(W_{\\text{post}}\\), linearly controlling for baseline body weight.\nSurprisingly, these two approaches can yield conflicting results. For example, the former method might suggest no difference, while the regression adjustment indicates a significant group effect.\n\n\nAn Explanation\nThis contradiction arises because the two methods implicitly address different causal questions.\n\nMethod 1 asks: ‚ÄúDo Groups \\(A\\) and \\(B\\) gain/lose different amounts of weight?‚Äù\nMethod 2 asks: ‚ÄúGiven the same initial weight, does any of the groups end up at different final weights?‚Äù The regression approach adjusts for baseline differences, assuming \\(W_{\\text{pre}}\\) is a confounder.\n\nThe key insight is that the choice of analysis reflects underlying assumptions about the causal structure of the data. If \\(W_{\\text{pre}}\\) is affected by group membership (e.g., due to selection bias), then adjusting for it may introduce bias rather than remove it. However, in practice we most often should control for baseline characteristics as they are critical in balancing the treatment and control groups.\n\n\nThe Simpson‚Äôs Paradox Once Again\nI recently illustrated the more commonly discussed Simpson‚Äôs paradox. Interestingly, a 2008 paper claims that two phenomena are closely related, with the Lord‚Äôs paradox being a ‚Äúcontinuous version‚Äù of Simpson‚Äôs paradox."
  },
  {
    "objectID": "blog/lord-paradox.html#an-example",
    "href": "blog/lord-paradox.html#an-example",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs look at some code illustrating Lord‚Äôs paradox in R and python. We start with simulating a dataset where two groups have identical distributions of \\(W_{\\text{pre}}\\) and \\(W_{\\text{post}}\\), yet differing relationships between the two variables.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\nn &lt;- 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup &lt;- factor(rep(c(\"A\", \"B\"), each = n/2))\n\n# Initial weight (pre). # Group A starts with higher average weight\nweight_pre &lt;- numeric(n)\nweight_pre[group == \"A\"] &lt;- rnorm(n/2, mean = 75, sd = 10)\nweight_pre[group == \"B\"] &lt;- rnorm(n/2, mean = 65, sd = 10)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain &lt;- rnorm(n, mean = 10, sd = 5)\nweight_post &lt;- weight_pre + gain\n\n# Create data frame\ndata &lt;- data.frame(group = group, pre = weight_pre, post = weight_post)\n\n# Analysis 1: Compare change scores between groups`\nt.test(post - pre ~ group, data = data)\n&gt; p-value = 0.6107\n\n# Analysis 2: Regression Adjustment`\nmodel &lt;- lm(post ~ group + pre, data = data)\nsummary(model)\n&gt; p-value = 0.08428742\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Set seed for reproducibility\nnp.random.seed(1988)\nn = 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup = np.array([\"A\"] * (n // 2) + [\"B\"] * (n // 2))\n\n# Initial weight (pre). Group A starts with higher average weight\nweight_pre = np.zeros(n)\nweight_pre[group == \"A\"] = np.random.normal(loc=75, scale=10, size=n // 2)\nweight_pre[group == \"B\"] = np.random.normal(loc=65, scale=10, size=n // 2)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain = np.random.normal(loc=10, scale=5, size=n)\nweight_post = weight_pre + gain\n\n# Create DataFrame\ndata = pd.DataFrame({\"group\": group, \"pre\": weight_pre, \"post\": weight_post})\n\n# Analysis 1: Compare change scores between groups\ndata[\"change\"] = data[\"post\"] - data[\"pre\"]\ngroup_a_change = data[data[\"group\"] == \"A\"][\"change\"]\ngroup_b_change = data[data[\"group\"] == \"B\"][\"change\"]\nt_stat, p_value = ttest_ind(group_a_change, group_b_change)\nprint(f\"Analysis 1: p-value = {p_value:.4f}\")\n\n# Analysis 2: Regression Adjustment\nmodel = smf.ols(\"post ~ group + pre\", data=data).fit()\nprint(model.summary())\n\n\n\nThe former approach shows lack of statistically significant differences in the body weight after the intervention between the two groups (\\(p\\)-value =$ 0.6107\\(). The results from the latter method do show meaningful differences (\\)p$-value = \\(0.0842\\)).\nThis illustrates the core of Lord‚Äôs paradox ‚Äì the statistical approach chosen can lead to different interpretations of the same underlying phenomenon."
  },
  {
    "objectID": "blog/lord-paradox.html#bottom-line",
    "href": "blog/lord-paradox.html#bottom-line",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLord‚Äôs paradox underscores the importance of aligning statistical methods with causal assumptions.\nDifferent methods answer different questions and may yield contradictory results if applied blindly.\nCareful consideration of the data-generating process and the role of potential confounders is crucial in choosing the appropriate analytical approach."
  },
  {
    "objectID": "blog/lord-paradox.html#references",
    "href": "blog/lord-paradox.html#references",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "References",
    "text": "References\nLord, E. M. (1967). A paradox in the interpretation of group comparisons. Psychological Bulletin, 68, 304‚Äì305. doi:10.1037/h0025105\nLord, F. M. (1969). Statistical adjustments when comparing preexisting groups. Psychological Bulletin, 72, 336‚Äì337. doi:10.1037/h0028108\nLord, E. M. (1975). Lord‚Äôs paradox. In S. B. Anderson, S. Ball, R. T. Murphy, & Associates, Encyclopedia of Educational Evaluation (pp.¬†232‚Äì236). San Francisco, CA: Jossey-Bass.\nTu, Y. K., Gunnell, D., & Gilthorpe, M. S. (2008). Simpson‚Äôs Paradox, Lord‚Äôs Paradox, and Suppression Effects are the same phenomenon‚Äìthe reversal paradox. Emerging themes in epidemiology, 5, 1-9."
  },
  {
    "objectID": "blog/hypo-testing-linear-ml.html",
    "href": "blog/hypo-testing-linear-ml.html",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "",
    "text": "Machine learning models are an indispensable part of data science. They are incredibly good at what they are designed for ‚Äì making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, p-values, or anything else related to statistical significance. Why?\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have already selected the most strongly correlated variables.\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The \\(t\\)-stats and \\(p\\)-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models ‚Äì Ridge, Elastic net, SCAD, etc."
  },
  {
    "objectID": "blog/hypo-testing-linear-ml.html#background",
    "href": "blog/hypo-testing-linear-ml.html#background",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "",
    "text": "Machine learning models are an indispensable part of data science. They are incredibly good at what they are designed for ‚Äì making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, p-values, or anything else related to statistical significance. Why?\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have already selected the most strongly correlated variables.\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The \\(t\\)-stats and \\(p\\)-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models ‚Äì Ridge, Elastic net, SCAD, etc."
  },
  {
    "objectID": "blog/hypo-testing-linear-ml.html#notation",
    "href": "blog/hypo-testing-linear-ml.html#notation",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Notation",
    "text": "Notation\nAs a reminder, \\(\\beta^{lasso}\\) is the solution to:\n\\[\\min_{\\beta} \\frac{1}{2} || Y-x\\beta|| ^2_2 + \\lambda ||\\beta||_1. \\]\nWe are trying to predict a vector \\(Y\\in \\mathbb{R}\\) with a set of features \\(X\\in \\mathbb{R}^{pxn}\\) with \\(p\\leq n\\), and \\(\\lambda\\) is a tuning parameter. When needed, I will use \\(j\\) to index individual columns (i.e., variables) of \\(X\\)."
  },
  {
    "objectID": "blog/hypo-testing-linear-ml.html#diving-deeper",
    "href": "blog/hypo-testing-linear-ml.html#diving-deeper",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nWe first need to discuss an important subtlety. There are two distinct ways of thinking about performing hypothesis testing in ML models.\nThe traditional view is that we have a true linear model which includes all variables:\n\\[Y=X\\beta_0+\\epsilon.\\]\nWe are interested in testing whether \\(\\beta_0=0\\) ‚Äì that is, inference on the full model. This is certainly an intuitive target. The interpretation is that this model encapsulates all relevant causal variables for \\(Y\\). Importantly, even if a given variable $X_j $ is not selected, it still belongs to the model and has a meaningful interpretation.\nThere is an alternative way to think about the problem. Imagine we run a variable selection algorithm (e.g., lasso), which selects a subset \\(M=\\{1,\\dots,p\\}\\) of all available predictors (\\(X\\)), \\(M&lt;p\\) leading to the alternative model:\n\n\\[Y=X_M\\beta_M+u.\\]\n\nNow we are interested in testing whether \\(\\beta_M=0\\) ‚Äì that is, inference on the selected model. Unlike the scenario above, here \\(\\beta_{Mj}\\) is interpreted as the change in \\(Y\\) for a unit change in \\(X_j\\) when all other variables in \\(X_M\\) (as opposed to all of \\(X\\)) are kept constant.\nWhich of the two targets is more intuitive?\nStatisticians argue vehemently about this. Some claim that the full model interpretation is inherently problematic. It is too na√Øve and perhaps even arrogant to think that (i) mother nature can be explained by a linear equation, (ii) we can measure and include the full set of relevant predictors. On top of this, there are also technical issues with this interpretation beyond the scope of this post.\nTo overcome these challenges, relatively recently, statisticians developed the idea of inference on the selected model. This introduces major technical challenges, however.\nLet us dive deeper and jump into the methods.\n\nThe Na√Øve Approach: What Not to Do\nFirst things first ‚Äì here is what we should not do.\n\nRun a Lasso regression.\nRun OLS regression on the subset of selected variables.\nPerform statistical inference with the estimated t-stats, confidence intervals, and p-values.\n\nThis is bad practice. Can you see why?\nIt is simply because we ignored the fact that we already peeked at the data when we ran the Lasso regression. Lasso already chose the variables that are strongly correlated with the outcome. Intuitively, we will need to inflate the \\(p\\)-values to account for the data exploration in the first step.\nIt turns out that, in general, both the finite- and large-sample distributions of these parameters are non-Gaussian and depend on unknown parameters in weird ways. Consequently, the calculated t-stats and p-values are all wrong, and there is little hope that anything simple can be done to save this approach. And no, the standard bootstrap cannot help us either.\nBut are there special cases when this approach might work? A recent paper titled ‚ÄúIn Defense of the Indefensible: A Very Na√Øve Approach to High-Dimensional Inference‚Äù argues that under very strict assumptions on \\(X\\) and \\(\\lambda\\), this method is actually kosher. The reason it works is that in the magical world of those assumptions, the set of variables that Lasso chooses is deterministic, and not random (hence circumventing the issue described above). The resulting estimator is unbiased and asymptotically normal ‚Äì hence hypothesis testing is trivial.\nHere is what we should do instead.\n\n\nThe Classical Approach: Inference on the Full Model\nRoughly speaking, there are at least four ways we can go about doing hypothesis testing for in equation (1).\n\nData Split\n\nSplit our data into two equal parts.\nRun a Lasso regression on the first part.\nRun OLS on the second part with the selected variables from Step 2.\nPerform inference using the computed \\(t\\)-stats, \\(p\\)-values, and confidence intervals.\n\nThis is simple and intuitive. The problem is that in small samples, the p-values can be quite sensitive to how we split the data in the first step. This is clearly undesirable, as we will be getting different results every time we run this algorithm for no apparent reason.\nPros: Simple, intuitive, fast, and easy to explain to non-technical audiences.\nCons: Needs a lot of data and gives p-values only for selected variables.\nSoftware package: We do not need any.\n\n\nMulti Split\nThis is a modification of the Data Split approach designed to solve the sensitivity issue and increase power.\n\nRepeat \\(B\\) times:\n\n\nReshuffle data.\nRun the Data Split method.\nSave the p-values.\n\n\nAggregate the B \\(p\\)-values into a single final one for each variable.\n\nInstead of splitting the data into two parts only once, we can do it many times, and each time, we get a \\(p\\)-value for every variable. The aggregation goes a long way to solving the instability of the simple data split approach. There is a lot of clever mathematics behind it. For example, there is a complicated expression for aggregating the \\(p\\)-values rather than taking a simple average.\nPros: Simple, intuitive, sort of easy to explain to non-technical audiences.\nCons: Can be computationally expensive.\nSoftware package: hdi\n\n\nBias Correction\nThis approach tackles the problem from a very different angle. The idea is to directly remove the bias from the na√Øve Lasso procedure without any subsampling or data splitting. Somewhat magically, the resulting estimator is unbiased and asymptotically normally distributed ‚Äì statistical inference is then straightforward.\nThe are several versions of this idea, but the general form of these estimators is:\n\\[\\hat{\\beta}^{\\text{bias cor}} = \\hat{\\beta}^{lasso} + \\hat{\\Theta} X'\\epsilon^{lasso},\\]\nWhere \\(\\hat{\\beta}^{lasso}\\) is the lasso estimator and \\(\\epsilon^{lasso}\\) are the residuals. The missing piece is the \\(\\hat{\\Theta}\\) matrix; there are several ways to estimate it depending on the setting. In its simplest form, \\(\\hat{\\Theta}\\) is the inverse of the sample variance-covariance matrix. Other examples include the matrix, which minimizes an error term related to the bias as well as the variance of its Gaussian component.\nSimilar bias-correction methods have been developed for Ridge regression as well.\nPros: Popular, simple, and sort of easy to explain to non-technical audiences.\nCons: N/A.\nSoftware package: hdi\n\n\nBootstrap\nAs in many other complicated settings for statistical inference, the bootstrap can come to the rescue. Still, the plain vanilla bootstrap will not do. Instead, here is the general idea of the leading version of the bootstrap estimator for Lasso:\n\nRun a Lasso regression.\nKeep only ^{lasso}‚Äôs larger than some magical threshold.\nCompute the associated residuals and center them around 0.\nRepeat B times:\n\n\ndraw random samples of these centered residuals,\ncompute new responses by adding them to the predictions X‚Äô^{lasso}, and\nobtain ^{lasso} coefficients from Lasso regressions on these new responses .\n\n\nUse the distribution of the obtained coefficients to conduct statistical inference.\n\nThis idea can be generalized to other settings and, for instance, be combined with the bias-corrected estimator.\nPros: N/A.\nCons: Challenging to explain to non-technical audiences.\nSoftware package: N/A\nThis wraps up our discussion of methods for performing hypothesis testing on equation (1) (i.e., the full model). We now move on to a more challenging topic ‚Äì inference on equation (2) (i.e., the selected model).\n\n\n\nThe Novel Approach: Inference on the Selected Model\n\nPoSI (Post Selection Inference)\nThe goal of the PoSI method is to construct confidence intervals that are valid regardless of the variable selection method and the selected submodel. The benefit is that we would be reaching the correct conclusion even if we did not select the true model. This luxury comes at the expense of often being too conservative (i.e., confidence intervals are ‚Äútoo wide‚Äù). Let me explain how this is done.\nTo take a step back, most confidence intervals in statistics take the form:\n\\[\\hat{\\beta} \\pm m \\times \\hat{SE}(\\hat{\\beta}).\\]\nEvery data scientist has seen a similar formula before. The question is usually one about choosing the constant m. When we work with two-sided hypotheses tests and large samples, we often use \\(m = 1.96\\) because this is roughly the 97.5th percentile of the \\(t\\)-distribution with many degrees of freedom. This gives a 2.5% false positive error on both tails of the distribution (5% in total) and hence the associated 95% confidence intervals. The larger m, the wider or more conservative the confidence interval.\nThere are a few ways to choose the constant m in the PoSI world. Vaguely speaking, PoSI says we should select this constant to equal the \\(97.5\\)th percentile of a distribution related to the largest t-statistic among all possible models. This is usually approximated with Monte Carlo simulations. Interestingly, we do not need the response variable to approximate the value.\n\\[m = \\max_{\\text{ \\{all models and vars\\} }} |t|.\\]\nAnother and even more conservative choice for m is the Scheffe constant.\n\\[m^{scheffe} = \\sqrt{rank(X) \\times F(rank(X),  n-p)},\\]\nwhere \\(F(\\dot)\\) denotes the 95th percentile of the \\(F\\) distribution with the respective degrees of freedom.\nUnfortunately, you guessed correctly that this method does not scale well. In some sense, it is a ‚Äúbrute force‚Äù method that scans through all possible model combinations and all variables within each model and picks the most conservative value. The authors recommend this procedure for datasets with roughly \\(p&lt;20\\). This rules out many practical applications where machine learning is most useful.\nPros: Valid regardless of whether or not we have selected the true model.\nCons: Does not work in high dimensions. Computationally very slow. Too conservative.\nSoftware package: PoSI\n\n\nEPoSI (Exact PoSI)\nOk, the name of this approach is not super original. The ‚ÄúE‚Äù here stands for ‚Äúexact.‚Äù Unlike its cousin, this approach is valid only in the selected submodel. Because we cover fewer scenarios, the intervals will generally be narrower than the PoSI ones. EPoSI produces valid finite sample (as opposed to asymptotic) confidence intervals and p-values. Like all methods described here, the math behind this is extremely technical. So, I will give you only a high-level description of how this works.\nThe idea is first to get the conditional distribution of given the selected model. A bit magically, it turns out it is a truncated normal distribution. Really, who would have guessed this? Do you even remember truncated probability distributions (hint: they are just like regular PDFs but bounded from at least one side. This requires further scaling so that the density area sums to 1.)?\nTo dig one layer deeper, the authors show that the selection of Lasso predictors can be recast as a ‚Äúpolyhedral region‚Äù of the form:\n\\[ AY\\leq b. \\]\nIn English, for fixed $X $and \\(\\lambda\\), the set of alternative outcome values \\(Y^*\\) which yields the same set of selected predictors, can be expressed by the simple inequality above. In it, \\(A\\) and \\(b\\) depend do not depend on \\(Y\\). Under this new result, the distribution of \\(\\hat{\\beta}_M^{\\text{EPoSI}}\\) is now well-understood and tractable, thus enabling valid hypothesis testing.\nThen, we can use the conditional distribution function to construct a whimsical test statistic that is uniformly distributed on the \\([0,1]\\) interval. And we can finally build confidence intervals based on that statistic.\nSelective inference is currently among the hottest topic in all of statistics. There have been a myriad of extensions and improvements on the original paper. Still, this literature is painstakingly technical. What is the polyhedral selection property or a union of polytopes?\nPros: Theoretically rigorous. It‚Äôs a cool kids‚Äô thing. Tons of theoretical work extends the idea to other settings.\nCons: Extremely technical.\nSoftware package: selectiveInference"
  },
  {
    "objectID": "blog/hypo-testing-linear-ml.html#an-example",
    "href": "blog/hypo-testing-linear-ml.html#an-example",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Refer to the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\nUnlike in Monte Carlo simulations, I do not know the ground truth here, so this exercise is not informative about which approaches work well and which do not. Rather, it just serves as an illustrative example.\nHere is a table displaying the number of statistically significant variables with \\(p &lt; .05\\) for various inference methods.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Number of Statistically Significant Predictors (p &lt; .05)\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n# vars p &lt; .05\n7\n5\n2\n3\n2\n2\n\n\n\nAs expected, the naive method results in the smallest \\(p\\)-values and hence the highest number of significant predictors ‚Äì seven. Data Split knocks down two of those seven variables, resulting in five significant ones. The rest are more conservative, leaving only two or three features with \\(p &lt; .05\\).\nBelow is the table with \\(p\\)-values for all variables and each method.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: p-values\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.01\n1.00\n\n\nage\n0.00\n0.04\n1.00\n0.01\n1.00\n1.00\n\n\nsibsp\n0.02\n0.03\n1.00\n0.21\n1.00\n1.00\n\n\nparch\n0.32\n0.64\n1.00\n1.00\n1.00\n1.00\n\n\nfare\n0.20\n0.21\n1.00\n1.00\n1.00\n1.00\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n\n\nembarkedS\n0.00\n0.01\n1.00\n0.06\n-\n1.00\n\n\ncabinA\n0.39\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinB\n0.35\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinD\n0.05\n0.27\n1.00\n0.44\n1.00\n-\n\n\ncabinE\n0.00\n0.64\n1.00\n0.06\n1.00\n1.00\n\n\ncabinF\n0.02\n0.52\n1.00\n0.21\n1.00\n1.00\n\n\nembarkedC\n-\n-\n1.00\n0.11\n1.00\n1.00\n\n\nembarkedQ\n-\n-\n1.00\n1.00\n1.00\n0.00\n\n\ncabinC\n-\n-\n1.00\n1.00\n1.00\n-\n\n\n\nYou can find the code for this exercise in this GitHub repo."
  },
  {
    "objectID": "blog/hypo-testing-linear-ml.html#bottom-line",
    "href": "blog/hypo-testing-linear-ml.html#bottom-line",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods mine through datasets to find strong correlations between the response and the features. Quantifying this strength is an open and challenging problem.\nThe naive approach to hypothesis testing is usually invalid.\nThere are two main approaches that work ‚Äì inference on the full model or on the selected model. The latter poses more technical challenges than the former.\nIf we are interested in the full model, the Multi Split approach is general enough to cover a wide range of models and settings.\nIf we believe you have to focus on the selected model, EPoSI is the state-of-the-art.\nSimulation exercises usually show no clear winner, as none of the methods consistently outperforms the rest."
  },
  {
    "objectID": "blog/hypo-testing-linear-ml.html#where-to-learn-more",
    "href": "blog/hypo-testing-linear-ml.html#where-to-learn-more",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nTaylor and Tibshirani (2015) give a non-technical introduction to the problem space along with a description of the POSI method ‚Äì a great read but focused on a single approach. Other studies both provide a relatively accessible overview of the various methods for statistical inference on the full model. For technical readers, Zhang et al.¬†(2022) provide an excellent up-to-date review of the literature, which I used extensively."
  },
  {
    "objectID": "blog/hypo-testing-linear-ml.html#references",
    "href": "blog/hypo-testing-linear-ml.html#references",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "References",
    "text": "References\nBerk, R., Brown, L., Buja, A., Zhang, K., & Zhao, L. (2013). Valid post-selection inference. The Annals of Statistics, 802-837.\nB√ºhlmann, P. (2013). Statistical significance in high-dimensional linear models. Bernoulli, 19(4), 1212-1242.\nDezeure, R., B√ºhlmann, P., Meier, L., & Meinshausen, N. (2015). High-dimensional inference: confidence intervals, p-values and R-software hdi. Statistical Science, 533-558.\nJavanmard, A., & Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15(1), 2869-2909.\nLee, J. D., Sun, D. L., Sun, Y., & Taylor, J. E. (2016). Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3), 907-927.\nLeeb, H., & P√∂tscher, B. M. (2005). Model selection and inference: Facts and fiction. Econometric Theory, 21(1), 21-59.\nLeeb, H., P√∂tscher, B. M., & Ewald, K. (2015). On various confidence intervals post-model-selection. Statistical Science, 30(2), 216-227.\nMeinshausen, N., Meier, L., & B√ºhlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\nTaylor, J., & Tibshirani, R. J. (2015). Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112(25), 7629-7634.\nVan de Geer, S., B√ºhlmann, P., Ritov, Y. A., & Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. The Annals of Statistics, 42(3), 1166-1202.\nWasserman, L., & Roeder, K. (2009). High dimensional variable selection. Annals of Statistics, 37(5A), 2178.\nZhang, D., Khalili, A., & Asgharian, M. (2022). Post-model-selection inference in linear regression models: An integrated review. Statistics Surveys, 16, 86-136.\nZhang, C. H., & Zhang, S. S. (2014). Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 217-242.\nZhao, S., Witten, D., & Shojaie, A. (2021). In defense of the indefensible: A very naive approach to high-dimensional inference. Statistical Science, 36(4), 562-577."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html",
    "href": "blog/overlapping-conf-intervals.html",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "",
    "text": "This is a mistake I‚Äôve made myself‚Äîmore times than I‚Äôd like to admit. Even seasoned professors and expert data scientists sometimes fall into the same trap.\nIt typically begins with a bar graph showing two sample means side by side, each accompanied by error bars representing 95% confidence intervals. The side-by-side placement suggests a comparison is imminent. Naturally, we check whether the confidence intervals overlap. If they don‚Äôt, we may quickly (and incorrectly) conclude that the difference between the means is statistically significant‚Äîand therefore meaningful.\nThis intuitive but flawed approach to evaluating significance is surprisingly common. Here‚Äôs why it doesn‚Äôt hold up."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#background",
    "href": "blog/overlapping-conf-intervals.html#background",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "",
    "text": "This is a mistake I‚Äôve made myself‚Äîmore times than I‚Äôd like to admit. Even seasoned professors and expert data scientists sometimes fall into the same trap.\nIt typically begins with a bar graph showing two sample means side by side, each accompanied by error bars representing 95% confidence intervals. The side-by-side placement suggests a comparison is imminent. Naturally, we check whether the confidence intervals overlap. If they don‚Äôt, we may quickly (and incorrectly) conclude that the difference between the means is statistically significant‚Äîand therefore meaningful.\nThis intuitive but flawed approach to evaluating significance is surprisingly common. Here‚Äôs why it doesn‚Äôt hold up."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#diving-deeper",
    "href": "blog/overlapping-conf-intervals.html#diving-deeper",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nThe Basics of Confidence Intervals\nLet‚Äôs use a simplified example adapted from Schenker and Gentleman (2001). Suppose we are comparing two quantities‚Äî\\(Y_1\\) and \\(Y_2\\)‚Äîsuch as average user engagement on Android vs.¬†iOS or sales in two different regions. We assume ideal conditions: large, random samples; well-behaved distributions; and reliable estimators.\nWe‚Äôre testing the null hypothesis:\n\\[H_0: Y_1 = Y_2.\\]\nWe denote our sample estimates as \\(\\hat{Y}_1\\) and \\(\\hat{Y}_2\\), with corresponding standard errors \\(\\hat{SE}(Y_1)\\) and \\(\\hat{SE}(Y_2)\\). The \\(95\\%\\) confidence intervals for these estimates are:\n\\[ \\hat{Y_1} \\pm 1.96 \\times \\hat{SE}(Y_1) \\]\nand \\[ \\hat{Y_2} \\pm 1.96 \\times \\hat{SE}(Y_2). \\]\nCrucially, we can also construct a confidence interval for the difference:\n\\[ (\\hat{Y_1} - \\hat{Y_2}) \\pm 1.96 \\times \\sqrt{ \\hat{SE}(Y_1)^2+ \\hat{SE}(Y_2)^2}. \\]\nThis is the interval we should be analyzing when testing whether \\(Y_1\\) and \\(Y_2\\) differ significantly.\n\n\nTwo Approaches, One Mistake\nThe Na√Øve Approach:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nLook at whether the confidence intervals for \\(Y_1\\) and \\(Y_2\\) overlap.\nReject \\(H_0\\) if they do not overlap; otherwise, do not reject.\n\n\n\nThe Correct Approach:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nCompute the confidence interval for the difference \\(Y_1 - Y_2\\).\nReject \\(H_0\\) if this interval does not contain 0; otherwise, do not reject.\n\n\n\n\n\nWhy the Na√Øve Method Fails\nTo understand the error, consider the following: under the na√Øve method, we‚Äôre implicitly relying on the interval:\n\\[ (\\hat{Y_1} - \\hat{Y_2}) \\pm 1.96 \\times (\\hat{SE}(Y_1) + \\hat{SE}(Y_2)). \\]\nCompare this to the statistically correct confidence interval for the difference:\n\\[ \\frac{\\hat{SE}(Y_1)+ \\hat{SE}(Y_2)}{\\sqrt{\\hat{SE}(Y_1)^2 + \\hat{SE}(Y_2)^2}} \\]\nThe ratio of the widths of these intervals is:\n\\[ \\frac{\\hat{SE}(Y_1)+ \\hat{SE}(Y_2)}{\\sqrt{\\hat{SE}(Y_1)^2 + \\hat{SE}(Y_2)^2}} \\]\nThis ratio is always greater than 1, meaning the na√Øve method uses a wider interval. It is more conservative when the null hypothesis is true (i.e., less likely to reject it), and less conservative when the null is false (i.e., more prone to false positives).\nThe discrepancy is largest when the standard errors are similar, and smallest when one standard error dominates."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#bottom-line",
    "href": "blog/overlapping-conf-intervals.html#bottom-line",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nVisual overlap of confidence intervals is an intuitive‚Äîbut unreliable‚Äîmethod for assessing statistical significance.\nThis rule of thumb often misleads, particularly when standard errors are similar.\nAlways test for significance by examining the confidence interval for the difference between two estimates."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#where-to-learn-more",
    "href": "blog/overlapping-conf-intervals.html#where-to-learn-more",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper exploration of this topic, including simulation results and discussion of error rates, see the two papers cited below."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#references",
    "href": "blog/overlapping-conf-intervals.html#references",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "References",
    "text": "References\nCole, S. R., & Blair, R. C. (1999). Overlapping confidence intervals. Journal of the American Academy of Dermatology, 41(6), 1051-1052.\nSchenker, N., & Gentleman, J. F. (2001). On judging the significance of differences by examining the overlap between confidence intervals. The American Statistician, 55(3), 182-186."
  },
  {
    "objectID": "blog/chatterjee-correlation.html",
    "href": "blog/chatterjee-correlation.html",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "",
    "text": "Much of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford published a paper outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\nIn this article I will go over the basics of Chatterjee‚Äôs correlation measure. Before we get there, let‚Äôs first review some of the more traditional approaches in assessing bivariate relationship strength.\nFor simplicity, let‚Äôs assume away ties. Let‚Äôs also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant."
  },
  {
    "objectID": "blog/chatterjee-correlation.html#background",
    "href": "blog/chatterjee-correlation.html#background",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "",
    "text": "Much of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford published a paper outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\nIn this article I will go over the basics of Chatterjee‚Äôs correlation measure. Before we get there, let‚Äôs first review some of the more traditional approaches in assessing bivariate relationship strength.\nFor simplicity, let‚Äôs assume away ties. Let‚Äôs also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant."
  },
  {
    "objectID": "blog/chatterjee-correlation.html#linear-relationships",
    "href": "blog/chatterjee-correlation.html#linear-relationships",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Linear Relationships",
    "text": "Linear Relationships\nWhen we simply say ‚Äúcorrelation‚Äù we refer to the so-called Pearson correlation coefficient. Virtually everyone working with data is familiar with it. Given a random sample \\((X_1,Y_1),\\dots,(X_n, Y_n)\\) of two random variables \\(X\\) and \\(Y\\) it is computed by:\n\\[corr^{P}(X,Y) = \\frac{ \\sum_i(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}},\\]\nwhere an upper bar denotes a sample mean.\nThis coefficient lives in the \\([-1,1]\\) interval. Larger values indicate stronger relationship between \\(X\\) and \\(Y\\), be it positive or negative. At the extreme, the Pearson coefficient will equal \\(1\\) when all observations can be perfectly lined up on a upward sloping line. Yes, you guessed it ‚Äì when it equals \\(-1\\) the line is sloping down.\nYou can easily calculate the Pearson correlation in R:\ncor(x,y, method = ‚Äòpearson‚Äô) cor.test(x,y, method = ‚Äòpearson‚Äô, alternative=‚Äòtwo.sided‚Äô).\nThis measure, while widely popular, suffers from a few shortcomings. First, outliers have an outsized impact in skewing its value. Sample means vulnerable to outliers are a key ingredient in the calculation, rendering the measure sensitive to data anomalies. Second, it is designed to detect only linear relationships. Two variables might have a strong but non-linear relationship which this measure will not detect. Lastly, it is not transformation-invariant, meaning that applying a monotone transformation to either of the variables will change the correlation value.\nLet‚Äôs discuss some improvements to the Pearson correlation measure. Enter Spearman correlation."
  },
  {
    "objectID": "blog/chatterjee-correlation.html#monotone-relationships",
    "href": "blog/chatterjee-correlation.html#monotone-relationships",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Monotone Relationships",
    "text": "Monotone Relationships\nSpearman correlation is the Pearson correlation among the ranks of \\(X\\) and \\(Y\\):\n\\[corr^{S}(X,Y) = corr^{P}(R(X),R(Y)),\\]\nwhere \\(R(\\cdot)\\) denotes an observation‚Äôs rank (or order) in the sample.\nSpearman correlation is thus a rank correlation measure. As such, it quantifies how well the relationship between \\(X\\) and \\(Y\\) can be described using a monotone (and not necessarily a linear) function. It is therefore a more flexible measure of association. Again, intuitively, Spearman correlation will take on a large positive value when the \\(X\\) and \\(Y\\) observations have similar ranks. This value will be negative when the ranks tend to go in opposite directions.\nSpearman correlation addresses some of the shortcomings associated with Pearson correlation. It is not easily influenced by outliers, and it does not change if we apply a monotone transformation of \\(X\\) and/or \\(Y\\). These benefits come at the expense of a loss in interpretation and potential issues when tied ranks are common, a scenario I ignore here.\nCalculating it in R is just as simple:\ncor(x,y, method = ‚Äòspearman‚Äô) cor.test(x,y, method = ‚Äòspearman‚Äô, alternative=‚Äòtwo.sided‚Äô).\nSpearman correlation is not the only rank correlation coefficient out there. –ê popular alternative is the Kendall rank coefficient which is computed slightly differently. Let‚Äôs define a pair of observations \\((X_i, Y_i)\\) and \\((X_j, Y_j)\\) to be agreeing (the technical term is concordant) if the differences \\((X_i - X_j)\\) and \\((Y_i - Y_j)\\) have the same sign (i.e., either both \\(X_i &gt; X_j\\) and $Y_i &gt; Y_j $or both \\(X_i &lt; X_j\\) and \\(Y_i &lt; Y_j\\) ).\nThen, Kendall‚Äôs coefficient is expressed as:\n\\[corr^{K} = \\frac{\\text{number of agreeing pairs} - \\text{number of disagreeing pairs}} {\\text{total number of pairs}}.\\]\nSo, it quantifies the degree of agreement between the ranks of \\(X\\) and \\(Y\\). Like the other coeffients described above, its range is \\([-1, 1]\\) and values away from zero indicate stronger relationship.\nAgain, this coefficient is similarly computed in R:\ncor(x,y, method = ‚Äòkendall‚Äô) cor.test(x,y, method = ‚Äòkendall‚Äô, alternative=‚Äòtwo.sided‚Äô).\nKendall‚Äôs measure improves on some of the shortcomings baked in the Spearman‚Äôs coefficients ‚Äì it has a clearer interpretation and it is less sensitive to rank ties.\nHowever, none of these rank correlation coefficients can detect non-monotonic relationships. For instance, \\(X\\) and \\(Y\\) can have a parabola- or wave-like pattern when plotted against each other. We would like a correlation measure flexible enough to capture such non-linear relationships.\nThis is where Chatterjee‚Äôs coefficient comes in."
  },
  {
    "objectID": "blog/chatterjee-correlation.html#more-general-relationships",
    "href": "blog/chatterjee-correlation.html#more-general-relationships",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "More General Relationships",
    "text": "More General Relationships\nChatterjee recently proposed a new correlation coefficient designed to detect non-monotonic relationships. He discovered a novel estimator of a population quantity first proposed by Dette et al.¬†(2013).\nLet‚Äôs start with the formula. The Chatterjee correlation coefficient is calculated as follows:\n\\[corr^{C}(X,Y) = 1-\\frac{3\\sum_{i=1}^{n-1}|R(Y_{k:R(X_k)=i+1}) - R(Y_{k:R(X_k)=i})|}{n^2-1},\\]\nwhere n is the sample size. This looks complicated, so let‚Äôs try to simplify the numerator. Let‚Äôs sort the data in an ascending order of \\(X\\) so that we have \\((X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})\\), where \\(X_{(1)}&lt;X_{(2)}&lt;\\dots &lt;X_{(n)}\\). Also, denote \\(R(Y_i)\\) be the rank of \\(Y_{(i)}\\). Then:\n\\[corr^{C}(X,Y) = 1 - \\frac{3\\sum_{i=1}^{n-1}|R(Y_{i+1})-R(Y_i)|}{n^2-1}.\\]\nSo, this new coefficient is a scaled version of the sum of the absolute differences in the consecutive ranks of \\(Y\\) when ordered by \\(X\\). It is perhaps best to think about Chatterjee‚Äôs method as a measure of dependence and not strictly a correlation coefficient.\nThere are some major difference comapred to the previous correlation measures. Chatterjee‚Äôs correlation coefficient lies in the \\([0,1]\\) interval. It is equal to zero if and only if \\(X\\) and \\(Y\\) are independent and to one if one of them is a function of the other. Unlike the coefficients descibed above, it is not symmetric in \\(X\\) and \\(Y\\), meaning \\(corr^{C}(X,Y) \\neq corr^{C}(Y,X)\\). This is understandable since we are interested in whether \\(X\\) is a function of \\(Y\\), which does not imply the opposite. The author also develops asymptotic theory for calculating p-values although some researchers have raised concerns about the coefficient‚Äôs power.\nHere is a sample R code to calculate its value:\nn &lt;- 1000 x &lt;- runif(n) y &lt;- 5 * sin(x) + rnorm(n)\ndata &lt;- data.frame(x=x, y=y) data\\(R &lt;- rank(data\\)y) data &lt;- data[order(data$x), ]\n1 - 3 * sum(abs(diff(data$R))) / (n^2-1)\nAlternatively, you can used the XICOR R package.\nThere you have it. You are now well-equipped to dive deeper into your datasets and find new exciting relationships."
  },
  {
    "objectID": "blog/chatterjee-correlation.html#bottom-line",
    "href": "blog/chatterjee-correlation.html#bottom-line",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThere are numerous ways of measuring association between two variables.\nThe most common methods measure only linear or monotonic relationships. These are often useful but do not capture more complex, non-linear associations.\nA new correlation measure, Chatterjee‚Äôs coeffient, is designed to go beyond monotonicty and assess more general bivariate relationships."
  },
  {
    "objectID": "blog/chatterjee-correlation.html#where-to-learn-more",
    "href": "blog/chatterjee-correlation.html#where-to-learn-more",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia has detailed entries on correlation, rank correlation, and Kendall‚Äôs coefficient which I found helpful. The R bloggers platform has articles exploring the Chatterjee‚Äôs correlation coefficient in detail. The more technically oriented folks will find Chatterjee‚Äôs original paper helpful."
  },
  {
    "objectID": "blog/chatterjee-correlation.html#references",
    "href": "blog/chatterjee-correlation.html#references",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "References",
    "text": "References\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\nDette, H., Siburg, K. F., & Stoimenov, P. A. (2013). A Copula‚ÄêBased Non‚Äêparametric Measure of Regression Dependence. Scandinavian Journal of Statistics, 40(1), 21-41.\nShi, H., Drton, M., & Han, F. (2022). On the power of Chatterjee‚Äôs rank correlation. Biometrika, 109(2), 317-333.\nhttps://www.r-bloggers.com/2021/12/exploring-the-xi-correlation-coefficient/"
  },
  {
    "objectID": "blog/multiple-testing-overview.html",
    "href": "blog/multiple-testing-overview.html",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "",
    "text": "The abundance of data around us is a major factor making the data science field so attractive. It enables all kinds of impactful, interesting, or fun analyses. I admit this is what got me into statistics and causal inference in the first place. However, this luxury has downsides ‚Äì it might require deeper methodological knowledge and attention.\nIf we are interested in measuring statistical significance, the standard frequentist framework relies on testing a single hypothesis on any given dataset. Once we start using the same data multiple times ‚Äì i.e., testing several hypotheses at once ‚Äì as we often do, we might have to make some additional adjustments.\nIn this article, I will walk you through the most popular multiple hypotheses (MH) testing corrections. Common examples of when these come to play include estimating the impact of several variables on a single outcome or estimating the effect of an intervention on several subgroups of users to mine for heterogeneous treatment effects. Both scenarios are ubiquitous in most data analysis projects.\nTo illustrate the problem, imagine we are testing seven hypotheses at the \\(5\\%\\) significance level. Then, the probability that we incorrectly reject at least one of these hypotheses is:\n\\[ 1 - (1-.05)^7 = .30. \\]\nThat is, roughly one in three times, we will reach a wrong conclusion.\nVaguely speaking, MH adjustments can be viewed as inflating the calculated p-values and hence decreasing the probability that we reject any given hypothesis. In other words, we are accounting for using the data multiple times by making the definition of statistical significance more stringent."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#background",
    "href": "blog/multiple-testing-overview.html#background",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "",
    "text": "The abundance of data around us is a major factor making the data science field so attractive. It enables all kinds of impactful, interesting, or fun analyses. I admit this is what got me into statistics and causal inference in the first place. However, this luxury has downsides ‚Äì it might require deeper methodological knowledge and attention.\nIf we are interested in measuring statistical significance, the standard frequentist framework relies on testing a single hypothesis on any given dataset. Once we start using the same data multiple times ‚Äì i.e., testing several hypotheses at once ‚Äì as we often do, we might have to make some additional adjustments.\nIn this article, I will walk you through the most popular multiple hypotheses (MH) testing corrections. Common examples of when these come to play include estimating the impact of several variables on a single outcome or estimating the effect of an intervention on several subgroups of users to mine for heterogeneous treatment effects. Both scenarios are ubiquitous in most data analysis projects.\nTo illustrate the problem, imagine we are testing seven hypotheses at the \\(5\\%\\) significance level. Then, the probability that we incorrectly reject at least one of these hypotheses is:\n\\[ 1 - (1-.05)^7 = .30. \\]\nThat is, roughly one in three times, we will reach a wrong conclusion.\nVaguely speaking, MH adjustments can be viewed as inflating the calculated p-values and hence decreasing the probability that we reject any given hypothesis. In other words, we are accounting for using the data multiple times by making the definition of statistical significance more stringent."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#when-are-multiple-hypothesis-adjustments-not-necessary",
    "href": "blog/multiple-testing-overview.html#when-are-multiple-hypothesis-adjustments-not-necessary",
    "title": "Multiple Testing: Methods Overview",
    "section": "When are Multiple Hypothesis Adjustments NOT Necessary",
    "text": "When are Multiple Hypothesis Adjustments NOT Necessary\nMultiple hypothesis adjustments are not required when:\n\nwe are interested in a purely predictive model, such as in machine learning settings,\nwe have an independent dataset for each correlated hypothesis,\nthe dependent variables we are testing are not correlated. An example is testing the impact of a new feature or intervention on a set of outcomes unrelated to each other ‚Äì e.g., health and education variables. In such cases, it is common to adjust for MH separately within the health- and education-related hypotheses.\n\nBefore we dive into the methods, let‚Äôs establish some basic terminology and notation."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#notation",
    "href": "blog/multiple-testing-overview.html#notation",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Notation",
    "text": "Notation\nWe are interested in testing a bunch of m null hypotheses about a population parameter \\(\\beta\\).\nAs a running example, you can think of \\(\\beta\\) as the causal impact of a new product feature on user engagement and \\(m\\) as indexing some geographical regions such as cities. We are interested in whether the new feature is more impactful in some cities than others. We will denote these hypotheses with \\(H_1, H_2, \\dots, H_m\\) and refer to their associated p-values with \\(p_1, p_2, \\dots, p_m\\).\nWe use \\(\\alpha\\) to denote the probability of a Type 1 error ‚Äì rejecting a true null (i.e., a false positive). In technical jargon, we refer to \\(\\alpha\\) as test size. We often choose \\(\\alpha=.05\\), meaning that we are allowing a 5% chance that we will make such an error. This corresponds to the 95% confidence intervals that we often see.\nNext, statistical power is the probability of correctly rejecting a false null hypothesis (i.e., a true positive). This is a desirable property ‚Äì the higher it is, the better. While this terminology does not describe the entire hypothesis testing framework, it does cover what is necessary to continue reading the article."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#two-types-of-multiple-hypotheses-adjustments",
    "href": "blog/multiple-testing-overview.html#two-types-of-multiple-hypotheses-adjustments",
    "title": "Multiple Testing: Methods Overview",
    "section": "Two Types of Multiple Hypotheses Adjustments",
    "text": "Two Types of Multiple Hypotheses Adjustments\nIn the MH world, there are two distinct types of targets that data scientists are after.\nThe first one was introduced in the 1950s by the famous statistician John Tukey and is called Familywise Error Rate (FWER). FWER is the probability of making at least one type 1 error:\n\\[ FWER = \\mathbb{P} (\\text{At Least 1 Type 1 Error}). \\]\nIn the example from the introduction, \\(FWER = 0.3\\).\nControlling the FWER is good, but sometimes it is not great. It does what it is supposed to do ‚Äì limit the number of false positives, but it is often too conservative. In practice, too few variables remain significant after such FWER adjustments, especially when testing many (as opposed to just a few) hypotheses.\nTo correct this, in the 1990s, Yoav Benjamini and Yosef Hochberg, conceptualized controlling the so-called False Discovery Rate (FDR). FDR is the expected proportion of false positives:\n\\[ FDR = \\mathbb{E} \\left[ \\frac{\\text{\\# False Positives}}{\\text{\\# False Positives + \\# True Positives} } \\right]. \\]\nTake a minute to notice the significant difference between these two approaches. FWER controls the probability of having at least one false positive at \\(\\alpha\\). At the same time, FDR is okay with having several false positives as long as they are (on average) less than \\(\\alpha\\) of all hypotheses.\nFWER is more conservative than FDR, resulting in fewer significant variables. In our example, applying an FWER adjustment as opposed to an FDR one will lead to fewer statistically significant differences in the impact of the new feature across various cities.\nLet‚Äôs go over the most popular ways to control the FWER."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#the-classic-approach-fwer-control",
    "href": "blog/multiple-testing-overview.html#the-classic-approach-fwer-control",
    "title": "Multiple Testing: Methods Overview",
    "section": "The Classic Approach: FWER Control",
    "text": "The Classic Approach: FWER Control\n\nBonferroni‚Äôs Procedure\nYou might have heard of this one. The procedure is extremely simple ‚Äì it goes like this:\n\nreject \\(H_i\\) if \\(p_i \\times m  \\leq \\alpha\\) for all \\(i\\).\n\nIn words, we multiply all p-values by m and reject the hypotheses with adjusted p-values that are smaller than or equal to \\(\\alpha\\).\nThe Bonferroni adjustment is often considered to control the FWER, but it actually controls an even more conservative target ‚Äì the per-family error rate (PFER). PFER is the sum of probabilities of Type 1 error for all the hypotheses.\nYou correctly guessed that PFER is considered too strict. Although it is common in practice, there are better alternatives. While the Bonferroni correction is simple and elegant, it is always dominated by the Holm-Bonferroni procedure, so there is really no reason ever to use it.\nPros: Simple.\nCons: Too conservative.\nSoftware Package: stats\n\n\nHolm & Bonferroni‚Äôs Procedure\nThis adjustment offers greater statistical power than the Bonferroni method regardless of the test size without significantly compromising on simplicity. In algorithmic language:\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m.\\)\nReject the null hypothesis \\(H_1,\\dots H_{k-1}\\), where \\(k\\) is the smallest index for which $ p_i (m+1-k) \\(.\nMuch like the Bonferroni method with a multiplication factor of (\\)m+1-k$) instead of \\(m\\).\n\nPros: Simple, more powerful than Bonferroni.\nCons: Still too conservative.\nSoftware Package: stats\nThese two procedures control the FWER by assuming a kind of a ‚Äòworst-case‚Äô scenario in which the \\(p\\)-values are close to being independent of each other. To see why this is the case, imagine the extreme opposite scenario in which there is perfect positive dependence. Then, there is effectively one test statistic, and FWER adjustment is unnecessary.\nIn practice, however, the \\(p\\)-values are often correlated, and exploiting this dependence can result in even more powerful FWER corrections. This can be achieved with various resampling methods, such as the bootstrap or permutation procedures.\n\n\nWestfall & Young‚Äòs Procedure\nThe Westfall & Young method, developed in the 1990s, was the first resampling procedure controlling the FWER. It uses the bootstrap to account for the dependence structure among the \\(p\\)-values and improves on Holm-Bonferroni‚Äôs method.\nIt goes roughly like this:\n\nOrder the \\(m\\) \\(p\\)-values in ascending order ‚Äì \\(p_1, \\dots, p_M\\).\nBegin with a \\(COUNTER_i = 0\\) for each hypothesis \\(i\\).\nTake a bootstrap sample and compute the m p-values (\\(p^*_1, \\dots, p^*_m\\))\nDefine the successive minima by starting with the largest p-value (\\(\\tilde{p}_M=p^*_M\\)) and for each subsequent hypothesis \\(i\\) taking the minimum between \\(\\tilde{p}_{i+1}\\) and \\(p^*_i\\).\n\n\nThis gives a list of \\(p\\)-values \\(\\tilde{p}_1, \\dots, \\tilde{p}_m\\).\n\n\nIf \\(\\tilde{p}_i \\leq p_i\\) add 1 unit to \\(COUNTER_i\\) for each \\(i\\).\nRepeat steps 3-5 above \\(B\\) times and compute the fraction \\(\\hat{p}_i = COUNT_i/B\\).\n\n\nThis gives a list of \\(p\\)-values \\(\\hat{p}_1, \\dots, \\hat{p}_m\\).\n\n\nEnforce monotonicity by starting with \\(\\hat{p}_1\\) and for each subsequent hypothesis \\(i\\) taking the maximum between \\(\\hat{p}_{i-1}\\) and \\(\\hat{p}_i\\).\n\n\nThis final vector presents the FWER-adjusted \\(p\\)-values.\n\n\nReject all null hypotheses \\(i\\) for which \\(\\hat{p}\\leq \\alpha\\).\n\nYes, this is a complicated procedure. But it does offer substantial improvements in power, especially if the hypotheses are positively correlated.\nImportantly, Westfall and Young‚Äôs method rests on a critical assumption called ‚Äúsubset pivotality.‚Äù Roughly speaking, this condition requires that the joint distribution of any subset of test statistics does not depend on whether the remaining hypotheses are true or not.\nPros: Can be more powerful than Bonferroni-Holm.\nCons: Rests on an uncomfortable ‚Äúsubset pivotality‚Äù assumption. Difficult to explain to non-technical audiences.\nSoftware Package: multtest\n\n\nRomano & Wolf‚Äòs Procedure(s)\nThis procedure improves on Westfall and Young‚Äôs method by not requiring a pivotality assumption. Here I am describing the method from Romano and Wolf‚Äôs 2005 Econometrica paper, although the authors have developed a few closely related FWER adjustments. This is the rough algorithm for it:\n\nOrder the test statistics in descending order, \\(t_1 \\geq t_2 \\geq, \\dots, t_m\\).\nTake B bootstrap samples, and for each hypothesis i compute the empirical distribution function of the successive maximum test statistic random variable.\n\n\nThat is, the random variable of the maximum test statistic between \\(t_i, t_{i+1}, \\dots, t_m\\).\nCall this distribution \\(c(i)\\).\n\n\nReject \\(H_i\\) if \\(t_i\\) is greater than the 1-quantile of \\(c(1)\\).\nDenote by h the number of rejected hypotheses. If \\(h=0\\) stop. Otherwise, for \\(H_{h+1},\\dots H_s\\):\nReject \\(H_i\\) if \\(t_i &gt; c(h+1)\\)\nIterate until no further hypotheses are rejected.\n\nAnd you thought the Westfall and Young correction was tricky. The good news is that there are software packages for all these adjustments, so you won‚Äôt have to program them yourself.\nPros: Arguably, one of the best ways to control FWER; does not rest on pivotality assumptions.\nCons: Difficult to explain to non-technical audiences; difficult to explain to anyone, really.\nSoftware Package: hdm\nThis wraps up the descriptions of the methods controlling the FWER."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#the-modern-approach-fdr-control",
    "href": "blog/multiple-testing-overview.html#the-modern-approach-fdr-control",
    "title": "Multiple Testing: Methods Overview",
    "section": "The Modern Approach: FDR Control",
    "text": "The Modern Approach: FDR Control\nAs a reminder, FDR procedures have greater power at the cost of increased rates of type 1 errors compared to FWER adjustments. This is the dominant framework in the modern literature on MH adjustment.\n\nBenjamini & Hochberg‚Äòs Procedure\nThe Benjamini-Hochberg (BH) method is simple:\n\nOrder the p-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m}{k} \\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\nBH is valid when the tests are independent and in some, but not all, dependence scenarios. In general, FDR control is tricky with arbitrary dependence.\nThis adjustment has an interesting geometric interpretation:\n\nplot \\(p\\) versus \\(k\\),\ndraw a line through the origin with a slope equal to \\(frac{\\alpha}{m}\\),\nreject all hypotheses associated with the p-values on the left up to the last point below this line.\n\nHow cool!\nPros: Simple.\nCons: Does not work under some dependence structures, hence it does not cover all cases.\nSoftware Package: stats\n\n\nBenjamini & Yekutieli‚Äòs Procedure\nBenjamini and Yekutieli (BY) developed a refinement of the BH procedure, which adds a magical constant \\(c(m)\\):\n\nOrder the p-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m.c(m)}{k}\\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\nThe obvious question is in choosing the optimal value for \\(c(m)\\). You can view BH as a special case of BY with \\(c(m)=1\\). The authors show that the following choice works under any arbitrary dependence assumption:\n\\[ c(m) = \\sum_{i=1}^m \\frac{1}{i} \\roughly log(m) + 0.57721 + \\frac{1}{2m}. \\]\nFascinating. For most applications, this simple procedure provides a credible way of taming the false positives rate. There are improvements to this method, but it feels like we are entering a world in which we get theoretical gains with only limited practical implications.\nPros: Valid under a wider range of scenarios than BH.\nCons: Can gain power by exploiting hypotheses dependence via resampling methods.\nSoftware Package: stats\nThe main way to improve on BH/BY is to know (or estimate) the share of false hypotheses among the ones we are testing. This, however, goes beyond the scope of the article.\n\n\nKnockoffs\nWhat if we would like to combine variable selection and control the FDR? BH and BY do not really tell us how to do that. Enter knockoffs. The knockoffs approach is the kid on the block, and it does more than just FDR control.\nThe idea behind the knockoffs is novel and unrelated to any of the methods I discussed above. Knockoffs are copies of the variables with specific properties. In particular, for each feature/covariate variable X, we create a knockoff copy \\(\\tilde{X}\\) such that the correlation structure between any two knockoff variables is the same as between their original counterparts. These knockoffs are constructed without looking at the outcome variable; hence, they are unrelated to it by design. Consequently, we can gauge whether each variable is statistically significant by comparing its test statistic to that of the one for its knockoff copy.\nKnockoffs are among the most active fields of research in statistics, with new papers coming out daily (ok, maybe weekly). These methods come in two flavors.\nThe fixed-design knockoff filter controls FDR for low-dimensional linear models regardless of the dependency structure of the features. This is great, but it does not work in high dimensions where variable selection is usually most needed.\nThe model-X knockoff filter solves this issue but requires exact knowledge of the joint distribution of the features. This is often infeasible in practice; even estimating that distribution in high dimensions can be really challenging as the curse of dimensionality lays its hammer on the data scientist.\nThe knockoff idea presents a big step forward because it solves two statistical problems at the same time ‚Äì selecting among a wide set of predictors and controlling the FDR. I am really excited to see where the current research will take us next.\nPros: Works for model selection and FDR control.\nCons: The baseline approach relies on knowing the joint distribution of the features.\nSoftware Package: knockoff\n\n\nOther Modern Methods\nThere is certainly no shortage of statistical methods in this field. Each week, there is at least one new paper dealing with some aspects of FDR or FWER control. Knockoffs, in particular, have gotten a lot of attention recently, with researchers extending the idea to all kinds of settings and models.\nI will briefly mention some of the newer methods that have caught my eye in recent months. Many of these control the FDR asymptotically, and some are valid under general dependency structures among the test statistics.\nOne recent variation of the knockoffs idea uses Gaussian Mirrors; while others rely on subsampling approaches ‚Äì Data Splitting (based on estimating two independent regression coefficients after splitting the dataset in half) and Data Aggregation. Clever ideas to improve power include using covariate information to optimally weigh hypotheses in the BH framework or incorporating them into FDR regressions.\nTime will show whether any of these newcomers will come to dominate in practice."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#an-example",
    "href": "blog/multiple-testing-overview.html#an-example",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Check the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\n\n\n\nNumber of Statistically Significant Variables\n\n\nHere is a table of the \\(p\\)-values for each feature and various MH adjustments.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaw\nBonferroni\nHolm\nRom.-Wolf\nBenj.-Hoch.\nBenj.-Yek.\n\n\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nage\n0.00\n0.02\n0.01\n0.01\n0.00\n0.01\n\n\nsibsp\n0.02\n0.34\n0.18\n0.16\n0.04\n0.14\n\n\nparch\n0.32\n1.00\n1.00\n0.77\n0.40\n1.00\n\n\nfare\n0.22\n1.00\n1.00\n0.66\n0.30\n0.98\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nembarkedC\n0.01\n0.23\n0.15\n0.14\n0.04\n0.13\n\n\nembarkedQ\n0.05\n0.76\n0.35\n0.29\n0.09\n0.28\n\n\ncabinA\n0.39\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinB\n0.37\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinC\n0.92\n1.00\n1.00\n0.92\n0.92\n1.00\n\n\ncabinD\n0.06\n0.89\n0.36\n0.29\n0.09\n0.30\n\n\ncabinE\n0.01\n0.07\n0.05\n0.05\n0.01\n0.05\n\n\ncabinF\n0.02\n0.31\n0.18\n0.16\n0.04\n0.14\n\n\n\nEight variables were statistically significant without any MH adjustment. As expected, the FWER adjustments lead to fewer significant variables than the FDR ones. Interestingly, there is a noticeable difference between the two FDR methods ‚Äì BH and BY, with the latter being more conservative.\nYou can find the code for this analysis in this GitHub repository."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#bottom-line",
    "href": "blog/multiple-testing-overview.html#bottom-line",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMultiple testing corrections are likely necessary when you are using a single dataset multiple times (i.e., testing multiple hypotheses).\nTwo major frameworks exist ‚Äì FWER and FDR control. The former is often considered too conservative, while the latter is the dominant way researchers and practitioners think about correcting for MH testing.\nIn most settings, the Benjamini-Yekuiteli approach offers a great balance between statistical power and technical simplicity.\nKnockoffs are a novel and exciting approach to FDR control."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#where-to-learn-more",
    "href": "blog/multiple-testing-overview.html#where-to-learn-more",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great resource on the classic approaches to tackling MH issues (both for FDR and FWER control), but it lacks material on modern methodologies. Emmanuel Cand√®s‚Äô website features an accessible introduction to the world of knockoffs. Clarke et al.¬†(2020) strip down many technicalities and provide accessible descriptions of both Westfall and Young‚Äôs, as well as Romano and Wolf‚Äôs methods. Korthauer et al.¬†(2019) compare some of the more recent approaches to controlling the FDR, which are beyond the scope of this blog post."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#references",
    "href": "blog/multiple-testing-overview.html#references",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics, 43(5), 2055-2085.\nBenjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of Statistics, 1165-1188.\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300.\nCandes, E., Fan, Y., Janson, L., & Lv, J. (2018). Panning for gold:‚Äômodel‚ÄêX‚Äôknockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3), 551-577.\nClarke, D., Romano, J. P., & Wolf, M. (2020). The Romano‚ÄìWolf multiple-hypothesis correction in Stata. The Stata Journal, 20(4), 812-843.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2022). False discovery rate control via data splitting. Journal of the American Statistical Association, 1-38.\nKorthauer, K., Kimes, P. K., Duvallet, C., Reyes, A., Subramanian, A., Teng, M., ‚Ä¶ & Hicks, S. C. (2019). A practical guide to methods controlling false discoveries in computational biology. Genome biology, 20(1), 1-21.\nRomano, J. P., & Wolf, M. (2005). Stepwise multiple testing as formalized data snooping. Econometrica, 73(4), 1237-1282.\nRomano, J. P., & Wolf, M. (2005). Exact and approximate stepdown methods for multiple hypothesis testing. Journal of the American Statistical Association, 100(469), 94-108.\nWestfall, P. H., & Young, S. S. (1993). Resampling-based multiple testing: Examples and methods for p-value adjustment (Vol. 279). John Wiley & Sons.\nXing, X., Zhao, Z., & Liu, J. S. (2021). Controlling false discovery rate using gaussian mirrors. Journal of the American Statistical Association, 1-20."
  },
  {
    "objectID": "blog/simpson-paradox.html",
    "href": "blog/simpson-paradox.html",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Simpson‚Äôs paradox is one of the most counterintuitive phenomena in data analysis. It describes situations where a trend observed within groups disappears‚Äîor even reverses‚Äîwhen the data is aggregated. The underlying cause is often a confounding variable that distorts the overall trend. Let‚Äôs examine a concrete example."
  },
  {
    "objectID": "blog/simpson-paradox.html#background",
    "href": "blog/simpson-paradox.html#background",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Simpson‚Äôs paradox is one of the most counterintuitive phenomena in data analysis. It describes situations where a trend observed within groups disappears‚Äîor even reverses‚Äîwhen the data is aggregated. The underlying cause is often a confounding variable that distorts the overall trend. Let‚Äôs examine a concrete example."
  },
  {
    "objectID": "blog/simpson-paradox.html#an-example",
    "href": "blog/simpson-paradox.html#an-example",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nImagine two hospitals, \\(A\\) and \\(B\\), treating patients for a particular condition with two treatment options, \\(T_1\\) and \\(T_2\\). Hospital \\(A\\), located in a higher-income neighborhood, primarily receives healthier patients, while Hospital \\(B\\), in a lower-income neighborhood, tends to treat sicker patients. The effectiveness of the treatments is measured as improvement in a continuous health score.\nWe are interested in examining whether one of the treatment options leads to better health outcomes. Consider the following data gathered across both hospitals.\nHospital Treatment Health Improvement \\(N\\) \\(A\\) \\(T_1\\) \\(20\\) \\(90\\) \\(A\\) \\(T_2\\) \\(20\\) \\(10\\) \\(B\\) \\(T_1\\) \\(10\\) \\(10\\) \\(B\\) \\(T_2\\) \\(10\\) \\(90\\) Health improvement by hospital and treatment type. Both treatments \\(T_1\\) and \\(T_2\\) are equally effective within each hospital. Let‚Äôs now look at what happens when we combine the data from both hospitals.\nTreatment \\(N\\) Health Improvement \\(T_1\\) \\(100\\) \\(19 = 20 * .9 + 10 * .1\\) \\(T_2\\) \\(100\\) \\(11 = 10 * .9 + 20 * .1\\) Health improvement by treatment type. Treatment T1 is more effective overall. Within each hospital, the data shows that both treatments are equally effective. However, combining the data across both hospitals reveals that treatment T1 appears to be significantly more effective overall. Why does this happen?\nThe confounding variable here is the underlying health status of patients. Hospital A treats mostly healthier patients, while Hospital B handles more severe cases. This difference in patient distribution influences the overall success rates of the treatments, even though both treatments perform identically within each hospital."
  },
  {
    "objectID": "blog/simpson-paradox.html#visualizing-the-paradox",
    "href": "blog/simpson-paradox.html#visualizing-the-paradox",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "Visualizing the Paradox",
    "text": "Visualizing the Paradox\nTo illustrate, imagine a scatter plot where each dot represents a patient. The color of the dot indicates the treatment they received, and the horizontal lines represent the average health improvement for each group. The vertical axis depicts the outcome variable (health improvement).\n[ADD IMAGE]\nIn the aggregated data, \\(T_1\\) shows a higher average improvement, creating the illusion of greater effectiveness. But when disaggregated by hospital, the averages for \\(T_1\\) and \\(T_2\\) are identical.\nYou can find the code to reproduce the figure in this GitHub repo."
  },
  {
    "objectID": "blog/simpson-paradox.html#where-to-learn-more",
    "href": "blog/simpson-paradox.html#where-to-learn-more",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nStart with the Wikipedia entry, where you will find all necessary additional resources."
  },
  {
    "objectID": "blog/simpson-paradox.html#bottom-line",
    "href": "blog/simpson-paradox.html#bottom-line",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nSimpson‚Äôs paradox manifests when an observable pattern within groups disappears if the data is aggregated.\nIt is a reminder of the critical role confounding variables play in data analysis.\nIt underscores the importance of stratifying data by meaningful subgroups and carefully considering the context before drawing conclusions from aggregated statistics."
  },
  {
    "objectID": "blog/new-dev-fdr.html",
    "href": "blog/new-dev-fdr.html",
    "title": "New Developments in False Discovery Rate",
    "section": "",
    "text": "A while back I wrote an article summarizing various approaches to correcting for multiple hypothesis testing. The dominant framework, False Discovery Rate (FDR), controls the share of hypotheses that are incorrectly rejected at a pre-specified level . Its foundations were laid out in 1995 by Benjamini and Hochberg (BH) and to date, their method remains the most popular approach for controlling FDR. Since then, the literature has gone in a few directions.\nOne strand of research generalizes the BH procedure to accommodate cases in which there is a dependency (i.e., correlation) among the hypotheses being tested. Another group of papers makes use of covariates that carry information about whether a given hypothesis is likely to be false. While intuitive in theory, in practice this idea is of limited use as such covariates are not often available.\nFinally, a relatively new class of methods builds on the notion of ‚Äúknockoff‚Äù (or fake) variables and performs variable selection while controlling FDR. The underlying idea is based on creating a fake variable and comparing its test statistics to that of the original variable. Since the fake one is, by definition, null, a small discrepancy between the two test statistics signals the original variable does not belong in the model. The baseline model-X knockoff method requires knowledge of the joint distribution of all covariates. Recent simulations show if this distribution is unknown and misspecified (which in practice it almost always is) there is a loss of statistical power and FDR increase.\nIn this article I will discuss a few new papers which aim to build on and improve the knockoff method. Like knockoffs, they are based on ‚Äúmirror statistics‚Äù, but unlike them they do not require exact knowledge or consistent estimation of any distribution. Specifically, I will discuss Gaussian Mirrors and Data Splitting for FDR control."
  },
  {
    "objectID": "blog/new-dev-fdr.html#background",
    "href": "blog/new-dev-fdr.html#background",
    "title": "New Developments in False Discovery Rate",
    "section": "",
    "text": "A while back I wrote an article summarizing various approaches to correcting for multiple hypothesis testing. The dominant framework, False Discovery Rate (FDR), controls the share of hypotheses that are incorrectly rejected at a pre-specified level . Its foundations were laid out in 1995 by Benjamini and Hochberg (BH) and to date, their method remains the most popular approach for controlling FDR. Since then, the literature has gone in a few directions.\nOne strand of research generalizes the BH procedure to accommodate cases in which there is a dependency (i.e., correlation) among the hypotheses being tested. Another group of papers makes use of covariates that carry information about whether a given hypothesis is likely to be false. While intuitive in theory, in practice this idea is of limited use as such covariates are not often available.\nFinally, a relatively new class of methods builds on the notion of ‚Äúknockoff‚Äù (or fake) variables and performs variable selection while controlling FDR. The underlying idea is based on creating a fake variable and comparing its test statistics to that of the original variable. Since the fake one is, by definition, null, a small discrepancy between the two test statistics signals the original variable does not belong in the model. The baseline model-X knockoff method requires knowledge of the joint distribution of all covariates. Recent simulations show if this distribution is unknown and misspecified (which in practice it almost always is) there is a loss of statistical power and FDR increase.\nIn this article I will discuss a few new papers which aim to build on and improve the knockoff method. Like knockoffs, they are based on ‚Äúmirror statistics‚Äù, but unlike them they do not require exact knowledge or consistent estimation of any distribution. Specifically, I will discuss Gaussian Mirrors and Data Splitting for FDR control."
  },
  {
    "objectID": "blog/new-dev-fdr.html#notation",
    "href": "blog/new-dev-fdr.html#notation",
    "title": "New Developments in False Discovery Rate",
    "section": "Notation",
    "text": "Notation\nAlthough many of the results generalize to more complex settings, I will work with the simple linear model:\n\\[Y = X\\beta + \\epsilon.\\]\nWe have n observations of an outcome \\(Y\\), and a covariate vector \\(X\\in \\mathbb{R}^p\\) with \\(p &lt; n\\). (Again, some of these results generalize to high-dimensional settings, but let‚Äôs keep it simple here.) I will index the variables in \\(X\\) by \\(j\\). My goal is to find a subset of relevant features from $ X$ while controlling the FDR at some level \\(\\alpha\\). In other words, I wil be testing the series of \\(p\\) null hypotheses of the kind \\(\\beta_j=0\\)."
  },
  {
    "objectID": "blog/new-dev-fdr.html#diving-deeper",
    "href": "blog/new-dev-fdr.html#diving-deeper",
    "title": "New Developments in False Discovery Rate",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nFDR Control with Mirror Statistics\nThe building block of these methods are the so-called mirror statistics, \\(M_j\\). They have the following two properties:\n\nVariables with larger mirror statistics (\\(M_j\\)‚Äôs) are more likely to be relevant.\nTheir distribution under the null hypothesis is (asymptotically) symmetric around \\(0\\).\n\nThese properties are simple and intuitive. For instance, the commonly used t-statistic for hypothesis testing in the linear model satisfies both. The first property suggests we can order the features and select ones with a mirror statistic exceeding some pre-defined threshold. The second one leads to an approximate upper bound on the number of false positives for any cutoff t:\n\\[FDP(t) = \\frac{\\#\\{j: \\text{j is irrelevant, but } M_j &gt; t\\}}{\\# \\{j:M_j &gt; t\\}} \\leq \\frac{\\# \\{j:M_j &lt;- t\\}}{\\# \\{j:M_j &gt; t\\}}. \\]\nNow that we know the mirror statistics‚Äô properties, I will discuss various ways of calculating them.\n\n\nConstructing the Mirror Statistics\nThe mirror statistics M_j take the following general form:\n\\[M_j = sign(\\tilde{\\beta}_j^1, \\tilde{\\beta}_j^2) f(|\\tilde{\\beta}_j^1|, |\\tilde{\\beta}_j^2|),\\]\nwhere the \\(\\tilde{\\beta}\\) denote (standardized) estimates of the true coefficient \\(\\beta\\) and \\(f(\\cdot)\\) is a nonnegative, exchangeable and monotically increasing function. For instance, convenient choices for \\(f(\\cdot)\\) include \\(f(a,b) = 2min(a,b)\\) (Xing et al.¬†2019), \\(f(a,b) = ab\\), and \\(f(a,b) = a+b\\) (Dai et al.¬†2022).\nLet‚Äôs now turn to calculating the \\(\\tilde{\\beta}\\)‚Äôs.\n\n\nConstructing the Regression Coefficients\nThe coefficients \\(\\tilde{\\beta}\\) ought to satisfy the following two conditions:\n\nIndependence ‚Äì The two regression coefficients are (asymptotically) independent.\nSymmetry ‚Äì Under the null hypothesis, the (marginal) distribution of either of the two coefficients is (asymptotically) symmetric around zero.\n\nI will now describe two approaches in constructing them.\nApproach #1 ‚Äì Gaussian Mirrors\nSoftware Package: GM\nThe main idea is to create a set of two perturbed mirror features for each variable \\(X_j\\). Namely,\n\\[X_j^+ = X_j + a_jZ_j, \\text{      and      } X_j^-=X_j -a_jZ_j,\\]\nwhere \\(a_j\\) is a scalar and \\(Z_j \\approx N(0,1)\\). The authors provide some guidance on how to select \\(a_j\\), but I will not get into that here.\nWhile it is possible to generate the mirror features for all columns in \\(X\\) simultaneously, the one-fit-per-feature approach shows better performance in simulations. So, the \\(\\tilde{\\beta}\\) are the estimates of \\(\\beta\\) in the following model:\n\\[ y = \\frac{\\beta_j}{2}X_j^+ +\\frac{\\beta_j}{2}X_j^- + X_{\\text{non-j}}\\beta_{\\text{non-j}} + \\epsilon. \\]\nApproach #2 ‚Äì Data Splitting\nAn alternative approach for getting two independent coefficient estimates \\(\\tilde{\\beta}\\) is through data splitting. When estimating the linear model in equation (1), we can get \\(\\tilde{\\beta}^1\\) from one half of the data and$ ^2$ from the other half of the data. While this is simple and intuitive it can result in loss of statistical power. To alleviate this concern, we can do repeated data splitting and aggregate the results in the end. This is reminiscent of the procedure suggested by Meinheusen et al.¬†(2012) in the context of hypothesis testing in for high-dimensional regression. We can then determine the feature importance based on the share of data splits in which it ends up being included. I will omit the technical details here.\nThere is a technical wrinkle I have omitted ‚Äì the regression coefficients have to be standardized so that the \\(M_j\\)‚Äôs have comparable variances across variables. Check the original papers for details on exactly how to do that. Instead, I will now turn to the final algorithm for variable selection with FDR control using the approaches outlined above.\n\n\nPutting it All Together\nThis framework sets the stage for the following general algorithm for variable selection with FDR control:\nCalculate the \\(j\\) mirror statistics, \\(M_j\\). Given a FDR level , set a threshold () such that \\[\\tau(\\alpha)= min\\{t &gt; 0 : \\hat{FDP}(t) \\leq \\alpha\\}\\]\nSelect the features \\(\\left{ j : M_j &gt;  \\tau(\\alpha) \\right}\\). In words, given calculate the \\(M_j\\)‚Äôs, find the magical threshold () and include the variables with \\(M_j &gt; \\tau(\\alpha)\\)."
  },
  {
    "objectID": "blog/new-dev-fdr.html#bottom-line",
    "href": "blog/new-dev-fdr.html#bottom-line",
    "title": "New Developments in False Discovery Rate",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe Benjamini-Hochberg approach is still the most popular way to control FDR.\nI discuss two novel approaches for variable selection and FDR control aimed at improving the knockoff filter."
  },
  {
    "objectID": "blog/new-dev-fdr.html#where-to-learn-more",
    "href": "blog/new-dev-fdr.html#where-to-learn-more",
    "title": "New Developments in False Discovery Rate",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThis research is currently ongoing, so the best thing you can do is look at the papers in the section below."
  },
  {
    "objectID": "blog/new-dev-fdr.html#references",
    "href": "blog/new-dev-fdr.html#references",
    "title": "New Developments in False Discovery Rate",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2018). Controlling the false discovery rate via knockoffs. Annals of Statistics\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300.\nBenjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of statistics, 1165-1188.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2022). False discovery rate control via data splitting. Journal of the American Statistical Association, 1-18.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2023). A scale-free approach for false discovery rate control in generalized linear models. Journal of the American Statistical Association, 1-15.\nIgnatiadis, N., Klaus, B., Zaugg, J. B., & Huber, W. (2016). Data-driven hypothesis weighting increases detection power in genome-scale multiple testing. Nature methods, 13(7), 577-580.\nKorthauer, K., Kimes, P. K., Duvallet, C., Reyes, A., Subramanian, A., Teng, M., ‚Ä¶ & Hicks, S. C. (2019). A practical guide to methods controlling false discoveries in computational biology. Genome biology, 20(1), 1-21.\nScott, J. G., Kelly, R. C., Smith, M. A., Zhou, P., & Kass, R. E. (2015). False discovery rate regression: an application to neural synchrony detection in primary visual cortex. Journal of the American Statistical Association, 110(510), 459-471.\nXing, X., Zhao, Z., & Liu, J. S. (2023). Controlling false discovery rate using gaussian mirrors. Journal of the American Statistical Association, 118(541), 222-241."
  },
  {
    "objectID": "blog/foci.html",
    "href": "blog/foci.html",
    "title": "FOCI: A New Variable Selection Method",
    "section": "",
    "text": "In our data-abundant world, we often have access to tens, hundreds, or even thousands of variables. Most of these features are usually irrelevant or redundant, leading to increased computational complexity and potentially overfitted models. Variable selection algorithms are designed to mitigate these challenges by selecting the subset of columns that is most relevant to the problem at hand. This, in turn, can result in simplified, more efficient, and interpretable machine learning (ML) models. Examples of such methods abound‚Äîlasso, ridge, elastic net utilizing \\(L1\\) and \\(L2\\) regularization or a linear combination of both; forward/backward stepwise regression algorithms, etc.\nIn this article, I will describe a new variable selection method built on the idea of Chatterjee‚Äôs correlation coefficient, which has been a hot topic of discussion among statisticians. The new method goes by the abbreviation FOCI, which stands for Feature Ordering by Conditional Independence. It is somewhat similar to forward stepwise regression but overcomes some of the critiques most often associated with it.\nLet‚Äôs start with setting up some basic notation. We are focused on studying the correlation between (\\(Y\\)) (e.g., income) and (\\(X\\)) (education level) while interested in conditioning on/controlling for (\\(Z\\)) (parental education and other factors). As always, we are armed with a random sample of size n of all these variables and assume everything is well-behaved. If you have not had a chance, I recommend you first read my earlier post on Chatterjee‚Äôs correlation measure, which lays some of the foundations necessary to understand the algorithm under the hood."
  },
  {
    "objectID": "blog/foci.html#background",
    "href": "blog/foci.html#background",
    "title": "FOCI: A New Variable Selection Method",
    "section": "",
    "text": "In our data-abundant world, we often have access to tens, hundreds, or even thousands of variables. Most of these features are usually irrelevant or redundant, leading to increased computational complexity and potentially overfitted models. Variable selection algorithms are designed to mitigate these challenges by selecting the subset of columns that is most relevant to the problem at hand. This, in turn, can result in simplified, more efficient, and interpretable machine learning (ML) models. Examples of such methods abound‚Äîlasso, ridge, elastic net utilizing \\(L1\\) and \\(L2\\) regularization or a linear combination of both; forward/backward stepwise regression algorithms, etc.\nIn this article, I will describe a new variable selection method built on the idea of Chatterjee‚Äôs correlation coefficient, which has been a hot topic of discussion among statisticians. The new method goes by the abbreviation FOCI, which stands for Feature Ordering by Conditional Independence. It is somewhat similar to forward stepwise regression but overcomes some of the critiques most often associated with it.\nLet‚Äôs start with setting up some basic notation. We are focused on studying the correlation between (\\(Y\\)) (e.g., income) and (\\(X\\)) (education level) while interested in conditioning on/controlling for (\\(Z\\)) (parental education and other factors). As always, we are armed with a random sample of size n of all these variables and assume everything is well-behaved. If you have not had a chance, I recommend you first read my earlier post on Chatterjee‚Äôs correlation measure, which lays some of the foundations necessary to understand the algorithm under the hood."
  },
  {
    "objectID": "blog/foci.html#diving-deeper",
    "href": "blog/foci.html#diving-deeper",
    "title": "FOCI: A New Variable Selection Method",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nI will be being with taking you on a tour of conditional and unconditional statistics. What‚Äôs the deal with conditional versus unconditional correlation, and is one of them always a better choice than the other?\n\nConditional and Unconditional Correlation\nMost times, when we talk about the correlation between \\(X\\) and \\(Y\\), knowingly or not, we mean unconditional correlation. This is the correlation between two variables without considering any additional or contextual factors. As such, it describes their relationship ‚Äúin general.‚Äù In my previous post, I illustrated several ways to measure such unconditional correlations, including the well-known Spearman correlation coefficient.\nIn some cases, however, we like to go one step further and consider contextual factors. This is where conditional correlation comes in. It considers the influence of one or more additional variables when measuring the relationship between X and Y. Thus, it provides information on how their relationship changes under specific conditions.\nConditional correlation is hard (pun intended); it‚Äôs even NP-hard in some contexts.\nUnder certain assumptions, it might be possible to go from conditional to unconditional correlation. Simplifying the example above, let‚Äôs say that Z (parental education) can take on two values‚Äîlow and high. If we have measures of the correlation between X and Y for each of these values (i.e., for families with low and high parental education), we can weigh these by their relative proportion and arrive at an estimate of the unconditional correlation between \\(X\\) and \\(Y\\).\nTo answer my own question above, it‚Äôs difficult to say that one is always better than the other. It really depends on your goal. If you want to know how \\(X\\) and \\(Y\\) vary in general, unconditional correlation is your friend; if, instead, you are interested in incorporating contextual information or simply controlling for other factors, you really want conditional correlation. My take is that most often we are after conditional correlations but use tools for measuring unconditional ones.\n\n\nSide Note: Quantile Regression\nQuantile regression is another setting where most often people confuse conditional and unconditional statistical inference. Interestingly, though, the situation is flipped. The classical quantile regression estimator, which most people utilize, measures the impact of \\(X\\) on the conditional quantile of \\(Y\\), but there results are commonly interpreted as having unconditional interpretation.\nEnough background; now back to this new variable selection algorithm.\n\n\nNew Coefficient of Conditional Independence\nAzadkia and Chatterjee (2021) develop a new coefficient of conditional correlation. The math behind is in extremely involved and I will not even be discussing its formula here. For simplicity, let‚Äôs just call it \\(T(\\cdot)\\):\n\\[T(Y,X|Z) = \\text{corr} (Y, X | Z).\\]\n\\(T(\\cdot)\\) enjoys the following attractive properties:\n\nIt is an extension of Chatterjee‚Äôs unconditional correlation coefficient.\nIt is non-parametric.\nIt has no tuning parameters.\nIt can be estimated quickly, in \\(O(n \\text{ log}n) time\\).\nAsymptotically converges to a limit in \\([0,1]\\). It‚Äôs limit is \\(0\\) when \\(Y\\) and \\(X\\) are conditionally independent, and it is \\(1\\) when \\(Y\\) is a measurable function of \\(X\\), conditionally.\nIt is a nonlinear generalization of the partial \\(R^2\\) coefficient in a linear regression of \\(Y\\) on \\(X\\) and \\(Z\\).\n\nAll this is to say ‚Äì \\(T(\\cdot)\\) has many things going for it, and it is a pretty good measure of conditional correlation.\n\n\nThe Variable Selection Algorithm\nThe key idea is to integrate \\(T(\\cdot)\\) into a forward stepwise variable selection procedure. Let‚Äôs add the \\(Z\\) variables into \\(X\\), so we only have \\(Y\\in \\mathbb{R} and X\\in \\mathbb{R}^p\\). Then the algorithm goes as follows:\nAlgorithm:\n\nStart with the index \\(j\\) that maximizes$ T(Y|X_j)$.\nGiven \\(j_1, \\dots j_k, select j_{k+1}\\) as the index \\(\\notin (j_1, \\dots j_k)\\) that maximizes \\(T(Y,X_j|X_{j_1}, \\dots, X_{j_k})\\).\nContinue until finding the first k such that T(Y,X_j_{k+1}|X_{j_1}, , X_{j_k}).\nDeclare the set of selected variables ={X_{j_1}, , X_{j_k}}.\n\nSoftware Package: FOCI\nIn words, we start with the variable j that maximizes \\(T(Y\\mid X_j)\\). In each subsequent step we select the variable that has not yet been selected and has the highest \\(T(Y \\mid \\cdot)\\) value up until \\(T(Y \\mid \\cdot)\\) is positive. That‚Äôs it. We then have the set of FOCI selected variables.\nAlthough it is not required theoretically, the predictor variables be standardized before running the algorithm. If computational time is not an issue, one can try to add \\(m \\geq  2\\) variables at each step instead of just one.\nThere you have it. Grab your data and see whether and how much FOCI improves on your favorite feature selection method."
  },
  {
    "objectID": "blog/foci.html#bottom-line",
    "href": "blog/foci.html#bottom-line",
    "title": "FOCI: A New Variable Selection Method",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nAzadkia and Chatterjee (2021) develop a new coefficient of conditional correlation featuring a host of attractive properties.\nThis coefficient can be used in a stepwise inclusion fashion as a variable selection algorithm potentially improving on well-established methods in the field."
  },
  {
    "objectID": "blog/foci.html#where-to-learn-more",
    "href": "blog/foci.html#where-to-learn-more",
    "title": "FOCI: A New Variable Selection Method",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMy earlier post on Chatterjee‚Äôs bivariate coefficient of (unconditional) correlation is a good starting point. Data scientists more deeply interested in FOCI should read Azadkia and Chatterjee‚Äôs paper which describes in detail the mathematics behind the new algorithm."
  },
  {
    "objectID": "blog/foci.html#references",
    "href": "blog/foci.html#references",
    "title": "FOCI: A New Variable Selection Method",
    "section": "References",
    "text": "References\nAlejo, J., Favata, F., Montes-Rojas, G., & Trombetta, M. (2021). Conditional vs Unconditional Quantile Regression Models: A Guide to Practitioners. Economia, 44(88), 76-93.\nAzadkia, M., & Chatterjee, S. (2021). A simple measure of conditional dependence. The Annals of Statistics, 49(6), 3070-3102.\nBorah, B. J., & Basu, A. (2013). Highlighting differences between conditional and unconditional quantile regression approaches through an application to assess medication adherence. Health economics, 22(9), 1052-1070.\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\nDagum, P., & Luby, M. (1993). Approximating probabilistic inference in Bayesian belief networks is NP-hard. Artificial intelligence, 60(1), 141-153.\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional quantile regressions. Econometrica, 77(3), 953-973.\nKoenker, R. (2017). Quantile regression: 40 years on. Annual review of economics, 9, 155-176."
  },
  {
    "objectID": "blog/causality-wo-experiments.html",
    "href": "blog/causality-wo-experiments.html",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "",
    "text": "Causality is central to many practical data-related questions. Conventional methods for isolating causal relationships rely on experimentation, assume unconfoundedness, or require instrumental variables. However, experimentation is often infeasible, costly, or ethically concerning; good instruments are notoriously difficult to find; and unconfoundedness can be an uncomfortable assumption in many settings.\nThis article highlights methods for measuring causality beyond these three paradigms. These underappreciated approaches exploit higher moments and heteroscedastic error structures (Lewbel 2012, Rigobon 2003), latent instrumental variables (IVs) (Ebbes et al.¬†2005), and copulas (Park and Gupta 2012). I will unite them in a common statistical framework and discuss the key assumptions underlying each one.\nThe focus will be on the ideas, intuition, and practical aspects of these methodologies, rather than technical details. Readers can find more in-depth information in the References section. This article assumes familiarity with econometric endogeneity and the basics of instrumental variables; without this background, some sections may be challenging to follow.\nNote: Regression discontinuity (RD) methods are excluded from this discussion, as they fall somewhere between instrument-based and instrument-free econometric methodologies. We know that in fuzzy RDs, the running variable can be viewed as an instrument."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#background",
    "href": "blog/causality-wo-experiments.html#background",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "",
    "text": "Causality is central to many practical data-related questions. Conventional methods for isolating causal relationships rely on experimentation, assume unconfoundedness, or require instrumental variables. However, experimentation is often infeasible, costly, or ethically concerning; good instruments are notoriously difficult to find; and unconfoundedness can be an uncomfortable assumption in many settings.\nThis article highlights methods for measuring causality beyond these three paradigms. These underappreciated approaches exploit higher moments and heteroscedastic error structures (Lewbel 2012, Rigobon 2003), latent instrumental variables (IVs) (Ebbes et al.¬†2005), and copulas (Park and Gupta 2012). I will unite them in a common statistical framework and discuss the key assumptions underlying each one.\nThe focus will be on the ideas, intuition, and practical aspects of these methodologies, rather than technical details. Readers can find more in-depth information in the References section. This article assumes familiarity with econometric endogeneity and the basics of instrumental variables; without this background, some sections may be challenging to follow.\nNote: Regression discontinuity (RD) methods are excluded from this discussion, as they fall somewhere between instrument-based and instrument-free econometric methodologies. We know that in fuzzy RDs, the running variable can be viewed as an instrument."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#notation-and-setup",
    "href": "blog/causality-wo-experiments.html#notation-and-setup",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Notation and Setup",
    "text": "Notation and Setup\nLet‚Äôs begin by establishing some basic notation. We aim to analyze the impact of a binary, endogenous treatment variable \\(X_1\\) on an outcome variable \\(Y\\), in a setting with exogenous variables \\(X_2\\). We have access to a well-behaved, representative iid sample of size \\(n\\) of \\(Y\\), and \\(X:=[X_1, X_2]\\). These variables are related as follows:\n\\[ Y = \\beta X_1 + \\gamma X_2 + \\epsilon, \\]\nwhere is a mean-zero error term. Our goal is to obtain a consistent estimate of \\(\\beta\\). For simplicity, we‚Äôre using the same notation for both single- and vector-valued quantities, as \\(X_2\\) can be in \\(\\mathbb{R}^p\\) with $ p&gt;1$.\nThe challenge arises because \\(X_1\\) and \\(\\epsilon\\) are correlated, rendering the standard OLS estimator inconsistent. Even in large samples, \\(\\hat{\\beta}_{OLS}\\) will be biased, and getting more data would not help. Specifically:\n\\[\\hat{\\beta}_{OLS} := (X'X)^{-1}X'Y \\nrightarrow \\beta. \\]\nStandard instrument-based methods rely on the existence of an instrumental variable \\(Z\\) which, conditional on \\(X_2\\), correlates with \\(X_1\\) but not with . Estimation then proceeds with 2SLS, LIML, or GMM, potentially yielding good estimates of \\(\\beta\\) given appropriate assumptions. Common issues with instrumental variables include implausibility of the exclusion restriction, weak correlation with \\(X_1\\), and challenges in interpreting \\(\\hat{\\beta}_{IV}\\). Formally:\n\\[\\hat{\\beta}_{IV} := (Z'X)^{-1}Z'Y \\rightarrow \\beta. \\]\nIn this article, we focus on obtaining correct estimates of \\(\\beta\\) in settings where we don‚Äôt have access to such an instrumental variable \\(Z\\)."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#diving-deeper",
    "href": "blog/causality-wo-experiments.html#diving-deeper",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nLet‚Äôs start with the heteroskedasticity-based approach of Lewbel (2012).\n\nMethod 1: Heteroskedasticity & Higher Moments\nThe main idea here is to construct valid instruments for \\(X_1\\) by using information contained in the heteroskedasticity of \\(\\epsilon\\). Intuitively, if \\(\\epsilon\\) exhibits heteroskedasticity related to $ X_2$, we can use to create instruments‚Äîspecifically by interacting \\(X_2\\) with the residuals of the endogenous regressor‚Äôs reduced form equation. So, this is an IV-based method, but the instrument is ‚Äúinternal‚Äù to the model and does not rely on any external information.\nThe key assumptions are:\nThe error term in the structural equation (\\(\\epsilon\\)) is heteroskedastic. This means \\(var(\\epsilon|X_2)\\) is not constant and depends on \\(X_2\\). Moreover, we need \\(cov(X_2,\\epsilon^2) \\neq 0\\). This is an analogue of the first stage assumption in IV methods. The exogenous variable (\\(X_2\\)) are uncorrelated with the product of the endogeneous variable (\\(X_1\\)) and the error term (\\(\\epsilon\\)). That is, \\(cov(X_2, X_1\\epsilon) = 0\\). This is a form of the standard exogeneity assumption in IV estimation. The heteroskedasticity-based estimator of Lewbel (2012) proceeds in two steps:\nRegress X_1 on X_2 and save the estimated residuals; call them \\(\\hat{u}\\). Construct an instrument for \\(X_1\\) as \\(\\tilde{Z}=(X_2-\\bar{X}_2)\\hat{u}\\), where \\(\\bar{X}_2\\) is the mean of \\(X_2\\). Use \\(\\tilde{Z}\\) as an instrument in a standard 2SLS estimation: \\[\\hat{\\beta}_{LEWBEL} = (X'P_{\\tilde{Z}}X)^{-1}X'P_{\\tilde{Z}}Y,\\]\nwhere \\(P_{\\tilde{Z}}\\) ‚Äãis the projection matrix onto the instrument.\nThis line of thought can also be extended to use higher moments as an alternative or additional way to construct instrumental variables. The original approach uses the variance of the error term, but we can also rely on skewness, kurtosis, etc. The assumptions then must be modified such that these higher moments are correlated with the endogenous variable, etc.\nSoftware Packages: REndo, ivlewbel.\nLet‚Äôs now move to the second set of methods.\n\n\nMethod 2: Latent IVs\nThe latent IV approach imposes distributional assumptions on the exogenous part of the endogenous variable and employs likelihood-based methods to estimate \\(\\beta\\).\nLet‚Äôs simplify the model above, so that we have:\n\\[ Y=\\beta X + \\epsilon, \\]\nwhere \\(X\\) is endogenous. The key idea is to decompose \\(X\\) into two components:\n\\[ X= \\theta + \\nu, \\]\nwith \\(cov(\\theta, \\epsilon)=0\\), \\(cov(\\theta, \\nu) = 0\\), and \\(cov(\\epsilon, \\nu)\\neq0\\). The first condition states that \\(\\theta\\) is the exogenous part of \\(X\\), and the last one gives rise to the endogeneity problem.\nWe then proceed with adding distributional assumptions. Importantly, \\(\\theta\\) must follow some discrete distribution with a finite number of mass points. A common example imposes:\n\\[ \\theta \\sim \\text{Multinomial}(\\cdot) \\]\nand\n\\[ (\\epsilon, \\nu) \\sim \\text{Gaussian}(\\cdot). \\]\nThese set of assumptions lead to analytical solutions for the conditional and unconditional distributions of \\((Y,X)\\) and all parameters of the model are identified. Maximum likelihood estimation can then give us an estimate of \\(\\beta_{LIV}\\).\nSoftware Packages: REndo.\nFinally, we turn our attention to the final set of instrument-free methods to solve endogeneity.\n\n\nMethod 3: Copulas\nFirst, a word on copulas. A copula is a multivariate cumulative distribution function (CDF) with uniform marginals on \\([0,1]\\). An old theorem states that any multivariate CDF can be expressed with uniform marginals and a copula function that represents the relationship between the variables. Specifically, if \\(A\\) and \\(B\\) are two random variables with marginal CDFs \\(F_A\\) and \\(F_B\\) and joint CDF \\(H\\), then there exists a copula \\(C\\) such that \\(H(a,b)=C(F_A(a), F_B(b))\\).\nHow does this fit into our context and framework? Park and Gupta (2012) introduced two estimation methods for \\(\\beta\\) under the assumption that \\(\\epsilon \\sim Gaussian(\\cdot)\\). The key idea is positing a Gaussian copula to link the marginal distributions of X and and obtain their joint distribution. We can then estimate $$in one of two ways: either impose distributional assumptions on these marginals and derive and maximize the joint likelihood function of \\(X\\) and \\(\\epsilon\\), or use a generated regressor approach. We will focus on the latter.\nIn the linear model, endogeneity is tackled by creating a novel variable \\(\\tilde{X}\\) and adding that as a control (i.e., a generated regressor). Using our simplified model where \\(X\\) is single-valued and endogenous, we now have:\n\\[Y=\\beta X + \\mu \\tilde{X} + \\eta, \\]\nwhere \\(\\eta\\) is the error term in this augmented model.\nWe construct \\(\\tilde{X}\\) as follows:\n\\[\\tilde{X}=\\Phi^{-1}(\\hat{F}_X(X)).\\]\nHere \\(\\Phi^{-1}(\\cdot)\\) is the inverse CDF of the standard normal distribution and \\(F_X(\\cdot)\\) is the marginal CDF of \\(X\\). We can estimate the latter using the empirical CDF by sorting the observations in ascending order and calculating the proportion of rows with smaller values for each observation. As you can guess, this introduces further uncertainty into the model, so the standard errors should be estimated using bootstrap.\nSoftware Packages: REndo, copula.\n\n\nComparison\nEach statistical method has its strengths and limitations. While the methods described here circumvent the traditional unconfoundedness and external instruments-based assumptions, they do not provide a magical panacea to the endogeneity problem. Instead, they rely on their own, different assumptions. These methods are not universally superior, but should be considered when traditional approaches do not fit your context.\nThe heteroskedasticity-based approach, as the name suggests, requires a considerable degree of heteroskedasticity to perform well. Latent IVs may offer efficiency advantages but come at the cost of imposing distributional assumptions and requiring a group structure of \\(X_1\\). The copula-based approach, while simple to implement, also requires strong assumptions about the distributions of \\(X\\) and \\(Y\\), as well as their relationship.\nThat‚Äôs it. You are now equipped with a set of new methods designed to identify causal relationships in your data."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#bottom-line",
    "href": "blog/causality-wo-experiments.html#bottom-line",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nConventional methods used to tease causality rely on experiments or ambitious assumptions such as unconfoudedness or the access to valid instrumental variables.\nResearchers have developed methods aimed at measuring causality without relying on these frameworks.\nNone of these are a panacea and they rely on their own assumptions that have to be checked on a case-by-case basis."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#where-to-learn-more",
    "href": "blog/causality-wo-experiments.html#where-to-learn-more",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nEbbes, Wedel, and Bockenholt (2009), Park and Gupta (2012), Papies, Ebbes, and Heerde (2017), and Rutz and Watson (2019) provide detailed comparisons of these IV-free methods with alternative methods. Also, Qian et al.¬†(2024) and Papadopolous (2022) and Baum and Lewbel (2019) have a practical angle that many data scientist will find accessible and attractive."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#references",
    "href": "blog/causality-wo-experiments.html#references",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "References",
    "text": "References\nBaum, C. F., & Lewbel, A. (2019). Advice on using heteroskedasticity-based identification. The Stata Journal, 19(4), 757-767.\nEbbes, P. (2004). Latent instrumental variables: a new approach to solve for endogeneity.\nEbbes, P., Wedel, M., & B√∂ckenholt, U. (2009). Frugal IV alternatives to identify the parameter for an endogenous regressor. Journal of Applied Econometrics, 24(3), 446-468.\nEbbes, P., Wedel, M., B√∂ckenholt, U., & Steerneman, T. (2005). Solving and testing for regressor-error (in) dependence when no IVs are available: With new evidence for the effect of education on income. Quantitative Marketing and Economics, 3, 365-392.\nErickson, T., & Whited, T. M. (2002). Two-step GMM estimation of the errors-in-variables model using high-order moments. Econometric Theory, 18(3), 776-799.\nGui, R., Meierer, M., Schilter, P., & Algesheimer, R. (2020). REndo: An R package to address endogeneity without external instrumental variables. Journal of Statistical Software.\nHueter, I. (2016). Latent instrumental variables: a critical review. Institute for New Economic Thinking Working Paper Series, (46).\nLewbel, A. (1997). Constructing instruments for regressions with measurement error when no additional data are available, with an application to patents and R&D. Econometrica, 1201-1213.\nLewbel, A. (2012). Using heteroscedasticity to identify and estimate mismeasured and endogenous regressor models. Journal of business & economic statistics, 30(1), 67-80.\nPapadopoulos, A. (2022). Accounting for endogeneity in regression models using Copulas: A step-by-step guide for empirical studies. Journal of Econometric Methods, 11(1), 127-154.\nPapies, D., Ebbes, P., & Van Heerde, H. J. (2017). Addressing endogeneity in marketing models. Advanced methods for modeling markets, 581-627.\nPark, S., & Gupta, S. (2012). Handling endogenous regressors by joint estimation using copulas. Marketing Science, 31(4), 567-586.\nRigobon, R. (2003). Identification through heteroskedasticity. Review of Economics and Statistics, 85(4), 777-792.\nQian, Y., Koschmann, A., & Xie, H. (2024). A Practical Guide to Endogeneity Correction Using Copulas (No.¬†w32231). National Bureau of Economic Research.\nRigobon, R. (2003). Identification through heteroskedasticity. Review of Economics and Statistics, 85(4), 777-792.\nRutz, O. J., & Watson, G. F. (2019). Endogeneity and marketing strategy research: An overview. Journal of the Academy of Marketing Science, 47, 479-498.\nTran, K. C., & Tsionas, E. G. (2015). Endogeneity in stochastic frontier models: Copula approach without external instruments. Economics Letters, 133, 85-88."
  },
  {
    "objectID": "blog/conformal-inference.html",
    "href": "blog/conformal-inference.html",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "",
    "text": "Traditional confidence intervals estimate the range in which a population parameter, such as a mean or regression coefficient, is likely to fall with a specified level of confidence (e.g., 95%). They reflect the uncertainty in estimating a fixed but unknown parameter based on observing only a sample of the population. But what if we are interested in measuring uncertainty in the context of making predictions? Can we use traditional methods, or do we need a new framework?\nPrediction intervals provide a range within which a future individual observation is expected to fall with a given probability. Since they account for both the uncertainty in estimating the mean and the inherent variability of individual observations, they are typically wider than confidence intervals. While a confidence interval narrows as sample size increases, a prediction interval remains relatively wide due to the irreducible noise in individual data points. For simplicity, I will use both terms interchangeably.\nConformal inference offers a formalized framework for building such prediction intervals in machine learning models. This article provides a gentle introduction to the main idea behind conformal inference, presenting a new way of thinking about uncertainty in the context of machine learning (i.e., prediction) models.\nAs a teaser, the basic idea behind the method rests on a simple result about sample quantiles. Let me explain."
  },
  {
    "objectID": "blog/conformal-inference.html#background",
    "href": "blog/conformal-inference.html#background",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "",
    "text": "Traditional confidence intervals estimate the range in which a population parameter, such as a mean or regression coefficient, is likely to fall with a specified level of confidence (e.g., 95%). They reflect the uncertainty in estimating a fixed but unknown parameter based on observing only a sample of the population. But what if we are interested in measuring uncertainty in the context of making predictions? Can we use traditional methods, or do we need a new framework?\nPrediction intervals provide a range within which a future individual observation is expected to fall with a given probability. Since they account for both the uncertainty in estimating the mean and the inherent variability of individual observations, they are typically wider than confidence intervals. While a confidence interval narrows as sample size increases, a prediction interval remains relatively wide due to the irreducible noise in individual data points. For simplicity, I will use both terms interchangeably.\nConformal inference offers a formalized framework for building such prediction intervals in machine learning models. This article provides a gentle introduction to the main idea behind conformal inference, presenting a new way of thinking about uncertainty in the context of machine learning (i.e., prediction) models.\nAs a teaser, the basic idea behind the method rests on a simple result about sample quantiles. Let me explain."
  },
  {
    "objectID": "blog/conformal-inference.html#notation",
    "href": "blog/conformal-inference.html#notation",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs imagine a size n i.i.d. sample of an outcome variable \\(Y\\) and a covariate vector \\(X\\), \\((X_1, Y_1) \\dots (X_n, Y_n)\\). Conformal inference is concerned with building a ‚Äúconfidence interval‚Äù for a new outcome observation \\(Y_{n+1}\\) from a new feature realization \\(X_{n+1}\\).\nImportantly, this interval should be valid:\n\nin finite samples (i.e., non-asymptotically),\nwithout assumptions on the data generating process, and\nfor any estimator of the regression function, \\(\\mu(x)=E[Y \\mid X=x]\\).\n\nIn mathematical notation, given a significance level , we want to construct a confidence interval \\(CI(X_{n+1})\\) satisfying the above properties and such that:\n\\[P(Y_{n+1} \\in CI(X_{n+1})) \\geq 1-\\alpha.\\]"
  },
  {
    "objectID": "blog/conformal-inference.html#diving-deeper",
    "href": "blog/conformal-inference.html#diving-deeper",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nA Refresher on Sample Quantiles\nI will start with reviewing sample quantiles. Given an i.i.d. sample, \\(U_1, \\dots, U_n\\), the (\\(1-\\alpha\\))th quantile is the value \\(\\hat{q}_{1-\\alpha}\\) such that approximately \\((1-\\alpha)\\times100\\%\\) of the data is smaller than it. For instance, the \\(95\\)th quantile (sometimes also called percentile) is the value for which \\(95\\%\\) of the observations are at least as small.\nSo, given a new observation \\(U_{n+1}\\), we know that:\n\\[P(U_{n+1}\\leq \\hat{q}_{1-\\alpha})\\geq 1-\\alpha.\\]\n\n\nThe Na√Øve Approach\nLet‚Äôs turn back to the regression example with Y and X. We are given a new observation \\(X_{n+1}\\) and our focus is on \\(Y_{n+1}\\). Following the fact described above, a na√Øve way to construct a confidence interval for \\(Y_{n+1}\\) is as follows:\n\\[CI^{\\text{na√Øve}}_{1-\\alpha}(X_{n+1}) = \\left[ \\hat{\\mu}(X_{n+1}) \\pm \\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha} \\right].\\]\nHere \\(\\mu(\\cdot)\\) is an estimate of the regression function \\(E[Y \\mid X]\\), and \\(\\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha}\\) is the \\((1-\\alpha)\\)th quantile of empirical distribution function of the fitted residuals \\(\\mid Y-\\hat{\\mu}(X) \\mid\\).\nPut simply, we can look at an interval around our best prediction for \\(Y_{n+1}\\) (i.e., \\(\\hat{\\mu}(X_{n+1})\\)) defined by the residuals estimated on the original data.\nIt turns out this interval is too narrow. In a series of papers Vladimir Vovk and co-authors show that the empirical distribution function of the fitted residuals is often biased downward and hence this interval is invalid. This is where conformal inference comes in.\n\n\nConformal Inference\nConsider the following strategy. For each \\(y\\) we fit a regression $_y $ on the sample \\((Y_1, X_1),\\dots (Y_n, X_n), (y, X_{n+1})\\). We calculate the residuals \\(R^y_i\\) for \\(i=1,\\dots,n\\) and \\(R^y_{n+1}\\) and count the proportion of \\(R^y_i\\)‚Äôs smaller than \\(R^y_{n+1}\\). Let‚Äôs call this number \\(\\sigma(y)\\). That is,\n\\[\\sigma(y) = \\frac{1}{n+1}\\sum_{i=1}^{n+1} I (R^y_i \\leq R^y_{n+1}),\\]\nwhere \\(I(\\cdot)\\) is the indicator function equal to one when the statement in the parenthesis is true and 0 if when it is not.\nThe test statistic \\(\\sigma({Y_{n+1}})\\) is uniformly distributed over the set \\(\\{ \\frac{1}{n+1}, \\frac{2}{n+1},\\dots, 1\\}\\), implying we can use \\(1-\\sigma({Y_{n+1}})\\) as a valid p-value for testing the null that \\(Y_{n+1}=y.\\) Then, using the sample quantiles logic outlined above we arrive at the following confidence interval for \\(Y_{n+1}\\):\n\\[ CI^{\\text{conformal}}_{1-\\alpha}(X_{n+1}) \\approx \\{ y\\in \\mathbb{R} : \\sigma(y)\\leq 1-\\alpha \\}.\\]\nThis is summarized in the following procedure:\nAlgorithm: For each value \\(y\\):\n\nFit the regression function \\(\\mu(\\cdot)\\) on \\((X_1, Y_1), \\dots, (X_n, Y_n), (X_{n+1}, y)\\) using your favorite estimator/learner.\nCalculate the \\(n+1\\) residuals.\nCalculate the proportion \\(\\sigma(y)\\).\nConstruct \\(CI = \\{y: \\sigma(y) \\leq (1-\\alpha)\\}\\).\n\nSoftware Package: conformalInference\nTwo notes. First, conformal inference guarantees unconditional coverage. This is conceptually different and should not be confused with the conditional statement \\(P(Y_{n+1}\\in CI(x) \\mid X_{n+1}=x)\\geq 1-\\alpha\\). The latter is stronger and more difficult to assert, requiring additional assumptions such as consistency of our estimator of \\(\\mu(\\cdot)\\).\nSecond, this procedure can be computationally expensive. For a given value \\(X_{n+1}\\) we need to fit a regression model and compute residuals for every \\(y\\) which we consider including in the confidence interval. This is where split conformal inference comes in.\n\n\nSplit Conformal Inference\nSplit conformal inference is a modification of the original algorithm that requires significantly less computation power. The idea is to split the fitting and ranking steps, so that the former is done only once. Here is the algorithm.\nAlgorithm:\n\nRandomly split the data in two equal-sized bins.\nGet \\(\\hat{\\mu}\\) on the first bin.\nCalculate the residuals for each observation in the second bin.\nLet d be the s-th smallest residual, where \\(s=(\\frac{n}{2}+1)(1-\\alpha)\\).\nConstruct \\(CI^{\\text{split}}=[\\hat{\\mu}-d,\\hat{\\mu}+d]\\).\n\nA downside of this splitting approach is the introduction of extra randomness. One way to mitigate this is to perform the split multiple times and construct a final confidence interval by taking the intersection of all intervals. The aggregation decreases the variability from a single data split and, as this paper shows, still remains valid. Similar random split aggregation has also been used in the context of statistical significance in high-dimensional models."
  },
  {
    "objectID": "blog/conformal-inference.html#an-example",
    "href": "blog/conformal-inference.html#an-example",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "An Example",
    "text": "An Example\nI used the popular iris dataset to try out the R package conformalInference. Like most of my data demos, this is meant to be a mere illustration and you should not take the results seriously.\nThe outcome variable was Sepal.Length, and the matrix \\(X\\) included sepal.width, petal.length, petal.width, species_setosa, species_versicolor, and species_virginica. Some of these were categorical in which case I converted them to a bunch of binary variables. I used the first \\(148\\) observations to estimate the regression function \\(\\mu(X)\\) using lasso and the \\(149\\)th row to form the prediction (i.e., the test set).\nHere is the code.\n# clear workspace and load packages\nrm(list=ls())\nlibrary(conformalInference)\nlibrary(glmnet)\nlibrary(tidyverse)\n\n# load the iris dataset\ndata &lt;- iris\n\n# clean data\ncolnames(data) &lt;- tolower(colnames(data))\n\n# one-hot encode the species variable\ndata &lt;- data %&gt;%\n  mutate(species_setosa = as.integer(species == 'setosa'),\n         species_versicolor = as.integer(species == 'versicolor'),\n         species_virginica = as.integer(species == 'virginica')) %&gt;%\n  dplyr::select(-species)\n\n# check for missing values (none in iris, but included for completeness)\ndata &lt;- na.omit(data)\n\n# split training/test data\ndata0 &lt;- data %&gt;% filter(row_number() == nrow(data))  # last row as test set\ndata &lt;- data %&gt;% filter(row_number() &lt; nrow(data))   # remaining rows as training set\n\n# select variables X, Y\ny &lt;- data$sepal.length  # target variable\nx &lt;- data %&gt;% dplyr::select(-sepal.length)  # predictors\nx &lt;- as.matrix(x)\nx0 &lt;- data0 %&gt;% dplyr::select(-sepal.length)  # test predictors\nx0 &lt;- as.matrix(x0)\nn &lt;- nrow(x)\n\n# use lasso to estimate mu\nout.gnet = glmnet(x, y, nlambda=100, lambda.min.ratio=1e-3)\nlambda = min(out.gnet$lambda)\nfuns = lasso.funs(lambda=lambda)\n\n# run conformal inference\nout.conf = conformal.pred(x, y, x0, \n                          alpha=0.1,\n                          train.fun=funs$train, \n                          predict.fun=funs$predict, \n                          verb=TRUE)\n\n# run split conformal inference\nout.split = conformal.pred.split(x, y, x0, \n                                 alpha=0.1,\n                                 train.fun=funs$train, \n                                 predict.fun=funs$predict, \n                                 verb=TRUE)\n\n# print results\npaste('The lower bound is', out.conf$lo, 'and the upper bound is', out.conf$up)\n&gt; [1] \"The lower bound is 5.89 and the upper bound is 6.68\"\nout.conf$pred\n&gt;         [,1]\n&gt; [1,] 6.316882\n\n# print results for split conformal inference\npaste('The lower bound is', out.split$lo, 'and the upper bound is', out.split$up)\n&gt; [1] \"The lower bound is 5.74 and the upper bound is 6.93\"\nout.split$pred\n&gt;          [,1]\n&gt; [1,] 6.33556\nThe actual age value in the test set was \\(6.2\\) while the conformal inference approach computed a confidence interval (\\(5.88, 6.68\\)). The splitting algorithm gave similar results."
  },
  {
    "objectID": "blog/conformal-inference.html#bottom-line",
    "href": "blog/conformal-inference.html#bottom-line",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nConformal inference offers a novel approach for constructing valid finite-sample prediction intervals in machine learning models."
  },
  {
    "objectID": "blog/conformal-inference.html#where-to-learn-more",
    "href": "blog/conformal-inference.html#where-to-learn-more",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nConformal inference in machine learning is an ongoing research topic and I do not know of any review papers or textbook treatments of the subject. If you are interested in learning more, check the paper referenced below."
  },
  {
    "objectID": "blog/conformal-inference.html#references",
    "href": "blog/conformal-inference.html#references",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "References",
    "text": "References\nLei, J., G‚ÄôSell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094-1111.\nLei, J., Rinaldo, A., & Wasserman, L. (2015). A conformal prediction approach to explore functional data. Annals of Mathematics and Artificial Intelligence, 74, 29-43.\nShafer, G., & Vovk, V. (2008). A Tutorial on Conformal Prediction. Journal of Machine Learning Research, 9(3).\nVovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world (Vol. 29). New York: Springer."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html",
    "href": "blog/gen-vars-predefined-corr.html",
    "title": "Generating Variables with Predefined Correlation",
    "section": "",
    "text": "Suppose you are working on a project where the relationship between two variables is influenced by an unobserved confounder, and you want to simulate data that reflects this dependency. Standard random number generators often assume independence between variables, making them unsuitable for this task. Instead, you need a method to introduce specific correlations into your data generation process.\nA powerful and efficient way to achieve this is through Cholesky decomposition. By decomposing a correlation matrix into its triangular components, you can transform independent random variables into correlated ones. This approach is versatile, efficient, and mathematically grounded, making it ideal for simulating realistic datasets with predefined (linear) relationships."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html#background",
    "href": "blog/gen-vars-predefined-corr.html#background",
    "title": "Generating Variables with Predefined Correlation",
    "section": "",
    "text": "Suppose you are working on a project where the relationship between two variables is influenced by an unobserved confounder, and you want to simulate data that reflects this dependency. Standard random number generators often assume independence between variables, making them unsuitable for this task. Instead, you need a method to introduce specific correlations into your data generation process.\nA powerful and efficient way to achieve this is through Cholesky decomposition. By decomposing a correlation matrix into its triangular components, you can transform independent random variables into correlated ones. This approach is versatile, efficient, and mathematically grounded, making it ideal for simulating realistic datasets with predefined (linear) relationships."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html#diving-deeper",
    "href": "blog/gen-vars-predefined-corr.html#diving-deeper",
    "title": "Generating Variables with Predefined Correlation",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nAlgorithm\nAssume we want to generate a vector Y with n observations and p variables with a target correlation matrix \\(\\Sigma\\). The algorithm to obtain \\(Y\\) is as follows:\n\nStart with Independent Variables: Create a matrix X of dimensions n p, where each column is independently drawn from N(0,1): \\[ X = \\begin{bmatrix}x_{11} & x_{12} & \\cdots & x_{1p} \\\\x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\x_{n1} & x_{n2} & \\cdots & x_{np}.\\end{bmatrix} \\]\nDecompose the Target Matrix: Perform Cholesky decomposition on the target correlation matrix \\(\\Sigma\\) as: \\[\\Sigma = LL^T,\\]\n\nwhere L is a lower triangular matrix. 3. Transform the Independent Variables: Multiply the independent variable matrix \\(X\\) by \\(L\\) to obtain the correlated variables: \\[Y = XL.\\]\nHere \\(Y\\) is an \\(n\\times p\\) matrix where the columns have the desired correlation structure defined by \\(\\Sigma\\). To ensure that \\(\\Sigma\\) is a valid correlation matrix, it must be positive-definite. This condition guarantees the success of Cholesky decomposition and the correctness of the resulting correlated variables.\n\n\nMathematical Explanation\nLet‚Äôs examine how and why this approach works. We know that $= LL^T $ and \\(E(XX^T)=I\\) by definition. We want to show that \\(E(YY^T)=LL^T\\). Here is the simplest way to get there:\n\\[E(YY^T)=E((LX)(LX)^T)=E(LXX^TL^T)=LE(XX^T)L^T=LL^T.\\]\nThere you have it ‚Äì the algorithm outlined above is mathematically grounded. The covariance matrix of \\(Y\\) is indeed equal to \\(\\Sigma\\). Let‚Äôs now look at an example."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html#an-example",
    "href": "blog/gen-vars-predefined-corr.html#an-example",
    "title": "Generating Variables with Predefined Correlation",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs implement this in R and python with \\(p=3\\) and \\(n=1,000\\). Our target correlation matrix defines the desired relationships between the variables in \\(Y\\). In our example, we have pairwise correlations equal to \\(0.8\\) (b/w \\(y_1\\) and \\(y_2\\)), \\(0.5\\) (b/w \\(y_1\\) and \\(y_3\\)), and \\(0.3\\) (b/w \\(y_2\\) and \\(y_3\\)).\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\n# Generate X, independent standard normal variables\nn &lt;- 1000 \np &lt;- 3   \nx &lt;- matrix(rnorm(n * p), nrow = n, ncol = p)\n\n# Define Sigma, the target correlation matrix\nsigma &lt;- matrix(c(\n  1.0, 0.8, 0.5,\n  0.8, 1.0, 0.3,\n  0.5, 0.3, 1.0\n), nrow = p, byrow = TRUE)\n\n# Cholesky decomposition\nL &lt;- t(chol(sigma))\ndiag &lt;- diag(c(1,1,1))\ny &lt;- t(diag %*% L %*% t(x))\n\n# Print the results\nprint(cor(y))\n          [,1]      [,2]      [,3]\n[1,] 1.0000000 0.7875707 0.5111323\n[2,] 0.7875707 1.0000000 0.3008518\n[3,] 0.5111323 0.3008518 1.0000000\n\n\nimport numpy as np\nnp.random.seed(1988)\n\n# Generate X, independent standard normal variables\nn = 1000\np = 3\nx = np.random.normal(size=(n, p))\n\n# Define Sigma, the target correlation matrix\nsigma = np.array([\n    [1.0, 0.8, 0.5],\n    [0.8, 1.0, 0.3],\n    [0.5, 0.3, 1.0]\n])\n\n# Cholesky decomposition\nL = np.linalg.cholesky(sigma)\ndiag = np.diag([1, 1, 1])\ny = (diag @ L @ x.T).T\n\n# Print results\n[[1.         0.78702913 0.48132289]\n [0.78702913 1.         0.27758356]\n [0.48132289 0.27758356 1.        ]]\n\n\n\nUsing our notation above we have:\n\\[\\Sigma = \\begin{bmatrix}1.0 & 0.8 & 0.5 \\\\0.8 & 1.0 & 0.3 \\\\ 0.5 & 0.3 &1.0\\end{bmatrix}. \\]\nThe chol function in R decomposes the matrix into a lower triangular matrix. In our example:\n\\[L^T = \\begin{bmatrix}1 & 0.8 & 0.5 \\\\0 & 0.6 & -0.17 \\\\0 & 0.0 & 0.85 \\end{bmatrix}. \\]\nMultiplying the independent variables \\(X\\) by the transpose of \\(L\\) ensures the output \\(Y\\) matches the specified correlation structure.\nThe cor function checks whether the generated data conforms to the target correlation matrix.\nThe two matrices match almost exactly. We can also visualize the three variables in a scatterplot matrix. Notice that higher correlation values (e.g., b/w \\(y_1\\) and \\(y_2\\)) correspond to stronger linear associations between."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html#bottom-line",
    "href": "blog/gen-vars-predefined-corr.html#bottom-line",
    "title": "Generating Variables with Predefined Correlation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nA common data practitioner‚Äôs need is to generate variables with a predefined correlation structure.\nCholesky decomposition offers a powerful and efficient way to achieve this."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html",
    "href": "blog/lasso-heterogeneous-effects.html",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "",
    "text": "Lasso is one of my favorite machine learning algorithms. It is so simple, elegant, and powerful. My feelings aside, Lasso indeed has a lot to offer. While, admittedly, it is outperformed by more complex, black-box type methods (e.g., boosting or neural networks), it has several advantages: interpretability, computational efficiency, and flexibility. Even when it comes to accuracy, theory tells us that under appropriate assumptions, Lasso can uncover the true submodel and we can even derive bounds on its prediction loss.\nIn this article I will briefly describe two ways researchers use Lasso to detect heterogeneous treatment effects. The underlying idea is to throw a bunch of covariates in the model and let the \\(L1\\) regularization do the difficult job of identifying which ones are important for treatment effect heterogeneity."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#background",
    "href": "blog/lasso-heterogeneous-effects.html#background",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "",
    "text": "Lasso is one of my favorite machine learning algorithms. It is so simple, elegant, and powerful. My feelings aside, Lasso indeed has a lot to offer. While, admittedly, it is outperformed by more complex, black-box type methods (e.g., boosting or neural networks), it has several advantages: interpretability, computational efficiency, and flexibility. Even when it comes to accuracy, theory tells us that under appropriate assumptions, Lasso can uncover the true submodel and we can even derive bounds on its prediction loss.\nIn this article I will briefly describe two ways researchers use Lasso to detect heterogeneous treatment effects. The underlying idea is to throw a bunch of covariates in the model and let the \\(L1\\) regularization do the difficult job of identifying which ones are important for treatment effect heterogeneity."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#notation",
    "href": "blog/lasso-heterogeneous-effects.html#notation",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Notation",
    "text": "Notation\nAs always, let‚Äôs start with some notation. Let \\(T\\) denote a binary treatment indicator, \\(Y(0), Y(1)\\) be the potential outcomes under each treatment state (\\(Y\\) is the observed one), and \\(X\\) be a covariate vector. Lastly, \\(p\\) is the share of units in the treatment group, \\(p=\\frac{1}{N}\\sum T\\), where \\(N\\) is the sample size.\nThe Lasso coefficient vector is commonly expressed as the solution to the following problem:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-X\\beta||_2^2 + \\lambda||\\beta||_1 \\},\\]\nwhere \\(\\lambda\\) is the regularization parameter governing the variance-bias trade-off.\nWe are interested in the heterogeneous treatment effect given \\(X (HTE(X))\\):\n\\[HTE(X) = E[Y(1)-Y(0)|X=x].\\]\nThat is, \\(HTE(X)\\) is the average treatment effect for units with covariate levels \\(X=x\\).\nMore precisely, our goal is identifying which variables in \\(X\\) divide the population of interest such that there are meaningful treatment effect differences across these groups. For instance, in the case of estimating the impact of school quality on test scores, \\(X\\) might be students‚Äô gender (e.g., girls benefit more than boys), or in the context of online A/B testing, \\(X\\) might denote previous product engagement (e.g., tenured users benefit more from a new feature than inexperienced users).\nBroadly speaking, there are two main approaches to using Lasso to solve this problem ‚Äî (i) a linear model with interactions between \\(T\\) and \\(X\\), and (ii) directly regressing the imputed unit-level treatment effects on \\(X\\)."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#diving-deeper",
    "href": "blog/lasso-heterogeneous-effects.html#diving-deeper",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nLasso with Treatment Variable Interactions\nIn a low-dimensional world where regularization is not necessary and researchers are interested in \\(HTE\\)s, they often use a linear model in which the treatment variable is interacted with the covariates. Statistically significant interaction coefficients identify the \\(X\\) variables for which the treatment has differential impact.\nMathematically, when all variables are properly interacted, this is analogous to splitting the sample into subgroups based on \\(X\\) and running OLS on each group separately. This is feasible and convenient when \\(X\\) is binary or categorical, but not when it is continuous. The advantage of this approach, however, is that the linear regression spits out p-values which can be (mis)used to determine statistical significance of these interaction variables.\nThe OLS model is then:\n\\[Y = \\beta_1 T + \\beta_2 X + \\beta_3 X \\times T + \\epsilon,\\]\nwhere \\(\\epsilon\\) is the error term. The attention here falls on the coefficient vector $_3 $ which identifies whether the treatment has had a differential impact on units with a particular characteristic \\(X\\).\nIn high-dimensional settings when we have access to a wide vector \\(X\\), this is again not feasible. Instead, we might want to use an algorithm to pick out the variables in \\(X\\) which are important for the treatment effect heterogeneity.\nImai and Ratkovic (2013) show us how to adapt the Lasso to this setting. It turns out you should not simply throw this model into the Lasso loss function. Can you guess why? Some variables might be predictive of the baseline outcome while others only of the treatment effect heterogeneity. The trick is to have two separate lasso constraints ‚Äì \\(\\lambda_1\\) and \\(\\lambda_2\\).\nSo, the loss function looks something like this:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-f(X,T)\\beta_1 - X\\times T \\beta_2 ||_2^2 + \\lambda_1||\\beta_1||_1 + \\lambda_2||\\beta_2||_1 \\}.\\]\nA simplified version of the algorithm is as follows:\n\nGenerate interaction variables, \\(\\tilde{X}(T)\\).\nRun Lasso of \\(Y\\) on \\(T\\), \\(X\\) and \\(\\tilde{X}(T)\\).\nIdentify statistically significant variables determining treatment effect heterogeneity.\n\nTwo notes. First, this requires some assurance that we are not overfitting, so that some form of sample splitting or cross validation is necessary. On top of this, Athey and Imbens (2017) suggest comparing these results with post-lasso OLS estimates to further guard against overfitting. Second, multiple testing is an issue as is the case with ML algorithms more generally. (You can check my earlier post on multiple hypothesis testing in linear machine learning models.) Options to take care of this include sample splitting and bootstrap, among others.\nSoftware Package: FindIt.\n\n\nLasso with Knockoffs\nAn alternative approach directly regresses the unit-level treatment effects on \\(X\\). To get there, we first have to model the outcome function and impute the missing potential outcome for each unit. See my previous post on using Machine Learning tools for causal inference for more information on how we might do that.\nThis approach was developed by Xie et al.¬†(2018). They recognized the multiple hypothesis issue and suggested using knockoffs to control the False Discovery Rate. This is still not completely kosher as it does not account for the fact that the outcome variable is estimated in an earlier step. Oh well. My guess and hope are that this is more of a theoretical concern, and empirically the inference we get is still ‚Äúcorrect.‚Äù\nHere is a simplified version of their algorithm:\n\nTransform the outcome variable \\(\\tilde{Y}=Y\\times\\frac{(T-p)}{p(1-p)}\\).\nCalculate unit-level treatment effects, \\(\\hat{Y}(1)-\\hat{Y}(0)\\).\nGenerate the difference \\(Y^*=\\tilde{Y}-\\frac{1}{N}\\sum \\hat{Y}(1)-\\hat{Y}(0)\\).\nRun Lasso of \\(Y^*\\) on \\(X\\) and \\(X^*\\) (the knockoff counterparts of \\(X\\)).\nFollow the knockoff method to obtain the set of significant variables.\n\nRemember that \\(\\tilde{Y}\\) has the special property that \\(E[\\tilde{Y} \\mid X=x]=\\text{HTE}(X)\\) under the unconfoundedness assumption.\nInterestingly, in the special case when \\(p=1/2\\) the algebra reduces further which provides computation scaling advantages. Tian et al.¬†(2014) showed this result first."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#an-example",
    "href": "blog/lasso-heterogeneous-effects.html#an-example",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate the latter method. As such, it is a mere depiction of the approach, and its results should certainly not be taken seriously.\nThe outcome variable was survived, and the treatment variable was male. As is well known, women were much more likely to survive than men (\\(74\\%\\) vs \\(19\\%\\) in this sample). So, I analyzed whether the gender difference in survival was impacted by other factors. I included the following covariates ‚Äì pclass (ticket class), age, sibsp (number of siblings aboard), parch (number of parents aboard), fare, embarked (port of Embarkation), and cabin. Some of these were categorical in which case I converted them to a bunch of binary variables.\nThe knockoff filter identified a single variable as having a significant impact on the treatment effect ‚Äì pclass. For the more affluent passengers (i.e., those in the higher ticket classes), this gender difference in survival was much smaller.\nYou can find the code in this GitHub repo."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#bottom-line",
    "href": "blog/lasso-heterogeneous-effects.html#bottom-line",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe core idea behind using Lasso in HTE estimation is to leverage \\(L_1\\) regularization to select which covariates explain differences in treatment responses.\nThere are two main ways researchers use Lasso to estimate HTEs. Use a linear model with all covariates interacted with the treatment indicator, and apply Lasso with two separate regularization constraints. Directly regress unit-level treatment effects on the covariates.\nI am not aware of simulation studies comparing both approaches.\nWhile Lasso is among the simplest and most popular machine learning algorithms, more suitable methods may exist for estimating HTEs."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#where-to-learn-more",
    "href": "blog/lasso-heterogeneous-effects.html#where-to-learn-more",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nHu (2022) is an excellent summary of the several ways researchers use Machine Learning to uncover heterogeneity in treatment effect estimation."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#references",
    "href": "blog/lasso-heterogeneous-effects.html#references",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics: 2055-2085.\nBarber, R. F., & Cand√®s, E. J. (2019). A knockoff filter for high-dimensional selective inference. The Annals of Statistics, 47(5), 2504-2537.\nChatterjee, A., & Lahiri, S. N. (2011). Bootstrapping lasso estimators. Journal of the American Statistical Association, 106(494), 608-625.\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. The Annals of Applied Statistics (2013): 443-470.\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\nMeinshausen, N., Meier, L., & B√ºhlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nXie, Y., Chen, N., & Shi, X. (2018). False discovery rate controlled heterogeneous treatment effect detection for online controlled experiments. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp.¬†876-885)."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html",
    "href": "blog/conformal-inference-var-selection.html",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "",
    "text": "Many machine learning (ML) methods operate as opaque systems, generating predictions when given a dataset as input. Identifying which variables have the greatest impact on these predictions is often crucial. This adds a touch of interpretability and transparency and aids stakeholders in better understanding the relevant context. Examples abound. For instance, identifying the house attributes most important for predicting home prices, the school or hospital characteristics most strongly associated with better students‚Äô and patients‚Äô outcomes, etc.\nConformal inference offers a novel way of measuring variable importance in ML. In an earlier article I introduced conformal inference as a tool for generating confidence intervals when making predictions for new observations, and here I will describe how we can adapt it to the context of feature importance. The approach is thus similar in spirit to the Gini Importance-based methods mentioned above."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#background",
    "href": "blog/conformal-inference-var-selection.html#background",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "",
    "text": "Many machine learning (ML) methods operate as opaque systems, generating predictions when given a dataset as input. Identifying which variables have the greatest impact on these predictions is often crucial. This adds a touch of interpretability and transparency and aids stakeholders in better understanding the relevant context. Examples abound. For instance, identifying the house attributes most important for predicting home prices, the school or hospital characteristics most strongly associated with better students‚Äô and patients‚Äô outcomes, etc.\nConformal inference offers a novel way of measuring variable importance in ML. In an earlier article I introduced conformal inference as a tool for generating confidence intervals when making predictions for new observations, and here I will describe how we can adapt it to the context of feature importance. The approach is thus similar in spirit to the Gini Importance-based methods mentioned above."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#notation",
    "href": "blog/conformal-inference-var-selection.html#notation",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs begin by setting up some notation. We have a size \\(n\\) i.i.d. random sample of a feature vector \\(X\\) and an outcome \\(Y\\). The focus of conformal inference is on constructing a ‚Äúconfidence interval‚Äù for predicting a new observation \\(Y_{n+1}\\) given a new feature realization \\(X_{n+1}\\). I denote the estimate of the mean function by \\(\\hat{\\mu}\\) and the same estimate when removing feature \\(j\\) from $X $ by \\(\\hat{\\mu}_{-j}\\).\nPlease refer to my previous article for more details on the conformal inference framework, methodology and its properties."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#diving-deeper",
    "href": "blog/conformal-inference-var-selection.html#diving-deeper",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nRefresher on Variable Importance\nThe idea of measuring which variables contribute most to a prediction model is not new. The data scientist‚Äôs toolbox contains some useful techniques designed to measure variable importance in ML models. Popular choices include:\n\nGini Importance and Information Gain in tree-based models (e.g., random forest, gradient boosting) measure the decrease in various within-leaf impurity indexes caused by excluding a certain variable. The larger the loss, the more important the variable.\nSHAP Values use a cooperative game-theoretic approach to measure each variable‚Äôs contribution to the final model‚Äôs prediction.\nPermutation Importance assesses a variable‚Äôs significance by randomly shuffling its values and comparing the change in the model‚Äôs performance. The larger the drop, the more important the variable.\nVariable Coefficients in linear ML models (e.g., Lasso, Ridge) can directly signal importance. This requires an appropriate standardization before fitting the model (to make sure all features are on a level playing field).\n\n\n\nConformal Inference & Variable Importance\nWe can measure the prediction error associated with dropping a feature j when predicting a new observation Y_{n+1} by:\n\\[\\Delta_j^{n+1} = |Y_{n+1} - \\hat{\\mu}_{-j}(X_{n+1})| - |Y_{n+1}-\\hat{\\mu}(X_{n+1})|.\\]\nThe main idea is to use conformal inference ideas to construct a confidence interval for this prediction loss, \\(\\Delta_j^{n+1}\\), as a signal whether that variable is relevant in predicting the outcome.\nSpecifically, let \\(CI(\\cdot)\\) denote the conformal inference interval for \\(Y_{n+1}\\) given \\(X_{n+1}\\). Then, the interval\n\\[S_j(x)=\\{ |y-\\hat{\\mu}_{-j}(x)|-|y-\\hat{\\mu}(x)| : y \\in CI(x) \\}\\]\n. has a valid finite-sample coverage in the sense that:\n\\[ P(\\Delta_j^{n+1} \\in S_j(X_{n+1})) \\geq 1-\\alpha, \\]\nwhere \\(\\alpha\\) is a pre-specified significance level. This holds for all \\(j\\).\nWe can plot the confidence intervals \\(S_j(X_i)\\) for \\(i=1 \\dots n\\) and roughly interpret them as measuring variable importance. The closer the intervals are to zero, the less important the variable is for predicting new outcomes. The opposite is true as well. The further and more often it is away from zero, the more important the variable.\nAnother, more global, approach to using conformal inference for variable importance focuses on the distribution of \\(\\Delta_j(X_{n+1}, Y_{n+1})\\) and conducts hypothesis testing on its median or mean. Intuitively, failing to reject a hyptohesis that these statistics are non-zero is evidence that variable \\(j\\) does not play a significant role in predicting \\(Y\\)."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#bottom-line",
    "href": "blog/conformal-inference-var-selection.html#bottom-line",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWhile many ML methods act as black boxes, attention often falls on measuring individual variable importance.\nConformal inference offers a new way for data scientists to quantify the influence of each variable to the model performance.\nThe main idea is to use conformal inference to construct a confidence interval for the loss in prediction accuracy associated with removing a feature from the dataset."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#where-to-learn-more",
    "href": "blog/conformal-inference-var-selection.html#where-to-learn-more",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nSee Section 6 in Lei et al.¬†(2018) and the references therein."
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#references",
    "href": "blog/conformal-inference-var-selection.html#references",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "References",
    "text": "References\nLei, J., G‚ÄôSell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094-1111.\nLei, J., Rinaldo, A., & Wasserman, L. (2015). A conformal prediction approach to explore functional data. Annals of Mathematics and Artificial Intelligence, 74, 29-43.\nShafer, G., & Vovk, V. (2008). A Tutorial on Conformal Prediction. Journal of Machine Learning Research, 9(3).\nVovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world (Vol. 29). New York: Springer."
  },
  {
    "objectID": "blog/variance-ps-matching.html",
    "href": "blog/variance-ps-matching.html",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "",
    "text": "Propensity score matching (PSM) is among the most popular methods for estimating causal effects with observational data. It lends its fame to both its power and simplicity. Under a certain set of commonly invoked assumptions, controlling for the propensity score alone (as opposed to the full set of covariates) is enough to remove bias between the treatment and control groups. The underlying idea of matching on the propensity score is incredibly simple and most data scientists can quickly estimate a treatment effect even from scratch.\nWhile estimating treatment effects is common and straightforward, calculating their variance along with the associated confidence intervals and p-values can be more challenging. So, how exactly do you compute the variance of PSM methods? In this article I will describe the most popular ways of doing that. My goal is to cover the intuition and I will spare the technical details. Interested readers can refer to the References section below for more detailed expositions.\nStatistical methods based on the propensity score function come in different flavors (e.g., weighting, matching, subclassification, etc.), but today I will focus on \\(1:1\\) or \\(1:N\\) matching. There are also several potential estimands of interest, such as ATE (average treatment effect), ATT (ATE for the treatment group) but I will talk specifically about the latter."
  },
  {
    "objectID": "blog/variance-ps-matching.html#background",
    "href": "blog/variance-ps-matching.html#background",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "",
    "text": "Propensity score matching (PSM) is among the most popular methods for estimating causal effects with observational data. It lends its fame to both its power and simplicity. Under a certain set of commonly invoked assumptions, controlling for the propensity score alone (as opposed to the full set of covariates) is enough to remove bias between the treatment and control groups. The underlying idea of matching on the propensity score is incredibly simple and most data scientists can quickly estimate a treatment effect even from scratch.\nWhile estimating treatment effects is common and straightforward, calculating their variance along with the associated confidence intervals and p-values can be more challenging. So, how exactly do you compute the variance of PSM methods? In this article I will describe the most popular ways of doing that. My goal is to cover the intuition and I will spare the technical details. Interested readers can refer to the References section below for more detailed expositions.\nStatistical methods based on the propensity score function come in different flavors (e.g., weighting, matching, subclassification, etc.), but today I will focus on \\(1:1\\) or \\(1:N\\) matching. There are also several potential estimands of interest, such as ATE (average treatment effect), ATT (ATE for the treatment group) but I will talk specifically about the latter."
  },
  {
    "objectID": "blog/variance-ps-matching.html#diving-deeper",
    "href": "blog/variance-ps-matching.html#diving-deeper",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nWe can classify the methods for estimating the PSM variance in three distinct buckets.\n\nAsymptotic Approximations\nRecall the Law of Total Variance stating that, given a conditioning variable \\(X\\), we can always decompose a random variable \\(Y\\)‚Äôs variance, into two components ‚Äì the expectation of the conditional variance and the variance of the conditional expectation: \\[Var(Y) = Var(E(Y|X)) + E(Var(Y|X)).\\]\nAbadie and Imbens (2006) use this idea to derive the asymptotic variance formula for one-to-one and one-to-many matching estimators. They provide an analytical expression for the variance as \\(n \\to \\infty\\) (and hence, this is an approximation) using the matching covariates as conditioning variables. I will not be going into their work in detail, but a couple of notes are worth discussing.\nThe original formulation assumes the true propensity score function is known. This is rarely true and in practice we rely on an estimate of it. Practitioners then replace this true propensity score with the estimated one. This is OK, but not great ‚Äì it brings me to the second issue.\nOne needs to account for the fact that this propensity score is estimated. As such, it comes with uncertainty, which, if ignored, can potentially understate the variance of your estimator. Hello, type 1 errors!\nIn follow-up work, Abadie and Imbens (2016) propose correction which accounts for this uncertainty. Interestingly, in a work that I will discuss below, Bodory et al.¬†(2020) report that in practice this correction might increase or decrease the ATT variance.\n\n\nApproximation Based on Weights\nWe can express all treatment effect estimators as a difference of weighted means:\n\\[\\hat{\\tau} = \\frac{1}{n} \\sum\\hat{w} D Y - \\frac{1}{n} \\sum\\hat{w} (1-D) Y,\\]\nwhere the weights \\(\\hat{w}\\) may depend on the propensity score.\nLechner (2002) suggested calculating the variance of PSM based on this formula, assuming the weights are non-stochastic. In other words, the approach ignores the fact that the propensity score is estimated in a first step. The PSM variance is then equal to the sum of two terms ‚Äì the one for the treated and the one for the control.\n\n\nBootstrap\nBootstrap is the go-to method for estimating variances in complex settings where analytical expressions are too messy or even do not exist. PSM seems like a natural environment where the bootstrap can help. Not so fast!\nAbadie and Imbens (2006) show that the standard nonparametric bootstrap is inconsistent for non-smooth estimators (such as propensity score matching) with fixed number of matches and continuous covariates. Nevertheless, practitioners routinely ignore this result. While the theory says we should not use the bootstrap here, the extent to which this is of practical relevance is unclear. For instance, if this only makes a difference in the fourth decimal, we might be willing to sacrifice some bias for ease of computation.\nThere are at least two ways one can go around using the bootstrap here. The first one is the standard plain vanilla approach.\n\nRandomly draw B samples of size \\(n\\).\n\n\nAlternatively, you can also sample directly from the matched pairs which works better in certain cases.\n\n\nCompute ATT each time.\nCompute the confidence interval: \\[ \\hat{\\tau}\\pm \\sqrt{\\hat{V}(\\hat{\\tau}) \\times c}, \\]\n\nwhere \\(\\hat{V}(\\tau) = \\frac{1}{B-1}\\sum_{b=1}^B(\\hat{\\tau}^b-\\frac{1}{B}\\sum_b\\hat{\\tau}^b)^2\\) is the bootstrap variance of \\(\\hat{\\tau}}\\) and \\(c\\) is the critical value associated with a confidence level \\(\\alpha\\). Alternatively, in this last step we can directly use the$ $ and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution of \\(\\hat{\\tau}\\).\nThe second approach uses the well-known fact that the bootstrap behaves better when it uses so called asymptotically pivotal statistics ‚Äì i.e., ones which asymptotic distribution does not depend on unknown quantities. In this setting a candidate would be the t-statistic which, as we know, under certain conditions has a standard normal asymptotic distribution. We proceed in three steps:\nCompute the t-stat in the main sample using one of the variance approximations. Draw B random samples of size \\(n\\) and we compute the recentered \\(t\\)-stat with respect to \\(\\hat{\\tau}\\) in the main sample, \\[T^b = \\frac{\\hat{\\tau}^b - \\hat{\\tau}}{\\sqrt{\\hat{V}(\\tau^b)}}.\\]\nThe \\(p\\)-value is the share of (absolute value) bootstrap t-stats larger than the absolute value of the t-stat in the main sample \\[p{\\text{-value}} = 1 - \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|\\leq |T|) = \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|&gt; |T|),\\]\nwhere \\(T\\) is the \\(t\\)-stat from the main sample.\nThere are also newer (wild) bootstrap ideas which are still making their way into the mainstream."
  },
  {
    "objectID": "blog/variance-ps-matching.html#monte-carlo-simulations",
    "href": "blog/variance-ps-matching.html#monte-carlo-simulations",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Monte Carlo Simulations",
    "text": "Monte Carlo Simulations\nBodory et al.¬†(2020) compare the finite sample performance of several of these methods (plus others). Their analysis is more thorough in the sense that they include other ATT estimators besides PSM, such as PS weighting or radius matching.\nOverall, their results do not indicate a clear winner which performs best in all scenarios. Interestingly, even some of the bootstrap methods often outperform the asymptotic approximations of Abadie and Imbens (2006) or Lechner (2002)."
  },
  {
    "objectID": "blog/variance-ps-matching.html#where-to-learn-more",
    "href": "blog/variance-ps-matching.html#where-to-learn-more",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nI based this post on Bodory et al.¬†(2020)‚Äôs paper, so that is a natural place to start. Many other studies have focused on the performance of propensity score treatment effect estimators (as opposed to their variance) ‚Äì Huber et al.¬†(2013) and Busso et al.¬†(2014) are great starting points. Lastly, Imbens (2015) is a great resource for an overview of matching methods more generally along with some of the problems they solve as well as the pitfalls they entail."
  },
  {
    "objectID": "blog/variance-ps-matching.html#bottom-line",
    "href": "blog/variance-ps-matching.html#bottom-line",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThere is no shortage of methods when it comes to estimating the variance of propensity score matching estimators.\nAbadie and Imbens (2006) showed that the standard bootstrap is inconsistent in this setting with continuous covariates and formally derived an asymptotic approximation of the true variance.\nMonte Carlo simulations, however, show that bootstrap methods offer a good tradeoff between simplicity and performance."
  },
  {
    "objectID": "blog/variance-ps-matching.html#references",
    "href": "blog/variance-ps-matching.html#references",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "References",
    "text": "References\nAbadie, A., & Imbens, G. W. (2006). Large sample properties of matching estimators for average treatment effects. Econometrica, 74(1), 235-267.\nAbadie, A., & Imbens, G. W. (2008). On the failure of the bootstrap for matching estimators. Econometrica, 76(6), 1537-1557.\nAustin, P. C., & Small, D. S. (2014). The use of bootstrapping when using propensity‚Äêscore matching without replacement: a simulation study. Statistics in medicine, 33(24), 4306-4319.\nBodory, H., Camponovo, L., Huber, M., & Lechner, M. (2020). The finite sample performance of inference methods for propensity score matching and weighting estimators. Journal of Business & Economic Statistics, 38(1), 183-200.\nBusso, M., DiNardo, J., & McCrary, J. (2014). New evidence on the finite sample properties of propensity score reweighting and matching estimators. Review of Economics and Statistics, 96(5), 885-897.\nCaliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of propensity score matching. Journal of economic surveys, 22(1), 31-72.\nHuber, M., Lechner, M., & Wunsch, C. (2013). The performance of estimators based on the propensity score. Journal of Econometrics, 175(1), 1-21.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nLechner, M. (2002). Some practical issues in the evaluation of heterogeneous labour market programmes by matching methods. Journal of the Royal Statistical Society: Series A (Statistics in Society), 165(1), 59-82."
  },
  {
    "objectID": "blog/gradient-boosting.html",
    "href": "blog/gradient-boosting.html",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "",
    "text": "Gradient boosting has emerged as one of the most powerful techniques for predictive modeling. In its simplest form, we can think of gradient boosting like having a team of detectives working in sequence, where each new detective specifically focuses on solving the cases where their predecessors stumbled. Each detective contributes their findings to the investigation, with earlier ones catching obvious clues and later ones piecing together the subtle evidence that was initially missed. The final solution emerges from combining all their work.\nWhile the term ‚Äúgradient boosting‚Äù is often used generically, there are notable differences among its implementations‚Äîeach with unique strengths, weaknesses, and practical applications. Understanding these nuances is essential for advanced data scientists aiming to choose the best method for their specific datasets and computational constraints.\nThis article aims to provide a brief overview of the most popular gradient boosting methods, delving into their mathematical foundations and unique characteristics. By the end, you will have a clearer understanding of when and how to use each method to achieve the best results in your predictive modeling tasks. At the end of the article, I‚Äôll provide a step-by-step Python example that implements these algorithms."
  },
  {
    "objectID": "blog/gradient-boosting.html#background",
    "href": "blog/gradient-boosting.html#background",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "",
    "text": "Gradient boosting has emerged as one of the most powerful techniques for predictive modeling. In its simplest form, we can think of gradient boosting like having a team of detectives working in sequence, where each new detective specifically focuses on solving the cases where their predecessors stumbled. Each detective contributes their findings to the investigation, with earlier ones catching obvious clues and later ones piecing together the subtle evidence that was initially missed. The final solution emerges from combining all their work.\nWhile the term ‚Äúgradient boosting‚Äù is often used generically, there are notable differences among its implementations‚Äîeach with unique strengths, weaknesses, and practical applications. Understanding these nuances is essential for advanced data scientists aiming to choose the best method for their specific datasets and computational constraints.\nThis article aims to provide a brief overview of the most popular gradient boosting methods, delving into their mathematical foundations and unique characteristics. By the end, you will have a clearer understanding of when and how to use each method to achieve the best results in your predictive modeling tasks. At the end of the article, I‚Äôll provide a step-by-step Python example that implements these algorithms."
  },
  {
    "objectID": "blog/gradient-boosting.html#notation",
    "href": "blog/gradient-boosting.html#notation",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Notation",
    "text": "Notation\nI assume a mathematical familiarity with machine learning (ML) basics and some minimal previous exposure to gradient boosting. If you need a refresher, grab any introductory ML textbook.\nBefore diving into the specifics of each method, let‚Äôs establish some common notation that will be used throughout this article:\n\n\\(\\mathbf{X}\\): Covariates/features matrix\n\\(\\mathbf{y}\\): Outcome/target variable\n\\(f(x)\\): Predictive model\n\\(L(y, \\hat{y})\\): Loss function\n\\(\\hat{y}\\): Predicted outcome/target value\n\\(\\gamma\\): Learning rate\n\\(n\\): Number of observations/instances\n\\(M\\): Number of algorithm iterations."
  },
  {
    "objectID": "blog/gradient-boosting.html#diving-deeper",
    "href": "blog/gradient-boosting.html#diving-deeper",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nRefresher on Gradient Boosting\nGradient boosting is an ensemble machine learning technique that builds models sequentially, each new model attempting to correct the errors of the previous ones. The general idea is to minimize a loss function by adding weak learners (typically short decision trees) in a stage-wise manner to arrive at a single strong learner. This iterative process allows the model to improve its performance gradually, making it highly effective for complex datasets. Gradient boosting methods are versatile in that they can be used both for regression and classifications problems. In the most common case when the weak learners are decision trees, the algorithm is known as gradient tree boosting.\nMathematically, the model is built as follows:\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m = 1\\) to \\(M\\):\n\n\nCompute the pseudo-residuals: \\(r_{im} = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}\\) for \\(i=1,\\dots,n\\). In regression tasks this is simply \\(y-\\hat{y}\\).\nFit a weak learner such as a tree, \\(h_m(x)\\), on \\(\\{(x_i, r_{im}) \\}_{i=1}^n.\\)\nCompute \\(\\gamma\\) by solving: \\(\\gamma=\\arg\\min\\sum_i L(y_i, f_{m-1}(x_i)+\\gamma h_m(x_i)).\\)\nUpdate the model: \\(f_m(x) = f_{m-1}(x) + \\gamma h_m(x).\\)\n\n\nThe final model is \\(f_M(x).\\)\n\nThis is the most generic recipe for a gradient boosting algorithm. Let‚Äôs now focus on more specific implementations of this idea.\n\n\nAdaBoost\nAdaBoost, short for Adaptive Boosting, is one of the earliest boosting algorithms. It adjusts the weights of incorrectly classified observations so that subsequent learners focus more on difficult ones. In other words, it works by learning from its mistakes ‚Äì after each round of predictions, it identifies which observartions it got wrong and pays special attention to them (i.e., gives them a higher weight) in the next round. Thus, AdaBoost is not strictly speaking a gradient boosting algorithm, in the modern sense of the term.\nLet‚Äôs consider a binary classification problem where the outcome variable takes values in \\(\\{ -1, 1\\}\\). Here is the general AdaBoost algorithm:\n\nInitialize weights \\(w_i = \\frac{1}{n}\\) for \\(i = 1, \\ldots, n.\\)\nFor \\(m = 1 \\dots M\\):\n\n\nTrain a weak learner \\(h_m(x)\\) using weights \\(w_i\\).\nCompute the weighted error: \\(\\epsilon_m = \\frac{\\sum_{i=1}^{n} w_i \\mathbb{I}(y_i \\neq h_m(x_i))}{\\sum_{i=1}^{n} w_i}\\), where \\(\\mathbb{I}(\\cdot)\\) is the indicator function.\nCompute the model weight:$ _m = ()$.\nUpdate and normalize the weights:$ w_i^{m+1}= w_i^{m} (-_m y_i h_m(x_i)))$, and \\(w_i^{m+1} = \\frac{ w_i^{m+1}}{\\sum_j  w_j^{m+1}} for i=1,\\dots,n\\).\n\n\nThe final model is \\(f(x)=sign \\left( \\sum_m \\alpha_m h_m(x) \\right)\\).\n\nAdaBoost is simple to implement while being relatively resistant to overfitting, making it especially effective for problems with clean, well-structured data. Its adaptive nature means it can identify and focus on the most challenging observations. However, AdaBoost has notable weaknesses: it‚Äôs highly sensitive to noisy data and outliers (since it increases weights on misclassified examples), can perform poorly when working with insufficient training data, and tends to be computationally intensive compared to newer, simpler algorithms.\nSoftware Packages. R: adabag, gbm. Python: scikit-learn.\n\n\nXGBoost\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting designed for speed and performance. It revolutionizes the method by using the Newton-Raphson method in function space, setting it apart from traditional gradient boosting‚Äôs simpler gradient descent approach. At its core, XGBoost leverages both the first and second derivatives of the loss function through a second-order Taylor approximation. This sophisticated approach enables faster convergence and more accurate predictions by making better-informed decisions about how to improve the model at each step. When building decision trees, XGBoost considers both the expected improvement and the uncertainty of potential splits, while simultaneously applying regularization to prevent overfitting.\nWith some simplifications, the regression version of the XGBoost is implemented as follows:\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m=1, \\dots M\\):\n\n\nCompute the gradients \\(grad_m(x_i)=\\left( \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right)_{f(x)=f_{m-1}(x)}\\) and hessians \\(hess_m(x_i)=\\left( \\frac{\\partial^2 L(y_i, f(x_i))}{\\partial^2 f(x_i)^2}\\right)_{f(x)=f_{m-1}(x)}\\) of the loss function for \\(i=1,\\dots,n\\).\nFit a weak learner such as a tree, \\(h_m(x)\\) on$ {(x_i, ) }_{i=1}^n$.\nUpdate the model \\(f_m(x)=f_{(m-1)} - \\alpha h_m(x).\\)\n\n\nThe final model is \\(f_M(x)=\\sum_m f_m(x)\\).\n\nXGBoost is ideal for large datasets and competitions where model performance is critical. It is also a good choice when you need a highly optimized and scalable solution as it is highly efficient, supports parallel and distributed computing. Nevertheless, this algorithm can be complex to tune, and may require significant computational resources for very large datasets.\nNext up is CatBoost.\nSoftware Packages: R: xgboost, Python: xgboost.\n\n\nCatBoost\nCatBoost, short for Categorical Boosting, is a gradient boosting library that handles categorical features efficiently. It uses ordered boosting to reduce overfitting and supports GPU training, making it both powerful and versatile. CatBoost modifies the standard gradient boosting algorithm by incorporating two novel techniques ‚Äì ordered boosting and target statistics for categorical features. Let‚Äôs examine each one in turn.\nRather than requiring preprocessing like one-hot encoding, CatBoost encodes categorical values based on the distribution of the target variable without introducing data leakage. This is achieved by encoding each data point as if it were unseen, preventing overfitting. Additionally, ordered boosting is a method that builds each new tree while treating each data point as ‚Äúout-of-fold‚Äù for itself. This helps reduce overfitting, particularly in small datasets, by preventing over-reliance on individual observations during training.\nWe now move on to a popular, faster gradient boosting implementation.\nSoftware Packages: R: catboost, Python: catboost.\n\n\nLightGBM\nLightGBM shares many of XGBoost‚Äôs benefits, such as support for sparse data, parallel training, multiple loss functions, regularization, and early stopping, but it also introduces a bunc of new features and improvements.\nFirst, rather than growing trees level-wise (row by row) as in most implementations, LightGBM grows trees leaf-wise, selecting the leaf that provides the greatest reduction in loss. Second, it does not use the typical sorted-based approach for finding split points, which sorts feature values to locate the best split. Instead, it relies on an efficient histogram-based method that significantly improves both speed and memory efficiency. Third, LightGBM incorporates the so-called Gradient-Based One-Side Sampling, which speeds up training by focusing on the most informative samples. Lastly, the algorithm uses Exclusive Feature Bundling which can group features to reduce dimensionality, enabling faster and more accurate model training.\nSoftware Packages: R: lightgmb, Python: lightgbm.\n\n\nChallenges\nGradinet boosting methods comes with a few common practical challenges. When a predicitve model learns the training data too precisely, it may perform poorly on new, unseen data ‚Äì a problem called overfitting. To combat this, various regularization methods are used to add helpful constraints to the learning process. A main parameter that regulates the model‚Äôs learning precision is the number of boosting rounds (M), which determines how many base models are created. While using more rounds reduces training errors, it also increases overfitting risk. To find the optimal M value, it‚Äôs common to use cross validation. The depth of decision trees is another important control parameter in tree boosting. Deeper trees can capture more complex patterns but are more prone to memorizing the training data rather than learning generalizable patterns.\nThe improved predictive performance of gradient boosting relative to simpler models comes at a cost of reduced model transparency. While a single decision tree‚Äôs reasoning can be easily traced and understood, tracking the decision-making process across numerous trees becomes extremely complex and challenging. Gradient boosting is an excellent example of the inherent tradeoff between model simplicity and performance.\nLet‚Äôs now look at how can we use these methods in practice."
  },
  {
    "objectID": "blog/gradient-boosting.html#an-example",
    "href": "blog/gradient-boosting.html#an-example",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "An Example",
    "text": "An Example\nHere is some sample python code illustrating the implementation of each algorithm described above on a common dataset. Let‚Äôs look at it in detail.\n# loading the necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\n# we load the data and split it into training and test parts.\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# define and implement the boosting algorithms. \nclassifiers = {\n    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n    \"XGBoost\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42),\n    \"LightGBM\": LGBMClassifier(n_estimators=100, random_state=42),\n    \"CatBoost\": CatBoostClassifier(n_estimators=100, verbose=0, random_state=42)\n}\n\n# save results\nresults = {}\nfor name, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = accuracy\nFinally, we print the accuracy results:\n\nfor name, accuracy in results.items():\n    print(f\"{name}: {accuracy:.4f}\")\n\n# results:\n&gt;AdaBoost: 0.9737\n&gt;XGBoost: 0.9561\n&gt;LightGBM: 0.9649\n&gt;CatBoost: 0.9649\nOverall each method performed reasonably well, with acuracy ranging from \\(95.6\\)% to \\(97.4\\)%. Interestingly, Adaboost outperformed the other more complex algorithms, at least in the in terms of accuracy.\nAnd that‚Äôs it. You are now familiar with the most popular implementations of gradient boosting along with their advantages and weaknesses. You also know how to employ them in practice. Have fun incorporating XGboost and the like into your predictive modeling tasks."
  },
  {
    "objectID": "blog/gradient-boosting.html#bottom-line",
    "href": "blog/gradient-boosting.html#bottom-line",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nGradient boosting is a powerful ensemble technique for predictive modeling that comes in a variety of flavors.\nAdaBoost focuses on misclassified instances by adjusting weights.\nXGBoost introduces regularization and optimization for speed and performance.\nCatBoost efficiently handles categorical features and reduces overfitting.\nLightGBM enjoys many of XGBoost‚Äôs strenghts while introducing a few novelties including a different way of building the underlying weak learners.\nCommon practical challenges when implementing gradient boosting include overfitting, decreased interpretability and computational costs."
  },
  {
    "objectID": "blog/gradient-boosting.html#where-to-learn-more",
    "href": "blog/gradient-boosting.html#where-to-learn-more",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great startint point, and it‚Äôs a resource I used extensively when preparing this article. ‚ÄúThe Elements of Statistical Learning‚Äù by Hastie, Tibshirani, and Friedman is a comprehensive guide that covers the theoretical foundations of machine learning, including gradient boosting. It is the de facto bible for statistical ML. While this book is phenomenal, it can be challenging for less technical practitioners for which I recommend its lighther versions, ‚ÄúAn Introduction to Statistical Learning‚Äù with R and Python code. All these books are available for free online. Lastly, if you want to dive even deeper into any of the algortihms descibe above, consider studying the papers in the References section below."
  },
  {
    "objectID": "blog/gradient-boosting.html#references",
    "href": "blog/gradient-boosting.html#references",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "References",
    "text": "References\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.¬†785-794).\nDorogush, A. V., Ershov, V., & Gulin, A. (2018). CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363.\nFreund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139.\nFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The elements of statistical learning: data mining, inference, and prediction.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An introduction to statistical learning: With applications in python. Springer Nature.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2013). An introduction to statistical learning: With applications in R. Springer Nature.\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ‚Ä¶ & Liu, T. Y. (2017). Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30.\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: unbiased boosting with categorical features. Advances in neural information processing systems, 31."
  },
  {
    "objectID": "blog/alphabet-het-treatment-effects.html",
    "href": "blog/alphabet-het-treatment-effects.html",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "",
    "text": "Numerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods ‚Äì the so-called \\(S\\)-, \\(T\\)-, \\(X\\)- and \\(R\\)-learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as the lasso, gradient boosting, or even neural networks. In some clever sense, \\(STXR\\) is the \\(ABC\\) of heterogeneous treatment effect estimation.\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The CausalML Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them."
  },
  {
    "objectID": "blog/alphabet-het-treatment-effects.html#background",
    "href": "blog/alphabet-het-treatment-effects.html#background",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "",
    "text": "Numerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods ‚Äì the so-called \\(S\\)-, \\(T\\)-, \\(X\\)- and \\(R\\)-learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as the lasso, gradient boosting, or even neural networks. In some clever sense, \\(STXR\\) is the \\(ABC\\) of heterogeneous treatment effect estimation.\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The CausalML Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them."
  },
  {
    "objectID": "blog/alphabet-het-treatment-effects.html#notation",
    "href": "blog/alphabet-het-treatment-effects.html#notation",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Notation",
    "text": "Notation\nAs usual, let‚Äôs begin by setting some mathematical notation. I use D to denote a binary treatment indicator, \\(Y\\) is the observed outcome and \\(X\\) is a covariate of interest. The potential outcomes under each treatment state are \\(Y(0)\\) and \\(Y(1)\\), and \\(p\\) is the (conditional) probability of assignment into the treatment (i.e., the propensity score).\nThe average treatment effect is then the mean difference in potential outcomes across all units:\n\\[ATE = E[Y(1)-E(0)].\\]\nInterest is, instead, in the ATE for units with values \\(X=x\\) which I refer to as the heterogeneous treatment effect, \\(HTE(X)\\):\n\\[HTE(X) = E[Y(1)-E(0)|X].\\]\nIt is also helpful to define the conditional outcome functions under each treatment state:\n\\[\\mu(X,d) = E[Y(d)|X].\\]\nIt then follows that \\(HTE(X)\\) can also be expressed as:\n\\[HTE(X) = \\mu(X,1) - \\mu(X,0).\\]"
  },
  {
    "objectID": "blog/alphabet-het-treatment-effects.html#diving-deeper",
    "href": "blog/alphabet-het-treatment-effects.html#diving-deeper",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nI will now briefly describe the four learners for heterogeneous treatment effects in ascending order of complexity.\n\n\\(S\\)(ingle) Learner\nThe idea behind the S learner is to estimate a single outcome function (X,D) and then calculate (X) by taking the difference in the predicted values between the units in the treatment and control groups.\nAlgorithm:\n\nUse the entire sample to estimate \\(\\hat{\\mu}(X,D)\\).\nCompute \\(\\hat{\\text{HTE}}(X)=\\hat{\\mu}(X,1)-\\hat{\\mu}(X,0)\\).\n\nThis is intuitive and fine. But the problem is that the treatment variable D might be excluded in step 1 when it is not highly correlated with the outcome. (Note that in observational data this does not necessarily imply a null treatment effect.) In this case, we cannot even move to step 2.\n\n\n\\(T\\)(wo) Learner\nThe T learner solves the above problem by forcing the response models to include D. The idea is to first estimate two separate (conditional) outcome functions ‚Äì one for the treatment and one for the control and proceed similarly.\nAlgorithm:\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for \\(\\hat{\\mu}(X,1)\\).\nThen \\(\\hat{\\text{HTE}}(X)=\\hat{\\mu}(X,1)- \\hat{\\mu}(X,0)\\)\n\nThis is better, but still not great. A potential problem is when there are different number of observations in the treatment and control groups. For instance, in the common case where the treatment group is much smaller, their \\(\\text{HTE}(X)\\) will be estimated much less precisely than the that of the control group. When combining the two to arrive at a final estimate of \\(\\text{HTE}(X)\\) the former should get a smaller weight than the latter. This is because the ML algorithms optimize for learning the \\(\\mu(\\cdot)\\) functions, and not the \\(\\text{HTE}(X)\\) function directly.\n\n\n\\(X\\) Learner\nThe \\(X\\) learner is designed to overcome the above concern. The procedure starts similarly to the T learner but then weighs differently the \\(\\text{HTE}(X)\\)‚Äôs for the treatment and control groups.\nAlgorithm:\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for $ (X,1)$.\nEstimate the unit-level treatment effect for the observations in the control group, \\(\\hat{\\mu}(X,1)-Y\\), and for the treatment group, \\(Y-\\hat{\\mu}(X,0)\\).\nCombine both estimates by weighing them using the predicted propensity score, \\(\\hat{p}\\): \\[\\text{HTE}(X)=\\hat{p} \\times HTE(X|D=1) + (1-\\hat{p})\\times HTE(X|D=0).\\]\n\nHere \\(\\hat{p}\\) balances the uncertainty associated with the \\(\\text{HTE}(X)\\)‚Äôs in each group and hence, this approach is particularly effective when there is a significant difference in the number of units between the two groups.\n\n\n\\(R\\)(obinson) Learner\nTo avoid getting too deep into technical details, I will not be describing the entire algorithm. The R learner models both the outcome, and the propensity score and begins by computing unit-level predicted outcomes and treatment probabilities using cross-fitting and leave-one-out estimation. The key innovation is plugging these into a novel loss function featuring squared deviations of these predictions as well as a regularization term. This ensures ‚Äúoptimality‚Äù in learning the treatment effect heterogeneity."
  },
  {
    "objectID": "blog/alphabet-het-treatment-effects.html#bottom-line",
    "href": "blog/alphabet-het-treatment-effects.html#bottom-line",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nML methods offer a promising way of determining which groups of units experience differential response to treatments.\nI summarized four such model-agnostic methods ‚Äì the \\(S\\)-, \\(T\\)-, \\(X\\)-, and \\(R\\)-learners.\nCompared to the simpler \\(S\\)- and \\(T\\)- learners, the \\(X\\)- and \\(R\\)-learners solve some common issues and are more attractive options in most settings."
  },
  {
    "objectID": "blog/alphabet-het-treatment-effects.html#where-to-learn-more",
    "href": "blog/alphabet-het-treatment-effects.html#where-to-learn-more",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nI have previously written on how ML can be useful in causal inference, more generally and how we can use the Lasso to estimate heterogeneous treatment effects. Hu (2022) offers a detailed summary of a bunch of ML methods for HTE estimation. A Statistical Odds and Ends blog post describes the \\(S\\)-, \\(T\\)- and \\(X\\)-learners and contains useful advice on when each of them is preferrable. Chapter 21 of Causal Inference for the Brave and True also discusses this material and provides useful examples."
  },
  {
    "objectID": "blog/alphabet-het-treatment-effects.html#references",
    "href": "blog/alphabet-het-treatment-effects.html#references",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "References",
    "text": "References\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\nK√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165. Nie,\nXie, N., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319."
  },
  {
    "objectID": "blog/bootstrap-limitations.html",
    "href": "blog/bootstrap-limitations.html",
    "title": "The Bootstrap and its Limitations",
    "section": "",
    "text": "The bootstrap is a powerful resampling technique used to estimate the sampling distribution of a statistic. By repeatedly drawing observations with replacement from the original dataset, it enables practitioners to perform tasks like hypothesis testing, computing standard errors, and constructing confidence intervals‚Äîwithout relying on strong parametric assumptions about the underlying population.\nIt is particularly valuable when analytical expressions for the variance of an estimator are unavailable or computationally complex. Moreover, the bootstrap is grounded in robust statistical theory and offers versatile adaptations, such as the wild bootstrap, which is commonly used for estimating cluster-robust variance. This combination of methodological flexibility and statistical rigor has established the bootstrap as a central tool in modern data science.\nHowever, like any statistical method, the bootstrap has its limitations. This article examines scenarios where the bootstrap will yield unreliable results."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#background",
    "href": "blog/bootstrap-limitations.html#background",
    "title": "The Bootstrap and its Limitations",
    "section": "",
    "text": "The bootstrap is a powerful resampling technique used to estimate the sampling distribution of a statistic. By repeatedly drawing observations with replacement from the original dataset, it enables practitioners to perform tasks like hypothesis testing, computing standard errors, and constructing confidence intervals‚Äîwithout relying on strong parametric assumptions about the underlying population.\nIt is particularly valuable when analytical expressions for the variance of an estimator are unavailable or computationally complex. Moreover, the bootstrap is grounded in robust statistical theory and offers versatile adaptations, such as the wild bootstrap, which is commonly used for estimating cluster-robust variance. This combination of methodological flexibility and statistical rigor has established the bootstrap as a central tool in modern data science.\nHowever, like any statistical method, the bootstrap has its limitations. This article examines scenarios where the bootstrap will yield unreliable results."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#diving-deeper",
    "href": "blog/bootstrap-limitations.html#diving-deeper",
    "title": "The Bootstrap and its Limitations",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nConsider the following scenarios:\n\nVery Small Sample Sizes ‚Äì The bootstrap relies on resampling the observed data to approximate the population distribution. With very small samples, there is not enough variability in the data to accurately capture the underlying distribution, leading to unreliable estimates.\nParameter at the Edge of the Parameter Space ‚Äì When the parameter being estimated lies at or near a boundary (e.g., estimating a proportion close to 0 or 1), the bootstrap may fail to reflect the true sampling distribution. The resampling process cannot fully mimic the constraints of the parameter space. This includes situations in which we are interested in learning more about the minimum or maximum value of some statistic.\nPresence of Outliers ‚Äì Outliers can heavily influence bootstrap resamples, leading to biased or overly variable estimates.\nDependence in the Data ‚Äì The bootstrap assumes the data are independent and identically distributed (i.i.d.). For time series or spatial data where observations are dependent, naive application of the bootstrap can yield incorrect inferences unless adapted for the structure (e.g., block bootstrap).\nExtreme Skewness or Rare Events ‚Äì When the data distribution is highly skewed or dominated by rare events, the bootstrap may struggle to approximate the tails of the distribution accurately, affecting confidence interval coverage and tail probability estimates.\nMisspecified Models ‚Äì If the bootstrap is applied to a statistic derived from a poorly specified model, the resulting inferences will inherit the same flaws. The bootstrap cannot correct for model misspecification.\n\nIn some of these cases theoretical approximation methods can provide analytical solutions that bypass the resampling challenges. The parametric bootstrap is like a more structured cousin of the standard bootstrap, generating samples based on a known probability distribution. It‚Äôs particularly helpful when you‚Äôve got a good sense of what your data looks like. Bayesian methods take things a step further, folding in prior knowledge to handle tricky statistical scenarios with flexibility.\nWhile the bootstrap is a versatile and often reliable tool, awareness of these limitations can help you avoid potential pitfalls and ensure more robust statistical analyses."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#bottom-line",
    "href": "blog/bootstrap-limitations.html#bottom-line",
    "title": "The Bootstrap and its Limitations",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe bootstrap is incredible versatile, but it has its limitations.\nBeware in relying on it when facing any of the situations described above."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#where-to-learn-more",
    "href": "blog/bootstrap-limitations.html#where-to-learn-more",
    "title": "The Bootstrap and its Limitations",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great place to start. If you have nailed the basics and are looking for a technical challenge on the bootstrap, Efron and Hastie (2021) is a hidden gem on all things statistical inference, especially the bootstrap. Econometrics geeks might want to dive into James Mackinnon‚Äôs papers cited below for seriously deep details."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#references",
    "href": "blog/bootstrap-limitations.html#references",
    "title": "The Bootstrap and its Limitations",
    "section": "References",
    "text": "References\nEfron, B. (2000). The bootstrap and modern statistics. Journal of the American Statistical Association, 95(452), 1293-1296.\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press.\nEfron, B., & Tibshirani, R. J. (1994). An introduction to the bootstrap. Chapman and Hall/CRC.\nMacKinnon, J. G. (2006). Bootstrap methods in econometrics. Economic Record, 82, S2-S18.\nMacKinnon, J. G., & Webb, M. D. (2017). Wild bootstrap inference for wildly different cluster sizes. Journal of Applied Econometrics, 32(2), 233-254.\nMacKinnon, J. G., & Webb, M. D. (2020). Clustering methods for statistical inference. Handbook of labor, human resources and population economics, 1-37."
  },
  {
    "objectID": "blog/hypothesis-testing-all-data.html#a-closer-look",
    "href": "blog/hypothesis-testing-all-data.html#a-closer-look",
    "title": "Hypothesis Testing with Population Data",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nAn Example\nLet‚Äôs focus on a specific example ‚Äì homicides in the US. According to the Crime in the US report published by the FBI, in 2018, there were \\(14,123\\) homicides, and for 2019 this number was \\(13,927\\). This is a decrease of \\(196\\) cases, or roughly equivalent to a \\(1.4\\%\\) drop.\nMother nature and the world around us are incredibly complex, so numbers around us can go up and down for no obvious reason. So, does this drop reflect a real change in the underlying crime rate?\nTo answer this question, it is helpful to model annual homicides as coming from a Poisson distribution from a figurative population of alternative US histories. This distribution has a mean \\(\\lambda\\) equal to the hypothetical true underlying homicide rate. We want to know whether \\(\\lambda\\) changed from 2018 to 2019.\nThe confidence interval for the change in this underlying homicide rate is:\n\\[ (14,123-13,927) \\pm 1.96 \\times \\sqrt{14,123+13,927}=(-132.26, 524.26). \\]\nThis interval clearly contains \\(0\\), so we cannot conclude that there was a real drop in the crime rate between 2018 and 2019. In other words, the \\(1.4\\%\\) drop in homicides between 2018 and 2019 was within the range consistent with the noise in our world. It should not be confused with increased underlying safety in the US.\n\n\nOne More Thing\nThis type of thinking is also helpful in a slightly different context. Let‚Äôs focus on 2018, when there were \\(14,123\\) homicides in the US, corresponding to an average daily rate of about \\(38.7\\) cases.\nImagine someone asked us to calculate the probability that there would be less than \\(25\\) cases on a given day, but no such day took place in 2018. It would still be na√Øve to conclude that the probability of this event was zero.\nWe can look at the left tail of the Poisson distribution with mean \\(\\lambda = 38.7\\) to answer this question:\n\nRPython\n\n\nppois(25, lambda=38.7)\n&gt;[1] 0.01270669\n\n\nfrom scipy.stats import poisson\n\n# Define the mean of the Poisson distribution\nlambda_value = 38.7\n\n# Calculate the cumulative probability for less than 25 cases\nprobability = poisson.cdf(25, mu=lambda_value)\nprint(probability)\n&gt;[1] 0.01270669\n\n\n\nThis gives us a \\(1.27%\\) probability of such an event, suggesting that, on average, there should be about \\(4.6\\) such days per year. Data would not help answer this question."
  },
  {
    "objectID": "blog/DONE-hypothesis-testing-all-data.html",
    "href": "blog/DONE-hypothesis-testing-all-data.html",
    "title": "Hypothesis Testing with Population Data",
    "section": "",
    "text": "Classical statistical theory is built on the idea of working with a sample of data from a given population of interest. Our software packages compute confidence intervals to reflect precisely this ‚Äì we observe only a small part of that population.\nIn modern times, however, we often work with all data points, and not just random samples. Examples abound, especially in the tech industry. Companies store all sales and website activity, the FBI records all homicides, and school records contain information on all students.\nHow can we interpret confidence intervals when we work with such datasets? More generally, how do we think about uncertainty in these settings? We know exactly how many items are sold or how many homicides occur each year; nothing is uncertain about that.\nThe short answer is that the confidence intervals in this setting have a fundamentally different interpretation ‚Äì one reflecting parameters of an underlying metaphorical population."
  },
  {
    "objectID": "blog/DONE-hypothesis-testing-all-data.html#background",
    "href": "blog/DONE-hypothesis-testing-all-data.html#background",
    "title": "Hypothesis Testing with Population Data",
    "section": "",
    "text": "Classical statistical theory is built on the idea of working with a sample of data from a given population of interest. Our software packages compute confidence intervals to reflect precisely this ‚Äì we observe only a small part of that population.\nIn modern times, however, we often work with all data points, and not just random samples. Examples abound, especially in the tech industry. Companies store all sales and website activity, the FBI records all homicides, and school records contain information on all students.\nHow can we interpret confidence intervals when we work with such datasets? More generally, how do we think about uncertainty in these settings? We know exactly how many items are sold or how many homicides occur each year; nothing is uncertain about that.\nThe short answer is that the confidence intervals in this setting have a fundamentally different interpretation ‚Äì one reflecting parameters of an underlying metaphorical population."
  },
  {
    "objectID": "blog/DONE-hypothesis-testing-all-data.html#a-closer-look",
    "href": "blog/DONE-hypothesis-testing-all-data.html#a-closer-look",
    "title": "Hypothesis Testing with Population Data",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nAn Example\nLet‚Äôs focus on a specific example ‚Äì homicides in the US. According to the Crime in the US report published by the FBI, in 2018, there were \\(14,123\\) homicides, and for 2019 this number was \\(13,927\\). This is a decrease of \\(196\\) cases, or roughly equivalent to a \\(1.4\\%\\) drop.\nMother nature and the world around us are incredibly complex, so numbers around us can go up and down for no obvious reason. So, does this drop reflect a real change in the underlying crime rate?\nTo answer this question, it is helpful to model annual homicides as coming from a Poisson distribution from a figurative population of alternative US histories. This distribution has a mean \\(\\lambda\\) equal to the hypothetical true underlying homicide rate. We want to know whether \\(\\lambda\\) changed from 2018 to 2019.\nThe confidence interval for the change in this underlying homicide rate is:\n\\[ (14,123-13,927) \\pm 1.96 \\times \\sqrt{14,123+13,927}=(-132.26, 524.26). \\]\nThis interval clearly contains \\(0\\), so we cannot conclude that there was a real drop in the crime rate between 2018 and 2019. In other words, the \\(1.4\\%\\) drop in homicides between 2018 and 2019 was within the range consistent with the noise in our world. It should not be confused with increased underlying safety in the US.\n\n\nOne More Thing\nThis type of thinking is also helpful in a slightly different context. Let‚Äôs focus on 2018, when there were \\(14,123\\) homicides in the US, corresponding to an average daily rate of about \\(38.7\\) cases.\nImagine someone asked us to calculate the probability that there would be less than \\(25\\) cases on a given day, but no such day took place in 2018. It would still be na√Øve to conclude that the probability of this event was zero.\nWe can look at the left tail of the Poisson distribution with mean \\(\\lambda = 38.7\\) to answer this question:\n\nRPython\n\n\nppois(25, lambda=38.7)\n\n\nfrom scipy.stats import poisson\n\n# Define the mean of the Poisson distribution\nlambda_value = 38.7\n\n# Calculate the cumulative probability for less than 25 cases\nprobability = poisson.cdf(25, mu=lambda_value)\nprint(probability)\n\n\n\nThis gives us a \\(1.27%\\) probability of such an event, suggesting that, on average, there should be about \\(4.6\\) such days per year. Data would not help answer this question."
  },
  {
    "objectID": "blog/DONE-hypothesis-testing-all-data.html#bottom-line",
    "href": "blog/DONE-hypothesis-testing-all-data.html#bottom-line",
    "title": "Hypothesis Testing with Population Data",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWorking with all data eliminates the uncertainty that usually arises in random samples.\nConfidence intervals in such settings are still meaningful ‚Äì they represent uncertainty associated with the underlying parameters of a metaphorical population."
  },
  {
    "objectID": "blog/DONE-hypothesis-testing-all-data.html#where-to-learn-more",
    "href": "blog/DONE-hypothesis-testing-all-data.html#where-to-learn-more",
    "title": "Hypothesis Testing with Population Data",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThis article was inspired by ‚ÄúThe Art of Statistics‚Äù (2019) which beautifully explains an impressively wide range of statistical topics in an engaging way. It is a non-technical read accessible to everyone interested in combining statistics and data to make inferences about the world."
  },
  {
    "objectID": "blog/DONE-hypothesis-testing-all-data.html#references",
    "href": "blog/DONE-hypothesis-testing-all-data.html#references",
    "title": "Hypothesis Testing with Population Data",
    "section": "References",
    "text": "References\nThe FBI (2018) Crime in the US.\nThe FBI (2019) Crime in the US.\nSpiegelhalter, D. (2019). The art of statistics: Learning from data. Penguin UK."
  },
  {
    "objectID": "blog/three-classes-stat-models.html#a-closer-look",
    "href": "blog/three-classes-stat-models.html#a-closer-look",
    "title": "The Three Classes of Statistical Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nParametric Models\nParametric models impose strong assumptions about the underlying data-generating process, making them less flexible but highly interpretable and data-efficient. They require fewer data points to estimate the target accurately compared to semiparametric or nonparametric models. Consequently, they are often the default choice in practice.\nA classic example is density estimation. Suppose you observe that your data appears symmetric around its mean. You might assume the data follows a normal (Gaussian) distribution. In this case, specifying the mean \\(\\mu\\) and variance \\(\\sigma^2\\) fully characterizes the distribution, allowing you to estimate the density parametrically. And then, you can make all kinds of calculations related to your variable, such as what is the probability that it will take values greater than a given number \\(c\\).\nFormally, a parametric model describes the data-generating process entirely using a finite-dimensional parameter vector. As yet another example, in the omnipresent linear model:\n\\[Y = X\\beta + \\epsilon,\\]\nthe parameter specifies the entire relationship between \\(X\\) and \\(Y\\). This imposes a strong assumption: the relationship is linear, meaning a unit change in \\(X\\) consistently results in a \\(\\beta\\) change in \\(Y\\), regardless of \\(X\\)‚Äôs magnitude (small or large). While convenient, this assumption may not hold in all practical scenarios, as real-world relationships often are complex and deviate from linearity.\n\n\nSemiparametric Models\nSemiparametric models combine the structure of parametric models with the flexibility of nonparametric methods. They specify certain aspects of the data-generating process parametrically while leaving other aspects unspecified or modeled flexibly.\nConsider the partially linear model:\n\\[Y = X \\beta + g(Z) + \\epsilon,\\]\nwhere \\(\\beta\\) is a parametric component describing the linear effect of \\(X\\), while \\(g(Z)\\) is an unknown and potentially complex function capturing the nonparametric effect of a covariate matrix \\(Z\\). Here, the model imposes linearity on \\(X\\)‚Äôs effect but allows \\(Z\\)‚Äôs effect to be fully flexible.\nSemiparametric models are highly versatile, balancing the interpretability of parametric models with the adaptability of nonparametric ones. However, estimating \\(\\beta\\) efficiently while accounting for the unknown \\(g(Z)\\) poses challenges, often requiring further assumptions.\nIn practice, semiparametric models play a crucial role in causal inference, particularly when working with observational cross-sectional data (see Imbens and Rubin 2015). A common strategy involves estimating propensity scores using parametric models (like logistic regression) and then employing nonparametric techniques such as kernel weighting, matching, or stratification to estimate treatment effects. This hybrid approach helps address confounding while maintaining computational tractability and interpretability.\n\n\nNonparametric Models\nNonparametric models make minimal assumptions about the underlying data-generating process, allowing the data to ‚Äúspeak for itself‚Äù. Unlike parametric and semiparametric models, these ones do not specify a finite-dimensional parameter vector or impose rigid structural assumptions. This flexibility makes them highly robust to model misspecification but often requires larger datasets to achieve accurate estimates.\nLet‚Äôs get back to the density estimation example. Histograms are a form of nonparametric models as they do not assume specific underlying functional form. Instead of fitting a parametric distribution (e.g., normal), you might use a kernel density estimator:\n\\[\\hat{f}(x)=\\frac{1}{nh}\\sum_i K \\left( \\frac{x-X_i}{h} \\right),\\]\nwhere \\(K(\\cdot)\\) is a kernel function, and \\(h\\) is a bandwidth parameter controlling the smoothness of the estimate. –ê kernel function assigns varying weight to observations around a data point. They are non-negative, symmetric around zero, and sum to one. Commonly employed kernel functions include:\n\nGaussian \\(K(\\cdot)=\\frac{1}{\\sqrt{2 \\pi}}e^{-0.5x^2}\\),\nEpanechnikov \\(K(\\cdot)=\\frac{3}{4}(1-x^2)\\),\nRectangular: \\(K(\\cdot)=0.5\\).\n\nThis approach does not rely on assumptions about the data‚Äôs shape, allowing it to adapt to various distributions.\nNonparametric regression provides another illustration. In it, the relationship between \\(X\\) and \\(Y\\) is modeled as:\n\\[Y=m(X)+\\epsilon,\\]\nwhere \\(m(X)\\) is an unknown function estimated directly from the data using methods like local polynomial regression, splines. Unlike the linearity assumption in parametric models, nonparametric regression allows \\(m(X)\\) to capture complex, nonlinear relationships. A commonly used variant of this is LOESS regression often overlayed in bivariate scatterplots.\nMany popular machine learning (ML) methods are also nonparametric. In this context it is important to distinguis between parameters which impose assumptions on the data and hyperparameters which serve to tune the algorithm. Tree-based techniques exemplify the nonparametric nature of ML: gradient boosting machines, random forests, and individual decision trees can all grow more complex as they encounter more data, creating flexible models that capture intricate patterns. Think even of unsupervised clustering models including \\(k\\)-means and hierarchical clustering.\nWhile nonparametric models are powerful, their flexibility comes at a cost: they can overfit small datasets and often require careful tuning (e.g., choosing the kernel bandwidth) to balance bias and variance. Nonetheless, they are invaluable for exploratory analysis and applications where minimal assumptions are desired.\nDid I also mention the curse of dimensionality?! You will quickly fall into its trap with even moderate number of variables. Yes, there is just so much you can do without adding some structure to your models."
  },
  {
    "objectID": "blog/DONE-three-classes-stat-models.html",
    "href": "blog/DONE-three-classes-stat-models.html",
    "title": "The Three Classes of Statistical Models",
    "section": "",
    "text": "Statistical modeling is among the most exciting elements of working with data. When mentoring junior data scientists, I never fail to see the spark in their eyes when our projects have overcome the data gathering and cleaning stages and have reached that precious Rubicon.\nA major goal in the craft of statistical modeling is extracting the most information from the data at hand. Statisticians and econometricians call this efficiency or precision. In simple terms, this means achieving the lowest possible variance from a class of available estimators. As an example, imagine two findings‚Äîboth centered at \\(3\\%\\) (e.g., the treatment effect of an intervention of interest)‚Äîbut one with a \\(95\\%\\) confidence interval of \\([2\\%, 4\\%]\\), and the other with \\([-7\\%, 13\\%]\\). The former is clearly more informative and useful than the latter.\nIn a series of four articles, I will dive deeper into the topic of efficiency with a varying degree of detail and complexity. As a first step in this exploration, I want to demystify the distinction between the three classes of statistcal models: parametric, semiparametric, and nonparametric. The goal of this article is to explore in depth the concepts of parameterizing a model and the distinctions between these three groups."
  },
  {
    "objectID": "blog/DONE-three-classes-stat-models.html#background",
    "href": "blog/DONE-three-classes-stat-models.html#background",
    "title": "The Three Classes of Statistical Models",
    "section": "",
    "text": "Statistical modeling is among the most exciting elements of working with data. When mentoring junior data scientists, I never fail to see the spark in their eyes when our projects have overcome the data gathering and cleaning stages and have reached that precious Rubicon.\nA major goal in the craft of statistical modeling is extracting the most information from the data at hand. Statisticians and econometricians call this efficiency or precision. In simple terms, this means achieving the lowest possible variance from a class of available estimators. As an example, imagine two findings‚Äîboth centered at \\(3\\%\\) (e.g., the treatment effect of an intervention of interest)‚Äîbut one with a \\(95\\%\\) confidence interval of \\([2\\%, 4\\%]\\), and the other with \\([-7\\%, 13\\%]\\). The former is clearly more informative and useful than the latter.\nIn a series of four articles, I will dive deeper into the topic of efficiency with a varying degree of detail and complexity. As a first step in this exploration, I want to demystify the distinction between the three classes of statistcal models: parametric, semiparametric, and nonparametric. The goal of this article is to explore in depth the concepts of parameterizing a model and the distinctions between these three groups."
  },
  {
    "objectID": "blog/DONE-three-classes-stat-models.html#a-closer-look",
    "href": "blog/DONE-three-classes-stat-models.html#a-closer-look",
    "title": "The Three Classes of Statistical Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nParametric Models\nParametric models impose strong assumptions about the underlying data-generating process, making them less flexible but highly interpretable and data-efficient. They require fewer data points to estimate the target accurately compared to semiparametric or nonparametric models. Consequently, they are often the default choice in practice.\nA classic example is density estimation. Suppose you observe that your data appears symmetric around its mean. You might assume the data follows a normal (Gaussian) distribution. In this case, specifying the mean \\(\\mu\\) and variance \\(\\sigma^2\\) fully characterizes the distribution, allowing you to estimate the density parametrically. And then, you can make all kinds of calculations related to your variable, such as what is the probability that it will take values greater than a given number \\(c\\).\nFormally, a parametric model describes the data-generating process entirely using a finite-dimensional parameter vector. As yet another example, in the omnipresent linear model:\n\\[Y = X\\beta + \\epsilon,\\]\nthe parameter specifies the entire relationship between \\(X\\) and \\(Y\\). This imposes a strong assumption: the relationship is linear, meaning a unit change in \\(X\\) consistently results in a \\(\\beta\\) change in \\(Y\\), regardless of \\(X\\)‚Äôs magnitude (small or large). While convenient, this assumption may not hold in all practical scenarios, as real-world relationships often are complex and deviate from linearity.\n\n\nSemiparametric Models\nSemiparametric models combine the structure of parametric models with the flexibility of nonparametric methods. They specify certain aspects of the data-generating process parametrically while leaving other aspects unspecified or modeled flexibly.\nConsider the partially linear model:\n\\[Y = X \\beta + g(Z) + \\epsilon,\\]\nwhere \\(\\beta\\) is a parametric component describing the linear effect of \\(X\\), while \\(g(Z)\\) is an unknown and potentially complex function capturing the nonparametric effect of a covariate matrix \\(Z\\). Here, the model imposes linearity on \\(X\\)‚Äôs effect but allows \\(Z\\)‚Äôs effect to be fully flexible.\nSemiparametric models are highly versatile, balancing the interpretability of parametric models with the adaptability of nonparametric ones. However, estimating \\(\\beta\\) efficiently while accounting for the unknown \\(g(Z)\\) poses challenges, often requiring further assumptions.\nIn practice, semiparametric models play a crucial role in causal inference, particularly when working with observational cross-sectional data (see Imbens and Rubin 2015). A common strategy involves estimating propensity scores using parametric models (like logistic regression) and then employing nonparametric techniques such as kernel weighting, matching, or stratification to estimate treatment effects. This hybrid approach helps address confounding while maintaining computational tractability and interpretability.\n\n\nNonparametric Models\nNonparametric models make minimal assumptions about the underlying data-generating process, allowing the data to ‚Äúspeak for itself‚Äù. Unlike parametric and semiparametric models, these ones do not specify a finite-dimensional parameter vector or impose rigid structural assumptions. This flexibility makes them highly robust to model misspecification but often requires larger datasets to achieve accurate estimates.\nLet‚Äôs get back to the density estimation example. Histograms are a form of nonparametric models as they do not assume specific underlying functional form. Instead of fitting a parametric distribution (e.g., normal), you might use a kernel density estimator:\n\\[\\hat{f}(x)=\\frac{1}{nh}\\sum_i K \\left( \\frac{x-X_i}{h} \\right),\\]\nwhere \\(K(\\cdot)\\) is a kernel function, and \\(h\\) is a bandwidth parameter controlling the smoothness of the estimate. –ê kernel function assigns varying weight to observations around a data point. They are non-negative, symmetric around zero, and sum to one. Commonly employed kernel functions include:\n\nGaussian \\(K(\\cdot)=\\frac{1}{\\sqrt{2 \\pi}}e^{-0.5x^2}\\),\nEpanechnikov \\(K(\\cdot)=\\frac{3}{4}(1-x^2)\\),\nRectangular: \\(K(\\cdot)=0.5\\).\n\nThis approach does not rely on assumptions about the data‚Äôs shape, allowing it to adapt to various distributions.\nNonparametric regression provides another illustration. In it, the relationship between \\(X\\) and \\(Y\\) is modeled as:\n\\[Y=m(X)+\\epsilon,\\]\nwhere \\(m(X)\\) is an unknown function estimated directly from the data using methods like local polynomial regression, splines. Unlike the linearity assumption in parametric models, nonparametric regression allows \\(m(X)\\) to capture complex, nonlinear relationships. A commonly used variant of this is LOESS regression often overlayed in bivariate scatterplots.\nMany popular machine learning (ML) methods are also nonparametric. In this context it is important to distinguis between parameters which impose assumptions on the data and hyperparameters which serve to tune the algorithm. Tree-based techniques exemplify the nonparametric nature of ML: gradient boosting machines, random forests, and individual decision trees can all grow more complex as they encounter more data, creating flexible models that capture intricate patterns. Think even of unsupervised clustering models including \\(k\\)-means and hierarchical clustering.\nWhile nonparametric models are powerful, their flexibility comes at a cost: they can overfit small datasets and often require careful tuning (e.g., choosing the kernel bandwidth) to balance bias and variance. Nonetheless, they are invaluable for exploratory analysis and applications where minimal assumptions are desired.\nDid I also mention the curse of dimensionality?! You will quickly fall into its trap with even moderate number of variables. Yes, there is just so much you can do without adding some structure to your models."
  },
  {
    "objectID": "blog/DONE-three-classes-stat-models.html#an-example",
    "href": "blog/DONE-three-classes-stat-models.html#an-example",
    "title": "The Three Classes of Statistical Models",
    "section": "An Example",
    "text": "An Example\nWe illustrate the distinctions between parametric, semiparametric, and nonparametric models using a toy example. We generate \\(1,000\\) observations of\n\\[Y=sin(X)+\\epsilon,\\]\nwhere \\(X\\) is uniformly distributed and is normally distributed noise. We then model the relationship between \\(Y\\) and \\(X\\) using three approaches.\nA parametric model, such as linear regression (although it could also be quadratic or a higher order polynomial), provides a simple, interpretable approximation but may miss crucial aspects of the true relationship. A ‚Äúsemiparametric‚Äù model, like piecewise linear regression, offers greater flexibility by allowing for changes in slope, capturing some curvature while maintaining a degree of interpretability. (One can cast this piecewise linear model as a parametric one, but for simplicity‚Äôs sake let‚Äôs go with this uncommon and imprecise definition of semiparametric.) Finally, a nonparametric model, such as LOESS, provides the most flexible representation, closely following the underlying sinusoidal pattern but potentially leading to overfitting.\n\nYou should not be surprised. This example demonstrates how the choice of model class significantly impacts the flexibility and interpretability of the fitted relationship. Do not take this example too seriously, it merely serves to illustrate the varying degree of complexity of statistical models.\nYou can download the code to reproduce the figure from this GitHub repo."
  },
  {
    "objectID": "blog/DONE-three-classes-stat-models.html#bottom-line",
    "href": "blog/DONE-three-classes-stat-models.html#bottom-line",
    "title": "The Three Classes of Statistical Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nParametric models impose the strongest assumptions and require the least amount of data. These are most models employed in practice. Think of linear regression.\nSemiparametric models strike balance between flexibility and interpretation while allowing for flexible relationships in the data. Think of (non-Augmented) Inverse Propensity Score Weighting.\nNonparametric models are flexible and data-hungry. They allow for flexible associations between your variables. Think of a histrogram or kernel density."
  },
  {
    "objectID": "blog/delta-method.html#a-closer-look",
    "href": "blog/delta-method.html#a-closer-look",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "A Closer Look",
    "text": "A Closer Look\nThe Delta Method builds on a simple premise: for a smooth function \\(g(\\cdot)\\), we can approximate \\(g(X)\\) around its mean \\(\\mu\\) using a first-order Taylor expansion:\n\\[g(X) \\approx g(\\mu) + \\nabla g(\\mu)^T (X - \\mu),\\]\nwhere \\(\\nabla g(\\mu)\\) is the gradient of \\(g(\\cdot)\\) evaluated at \\(\\mu\\), i.e., a \\(k\\times1\\) vector of partial derivatives:\n\\[\\nabla g(\\mu) = \\left[ \\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2}, \\dots, \\frac{\\partial g}{\\partial x_k} \\right]^T.\\]\nBy substituting this into the approximation, the variance of \\(g(X)\\) becomes:\n\\[\\begin{align*} \\text{Var}(g(X)) & = \\text{Var}(g(\\mu) + \\nabla g(\\mu)^T (X - \\mu)) \\\\ & = \\text{Var}(g(\\mu) + \\nabla g(\\mu)^T X -  \\nabla g(\\mu)^T  \\mu) \\\\  &= \\text{Var}(g(\\mu)^T X)  \\\\ &=  \\nabla g(\\mu)^T \\Sigma \\nabla g(\\mu).  \\end{align*}\\]\nIn the univariate \\(k=1\\) case, we have:\n\\[\\text{Var}(g(X)) = \\sigma^2 [g(\\cdot)']^2.\\]\nIf \\(X\\) is a sample-based estimator (e.g., sample mean, regression coefficients), then \\(\\Sigma\\) would be its estimated covariance matrix, and the Delta Method gives us an approximate standard error for \\(g(X)\\). This approximation works well for large samples but may break down when variances are high or sample sizes are small."
  },
  {
    "objectID": "blog/foci.html#a-closer-look",
    "href": "blog/foci.html#a-closer-look",
    "title": "FOCI: A New Variable Selection Method",
    "section": "A Closer Look",
    "text": "A Closer Look\nI will be being with taking you on a tour of conditional and unconditional statistics. What‚Äôs the deal with conditional versus unconditional correlation, and is one of them always a better choice than the other?\n\nConditional and Unconditional Correlation\nMost times, when we talk about the correlation between \\(X\\) and \\(Y\\), knowingly or not, we mean unconditional correlation. This is the correlation between two variables without considering any additional or contextual factors. As such, it describes their relationship ‚Äúin general.‚Äù In my previous post, I illustrated several ways to measure such unconditional correlations, including the well-known Spearman correlation coefficient.\nIn some cases, however, we like to go one step further and consider contextual factors. This is where conditional correlation comes in. It considers the influence of one or more additional variables when measuring the relationship between \\(X\\) and \\(Y\\). Thus, it provides information on how their relationship changes under specific conditions.\nConditional correlation is hard (pun intended); it‚Äôs even NP-hard in some contexts.\nUnder certain assumptions, it might be possible to go from conditional to unconditional correlation. Simplifying the example above, let‚Äôs say that \\(Z\\) (parental education) can take on two values‚Äîlow and high. If we have measures of the correlation between \\(X\\) and \\(Y\\) for each of these values (i.e., for families with low and high parental education), we can weigh these by their relative proportion and arrive at an estimate of the unconditional correlation between \\(X\\) and \\(Y\\).\nTo answer my own question above, it‚Äôs difficult to say that one is always better than the other. It really depends on your goal. If you want to know how \\(X\\) and \\(Y\\) vary in general, unconditional correlation is your friend; if, instead, you are interested in incorporating contextual information or simply controlling for other factors, you really want conditional correlation. My take is that most often we are after conditional correlations but use tools for measuring unconditional ones.\n\n\nQuantile Regression\n\n\n\n\n\n\nSide Note on Quantile Regression\n\n\n\nQuantile regression is another setting where most often people confuse conditional and unconditional statistical inference. Interestingly, though, the situation is flipped. The classical quantile regression estimator, which most people utilize, measures the impact of \\(X\\) on the conditional quantile of \\(Y\\), but there results are commonly interpreted as having unconditional interpretation.\n\n\n\n\nNew Coefficient of Conditional Independence\nAzadkia and Chatterjee (2021) develop a new coefficient of conditional correlation. The math behind is in extremely involved and I will not even be discussing its formula here. For simplicity, let‚Äôs just call it \\(T(\\cdot)\\):\n\\[T(Y,X|Z) = \\text{corr} (Y, X | Z).\\]\n\\(T(\\cdot)\\) enjoys the following attractive properties:\n\nIt is an extension of Chatterjee‚Äôs unconditional correlation coefficient.\nIt is non-parametric.\nIt has no tuning parameters.\nIt can be estimated quickly, in \\(O(n \\text{ log}n)\\) time.\nAsymptotically converges to a limit in \\([0,1]\\). It‚Äôs limit is \\(0\\) when \\(Y\\) and \\(X\\) are conditionally independent, and it is \\(1\\) when \\(Y\\) is a measurable function of \\(X\\), conditionally.\nIt is a nonlinear generalization of the partial \\(R^2\\) coefficient in a linear regression of \\(Y\\) on \\(X\\) and \\(Z\\).\n\nAll this is to say ‚Äì \\(T(\\cdot)\\) has many things going for it, and it is a pretty good measure of conditional correlation.\n\n\nThe Variable Selection Algorithm\nThe key idea is to integrate \\(T(\\cdot)\\) into a forward stepwise variable selection procedure. Let‚Äôs add the \\(Z\\) variables into \\(X\\), so we only have \\(Y\\in \\mathbb{R}\\) and \\(X\\in \\mathbb{R}^p\\). Then the algorithm goes as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nStart with the index \\(j\\) that maximizes \\(T(Y \\mid X_j)\\).\nGiven \\(j_1, \\dots j_k\\), select \\(j_{k+1}\\) as the index \\(\\notin (j_1, \\dots j_k)\\) that maximizes \\(T(Y,X_j \\mid X_{j_1}, \\dots, X_{j_k})\\).\nContinue until finding the first \\(k\\) such that \\(T(Y, X_{j_{k+1}} \\mid X_{j_1}, \\dots, X_{j_k}) \\leq 0\\).\nDeclare the set of selected variables \\(\\hat{S} ={X_{j_1}, \\dots, X_{j_k}}\\).\n\n\n\nSoftware Package: FOCI\nIn words, we start with the variable j that maximizes \\(T(Y\\mid X_j)\\). In each subsequent step we select the variable that has not yet been selected and has the highest \\(T(Y \\mid \\cdot)\\) value up until \\(T(Y \\mid \\cdot)\\) is positive. That‚Äôs it. We then have the set of FOCI selected variables.\nAlthough it is not required theoretically, the predictor variables be standardized before running the algorithm. If computational time is not an issue, one can try to add \\(m \\geq  2\\) variables at each step instead of just one.\nThere you have it. Grab your data and see whether and how much FOCI improves on your favorite feature selection method."
  },
  {
    "objectID": "blog/DONE-delta-method.html",
    "href": "blog/DONE-delta-method.html",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "",
    "text": "You‚Äôve likely encountered this scenario: you‚Äôve calculated an estimate for a particular parameter, and now you require a confidence interval. Seems straightforward, doesn‚Äôt it? However, the task becomes considerably more challenging if your estimator is a nonlinear function of other random variables. Whether you‚Äôre dealing with ratios, transformations, or intricate functional relationships, directly deriving the variance for your estimator can feel incredibly daunting. In some instances, the bootstrap might offer a solution, but it can also be computationally demanding.\nEnter the Delta Method, a technique that harnesses the power of Taylor series approximations to assist in calculating confidence intervals within complex scenarios. By linearizing a function of random variables around their mean, the Delta Method provides a way to approximate their variance (and consequently, confidence intervals). This effectively transforms a convoluted problem into a more manageable one. Let‚Äôs delve deeper together, assuming you already have a foundational understanding of hypothesis testing."
  },
  {
    "objectID": "blog/DONE-delta-method.html#background",
    "href": "blog/DONE-delta-method.html#background",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "",
    "text": "You‚Äôve likely encountered this scenario: you‚Äôve calculated an estimate for a particular parameter, and now you require a confidence interval. Seems straightforward, doesn‚Äôt it? However, the task becomes considerably more challenging if your estimator is a nonlinear function of other random variables. Whether you‚Äôre dealing with ratios, transformations, or intricate functional relationships, directly deriving the variance for your estimator can feel incredibly daunting. In some instances, the bootstrap might offer a solution, but it can also be computationally demanding.\nEnter the Delta Method, a technique that harnesses the power of Taylor series approximations to assist in calculating confidence intervals within complex scenarios. By linearizing a function of random variables around their mean, the Delta Method provides a way to approximate their variance (and consequently, confidence intervals). This effectively transforms a convoluted problem into a more manageable one. Let‚Äôs delve deeper together, assuming you already have a foundational understanding of hypothesis testing."
  },
  {
    "objectID": "blog/DONE-delta-method.html#notation",
    "href": "blog/DONE-delta-method.html#notation",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "Notation",
    "text": "Notation\nBefore diving into the technical weeds, let‚Äôs set up some notation to keep things grounded. Let \\(X=(x_1, \\dots, x_k)\\) be a random vector of dimension \\(k\\), with mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\) (or simply a scalar \\(\\sigma^2\\) when \\(k=1\\)). Suppose you have a continuous, differentiable function \\(g(\\cdot)\\), and you‚Äôre interested in approximating the variance of \\(g(X)\\), denoted as \\(\\text{Var}(g(X))\\)."
  },
  {
    "objectID": "blog/DONE-delta-method.html#a-closer-look",
    "href": "blog/DONE-delta-method.html#a-closer-look",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "A Closer Look",
    "text": "A Closer Look\nThe Delta Method builds on a simple premise: for a smooth function \\(g(\\cdot)\\), we can approximate \\(g(X)\\) around its mean \\(\\mu\\) using a first-order Taylor expansion:\n\\[g(X) \\approx g(\\mu) + \\nabla g(\\mu)^T (X - \\mu),\\]\nwhere \\(\\nabla g(\\mu)\\) is the gradient of \\(g(\\cdot)\\) evaluated at \\(\\mu\\), i.e., a \\(k\\times1\\) vector of partial derivatives:\n\\[\\nabla g(\\mu) = \\left[ \\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2}, \\dots, \\frac{\\partial g}{\\partial x_k} \\right]^T.\\]\nBy substituting this into the approximation, the variance of \\(g(X)\\) becomes:\n\\[\\begin{align*} \\text{Var}(g(X)) & = \\text{Var}(g(\\mu) + \\nabla g(\\mu)^T (X - \\mu)) \\\\ & = \\text{Var}(g(\\mu) + \\nabla g(\\mu)^T X -  \\nabla g(\\mu)^T  \\mu) \\\\  &= \\text{Var}(g(\\mu)^T X)  \\\\ &=  \\nabla g(\\mu)^T \\Sigma \\nabla g(\\mu).  \\end{align*}\\]\nIn the univariate \\(k=1\\) case, we have:\n\\[\\text{Var}(g(X)) = \\sigma^2 [g(\\cdot)']^2.\\]\nIf \\(X\\) is a sample-based estimator (e.g., sample mean, regression coefficients), then \\(\\Sigma\\) would be its estimated covariance matrix, and the Delta Method gives us an approximate standard error for \\(g(X)\\). This approximation works well for large samples but may break down when variances are high or sample sizes are small."
  },
  {
    "objectID": "blog/DONE-delta-method.html#an-example",
    "href": "blog/DONE-delta-method.html#an-example",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs walk through an example to make this concrete. Suppose you‚Äôre studying the ratio of two independent random variables: \\(R = \\frac{X_1}{X_2}\\), where \\(X_1 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim N(\\mu_2, \\sigma_2^2)\\). I know some of you want specific numbers, so we can set \\(\\mu_1 = 5\\), \\(\\mu_2 = 10\\), \\(\\sigma_1 = 2\\), and \\(\\sigma_2=1\\).\nWe want to approximate the variance of \\(R\\) using the Delta Method. Here is the step-by-step procedure to get there.\n\nDefine \\(g(X)\\) and obtain its gradient. Here, \\(g(X) = \\frac{X_1}{X_2}\\) and the gradient is: \\[\\nabla g(\\mu) = \\left[ \\frac{1}{\\mu_2}, -\\frac{\\mu_1}{\\mu_2^2} \\right]^T.\\]\nEvaluate \\(g(\\mu)\\) at _1 and _2. In our example \\[\\nabla g(\\mu) = [0.1, -0.5]^T.\\]\nCompute the variance approximation. We have \\[\\Sigma = \\begin{bmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_2^2 \\end{bmatrix} = \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix}.\\] Thus, the approximate variance of \\(R\\) is: \\[\\text{Var}(R) \\approx \\nabla g(\\mu)^T \\Sigma \\nabla g(\\mu) = \\frac{\\sigma_1^2}{\\mu_2^2} + \\frac{\\mu_1^2 \\sigma_2^2}{\\mu_2^4}=\\frac{4}{100}+\\frac{25}{625}=0.08.\\]\n\nAnd that‚Äôs it. We used the Delta Method to compute the approximate variance of \\(R = \\frac{X_1}{X_2}\\)."
  },
  {
    "objectID": "blog/DONE-delta-method.html#bottom-line",
    "href": "blog/DONE-delta-method.html#bottom-line",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe Delta Method is a generic way of computing confidence intervals in non-standard situations.\nIt works by linearizing nonlinear functions to approximate variances and standard errors.\nThis technique works for any smooth function, making it a go-to tool in econometrics, biostatistics, and machine learning."
  },
  {
    "objectID": "blog/DONE-delta-method.html#references",
    "href": "blog/DONE-delta-method.html#references",
    "title": "The Delta Method: Simplifying Confidence Intervals for Complex Estimators",
    "section": "References",
    "text": "References\nCasella, G., & Berger, R. L. (2002). Statistical Inference.\nGreene, W. H. (2018). Econometric Analysis."
  },
  {
    "objectID": "blog/DONE-causation-without-correlation.html",
    "href": "blog/DONE-causation-without-correlation.html",
    "title": "Causation without Correlation",
    "section": "",
    "text": "While most people understand that correlation doesn‚Äôt imply causation, it might surprise many to learn that causation doesn‚Äôt always result in correlation. In the absence of randomization, causal relationships do not require observable correlation. This counterintuitive concept challenges our natural tendency to expect that when one variable causes change in another, we should see a clear (linear) relationship between them. The core idea is that confounding variables or other statistical phenomena can obscure the causal link. Let‚Äôs explore this concept through a few examples."
  },
  {
    "objectID": "blog/DONE-causation-without-correlation.html#background",
    "href": "blog/DONE-causation-without-correlation.html#background",
    "title": "Causation without Correlation",
    "section": "",
    "text": "While most people understand that correlation doesn‚Äôt imply causation, it might surprise many to learn that causation doesn‚Äôt always result in correlation. In the absence of randomization, causal relationships do not require observable correlation. This counterintuitive concept challenges our natural tendency to expect that when one variable causes change in another, we should see a clear (linear) relationship between them. The core idea is that confounding variables or other statistical phenomena can obscure the causal link. Let‚Äôs explore this concept through a few examples."
  },
  {
    "objectID": "blog/DONE-causation-without-correlation.html#a-closer-look",
    "href": "blog/DONE-causation-without-correlation.html#a-closer-look",
    "title": "Causation without Correlation",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn the popular book Causal Inference: The Mixtape, Scott Cunningham gives an example of a sailor steering a boat in stormy waters. The wind may be so strong as to offset the boat‚Äôs natural moving direction. For instance, the sailor might steer (treatment, \\(T\\)) the boat north, while a southward wind (confounder, \\(Z\\)) causes the boat to move east (outcome, \\(Y\\)). An onlooker would not observe any direct relationship between \\(T\\) and \\(Y\\), even though \\(T\\) causes \\(Y\\).\nAt first, this sounds counterintuitive. On second thought, such patterns are everywhere. Consider the following.\n\nExample 1: Parenting Styles and Children‚Äôs Behavior\nA parent might adopt a stricter parenting style (\\(T\\)) in response to a child‚Äôs behavioral issues (\\(Y\\)). However, other influences, like peer pressure or school environment (\\(Z\\)), may also shape the child‚Äôs behavior, sometimes overriding the parent‚Äôs efforts. The net observable outcome could show no correlation between stricter parenting and improved behavior, even though the stricter parenting is causally effective in certain contexts.\nThis idea can be taken one step further. An observable relationship might even appear positive when the causal relationship is negative.\n\n\nExample 2: Ice Cream Sales and Shark Attacks\nImagine two beaches with vastly different safety protocols (\\(Z\\)): one has lifeguards trained to prevent shark attacks, while the other does not. On the safer beach, higher ice cream sales (\\(T\\)) correlate positively with shark attacks (\\(Y\\)), because more people visit the beach when safety protocols are in place. This hides the fact that proper safety protocols causally reduce shark attacks. The observed positive correlation between \\(T\\) and \\(Y\\) masks the negative causal relationship.\n\n\nExample 3: Nonlinearity\nA more trivial scenario leading to the lack of correlation in causal relationships is non-linearity. I do not find this scenario too insightful simply because it can be avoided by using more sophisticated measures of correlation. See my earlier post on the Chatterjee correlation coefficient.\nExamples of such relationships abound. Consider a parabolic relationship, where increasing a drug‚Äôs dosage initially improves patient outcomes but becomes harmful at higher doses. Despite a clear causal relationship, the (Pearson) correlation coefficient might be close to zero because the relationship is not linear.\n\n\nExample 4: Threshold Effects and Phase Transitions\nTake the classic example of temperature and water‚Äôs state. Increasing temperature causes water to change state at exactly 100¬∞C. Below and above this point, temperature changes cause minimal effects on the water‚Äôs state. Aggregating these observations leads to a weak correlation, despite the temperature being the direct cause of the phase transition.\nOther fascinating scenarios include Lord‚Äôs Paradox, and Simpson‚Äôs Paradox, where a causal relationship can appear to reverse or disappear when data is aggregated.\n\n\nExample 5: Hospital Mortality Rates\nSuppose two hospitals treat patients with different levels of severity. Hospital \\(A\\) specializes in high-risk patients, while Hospital \\(B\\) treats mostly low-risk cases. When comparing raw mortality rates (\\(Y\\)), Hospital \\(A\\) might appear worse, even though it provides superior care (\\(T\\)). Disaggregating the data by risk level reveals the causal effect of Hospital \\(A\\)‚Äôs superior treatment within each group."
  },
  {
    "objectID": "blog/DONE-causation-without-correlation.html#bottom-line",
    "href": "blog/DONE-causation-without-correlation.html#bottom-line",
    "title": "Causation without Correlation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nIn observational data causation does not require correlation.\nCorrelation‚Äîor the lack thereof‚Äîcan obscure our understanding of causal relationships.\nWith the right tools and frameworks, we can disentangle the true causal effects, even when correlation gives us a wrong answer."
  },
  {
    "objectID": "blog/DONE-causation-without-correlation.html#references",
    "href": "blog/DONE-causation-without-correlation.html#references",
    "title": "Causation without Correlation",
    "section": "References",
    "text": "References\nCunningham, S. (2021). Causal inference: The mixtape. Yale university press."
  },
  {
    "objectID": "blog/DONE-foci.html",
    "href": "blog/DONE-foci.html",
    "title": "FOCI: A New Variable Selection Method",
    "section": "",
    "text": "In our data-abundant world, we often have access to tens, hundreds, or even thousands of variables. Most of these features are usually irrelevant or redundant, leading to increased computational complexity and potentially overfitted models. Variable selection algorithms are designed to mitigate these challenges by selecting the subset of columns that is most relevant to the problem at hand. This, in turn, can result in simplified, more efficient, and interpretable machine learning (ML) models. Examples of such methods abound‚Äîlasso, ridge, elastic net utilizing \\(L1\\) and \\(L2\\) regularization or a linear combination of both; forward/backward stepwise regression algorithms, etc.\nIn this article, I will describe a new variable selection method built on the idea of Chatterjee‚Äôs correlation coefficient, which has been a hot topic of discussion among statisticians. The new method goes by the abbreviation FOCI, which stands for Feature Ordering by Conditional Independence. It is somewhat similar to forward stepwise regression but overcomes some of the critiques most often associated with it.\nLet‚Äôs start with setting up some basic notation. We are focused on studying the correlation between (\\(Y\\)) (e.g., income) and (\\(X\\)) (education level) while interested in conditioning on/controlling for (\\(Z\\)) (parental education and other factors). As always, we are armed with a random sample of size n of all these variables and assume everything is well-behaved. If you have not had a chance, I recommend you first read my earlier post on Chatterjee‚Äôs correlation measure, which lays some of the foundations necessary to understand the algorithm under the hood."
  },
  {
    "objectID": "blog/DONE-foci.html#background",
    "href": "blog/DONE-foci.html#background",
    "title": "FOCI: A New Variable Selection Method",
    "section": "",
    "text": "In our data-abundant world, we often have access to tens, hundreds, or even thousands of variables. Most of these features are usually irrelevant or redundant, leading to increased computational complexity and potentially overfitted models. Variable selection algorithms are designed to mitigate these challenges by selecting the subset of columns that is most relevant to the problem at hand. This, in turn, can result in simplified, more efficient, and interpretable machine learning (ML) models. Examples of such methods abound‚Äîlasso, ridge, elastic net utilizing \\(L1\\) and \\(L2\\) regularization or a linear combination of both; forward/backward stepwise regression algorithms, etc.\nIn this article, I will describe a new variable selection method built on the idea of Chatterjee‚Äôs correlation coefficient, which has been a hot topic of discussion among statisticians. The new method goes by the abbreviation FOCI, which stands for Feature Ordering by Conditional Independence. It is somewhat similar to forward stepwise regression but overcomes some of the critiques most often associated with it.\nLet‚Äôs start with setting up some basic notation. We are focused on studying the correlation between (\\(Y\\)) (e.g., income) and (\\(X\\)) (education level) while interested in conditioning on/controlling for (\\(Z\\)) (parental education and other factors). As always, we are armed with a random sample of size n of all these variables and assume everything is well-behaved. If you have not had a chance, I recommend you first read my earlier post on Chatterjee‚Äôs correlation measure, which lays some of the foundations necessary to understand the algorithm under the hood."
  },
  {
    "objectID": "blog/DONE-foci.html#a-closer-look",
    "href": "blog/DONE-foci.html#a-closer-look",
    "title": "FOCI: A New Variable Selection Method",
    "section": "A Closer Look",
    "text": "A Closer Look\nI will be being with taking you on a tour of conditional and unconditional statistics. What‚Äôs the deal with conditional versus unconditional correlation, and is one of them always a better choice than the other?\n\nConditional and Unconditional Correlation\nMost times, when we talk about the correlation between \\(X\\) and \\(Y\\), knowingly or not, we mean unconditional correlation. This is the correlation between two variables without considering any additional or contextual factors. As such, it describes their relationship ‚Äúin general.‚Äù In my previous post, I illustrated several ways to measure such unconditional correlations, including the well-known Spearman correlation coefficient.\nIn some cases, however, we like to go one step further and consider contextual factors. This is where conditional correlation comes in. It considers the influence of one or more additional variables when measuring the relationship between \\(X\\) and \\(Y\\). Thus, it provides information on how their relationship changes under specific conditions.\nConditional correlation is hard (pun intended); it‚Äôs even NP-hard in some contexts.\nUnder certain assumptions, it might be possible to go from conditional to unconditional correlation. Simplifying the example above, let‚Äôs say that \\(Z\\) (parental education) can take on two values‚Äîlow and high. If we have measures of the correlation between \\(X\\) and \\(Y\\) for each of these values (i.e., for families with low and high parental education), we can weigh these by their relative proportion and arrive at an estimate of the unconditional correlation between \\(X\\) and \\(Y\\).\nTo answer my own question above, it‚Äôs difficult to say that one is always better than the other. It really depends on your goal. If you want to know how \\(X\\) and \\(Y\\) vary in general, unconditional correlation is your friend; if, instead, you are interested in incorporating contextual information or simply controlling for other factors, you really want conditional correlation. My take is that most often we are after conditional correlations but use tools for measuring unconditional ones.\n\n\nQuantile Regression\n\n\n\n\n\n\nSide Note on Quantile Regression\n\n\n\nQuantile regression is another setting where most often people confuse conditional and unconditional statistical inference. Interestingly, though, the situation is flipped. The classical quantile regression estimator, which most people utilize, measures the impact of \\(X\\) on the conditional quantile of \\(Y\\), but there results are commonly interpreted as having unconditional interpretation.\n\n\n\n\nNew Coefficient of Conditional Independence\nAzadkia and Chatterjee (2021) develop a new coefficient of conditional correlation. The math behind is in extremely involved and I will not even be discussing its formula here. For simplicity, let‚Äôs just call it \\(T(\\cdot)\\):\n\\[T(Y,X|Z) = \\text{corr} (Y, X | Z).\\]\n\\(T(\\cdot)\\) enjoys the following attractive properties:\n\nIt is an extension of Chatterjee‚Äôs unconditional correlation coefficient.\nIt is non-parametric.\nIt has no tuning parameters.\nIt can be estimated quickly, in \\(O(n \\text{ log}n)\\) time.\nAsymptotically converges to a limit in \\([0,1]\\). It‚Äôs limit is \\(0\\) when \\(Y\\) and \\(X\\) are conditionally independent, and it is \\(1\\) when \\(Y\\) is a measurable function of \\(X\\), conditionally.\nIt is a nonlinear generalization of the partial \\(R^2\\) coefficient in a linear regression of \\(Y\\) on \\(X\\) and \\(Z\\).\n\nAll this is to say ‚Äì \\(T(\\cdot)\\) has many things going for it, and it is a pretty good measure of conditional correlation.\n\n\nThe Variable Selection Algorithm\nThe key idea is to integrate \\(T(\\cdot)\\) into a forward stepwise variable selection procedure. Let‚Äôs add the \\(Z\\) variables into \\(X\\), so we only have \\(Y\\in \\mathbb{R}\\) and \\(X\\in \\mathbb{R}^p\\). Then the algorithm goes as follows:\n\nStart with the index \\(j\\) that maximizes \\(T(Y \\mid X_j)\\).\nGiven \\(j_1, \\dots j_k\\), select \\(j_{k+1}\\) as the index \\(\\notin (j_1, \\dots j_k)\\) that maximizes \\(T(Y,X_j \\mid X_{j_1}, \\dots, X_{j_k})\\).\nContinue until finding the first \\(k\\) such that \\(T(Y, X_{j_{k+1}} \\mid X_{j_1}, \\dots, X_{j_k}) \\leq 0\\).\nDeclare the set of selected variables \\(\\hat{S} ={X_{j_1}, \\dots, X_{j_k}}\\).\n\nSoftware Package: FOCI\nIn words, we start with the variable j that maximizes \\(T(Y\\mid X_j)\\). In each subsequent step we select the variable that has not yet been selected and has the highest \\(T(Y \\mid \\cdot)\\) value up until \\(T(Y \\mid \\cdot)\\) is positive. That‚Äôs it. We then have the set of FOCI selected variables.\nAlthough it is not required theoretically, the predictor variables be standardized before running the algorithm. If computational time is not an issue, one can try to add \\(m \\geq  2\\) variables at each step instead of just one.\nThere you have it. Grab your data and see whether and how much FOCI improves on your favorite feature selection method."
  },
  {
    "objectID": "blog/DONE-foci.html#bottom-line",
    "href": "blog/DONE-foci.html#bottom-line",
    "title": "FOCI: A New Variable Selection Method",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nAzadkia and Chatterjee (2021) develop a new coefficient of conditional correlation featuring a host of attractive properties.\nThis coefficient can be used in a stepwise inclusion fashion as a variable selection algorithm potentially improving on well-established methods in the field."
  },
  {
    "objectID": "blog/DONE-foci.html#where-to-learn-more",
    "href": "blog/DONE-foci.html#where-to-learn-more",
    "title": "FOCI: A New Variable Selection Method",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMy earlier post on Chatterjee‚Äôs bivariate coefficient of (unconditional) correlation is a good starting point. Data scientists more deeply interested in FOCI should read Azadkia and Chatterjee‚Äôs paper which describes in detail the mathematics behind the new algorithm."
  },
  {
    "objectID": "blog/DONE-foci.html#references",
    "href": "blog/DONE-foci.html#references",
    "title": "FOCI: A New Variable Selection Method",
    "section": "References",
    "text": "References\nAlejo, J., Favata, F., Montes-Rojas, G., & Trombetta, M. (2021). Conditional vs Unconditional Quantile Regression Models: A Guide to Practitioners. Economia, 44(88), 76-93.\nAzadkia, M., & Chatterjee, S. (2021). A simple measure of conditional dependence. The Annals of Statistics, 49(6), 3070-3102.\nBorah, B. J., & Basu, A. (2013). Highlighting differences between conditional and unconditional quantile regression approaches through an application to assess medication adherence. Health economics, 22(9), 1052-1070.\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\nDagum, P., & Luby, M. (1993). Approximating probabilistic inference in Bayesian belief networks is NP-hard. Artificial intelligence, 60(1), 141-153.\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional quantile regressions. Econometrica, 77(3), 953-973.\nKoenker, R. (2017). Quantile regression: 40 years on. Annual review of economics, 9, 155-176."
  },
  {
    "objectID": "blog/DONE-correlation-is-cosine.html",
    "href": "blog/DONE-correlation-is-cosine.html",
    "title": "Correlation is a Cosine",
    "section": "",
    "text": "You might have come across the statement, ‚Äúcorrelation is a cosine,‚Äù but never taken the time to explore its precise meaning. It certainly sounds intriguing‚Äîhow can the simplest bivariate summary statistic be connected to a trigonometric function you first encountered in sixth grade? What exactly is the relationship between correlation and cosines?"
  },
  {
    "objectID": "blog/DONE-correlation-is-cosine.html#background",
    "href": "blog/DONE-correlation-is-cosine.html#background",
    "title": "Correlation is a Cosine",
    "section": "",
    "text": "You might have come across the statement, ‚Äúcorrelation is a cosine,‚Äù but never taken the time to explore its precise meaning. It certainly sounds intriguing‚Äîhow can the simplest bivariate summary statistic be connected to a trigonometric function you first encountered in sixth grade? What exactly is the relationship between correlation and cosines?"
  },
  {
    "objectID": "blog/DONE-correlation-is-cosine.html#a-closer-look",
    "href": "blog/DONE-correlation-is-cosine.html#a-closer-look",
    "title": "Correlation is a Cosine",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Law of Cosines\nThe law of cosines states that in any triangle with sides \\(x\\), \\(y\\), and \\(z\\) and an angle (between \\(x\\) and \\(y\\)) \\(\\theta\\), we have:\n\\[ z^2 = x^2 + y^2 - 2 x y cos(\\theta), \\]\nIn the special case when \\(\\theta=\\frac{\\pi}{2}\\), the term on the right-hand side equals \\(0\\) and the equation reduces to the well-known Pythagorean Theorem.\n\n\nThe Variance of the \\(A+B\\)\nLet‚Äôs imagine two random variables \\(A\\), \\(B\\). The variance of their sum is given by:\n\\[ var(A+B) = var(A)+var(B)+2 cov(A,B), \\]\nwhere \\(cov(\\cdot)\\), denotes covariance. We can substitute the last term with its definition as follows:\n\\[ var(A+B) = var(A)+var(B)+2 corr(A,B) sd(A) sd(B). \\]\nNext, we know that \\(var(\\cdot)=sd^2(\\cdot)\\). Substituting, we get:\n\\[ sd^2(A+B) = sd^2 (A)+ sd^2 (B)+2 corr(A,B) sd(A) sd(B).\\]\n\n\nPutting the Two Together\nSetting \\(x=sd(A)\\), \\(y=sd(B)\\), and \\(z=sd(A+B)\\) in the first equation gives the desired result. With one small caveat ‚Äì the negative sign on the cosine term. To get around this we can simply look at the complementary angle \\(\\delta = \\pi - \\theta\\).\nThat is, we imagine a triangle with sides equal to \\(sd(A)\\), \\(sd(B)\\) and \\(sd(A+B)\\), where \\(\\theta\\) is the angle between \\(sd(A)\\), \\(sd(B)\\). When this angle is small (\\(\\theta &lt; \\frac{\\pi}{2}\\)), the two sides point in the same direction and A and B are positively correlated. The opposite is true for \\(\\theta &gt; \\frac{\\pi}{2}\\). As mentioned above, \\(\\theta = \\frac{\\pi}{2}\\) kills the correlation term, consistent with \\(A\\) and \\(B\\) being independent.\n\n\nCorrelation as a Dot Product\nThere‚Äôs another way to see this connection that makes it even clearer.\nIf you think of \\(A\\) and \\(B\\) as vectors in \\(n\\)-dimensional space (e.g., \\(A = (a_1, a_2, \\ldots, a_n)\\)), the cosine of the angle between them is given by:\n\\[\n\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|},\n\\]\nwhere \\(A \\cdot B\\) is the dot product, and \\(\\|\\cdot\\|\\) denotes the Euclidean norm. When \\(A\\) and \\(B\\) are standardized (i.e., mean zero and unit variance), this cosine becomes the Pearson correlation coefficient:\n\\[\n\\text{corr}(A, B) = \\frac{1}{n} \\sum_{i=1}^n A_i^* B_i^* \\approx \\cos(\\theta),\n\\]\nwhere \\(A^*\\) and \\(B^*\\) are the standardized versions of \\(A\\) and \\(B\\).\n\n\nCosine Similarity vs.¬†Correlation\n\n\n\n\n\n\nCosine similarity vs.¬†correlation\n\n\n\nCosine similarity and Pearson correlation are closely related, but not always the same:\n\nCosine similarity considers only the angle between vectors. It‚Äôs scale-invariant but not shift-invariant.\nCorrelation removes both mean and scale, making it invariant to affine transformations.\n\nSo, while ‚Äúcorrelation is a cosine,‚Äù the statement is strictly true when you‚Äôre working with standardized vectors."
  },
  {
    "objectID": "blog/DONE-correlation-is-cosine.html#where-to-learn-more",
    "href": "blog/DONE-correlation-is-cosine.html#where-to-learn-more",
    "title": "Correlation is a Cosine",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAs with anything else, a Google search is your friend here, with multiple Stack Overflow posts explaining this connection from all sorts of angles. However, I do find John D. Cook‚Äôs blog post most helpful, and I am following his exposition closely."
  },
  {
    "objectID": "blog/DONE-correlation-is-cosine.html#bottom-line",
    "href": "blog/DONE-correlation-is-cosine.html#bottom-line",
    "title": "Correlation is a Cosine",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe variance formula mirrors the law of cosines.\nStandardizing the variables makes correlation equal the cosine of the angle.\nSo: ‚ÄúCorrelation is a cosine‚Äù ‚Äî literally!"
  },
  {
    "objectID": "blog/DONE-correlation-transitive.html",
    "href": "blog/DONE-correlation-transitive.html",
    "title": "Correlation is Not (Always) Transitive",
    "section": "",
    "text": "At first, I found this really puzzling. \\(X\\) is correlated (Pearson) with Y, and Y is correlated with \\(Z\\). Does this mean X is necessarily correlated with \\(Z\\)? Intuitively, this totally makes sense. The answer, however, is ‚Äúno.‚Äù\nPerhaps the strangest thing is how easy it is to rationalize this ‚Äúpuzzle.‚Äù I drink more beer (\\(X\\)) and read more books (\\(Z\\)) when I am on a vacation (Y). That is, both pairs ‚Äì \\(X\\) and \\(Y\\) and \\(Z\\) and \\(Y\\) ‚Äì are positively correlated. But I do not drink more beer when I read more books ‚Äì \\(X\\) and \\(Z\\) are not correlated. It is now obvious that correlation is not (always) transitive, but a second ago, this sounded bizarre."
  },
  {
    "objectID": "blog/DONE-correlation-transitive.html#background",
    "href": "blog/DONE-correlation-transitive.html#background",
    "title": "Correlation is Not (Always) Transitive",
    "section": "",
    "text": "At first, I found this really puzzling. \\(X\\) is correlated (Pearson) with Y, and Y is correlated with \\(Z\\). Does this mean X is necessarily correlated with \\(Z\\)? Intuitively, this totally makes sense. The answer, however, is ‚Äúno.‚Äù\nPerhaps the strangest thing is how easy it is to rationalize this ‚Äúpuzzle.‚Äù I drink more beer (\\(X\\)) and read more books (\\(Z\\)) when I am on a vacation (Y). That is, both pairs ‚Äì \\(X\\) and \\(Y\\) and \\(Z\\) and \\(Y\\) ‚Äì are positively correlated. But I do not drink more beer when I read more books ‚Äì \\(X\\) and \\(Z\\) are not correlated. It is now obvious that correlation is not (always) transitive, but a second ago, this sounded bizarre."
  },
  {
    "objectID": "blog/DONE-correlation-transitive.html#a-closer-look",
    "href": "blog/DONE-correlation-transitive.html#a-closer-look",
    "title": "Correlation is Not (Always) Transitive",
    "section": "A Closer Look",
    "text": "A Closer Look\nLet‚Äôs denote the respective correlations between \\(X\\), \\(Y\\) and \\(Z\\) by \\(cor(X,Y)\\), \\(cor(X,Z)\\), and \\(cor(Y,Z)\\). For simplicity (and without loss of generality), let‚Äôs work with standardized versions of these variables ‚Äì that is, means of \\(0\\) and variances of \\(1\\). This implies, \\(cov(X,Y) = cor(X,Y)\\) for any pair.\nWe can write the linear projections of \\(X\\) and \\(Z\\) on \\(Y\\) as follows:\n\\[ X = cor(X,Y)Y + \\epsilon^{X,Y}, \\]\n\\[ Z = cor(Z,Y)Y + \\epsilon^{Z,Y}. \\]\nThen, we have:\n\\[ cor(X,Z)=cor(X,Y)cor(Z,Y)+cor(\\epsilon^{X,Y},\\epsilon^{Z,Y}).\\]\nWe can use the Cauchy-Schwarz inequality to bound the last term, which gives the final range of possible values for cor(X,Z):\n\\[cor(X,Y)cor(Z,Y) - \\sqrt{(1-cor(X,Y)^2) (1-cor(Z,Y)^2)}\\]\n\\[\\leq cor(X,Z) \\leq  \\]\n\\[cor(X,Y)cor(Z,Y) + \\sqrt{(1-cor(X,Y)^2) (1-cor(Z,Y)^2)}\\]\nFor instance, if we set \\(cor(X,Y)=cor(Z,Y)=0.6\\), then we get:\n\\[-.28 \\leq cor(X,Z) \\leq 1.\\]\nThat is, \\(cor(X,Z)\\) can be negative.\n\nAn (Extremely Simple) Example\nPerhaps the simplest example to illustrate this is:\n\n\\(X\\) and \\(Z\\) are independent random variables,\n\\(Y=X+Z\\). The result follows.\n\nThe following code sets up this example in R and python.\n\nRPython\n\n\nrm(list=ls())\nset.seed(68493)\n\nx &lt;- runif(n=1000)\nz &lt;- runif(n=1000)\ny &lt;- x + z\n\ncor(y, x)\ncor(y, z)\ncor(z, x)\n\ncor.test(y, x, alternative='two.sided', method='pearson')\ncor.test(y, z, alternative='two.sided', method='pearson')\ncor.test(z, x, alternative='two.sided', method='pearson')\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr\n\n# Set seed for reproducibility\nnp.random.seed(68493)\n\n# Generate random variables\nx = np.random.uniform(size=1000)\nz = np.random.uniform(size=1000)\ny = x + z\n\n# Compute correlations\nprint(\"cor(y, x):\", np.corrcoef(y, x)[0, 1])\nprint(\"cor(y, z):\", np.corrcoef(y, z)[0, 1])\nprint(\"cor(z, x):\", np.corrcoef(z, x)[0, 1])\n\n# Perform correlation tests\nprint(\"cor.test(y, x):\", pearsonr(y, x))\nprint(\"cor.test(y, z):\", pearsonr(y, z))\nprint(\"cor.test(z, x):\", pearsonr(z, x))\n\n\n\nBelow is a table with correlation coefficients and \\(p\\)-values associated with the null hypotheses that they are equal to zero.\n\n\n\n\nvars\ncor. coef.\n\\(p\\)-value\n\n\n\n\n\\(cor(X,Y)\\)\n0.68\n0.00\n\n\n\\(cor(Z,Y)\\)\n0.70\n0.00\n\n\n\\(cor(X,Z)\\)\n-0.05\n0.15"
  },
  {
    "objectID": "blog/DONE-correlation-transitive.html#when-is-correlation-transitive",
    "href": "blog/DONE-correlation-transitive.html#when-is-correlation-transitive",
    "title": "Correlation is Not (Always) Transitive",
    "section": "When Is Correlation Transitive",
    "text": "When Is Correlation Transitive\nFrom the equation above it follows that when both \\(cor(X,Y)\\) and \\(cor(Z,Y)\\) are sufficiently large, then \\(cor(X,Z)\\) is sure to be positive (i.e., bounded below by \\(0\\)).\nIn the example above, if we fix \\(cor(X,Y)=.6\\), then we need \\(cor(Z,Y)&gt;.8\\) to guarantee that \\(cor(X,Z)&gt;0\\)."
  },
  {
    "objectID": "blog/DONE-correlation-transitive.html#where-to-learn-more",
    "href": "blog/DONE-correlation-transitive.html#where-to-learn-more",
    "title": "Correlation is Not (Always) Transitive",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMultiple Stack Overflow threads explain this phenomenon from various angles. Olkin (1981) derives some further mathematical results related to transitivity in higher dimensions."
  },
  {
    "objectID": "blog/DONE-correlation-transitive.html#bottom-line",
    "href": "blog/DONE-correlation-transitive.html#bottom-line",
    "title": "Correlation is Not (Always) Transitive",
    "section": "Bottom Line",
    "text": "Bottom Line\n\n\\(X\\) and \\(Z\\) both being correlated with \\(Y\\) does not guarantee that \\(X\\) and \\(Z\\) are correlated with each other.\nThis is the case when the former two correlations are ‚Äúlarge enough.‚Äù"
  },
  {
    "objectID": "blog/DONE-correlation-transitive.html#references",
    "href": "blog/DONE-correlation-transitive.html#references",
    "title": "Correlation is Not (Always) Transitive",
    "section": "References",
    "text": "References\nOlkin, I. (1981). Range restrictions for product-moment correlation matrices. Psychometrika, 46, 469-472. doi:10.1007/BF02293804"
  },
  {
    "objectID": "blog/DONE-bootstrap-limitations.html",
    "href": "blog/DONE-bootstrap-limitations.html",
    "title": "The Bootstrap and its Limitations",
    "section": "",
    "text": "The bootstrap is a powerful resampling technique used to estimate the sampling distribution of a statistic. By repeatedly drawing observations with replacement from the original dataset, it enables practitioners to perform tasks like hypothesis testing, computing standard errors, and constructing confidence intervals‚Äîwithout relying on strong parametric assumptions about the underlying population.\nIt is particularly valuable when analytical expressions for the variance of an estimator are unavailable or computationally complex. Moreover, the bootstrap is grounded in robust statistical theory and offers versatile adaptations, such as the wild bootstrap, which is commonly used for estimating cluster-robust variance. This combination of methodological flexibility and statistical rigor has established the bootstrap as a central tool in modern data science.\nHowever, like any statistical method, the bootstrap has its limitations. This article examines scenarios where the bootstrap will yield unreliable results."
  },
  {
    "objectID": "blog/DONE-bootstrap-limitations.html#background",
    "href": "blog/DONE-bootstrap-limitations.html#background",
    "title": "The Bootstrap and its Limitations",
    "section": "",
    "text": "The bootstrap is a powerful resampling technique used to estimate the sampling distribution of a statistic. By repeatedly drawing observations with replacement from the original dataset, it enables practitioners to perform tasks like hypothesis testing, computing standard errors, and constructing confidence intervals‚Äîwithout relying on strong parametric assumptions about the underlying population.\nIt is particularly valuable when analytical expressions for the variance of an estimator are unavailable or computationally complex. Moreover, the bootstrap is grounded in robust statistical theory and offers versatile adaptations, such as the wild bootstrap, which is commonly used for estimating cluster-robust variance. This combination of methodological flexibility and statistical rigor has established the bootstrap as a central tool in modern data science.\nHowever, like any statistical method, the bootstrap has its limitations. This article examines scenarios where the bootstrap will yield unreliable results."
  },
  {
    "objectID": "blog/DONE-bootstrap-limitations.html#diving-deeper",
    "href": "blog/DONE-bootstrap-limitations.html#diving-deeper",
    "title": "The Bootstrap and its Limitations",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nConsider the following scenarios:\n\nVery Small Sample Sizes ‚Äì The bootstrap relies on resampling the observed data to approximate the population distribution. With very small samples, there is not enough variability in the data to accurately capture the underlying distribution, leading to unreliable estimates.\nParameter at the Edge of the Parameter Space ‚Äì When the parameter being estimated lies at or near a boundary (e.g., estimating a proportion close to 0 or 1), the bootstrap may fail to reflect the true sampling distribution. The resampling process cannot fully mimic the constraints of the parameter space. This includes situations in which we are interested in learning more about the minimum or maximum value of some statistic.\nPresence of Outliers ‚Äì Outliers can heavily influence bootstrap resamples, leading to biased or overly variable estimates.\nDependence in the Data ‚Äì The bootstrap assumes the data are independent and identically distributed (i.i.d.). For time series or spatial data where observations are dependent, naive application of the bootstrap can yield incorrect inferences unless adapted for the structure (e.g., block bootstrap).\nExtreme Skewness or Rare Events ‚Äì When the data distribution is highly skewed or dominated by rare events, the bootstrap may struggle to approximate the tails of the distribution accurately, affecting confidence interval coverage and tail probability estimates.\nMisspecified Models ‚Äì If the bootstrap is applied to a statistic derived from a poorly specified model, the resulting inferences will inherit the same flaws. The bootstrap cannot correct for model misspecification.\n\nIn some of these cases theoretical approximation methods can provide analytical solutions that bypass the resampling challenges. The parametric bootstrap is like a more structured cousin of the standard bootstrap, generating samples based on a known probability distribution. It‚Äôs particularly helpful when you‚Äôve got a good sense of what your data looks like. Bayesian methods take things a step further, folding in prior knowledge to handle tricky statistical scenarios with flexibility.\nWhile the bootstrap is a versatile and often reliable tool, awareness of these limitations can help you avoid potential pitfalls and ensure more robust statistical analyses."
  },
  {
    "objectID": "blog/DONE-bootstrap-limitations.html#bottom-line",
    "href": "blog/DONE-bootstrap-limitations.html#bottom-line",
    "title": "The Bootstrap and its Limitations",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe bootstrap is incredible versatile, but it has its limitations.\nBeware in relying on it when facing any of the situations described above."
  },
  {
    "objectID": "blog/DONE-bootstrap-limitations.html#where-to-learn-more",
    "href": "blog/DONE-bootstrap-limitations.html#where-to-learn-more",
    "title": "The Bootstrap and its Limitations",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great place to start. If you have nailed the basics and are looking for a technical challenge on the bootstrap, Efron and Hastie (2021) is a hidden gem on all things statistical inference, especially the bootstrap. Econometrics geeks might want to dive into James Mackinnon‚Äôs papers cited below for seriously deep details."
  },
  {
    "objectID": "blog/DONE-bootstrap-limitations.html#references",
    "href": "blog/DONE-bootstrap-limitations.html#references",
    "title": "The Bootstrap and its Limitations",
    "section": "References",
    "text": "References\nEfron, B. (2000). The bootstrap and modern statistics. Journal of the American Statistical Association, 95(452), 1293-1296.\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press.\nEfron, B., & Tibshirani, R. J. (1994). An introduction to the bootstrap. Chapman and Hall/CRC.\nMacKinnon, J. G. (2006). Bootstrap methods in econometrics. Economic Record, 82, S2-S18.\nMacKinnon, J. G., & Webb, M. D. (2017). Wild bootstrap inference for wildly different cluster sizes. Journal of Applied Econometrics, 32(2), 233-254.\nMacKinnon, J. G., & Webb, M. D. (2020). Clustering methods for statistical inference. Handbook of labor, human resources and population economics, 1-37."
  },
  {
    "objectID": "blog/DONE-bootstrap-limitations.html#a-closer-look",
    "href": "blog/DONE-bootstrap-limitations.html#a-closer-look",
    "title": "The Bootstrap and its Limitations",
    "section": "A Closer Look",
    "text": "A Closer Look\nConsider the following scenarios:\n\nVery Small Sample Sizes ‚Äì The bootstrap relies on resampling the observed data to approximate the population distribution. With very small samples, there is not enough variability in the data to accurately capture the underlying distribution, leading to unreliable estimates.\nParameter at the Edge of the Parameter Space ‚Äì When the parameter being estimated lies at or near a boundary (e.g., estimating a proportion close to \\(0\\) or \\(1\\)), the bootstrap may fail to reflect the true sampling distribution. The resampling process cannot fully mimic the constraints of the parameter space. This includes situations in which we are interested in learning more about the minimum or maximum value of some statistic.\nPresence of Outliers ‚Äì Outliers can heavily influence bootstrap resamples, leading to biased or overly variable estimates.\nDependence in the Data ‚Äì The bootstrap assumes the data are independent and identically distributed (i.i.d.). For time series or spatial data where observations are dependent, naive application of the bootstrap can yield incorrect inferences unless adapted for the structure (e.g., block bootstrap).\nExtreme Skewness or Rare Events ‚Äì When the data distribution is highly skewed or dominated by rare events, the bootstrap may struggle to approximate the tails of the distribution accurately, affecting confidence interval coverage and tail probability estimates.\nMisspecified Models ‚Äì If the bootstrap is applied to a statistic derived from a poorly specified model, the resulting inferences will inherit the same flaws. The bootstrap cannot correct for model misspecification.\n\nIn some of these cases theoretical approximation methods can provide analytical solutions that bypass the resampling challenges. The parametric bootstrap is like a more structured cousin of the standard bootstrap, generating samples based on a known probability distribution. It‚Äôs particularly helpful when you‚Äôve got a good sense of what your data looks like. Bayesian methods take things a step further, folding in prior knowledge to handle tricky statistical scenarios with flexibility.\nWhile the bootstrap is a versatile and often reliable tool, awareness of these limitations can help you avoid potential pitfalls and ensure more robust statistical analyses."
  },
  {
    "objectID": "blog/DONE-binscatter.html",
    "href": "blog/DONE-binscatter.html",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "",
    "text": "In the realm of data visualization, the classical scatter plot has long been a staple for exploring bivariate relationships. However, as datasets grow larger and more complex, traditional scatter plots can become cluttered and less informative. Privacy concerns may also limit the ability to plot raw data, and simple bivariate plots often fail to reveal causal relationships. This is where binscatter, or binned scatter plots, come into play.\nBinscatter offers a cleaner, more interpretable way to visualize the relationship between two variables, especially when dealing with large datasets. By aggregating data points into bins and plotting the average outcome within each bin, binscatter simplifies the visualization, making it easier to discern patterns and trends. It‚Äôs particularly useful for:\n\nIntuitive visualization for large datasets by grouping data into bins.\nHighlighting trends and relationship between variables effectively.\nExtending these ideas to control for covariates.\n\nIn this article, I will introduce binscatter, explore its mathematical foundation, and demonstrate its utility with an example in R and python."
  },
  {
    "objectID": "blog/DONE-binscatter.html#background",
    "href": "blog/DONE-binscatter.html#background",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "",
    "text": "In the realm of data visualization, the classical scatter plot has long been a staple for exploring bivariate relationships. However, as datasets grow larger and more complex, traditional scatter plots can become cluttered and less informative. Privacy concerns may also limit the ability to plot raw data, and simple bivariate plots often fail to reveal causal relationships. This is where binscatter, or binned scatter plots, come into play.\nBinscatter offers a cleaner, more interpretable way to visualize the relationship between two variables, especially when dealing with large datasets. By aggregating data points into bins and plotting the average outcome within each bin, binscatter simplifies the visualization, making it easier to discern patterns and trends. It‚Äôs particularly useful for:\n\nIntuitive visualization for large datasets by grouping data into bins.\nHighlighting trends and relationship between variables effectively.\nExtending these ideas to control for covariates.\n\nIn this article, I will introduce binscatter, explore its mathematical foundation, and demonstrate its utility with an example in R and python."
  },
  {
    "objectID": "blog/DONE-binscatter.html#notation",
    "href": "blog/DONE-binscatter.html#notation",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Notation",
    "text": "Notation\nTo formalize binscatter, let‚Äôs define the following:\n\n\\(X\\): The independent/predictor variable.\n\\(Y\\): The dependent/outcome/response variable.\n\\(n\\): The number of observations in the dataset.\n\\(K\\): The number of bins into which \\(X\\) is divided.\n\\(\\bar{Y}_k\\): The mean of \\(Y\\) for observations falling in the \\(k\\)-th bin of \\(X\\). Similarly for \\(\\bar{X}_k\\).\n\\(B_k\\)‚Äã: The observations falling in the \\(k\\)-th bin.\n\\(W\\): The covariate to be controlled. This can be a vector too."
  },
  {
    "objectID": "blog/DONE-binscatter.html#diving-deeper",
    "href": "blog/DONE-binscatter.html#diving-deeper",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nFormal Definition\nA binscatter plot is constructed by partitioning the range of the independent variable X into a fixed number of \\(K\\) bins, \\(B_1,\\dots,B_K\\) typically using empirical quantiles. This ensures each bin is of roughly the same size. Within each bin, the average value of the dependent variable \\(Y\\) is calculated. These averages are then plotted against the midpoint of each bin, \\(\\bar{X}\\), resulting in a series of points that represent an estimate of conditional mean of \\(Y\\) given \\(X\\), $ E[Y|X]$.\nIn technical jargon binscatter provides a nonparametric estimate of the conditional mean function, offering a visual summary of the relationship between the two variables. The resulting graph allows assessment of linearity, monotonicity, convexity, etc.\nHere is the step-by-step recipe for construcing a binscatter plot.\n\n\nAlgorithm:\nBin construction: Divide the range of X into K equal-width bins, or use quantile-based bins for equal sample sizes within bins. For example, with \\(K=10\\), the observations in B_1 would be those between the minimimum value of \\(X\\) and that of its \\(10\\)th percentile. Mean calculation: Compute the mean of Y within each bin:\n\\[\\bar{Y}_k= \\frac{1}{|B_k|} \\sum_{i \\in B_k} Y_i,\\]\nwhere \\(\\midB_k\\mid\\) is the number of observations in bin \\(B_k\\)‚Äã. Plotting: Plot {Y}_k against the midpoints of each bin, \\(\\bar{X}_k\\). Software Package: binsreg.\nQuite simple, right? Let‚Äôs explore certain useful extensions of this idea.\n\n\nAdjusting for Covariates: The Wrong Way\nIn many applications, it is essential to control for additional covariates \\(W\\) to isolate the relationship between the primary variables of interest. The object of interest then becomes the conditional mean \\(E[Y\\mid W,X]\\). An example would be focusing on the relationship between income (\\(Y\\)) and education level (\\(X\\)) when controling for parental education (W).\nA common but flawed approach to incorporating covariates in binscatter is residualized binscatter. This method involves first regressing separately both \\(Y\\) and \\(X\\) on the covariates \\(W\\) to obtain residuals \\(\\hat{u}_Y\\)‚Äã and \\(\\hat{u}_X\\)‚Äã, and then applying the binscatter method to these residuals:\n\\[\\bar{\\hat{u}}_{Y,k} = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\hat{u}_{X,i}.\\]\nWhile this approach is motivated by the Frisch-Waugh-Lovell theorem in linear regression, it can lead to incorrect conclusions in more general settings. The residualized binscatter may not accurately reflect the true conditional mean function, especially if the underlying relationship is nonlinear. Therefore, it is generally not recommended for empirical work.\n\n\nAdjusting for Covariates: The Right Way\nInstead, this should be done using a semi-parametric partially linear regression model. This is achieved by modeling the conditional mean function as\n\\[Y = \\mu_0(X) + W \\gamma_0 + \\varepsilon,\\]\nwhere \\(\\mu_0(X)\\) captures the main effect of \\(X\\), and \\(W' \\gamma_0\\) adjusts for the influence of additional covariates. Rather than residualizing, we estimate \\(\\mu_0(X)\\) using the least-squares approach:\n\\[(\\hat{\\beta}, \\hat{\\gamma}) = \\arg\\min_{\\beta, \\gamma} \\sum (Y- b(X)' \\beta - W' \\gamma)^2,\\]\nwhere \\(b(X)\\) represents the binning basis functions. The final binscatter plot displays the estimated conditional mean function\n\\[\\hat{\\mu}(X_k) = b(X_k)' \\hat{\\beta}\\]\nagainst \\(\\bar{X}_k\\), ensuring a correct visualization of the relationship between \\(X\\) and \\(Y\\) after accounting for the covariates \\(W\\)."
  },
  {
    "objectID": "blog/DONE-binscatter.html#practical-considerations",
    "href": "blog/DONE-binscatter.html#practical-considerations",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Practical Considerations",
    "text": "Practical Considerations\nA key decision is the choice of the number of bins \\(K\\). Too few bins can oversmooth the data, masking important features, while too many bins can lead to undersmoothing, resulting in a noisy and less interpretable plot. An optimal choice of K balances bias and variance, often determined using data-driven methods. To address this, Cattaneo et al.¬†(2024) propose an adaptive, Integrated Mean Squared Error (IMSE)-optimal choice of K for which get a plug-in formula.\nThoughtful data scientist always have variance in their mind. If, for instance, we see some linear relationship between \\(Y\\) and \\(X\\), how can we determine whether it is statistically significant? Quantifying the uncertainty around binscatter estimates is crucial. The authors also discuss constructing confidence bands, which can be added to the plot to visually represent estimation uncertainty, enhancing both interpretability and reliability."
  },
  {
    "objectID": "blog/DONE-binscatter.html#an-example",
    "href": "blog/DONE-binscatter.html#an-example",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "An Example",
    "text": "An Example\nAs an example let‚Äôs examine the relationship between the variables Sepal.Length and Petal.Length in the popular iris dataset. We will use a fixed number of ten bins. Alternatively, the package binsreg will automatically calculate the optimal \\(K\\).\n\nRPython\n\n\n# clear the workspace and load libraries\nrm(list=ls())\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(binsreg)\ndata(iris)\n\n# define the number of bins\nbins &lt;- 10\n\n# create binned data\niris_binned &lt;- iris %&gt;%\n  mutate(bin = cut(Sepal.Length, breaks = bins, include.lowest = TRUE)) %&gt;%\n  group_by(bin) %&gt;%\n  summarize(\n    bin_mid = mean(as.numeric(as.character(bin))),\n    mean_petal_length = mean(Petal.Length)\n  )\n\n# Add a panel label for the raw scatter plot\niris_raw &lt;- iris %&gt;% \n    mutate(panel = \"1. Raw Scatter Plot\")\n\n# Add a panel label for the binned scatter plot\niris_binned &lt;- iris_binned %&gt;%\n  mutate(panel = \"2. Binned Scatter Plot\")\nWe have split the data into ten bins, now let‚Äôs plot it.\n\n# Combine raw and binned data into a single dataset for plotting\nplot_data &lt;- bind_rows(\niris_raw %&gt;% rename(x = Sepal.Length, y = Petal.Length),\n  iris_binned %&gt;% rename(x = bin_mid, y = mean_petal_length)\n)\n\n# Create the plot\nggplot(plot_data, aes(x = x, y = y)) +\n  geom_point() +\n  facet_wrap(~ panel, scales = \"free_x\", ncol = 2) +\n  labs(title = \"Comparison of Raw and Binned Scatter Plots\",\n  x = \"Sepal Length\",\n  y = \"Petal Length\") +\n  theme_minimal()\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Number of bins\nbins = 10\n\n# Create binned data\niris['bin'] = pd.cut(iris['Sepal.Length'], bins=bins, include_lowest=True)\niris_binned = iris.groupby('bin').agg(\n    bin_mid=('Sepal.Length', lambda x: (x.min() + x.max()) / 2),\n    mean_petal_length=('Petal.Length', 'mean')\n).reset_index()\n\n# Add panel labels\niris_raw = iris[['Sepal.Length', 'Petal.Length']].copy()\niris_raw['panel'] = \"1. Raw Scatter Plot\"\n\niris_binned = iris_binned.rename(columns={'bin_mid': 'Sepal.Length', 'mean_petal_length': 'Petal.Length'})\niris_binned['panel'] = \"2. Binned Scatter Plot\"\n\n# Combine raw and binned data\nplot_data = pd.concat([iris_raw, iris_binned], ignore_index=True)\n\n# Plot\nsns.set_theme(style=\"whitegrid\")\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Raw scatter plot\nsns.scatterplot(\n    data=plot_data[plot_data['panel'] == \"1. Raw Scatter Plot\"],\n    x='Sepal.Length', y='Petal.Length', ax=axes[0]\n)\naxes[0].set_title(\"1. Raw Scatter Plot\")\naxes[0].set_xlabel(\"Sepal Length\")\naxes[0].set_ylabel(\"Petal Length\")\n\n# Binned scatter plot\nsns.scatterplot(\n    data=plot_data[plot_data['panel'] == \"2. Binned Scatter Plot\"],\n    x='Sepal.Length', y='Petal.Length', ax=axes[1]\n)\naxes[1].set_title(\"2. Binned Scatter Plot\")\naxes[1].set_xlabel(\"Sepal Length\")\n\n# Adjust layout\nplt.suptitle(\"Comparison of Raw and Binned Scatter Plots\")\nplt.tight_layout()\nplt.show()\n\n\n\nHere is the resulting image. The left scatter plot displays the raw data and the right one shows the binscatter. Binscatter removes some of the clutter and highlights the linear relationship more directly."
  },
  {
    "objectID": "blog/DONE-binscatter.html#bottom-line",
    "href": "blog/DONE-binscatter.html#bottom-line",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBinscatter simplifies scatterplots by aggregating data into bins and plotting means.\nIt is a powerful tool for visualizing relationships in large or noisy datasets.\nConditional and residualized binscatter extend its utility to controlling for covariates.\nWhile intuitive, binscatter is sensitive to binning choices and may obscure nuances."
  },
  {
    "objectID": "blog/DONE-binscatter.html#where-to-learn-more",
    "href": "blog/DONE-binscatter.html#where-to-learn-more",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nBoth papers cited below are relatively accessible and will answer your questions. Start with Starr and Goldfarb (2020)."
  },
  {
    "objectID": "blog/DONE-binscatter.html#references",
    "href": "blog/DONE-binscatter.html#references",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "References",
    "text": "References\nCattaneo, M. D., Crump, R. K., Farrell, M. H., & Feng, Y. (2024). On Binscatter Regressions. American Economic Review, 111(3), 718‚Äì748.\nStarr, E., & Goldfarb, B. (2020). Binned scatterplots: A simple tool to make research easier and better. Strategic Management Journal, 41(12), 2261-2274."
  },
  {
    "objectID": "blog/DONE-binscatter.html#a-closer-look",
    "href": "blog/DONE-binscatter.html#a-closer-look",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFormal Definition\nA binscatter plot is constructed by partitioning the range of the independent variable X into a fixed number of \\(K\\) bins, \\(B_1,\\dots,B_K\\) typically using empirical quantiles. This ensures each bin is of roughly the same size. Within each bin, the average value of the dependent variable \\(Y\\) is calculated. These averages are then plotted against the midpoint of each bin, \\(\\bar{X}\\), resulting in a series of points that represent an estimate of conditional mean of \\(Y\\) given \\(X\\), \\(E[Y\\mid X]\\).\nIn technical jargon binscatter provides a nonparametric estimate of the conditional mean function, offering a visual summary of the relationship between the two variables. The resulting graph allows assessment of linearity, monotonicity, convexity, etc.\nHere is the step-by-step recipe for construcing a binscatter plot.\n\n\nThe Algorithm\n\nBin construction: Divide the range of \\(X\\) into \\(K\\) equal-width bins, or use quantile-based bins for equal sample sizes within bins. For example, with \\(K=10\\), the observations in \\(B_1\\) would be those between the minimimum value of \\(X\\) and that of its tenth percentile.\nMean calculation: Compute the mean of \\(Y\\) within each bin:\n\n\\[\\bar{Y}_k= \\frac{1}{|B_k|} \\sum_{i \\in B_k} Y_i,\\]\nwhere \\(|B_k|\\) is the number of observations in bin \\(B_k\\)‚Äã.\n\nPlotting: Plot \\(\\bar{Y}_k\\) against the midpoints of each bin, \\(\\bar{X}_k\\).\n\nSoftware Package: binsreg.\nQuite simple, right? Let‚Äôs explore certain useful extensions of this idea.\n\n\nAdjusting for Covariates: The Wrong Way\nIn many applications, it is essential to control for additional covariates \\(W\\) to isolate the relationship between the primary variables of interest. The object of interest then becomes the conditional mean \\(E[Y\\mid W,X]\\). An example would be focusing on the relationship between income (\\(Y\\)) and education level (\\(X\\)) when controling for parental education (\\(W\\)).\nA common but flawed approach to incorporating covariates in binscatter is residualized binscatter. This method involves first regressing separately both \\(Y\\) and \\(X\\) on the covariates \\(W\\) to obtain residuals \\(\\hat{u}_Y\\)‚Äã and \\(\\hat{u}_X\\)‚Äã, and then applying the binscatter method to these residuals:\n\\[\\bar{\\hat{u}}_{Y,k} = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\hat{u}_{X,i}.\\]\nWhile this approach is motivated by the Frisch-Waugh-Lovell theorem in linear regression, it can lead to incorrect conclusions in more general settings. The residualized binscatter may not accurately reflect the true conditional mean function, especially if the underlying relationship is nonlinear. Therefore, it is generally not recommended for empirical work.\n\n\nAdjusting for Covariates: The Right Way\nInstead, this should be done using a semi-parametric partially linear regression model. This is achieved by modeling the conditional mean function as\n\\[Y = \\mu_0(X) + W \\gamma_0 + \\varepsilon,\\]\nwhere \\(\\mu_0(X)\\) captures the main effect of \\(X\\), and \\(W' \\gamma_0\\) adjusts for the influence of additional covariates. Rather than residualizing, we estimate \\(\\mu_0(X)\\) using the least-squares approach:\n\\[(\\hat{\\beta}, \\hat{\\gamma}) = \\arg\\min_{\\beta, \\gamma} \\sum (Y- b(X)' \\beta - W' \\gamma)^2,\\]\nwhere \\(b(X)\\) represents the binning basis functions. The final binscatter plot displays the estimated conditional mean function\n\\[\\hat{\\mu}(X_k) = b(X_k)' \\hat{\\beta}\\]\nagainst \\(\\bar{X}_k\\), ensuring a correct visualization of the relationship between \\(X\\) and \\(Y\\) after accounting for the covariates \\(W\\).\n\n\nPractical Considerations\nA key decision is the choice of the number of bins \\(K\\). Too few bins can oversmooth the data, masking important features, while too many bins can lead to undersmoothing, resulting in a noisy and less interpretable plot. An optimal choice of \\(K\\) balances bias and variance, often determined using data-driven methods. To address this, Cattaneo et al.¬†(2024) propose an adaptive, Integrated Mean Squared Error (IMSE)-optimal choice of \\(K\\) for which get a plug-in formula.\nThoughtful data scientist always have variance in their mind. If, for instance, we see some linear relationship between \\(Y\\) and \\(X\\), how can we determine whether it is statistically significant? Quantifying the uncertainty around binscatter estimates is crucial. The authors also discuss constructing confidence bands, which can be added to the plot to visually represent estimation uncertainty, enhancing both interpretability and reliability."
  },
  {
    "objectID": "blog/DONE-gen-vars-predefined-corr.html",
    "href": "blog/DONE-gen-vars-predefined-corr.html",
    "title": "Generating Variables with Predefined Correlation",
    "section": "",
    "text": "Suppose you are working on a project where the relationship between two variables is influenced by an unobserved confounder, and you want to simulate data that reflects this dependency. Standard random number generators often assume independence between variables, making them unsuitable for this task. Instead, you need a method to introduce specific correlations into your data generation process.\nA powerful and efficient way to achieve this is through Cholesky decomposition. By decomposing a correlation matrix into its triangular components, you can transform independent random variables into correlated ones. This approach is versatile, efficient, and mathematically grounded, making it ideal for simulating realistic datasets with predefined (linear) relationships."
  },
  {
    "objectID": "blog/DONE-gen-vars-predefined-corr.html#background",
    "href": "blog/DONE-gen-vars-predefined-corr.html#background",
    "title": "Generating Variables with Predefined Correlation",
    "section": "",
    "text": "Suppose you are working on a project where the relationship between two variables is influenced by an unobserved confounder, and you want to simulate data that reflects this dependency. Standard random number generators often assume independence between variables, making them unsuitable for this task. Instead, you need a method to introduce specific correlations into your data generation process.\nA powerful and efficient way to achieve this is through Cholesky decomposition. By decomposing a correlation matrix into its triangular components, you can transform independent random variables into correlated ones. This approach is versatile, efficient, and mathematically grounded, making it ideal for simulating realistic datasets with predefined (linear) relationships."
  },
  {
    "objectID": "blog/DONE-gen-vars-predefined-corr.html#a-closer-look",
    "href": "blog/DONE-gen-vars-predefined-corr.html#a-closer-look",
    "title": "Generating Variables with Predefined Correlation",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Algorithm\nAssume we want to generate a vector Y with n observations and p variables with a target correlation matrix \\(\\Sigma\\). The algorithm to obtain \\(Y\\) is as follows:\n\nStart with Independent Variables: Create a matrix \\(X\\) of dimensions \\(n \\times p\\), where each column is independently drawn from N(0,1): \\[ X = \\begin{bmatrix}x_{11} & x_{12} & \\cdots & x_{1p} \\\\x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\x_{n1} & x_{n2} & \\cdots & x_{np}.\\end{bmatrix} \\]\nDecompose the Target Matrix: Perform Cholesky decomposition on the target correlation matrix \\(\\Sigma\\) as: \\[\\Sigma = LL^T,\\] where \\(L\\) is a lower triangular matrix.\nTransform the Independent Variables: Multiply the independent variable matrix \\(X\\) by \\(L\\) to obtain the correlated variables: \\[Y = XL.\\]\n\nHere \\(Y\\) is an \\(n\\times p\\) matrix where the columns have the desired correlation structure defined by \\(\\Sigma\\). To ensure that \\(\\Sigma\\) is a valid correlation matrix, it must be positive-definite. This condition guarantees the success of Cholesky decomposition and the correctness of the resulting correlated variables.\n\n\nMathematical Explanation\nLet‚Äôs examine how and why this approach works. We know that \\(\\Sigma = LL^T\\) and \\(E(XX^T)=I\\) by definition. We want to show that \\(E(YY^T)=LL^T\\). Here is the simplest way to get there:\n\\[\\begin{align*}\nE(YY^T) &= E((LX)(LX)^T) \\\\\n        &= E(LXX^TL^T) \\\\\n        &= LE(XX^T)L^T \\\\\n        &= LL^T.\n\\end{align*}\\]\nThere you have it ‚Äì the algorithm outlined above is mathematically grounded. The covariance matrix of \\(Y\\) is indeed equal to \\(\\Sigma\\). Let‚Äôs now look at an example."
  },
  {
    "objectID": "blog/DONE-gen-vars-predefined-corr.html#an-example",
    "href": "blog/DONE-gen-vars-predefined-corr.html#an-example",
    "title": "Generating Variables with Predefined Correlation",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs implement this in R and python with \\(p=3\\) and \\(n=1,000\\). Our target correlation matrix defines the desired relationships between the variables in \\(Y\\). In our example, we have pairwise correlations equal to \\(0.8\\) (b/w \\(y_1\\) and \\(y_2\\)), \\(0.5\\) (b/w \\(y_1\\) and \\(y_3\\)), and \\(0.3\\) (b/w \\(y_2\\) and \\(y_3\\)).\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\n# Generate X, independent standard normal variables\nn &lt;- 1000 \np &lt;- 3   \nx &lt;- matrix(rnorm(n * p), nrow = n, ncol = p)\n\n# Define Sigma, the target correlation matrix\nsigma &lt;- matrix(c(\n  1.0, 0.8, 0.5,\n  0.8, 1.0, 0.3,\n  0.5, 0.3, 1.0\n), nrow = p, byrow = TRUE)\n\n# Cholesky decomposition\nL &lt;- t(chol(sigma))\ndiag &lt;- diag(c(1,1,1))\ny &lt;- t(diag %*% L %*% t(x))\n\n\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(1988)\n\n# Generate X, independent standard normal variables\nn = 1000\np = 3\nx = np.random.normal(size=(n, p))\n\n# Define Sigma, the target correlation matrix\nsigma = np.array([\n    [1.0, 0.8, 0.5],\n    [0.8, 1.0, 0.3],\n    [0.5, 0.3, 1.0]\n])\n\n# Cholesky decomposition\nL = np.linalg.cholesky(sigma).T\ndiag = np.diag([1, 1, 1])\ny = (diag @ L @ x.T).T\n\n\n\nUsing our notation above we have:\n\\[\\Sigma = \\begin{bmatrix}1.0 & 0.8 & 0.5 \\\\0.8 & 1.0 & 0.3 \\\\ 0.5 & 0.3 &1.0\\end{bmatrix}. \\]\nThe chol function in R decomposes the matrix into a lower triangular matrix. In our example:\n\\[L^T = \\begin{bmatrix}1 & 0.8 & 0.5 \\\\0 & 0.6 & -0.17 \\\\0 & 0.0 & 0.85 \\end{bmatrix}. \\]\nMultiplying the independent variables \\(X\\) by the transpose of \\(L\\) ensures the output \\(Y\\) matches the specified correlation structure.\nThe cor function checks whether the generated data conforms to the target correlation matrix.\nThe two matrices match almost exactly. We can also visualize the three variables in a scatterplot matrix. Notice that higher correlation values (e.g., b/w \\(y_1\\) and \\(y_2\\)) correspond to stronger linear associations between."
  },
  {
    "objectID": "blog/DONE-gen-vars-predefined-corr.html#bottom-line",
    "href": "blog/DONE-gen-vars-predefined-corr.html#bottom-line",
    "title": "Generating Variables with Predefined Correlation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nA common data practitioner‚Äôs need is to generate variables with a predefined correlation structure.\nCholesky decomposition offers a powerful and efficient way to achieve this."
  },
  {
    "objectID": "blog/DONE-correlation-is-cosine.html#correlation-as-a-dot-product",
    "href": "blog/DONE-correlation-is-cosine.html#correlation-as-a-dot-product",
    "title": "Correlation is a Cosine",
    "section": "Correlation as a Dot Product",
    "text": "Correlation as a Dot Product\nThere‚Äôs another way to see this connection that makes it even clearer.\nIf you think of \\(A\\) and \\(B\\) as vectors in \\(n\\)-dimensional space (e.g., \\(A = (a_1, a_2, \\ldots, a_n)\\)), the cosine of the angle between them is given by:\n\\[\n\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|},\n\\]\nwhere \\(A \\cdot B\\) is the dot product, and \\(\\|\\cdot\\|\\) denotes the Euclidean norm. When \\(A\\) and \\(B\\) are standardized (i.e., mean zero and unit variance), this cosine becomes the Pearson correlation coefficient:\n\\[\n\\text{corr}(A, B) = \\frac{1}{n} \\sum_{i=1}^n A_i^* B_i^* \\approx \\cos(\\theta),\n\\]\nwhere \\(A^*\\) and \\(B^*\\) are the standardized versions of \\(A\\) and \\(B\\)."
  },
  {
    "objectID": "blog/DONE-correlation-is-cosine.html#cosine-similarity-vs.-correlation",
    "href": "blog/DONE-correlation-is-cosine.html#cosine-similarity-vs.-correlation",
    "title": "Correlation is a Cosine",
    "section": "Cosine Similarity vs.¬†Correlation",
    "text": "Cosine Similarity vs.¬†Correlation\n\n\n\n\n\n\nCosine similarity vs.¬†correlation\n\n\n\nCosine similarity and Pearson correlation are closely related, but not always the same:\n\nCosine similarity considers only the angle between vectors. It‚Äôs scale-invariant but not shift-invariant.\nCorrelation removes both mean and scale, making it invariant to affine transformations.\n\nSo, while ‚Äúcorrelation is a cosine,‚Äù the statement is strictly true when you‚Äôre working with standardized vectors."
  },
  {
    "objectID": "blog/DONE-correlation-is-cosine.html#an-example",
    "href": "blog/DONE-correlation-is-cosine.html#an-example",
    "title": "Correlation is a Cosine",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs generate two random vectors, standardize them, compute their correlation and angle, and plot them as vectors. We‚Äôll also see how correlation equals the cosine of the angle between the vectors.\n\nRPython\n\n\nset.seed(1988)\n\n# Generate two random vectors\nA &lt;- rnorm(100)\nB &lt;- 0.8 * A + sqrt(1 - 0.8^2) * rnorm(100)  # Correlated with A\n\n# Standardize\nA_std &lt;- scale(A)\nB_std &lt;- scale(B)\n\n# Correlation and cosine\ncorrelation &lt;- cor(A, B)\ncosine &lt;- sum(A_std * B_std) / (sqrt(sum(A_std^2)) * sqrt(sum(B_std^2)))\nangle_deg &lt;- acos(cosine) * 180 / pi\n\n# Print results\ncat(\"Correlation:\", round(correlation, 3), \"\\n\")\ncat(\"Angle (degrees):\", round(angle_deg, 1), \"\\n\")\n\n# Plot vectors\nplot(c(0, A_std[1]), c(0, B_std[1]), type = \"n\", xlab = \"A (standardized)\", ylab = \"B (standardized)\",\n     main = \"First Vectors from A and B\")\narrows(0, 0, A_std[1], 0, col = \"blue\", lwd = 2)\narrows(0, 0, A_std[1], B_std[1], col = \"red\", lwd = 2)\nlegend(\"topright\", legend = c(\"A\", \"B\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1988)\n\n# Generate two correlated vectors\nA = np.random.randn(100)\nB = 0.8 * A + np.sqrt(1 - 0.8**2) * np.random.randn(100)\n\n# Standardize\nA_std = (A - np.mean(A)) / np.std(A)\nB_std = (B - np.mean(B)) / np.std(B)\n\n# Correlation and cosine\ncorrelation = np.corrcoef(A, B)[0, 1]\ncosine = np.dot(A_std, B_std) / (np.linalg.norm(A_std) * np.linalg.norm(B_std))\nangle = np.arccos(np.clip(cosine, -1, 1)) * 180 / np.pi\n\nprint(f\"Correlation: {correlation:.3f}\")\nprint(f\"Angle (degrees): {angle:.1f}\")\n\n# Plot first two vectors\nplt.figure(figsize=(5, 5))\nplt.quiver(0, 0, A_std[0], 0, angles='xy', scale_units='xy', scale=1, color='blue', label='A')\nplt.quiver(0, 0, A_std[0], B_std[0], angles='xy', scale_units='xy', scale=1, color='red', label='B')\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nplt.xlabel(\"A (standardized)\")\nplt.ylabel(\"B (standardized)\")\nplt.title(\"First Vectors from A and B\")\nplt.legend()\nplt.grid(True)\nplt.gca().set_aspect('equal')\nplt.show()\n\n\n\nIn this example, \\(cor(A,B)=0.825\\) and the angle between the two vectors is \\(34.4^\\circ\\), and we have \\(cos(34.4^\\circ) = cor(A,B)=0.825\\). You can also visualize these vectors, but I am not showing that graph here."
  },
  {
    "objectID": "blog/DONE-correlation-is-cosine.html#references",
    "href": "blog/DONE-correlation-is-cosine.html#references",
    "title": "Correlation is a Cosine",
    "section": "References",
    "text": "References\nCosines and correlation, Cook 2010, blog post"
  },
  {
    "objectID": "blog/TODO-column-sampling-bootstrap.html",
    "href": "blog/TODO-column-sampling-bootstrap.html",
    "title": "Column-Sampling Bootstrap?",
    "section": "",
    "text": "The bootstrap is a versatile resampling technique traditionally focused on rows. Let‚Äôs add a twist to the plain vanilla bootstrap. Imagine you have a wide dataset‚Äîmany variables but few rows‚Äîand want to test the statistical significance of a correlation between two variables. An example is a genetic dataset with thousands of columns (genetic information and outcomes) but a limited number of rows (patients). Can you use the bootstrap to determine if the correlation between a specific gene-outcome pair is statistically significant?\nOne creative approach is resampling columns instead of rows, generating a distribution of correlation coefficients to assess the significance of your observed correlation."
  },
  {
    "objectID": "blog/TODO-column-sampling-bootstrap.html#background",
    "href": "blog/TODO-column-sampling-bootstrap.html#background",
    "title": "Column-Sampling Bootstrap?",
    "section": "",
    "text": "The bootstrap is a versatile resampling technique traditionally focused on rows. Let‚Äôs add a twist to the plain vanilla bootstrap. Imagine you have a wide dataset‚Äîmany variables but few rows‚Äîand want to test the statistical significance of a correlation between two variables. An example is a genetic dataset with thousands of columns (genetic information and outcomes) but a limited number of rows (patients). Can you use the bootstrap to determine if the correlation between a specific gene-outcome pair is statistically significant?\nOne creative approach is resampling columns instead of rows, generating a distribution of correlation coefficients to assess the significance of your observed correlation."
  },
  {
    "objectID": "blog/TODO-column-sampling-bootstrap.html#diving-deeper",
    "href": "blog/TODO-column-sampling-bootstrap.html#diving-deeper",
    "title": "Column-Sampling Bootstrap?",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nDefinition\nHere‚Äôs the basic algorithm:\n\nRandomly sample columns from your dataset with replacement to create fake dataset.\nCompute their correlation coefficient.\nRepeat this many times.\nCompare your observed correlation to the distribution of these synthetic correlations.\nDeclare statistical significance if the observed correlation appears as an ‚Äúoutlier‚Äù in this synthetic distribution.\n\nThis approach allows you to explore a large number of possible correlations in a computationally efficient way. But does it actually work? Let‚Äôs unpack the key considerations.\nThe column-sampling bootstrap is most valuable when a dataset has many columns but too few rows for traditional bootstrap methods. The abundance of columns provides a rich sampling landscape. The goal is determining whether a correlation is significantly stronger or weaker than what might occur by chance. Let‚Äôs simplify the problem and ignore any issues stemming from being unable to estimate the correlation coefficients well enough.\n\n\nChallenges\nHowever, several critical challenges exist. The method assumes columns are independent and identically distributed (i.i.d.), which rarely holds in practice. Columns often represent related variables‚Äîlike gene measurements or interconnected phenomena‚Äîand these dependencies can bias resampled correlations. Moreover, by resampling columns, you ignore row-level relationships, such as connections in time series or grouped data (like patients from the same household).\nInterpreting the null distribution presents another significant challenge. Synthetic correlation coefficients generated through column resampling might not represent a meaningful null hypothesis. If your dataset contains highly correlated features, the null distribution could shift, potentially leading to misleading conclusions. Unlike traditional bootstrapping‚Äîwhere samples reflect a subpopulation‚Äîthis method lacks that fundamental connection.\n\n\nThe Verdict\nWhile the column-sampling bootstrap is an intriguing concept, it will likely prove useful only in very specific, carefully constrained settings."
  },
  {
    "objectID": "blog/TODO-column-sampling-bootstrap.html#an-example",
    "href": "blog/TODO-column-sampling-bootstrap.html#an-example",
    "title": "Column-Sampling Bootstrap?",
    "section": "An Example",
    "text": "An Example\nWhile we should be skeptical of the column-sampling bootstrap in practical applications, it can be instructive to see how we might implement it.\nBelow is a sample R and python code illustrating the main concept. We begin with setting up a synthetic dataset.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\ndata &lt;- as.data.frame(matrix(rnorm(1000), nrow = 50, ncol = 20))\nobserved_correlation &lt;- cor(data[[1]], data[[2]])\n\n# Perform the resampling\nn_bootstrap &lt;- 1000  # Number of bootstrap iterations\nn_columns &lt;- ncol(data)  # Total number of columns in the dataset\nbootstrap_correlations &lt;- numeric(n_bootstrap)\n\nfor (i in 1:n_bootstrap) {\n  resampled_columns &lt;- sample(1:n_columns, size = n_columns, replace = TRUE)\n  resampled_data &lt;- data[, resampled_columns]\n  bootstrap_correlations[i] &lt;- cor(resampled_data[[1]], resampled_data[[2]])\n}\n\n# Test the significance of the observed correlation\np_value &lt;- mean(abs(bootstrap_correlations) &gt;= abs(observed_correlation))\n\n# Print the results\ncat(\"Observed Correlation:\", observed_correlation, \"\\n\")\ncat(\"P-value:\", p_value, \"\\n\")\n\n\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(1988)\n\n# Generate synthetic dataset\ndata = np.random.normal(size=(50, 20))  # 50 rows, 20 columns\nobserved_correlation = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n\n# Perform the resampling\nn_bootstrap = 1000  # Number of bootstrap iterations\nn_columns = data.shape[1]  # Total number of columns in the dataset\nbootstrap_correlations = []\n\nfor _ in range(n_bootstrap):\n    # Resample columns with replacement\n    resampled_columns = np.random.choice(n_columns, size=n_columns, replace=True)\n    resampled_data = data[:, resampled_columns]\n    # Compute correlation between the first two columns of the resampled data\n    bootstrap_correlations.append(np.corrcoef(resampled_data[:, 0], resampled_data[:, 1])[0, 1])\n\n# Test the significance of the observed correlation\nbootstrap_correlations = np.array(bootstrap_correlations)\np_value = np.mean(np.abs(bootstrap_correlations) &gt;= np.abs(observed_correlation))\n\n# Print the results\nprint(\"Observed Correlation:\", observed_correlation)\nprint(\"P-value:\", p_value)\n\n\n\nThe observed correlation is quite low and equal to \\(0.58\\). Its associated p-value is \\(0.676\\), consistent with the value not being statistically significant."
  },
  {
    "objectID": "blog/TODO-column-sampling-bootstrap.html#bottom-line",
    "href": "blog/TODO-column-sampling-bootstrap.html#bottom-line",
    "title": "Column-Sampling Bootstrap?",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe column-sampling bootstrap is a thought-proviking twist on traditional resampling techniques that leverages the width of your dataset.\nWhile it offers computational efficiency and flexibility, its reliance on the i.i.d. assumption and potential to overlook row-level dependencies highlight the need for careful application.\nThe column-sampling bootrap should not be your go-to method to assess statistical significance."
  },
  {
    "objectID": "blog/TODO-limits-parametric-models.html",
    "href": "blog/TODO-limits-parametric-models.html",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "",
    "text": "[This article is part of a series of explorations of the limits of statistical models. If the material below is too challenging, I suggest you start here.]"
  },
  {
    "objectID": "blog/TODO-limits-parametric-models.html#background",
    "href": "blog/TODO-limits-parametric-models.html#background",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Background",
    "text": "Background\nObtaining the lowest possible variance is a primary goal for anyone working with statistical models. Efficiency (or precision), as is the jargon, is a cornerstone of statistics and econometrics, guiding us toward estimators that extract the maximum possible information from the data. It can make or break a data project.\nThe Cram√©r-Rao lower bound (CRLB) plays a pivotal role in this context by establishing a theoretical limit on the variance of unbiased estimators. Unbiased estimators are those that yield the true answer (on average), rendering them a highly attractive class of methods. The CRLB highlights the best achievable precision for parameter estimation based on the Fisher information in the data. This article explores the theoretical foundation of the CRLB, its computation, and its implications for practical estimation.\nIn what follows, I am concerned with unbiased estimators, a common practice that should not be taken for granted. As a counterexample, consider the James-Stein estimator‚Äîa biased but attractive technique."
  },
  {
    "objectID": "blog/TODO-limits-parametric-models.html#notation",
    "href": "blog/TODO-limits-parametric-models.html#notation",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Notation",
    "text": "Notation\nBefore diving in, let‚Äôs establish a unified notation to structure the mathematical discussion:\n\nLet X denote the observed data, with \\(X_1, X_2, \\dots, X_n\\) being n independent and identically distributed (i.i.d.) observations.\nThe model governing the data is characterized by a (finite-dimensional) parameter \\(\\theta \\in \\mathbb{R}^d\\) which we aim to estimate.\nThe likelihood of the data is \\(f(x; \\theta)\\), fully specified by the parameter \\(\\theta\\)."
  },
  {
    "objectID": "blog/TODO-limits-parametric-models.html#diving-deeper",
    "href": "blog/TODO-limits-parametric-models.html#diving-deeper",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nThe Cram√©r-Rao lower bound provides a theoretical benchmark for how precise an unbiased estimator can be. It sets the minimum variance that any unbiased estimator of a parameter \\(\\theta\\) can achieve, given a specific data-generating process.\n\nThe CRLB Formula\nFor a parameter \\(\\theta\\) in a parametric model with likelihood \\(f(x; \\theta)\\), the CRLB is expressed as:\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{I(\\theta)},\\]\nwhere \\(I(\\theta)\\) is the Fisher information (FI), defined as:\n\\[I(\\theta) = \\mathbb{E}\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 \\right].\\]\n\n\nIntuition\nTo understand the CRLB, we must delve into the concept of Fisher information named after one of the modern fathers of statistics R.A. Fisher. Intuitively, FI quantifies how much information the observed data carries about the parameter \\(\\theta\\).\nThink of the likelihood function \\(f(x; \\theta)\\) as describing the probability of observing a given dataset \\(x\\) for a particular value of \\(\\theta\\). If the likelihood changes sharply with \\(\\theta\\) (i.e., \\(\\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta)\\) is large), small changes in \\(\\theta\\) lead to noticeable differences in the likelihood. This variability reflects high information: the data can ‚Äúpinpoint‚Äù \\(\\theta\\) with greater precision. Conversely, if the likelihood changes slowly with \\(\\theta\\), the data offers less information about its true value.\nMathematically, the Fisher information \\(I(\\theta)\\) is the variance of the the partial derivative\n\\[\\frac{\\partial}{\\partial \\theta} logf(x;\\theta),\\]\nwhich we refer to as the score function. This score measures how sensitive the likelihood function is to changes in \\(\\theta\\). Higher variance in the score corresponds to more precise information about \\(\\theta\\).\n\n\nPractical Application\nThe CRLB provides a benchmark for evaluating the performance of estimators. For example, if you propose an unbiased estimator \\(\\hat{\\theta}\\), you can compare its variance to the CRLB. If \\(\\text{Var}(\\hat{\\theta}) = \\frac{1}{I(\\theta)}\\), we say the estimator is efficient. However, if the variance is higher, there may be room to improve the estimation method.\nMoreover, the CRLB also offers insight into the difficulty of estimating a parameter. If \\(I(\\theta)\\) is ‚Äúsmall‚Äù, so that the bound on the variance is high, then no unbiased estimator can achieve high precision with the available data. It is possible to develop a biased estimator for \\(\\theta\\) with lower variance, but it is not clear why you would do that."
  },
  {
    "objectID": "blog/TODO-limits-parametric-models.html#an-example",
    "href": "blog/TODO-limits-parametric-models.html#an-example",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "An Example",
    "text": "An Example\nImagine you are estimating the mean of a normal distribution, where \\(X \\sim N(\\mu, \\sigma^2)\\), and \\(\\sigma^2\\) is known. The likelihood for a single observation \\(x_i\\) is:\n\\[f(x_i;\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2 \\sigma^2}}.\\]\nUsing the Fisher information defintion given above, taking the derivative and simplifying, we find:\n\\[I(\\mu)=  \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 = \\frac{1}{\\sigma^2}.\\]\nFor n independent observations, this expression becomes:\n\\[I(\\mu)=\\frac{n}{\\sigma^2}.\\]\nThe CRLB for the variance of any unbiased estimator of is:\n\\[\\text{Var}(\\hat{\\mu})\\geq \\frac{\\sigma^2}{n}\\]\nThis result aligns with our intuition: as n increases, the precision of our estimate improves. In other words, more data leads to more informative results."
  },
  {
    "objectID": "blog/TODO-limits-parametric-models.html#where-to-learn-more",
    "href": "blog/TODO-limits-parametric-models.html#where-to-learn-more",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAny graduate econometrics textbook will do. Personally, my grad school nightmares were induced by Greene‚Äôs textbook (cited below). It can be dry but certainly contains what you need to know."
  },
  {
    "objectID": "blog/TODO-limits-parametric-models.html#bottom-line",
    "href": "blog/TODO-limits-parametric-models.html#bottom-line",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe CRLB establishes a theoretical lower limit on the variance of unbiased estimators, serving as a benchmark for efficiency.\nFisher information measures the sensitivity of the likelihood to changes in the parameter \\(\\theta\\), linking the amount of information in the data to the precision of estimation.\nEfficient estimators achieve the CRLB and are optimal under the given model assumptions."
  },
  {
    "objectID": "blog/TODO-limits-parametric-models.html#references",
    "href": "blog/TODO-limits-parametric-models.html#references",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "References",
    "text": "References\nGreene, William H. ‚ÄúEconometric analysis‚Äù. New Jersey: Prentice Hall (2000): 201-215."
  },
  {
    "objectID": "blog/TODO-weights-statistics.html",
    "href": "blog/TODO-weights-statistics.html",
    "title": "Weights in Statistical Analyses",
    "section": "",
    "text": "Weights in statistical analyses offer a way to assign varying importance to observations in a dataset. Although powerful, they can be quite confusing due to the various types of weights available. In this article, I will unpack the details behind the most common types of weights used in data science.\nMost types of statistical analyses can be performed with weights. These include calculating summary statistics, regression models, bootstrap, etc. Even maximum likelihood estimation is minimally affected. In this article, I will keep it simple and only discuss mean and variance estimation.\nThe two primary types of weights encountered in practice are sampling weights and frequency weights. I will explore each in turn and conclude with a comparative example.\nTo begin, let‚Äôs imagine a well-behaved random variable X of which we have an iid sample of size n.¬†I will use w to denote the relevant weighting variable."
  },
  {
    "objectID": "blog/TODO-weights-statistics.html#background",
    "href": "blog/TODO-weights-statistics.html#background",
    "title": "Weights in Statistical Analyses",
    "section": "",
    "text": "Weights in statistical analyses offer a way to assign varying importance to observations in a dataset. Although powerful, they can be quite confusing due to the various types of weights available. In this article, I will unpack the details behind the most common types of weights used in data science.\nMost types of statistical analyses can be performed with weights. These include calculating summary statistics, regression models, bootstrap, etc. Even maximum likelihood estimation is minimally affected. In this article, I will keep it simple and only discuss mean and variance estimation.\nThe two primary types of weights encountered in practice are sampling weights and frequency weights. I will explore each in turn and conclude with a comparative example.\nTo begin, let‚Äôs imagine a well-behaved random variable X of which we have an iid sample of size n.¬†I will use w to denote the relevant weighting variable."
  },
  {
    "objectID": "blog/TODO-weights-statistics.html#diving-deeper",
    "href": "blog/TODO-weights-statistics.html#diving-deeper",
    "title": "Weights in Statistical Analyses",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nMean estimation does not depend on the type of weights. We have:\n\\[ \\bar{X} = \\frac{\\sum_i wX}{\\sum_i w}. \\]\nVariance estimation depends on the weight type.\nRemember that we estimate the standard error of \\(\\bar{X}\\) as:\n\\[ SE(\\bar{X})=\\frac{s}{\\sqrt{n}}, \\hspace{.3cm} \\text{where} \\hspace{.3cm} s=\\sqrt{\\frac{1}{N-1}\\sum_i(X-\\bar{X})^2} \\]\nis an estimate of the standard deviation of \\(X\\). I will explain below how estimation of \\(SE(\\bar{X})\\) differs for sampling and frequency weights. Imporantly, s_w will denote the weighted version of this standard error.\n\nSampling Weights\nSampling weights measure the inverse probability of an observation entering the sample. They are particularly relevant in surveys when some units in the population are sampled more frequently than others. These weights adjust for sampling discrepancies to ensure that survey estimates are representative of the entire population. Sampling weights are also sometimes called probability weights.\nIntuitively, if we want to use the survey to accurately represent the population, we should assign higher importance to observations that are less likely to be sampled and lower importance to those more likely to appear in our data. This is exactly what sampling weights achieve. Sampling weights are, thus, inversely proportional to the probability of selection. Therefore, a smaller weight indicates a higher probability of being sampled, and vice versa.\nFor example, households in rural areas might be less likely to enter a survey due to the increased resources required to reach remote locations. Conversely, urban households are typically easier to sample and thus more likely to appear in the data.\nThis weighting approach is also common in causal inference analyses using propensity score methods. Similarly, in machine learning, weighting is often employed to adjust for unbalanced samples in the context of rare events (e.g., fraud detection, cancer diagnosis).\nLet‚Äôs now get back to the discussion on variance estimation. With sampling weights, we have:\n\\[ SE^{\\text{sampling}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{S}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\frac{(\\sum_i w)^2}{\\sum_i w^2}}}\\]\nThe numerator is the weighted version of the sum of squared deviations from the mean, emphasizing observations with higher weights. The denominator normalizes the standard error based on the total sum of the weights. When the weights differ greatly‚Äîhigh weights make some observations much more influential, increasing uncertainty about the population mean and so the sampling-weighted standard error is often larger.\nI will move on to describing frequency weights.\nSoftware Package: survey.\n\n\nFrequency Weights\nFrequency weights measure the number of times an observation should be counted in the analysis. They are most relevant when there are multiple identical observations or when duplicating observations is possible. Naturally, higher weights correspond to observations that appear more frequently. Frequency weights are common in aggregated datasets. For instance, with market- or city-level data, we might want to weight the rows by market size, thus assigning more importance to larger units.\nWe can gain intuition from a linear algebra perspective. In a regression context, the design matrix X must be of full rank (to ensure invertibility), which implies no two rows can be identical. We thus need to collapse X to keep only distinct rows and record the number of times each row appears in the original data. This record constitutes the frequency weights.\nThe formula for estimating the standard error of X with frequency weights is:\n\\[ SE^{\\text{frequency}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{F}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\sum_i w}}.\\]\nHere \\(n_{\\text{eff}}\\) is the effective sample size (i.e., the sum of all weights) and \\(s_w\\) is the weighted standard deviation of \\(X\\).\nThe frequency-weighted standard error is typically the smaller because it increases the effective sample size without introducing variability in the contribution of different observations. In contrast, the sampling-weighted standard error could be larger if some observations are given much higher weights, increasing the variability and lowering the precision.\nSoftware Package: survey\n\n\nOne More Thing\nThere is actually one more type of weights which are not so commonly used in practice, precision weights. They represent the precision (\\(1/\\text{variance}\\)) of observations. A weight of 5, for example, reflects 5 times the precision of a weight of 1, originally based on averaging 5 replicate observations. Precision weights often come up in statistical theory in places such as Generalized Least Squares where they promise efficiency gains (i.e., lower variance) relative to traditional Ordinary Least Squares estimation. In practice, precision weights are in fact frequency weights normalized to sum to n.¬†Lastly, Stata‚Äôs user guide refers to them as analytic weights."
  },
  {
    "objectID": "blog/TODO-weights-statistics.html#an-example",
    "href": "blog/TODO-weights-statistics.html#an-example",
    "title": "Weights in Statistical Analyses",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs see all of this in practice. We begin by creating a fake dataset of a variable \\(X\\) with both sampling and frequency weights. The weights are randomly created and hence have no underlying meaning, so think of this example as a tutorial, without a strong focus on the results.\nlibrary(survey) rm(list=ls()) set.seed(681) n &lt;- 100000\ndata &lt;- data.frame( x = rnorm(n), prob_selection = runif(n, .1, .9), freq_weight = rpois(n, 3) ) data\\(samp_weight &lt;- 1 / data\\)prob_selection\nNow let‚Äôs calculate the average value of \\(X\\) using both types of weights.\ndesign_unweight &lt;- svydesign(ids = ~1, data = data, weights = ~1) design_samp &lt;- svydesign(ids = ~1, data = data, weights = ~samp_weight) design_freq &lt;- svydesign(ids = ~1, data = data, weights = ~freq_weight)\nmean_unweight &lt;- svymean(~x, design_unweight) mean_samp &lt;- svymean(~x, design_samp) mean_freq &lt;- svymean(~x, design_freq)\nIn the code above I assigned the associated weight to a given survey design before computing the means. Now it‚Äôs time to see the results.\n\nprint(round(mean_unweight, digits=3)) mean SE x -0.002 0.0032\n\n\nprint(round(mean_samp, digits=3)) mean SE x 0 0.0038\n\n\nprint(round(mean_freq, digits=3)) mean SE x -0.002 0.0037\n\nThe unweighted and frequency-weighted means match exactly, (I am not sure why the sampling-weighted mean is slightly lower.) while the variances are different. The variance with frequency weights is lower than that of sampling weights.\nYou can find my code in this GitHub repo."
  },
  {
    "objectID": "blog/TODO-weights-statistics.html#bottom-line",
    "href": "blog/TODO-weights-statistics.html#bottom-line",
    "title": "Weights in Statistical Analyses",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWeights are one of the most confusing aspects of working with data.\nSampling and frequency weights are the most common types of weights found in practice.\nThe former measure the inverse probability of being sampled, while the latter represent the number of times an observation enters the sample.\nWhile weighting usually does not impact point estimates (e.g., regression coefficients, means), incorrect usage of weights can lead to inaccurate confidence intervals and p-values.\nThis is relevant only if (i) your dataset contains weights, and (ii) you are interested in population-level statistics."
  },
  {
    "objectID": "blog/TODO-weights-statistics.html#where-to-learn-more",
    "href": "blog/TODO-weights-statistics.html#where-to-learn-more",
    "title": "Weights in Statistical Analyses",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nGoogle is a great starting place. Lumley‚Äôs blog post titled Weights in Statistics was incredibly helpful in preparing this article. Stata‚Äôs manuals which are publically available contain more detailed information on various types of weighting schemes. See also Solon et al.¬†(2015) for using weights in causal inference."
  },
  {
    "objectID": "blog/TODO-weights-statistics.html#references",
    "href": "blog/TODO-weights-statistics.html#references",
    "title": "Weights in Statistical Analyses",
    "section": "References",
    "text": "References\nDupraz, Y. (2013). Using Weights in Stata. Memo, 54(2)\nLumley, T. (2020), Weights in Statistics, Blog Post\nSolon, G., Haider, S. J., & Wooldridge, J. M. (2015). What are we weighting for?. Journal of Human resources, 50(2), 301-316.\nStata User‚Äôs Guide (2023) https://www.stata.com/manuals/u.pdf"
  },
  {
    "objectID": "blog/TODO-chatterjee-correlation.html",
    "href": "blog/TODO-chatterjee-correlation.html",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "",
    "text": "Much of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford published a paper outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\nIn this article I will go over the basics of Chatterjee‚Äôs correlation measure. Before we get there, let‚Äôs first review some of the more traditional approaches in assessing bivariate relationship strength.\nFor simplicity, let‚Äôs assume away ties. Let‚Äôs also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant."
  },
  {
    "objectID": "blog/TODO-chatterjee-correlation.html#background",
    "href": "blog/TODO-chatterjee-correlation.html#background",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "",
    "text": "Much of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford published a paper outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\nIn this article I will go over the basics of Chatterjee‚Äôs correlation measure. Before we get there, let‚Äôs first review some of the more traditional approaches in assessing bivariate relationship strength.\nFor simplicity, let‚Äôs assume away ties. Let‚Äôs also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant."
  },
  {
    "objectID": "blog/TODO-chatterjee-correlation.html#linear-relationships",
    "href": "blog/TODO-chatterjee-correlation.html#linear-relationships",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Linear Relationships",
    "text": "Linear Relationships\nWhen we simply say ‚Äúcorrelation‚Äù we refer to the so-called Pearson correlation coefficient. Virtually everyone working with data is familiar with it. Given a random sample \\((X_1,Y_1),\\dots,(X_n, Y_n)\\) of two random variables \\(X\\) and \\(Y\\) it is computed by:\n\\[corr^{P}(X,Y) = \\frac{ \\sum_i(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}},\\]\nwhere an upper bar denotes a sample mean.\nThis coefficient lives in the \\([-1,1]\\) interval. Larger values indicate stronger relationship between \\(X\\) and \\(Y\\), be it positive or negative. At the extreme, the Pearson coefficient will equal \\(1\\) when all observations can be perfectly lined up on a upward sloping line. Yes, you guessed it ‚Äì when it equals \\(-1\\) the line is sloping down.\nYou can easily calculate the Pearson correlation in R:\ncor(x,y, method = ‚Äòpearson‚Äô) cor.test(x,y, method = ‚Äòpearson‚Äô, alternative=‚Äòtwo.sided‚Äô).\nThis measure, while widely popular, suffers from a few shortcomings. First, outliers have an outsized impact in skewing its value. Sample means vulnerable to outliers are a key ingredient in the calculation, rendering the measure sensitive to data anomalies. Second, it is designed to detect only linear relationships. Two variables might have a strong but non-linear relationship which this measure will not detect. Lastly, it is not transformation-invariant, meaning that applying a monotone transformation to either of the variables will change the correlation value.\nLet‚Äôs discuss some improvements to the Pearson correlation measure. Enter Spearman correlation."
  },
  {
    "objectID": "blog/TODO-chatterjee-correlation.html#monotone-relationships",
    "href": "blog/TODO-chatterjee-correlation.html#monotone-relationships",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Monotone Relationships",
    "text": "Monotone Relationships\nSpearman correlation is the Pearson correlation among the ranks of \\(X\\) and \\(Y\\):\n\\[corr^{S}(X,Y) = corr^{P}(R(X),R(Y)),\\]\nwhere \\(R(\\cdot)\\) denotes an observation‚Äôs rank (or order) in the sample.\nSpearman correlation is thus a rank correlation measure. As such, it quantifies how well the relationship between \\(X\\) and \\(Y\\) can be described using a monotone (and not necessarily a linear) function. It is therefore a more flexible measure of association. Again, intuitively, Spearman correlation will take on a large positive value when the \\(X\\) and \\(Y\\) observations have similar ranks. This value will be negative when the ranks tend to go in opposite directions.\nSpearman correlation addresses some of the shortcomings associated with Pearson correlation. It is not easily influenced by outliers, and it does not change if we apply a monotone transformation of \\(X\\) and/or \\(Y\\). These benefits come at the expense of a loss in interpretation and potential issues when tied ranks are common, a scenario I ignore here.\nCalculating it in R is just as simple:\ncor(x,y, method = ‚Äòspearman‚Äô) cor.test(x,y, method = ‚Äòspearman‚Äô, alternative=‚Äòtwo.sided‚Äô).\nSpearman correlation is not the only rank correlation coefficient out there. –ê popular alternative is the Kendall rank coefficient which is computed slightly differently. Let‚Äôs define a pair of observations \\((X_i, Y_i)\\) and \\((X_j, Y_j)\\) to be agreeing (the technical term is concordant) if the differences \\((X_i - X_j)\\) and \\((Y_i - Y_j)\\) have the same sign (i.e., either both \\(X_i &gt; X_j\\) and $Y_i &gt; Y_j $or both \\(X_i &lt; X_j\\) and \\(Y_i &lt; Y_j\\) ).\nThen, Kendall‚Äôs coefficient is expressed as:\n\\[corr^{K} = \\frac{\\text{number of agreeing pairs} - \\text{number of disagreeing pairs}} {\\text{total number of pairs}}.\\]\nSo, it quantifies the degree of agreement between the ranks of \\(X\\) and \\(Y\\). Like the other coeffients described above, its range is \\([-1, 1]\\) and values away from zero indicate stronger relationship.\nAgain, this coefficient is similarly computed in R:\ncor(x,y, method = ‚Äòkendall‚Äô) cor.test(x,y, method = ‚Äòkendall‚Äô, alternative=‚Äòtwo.sided‚Äô).\nKendall‚Äôs measure improves on some of the shortcomings baked in the Spearman‚Äôs coefficients ‚Äì it has a clearer interpretation and it is less sensitive to rank ties.\nHowever, none of these rank correlation coefficients can detect non-monotonic relationships. For instance, \\(X\\) and \\(Y\\) can have a parabola- or wave-like pattern when plotted against each other. We would like a correlation measure flexible enough to capture such non-linear relationships.\nThis is where Chatterjee‚Äôs coefficient comes in."
  },
  {
    "objectID": "blog/TODO-chatterjee-correlation.html#more-general-relationships",
    "href": "blog/TODO-chatterjee-correlation.html#more-general-relationships",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "More General Relationships",
    "text": "More General Relationships\nChatterjee recently proposed a new correlation coefficient designed to detect non-monotonic relationships. He discovered a novel estimator of a population quantity first proposed by Dette et al.¬†(2013).\nLet‚Äôs start with the formula. The Chatterjee correlation coefficient is calculated as follows:\n\\[corr^{C}(X,Y) = 1-\\frac{3\\sum_{i=1}^{n-1}|R(Y_{k:R(X_k)=i+1}) - R(Y_{k:R(X_k)=i})|}{n^2-1},\\]\nwhere n is the sample size. This looks complicated, so let‚Äôs try to simplify the numerator. Let‚Äôs sort the data in an ascending order of \\(X\\) so that we have \\((X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})\\), where \\(X_{(1)}&lt;X_{(2)}&lt;\\dots &lt;X_{(n)}\\). Also, denote \\(R(Y_i)\\) be the rank of \\(Y_{(i)}\\). Then:\n\\[corr^{C}(X,Y) = 1 - \\frac{3\\sum_{i=1}^{n-1}|R(Y_{i+1})-R(Y_i)|}{n^2-1}.\\]\nSo, this new coefficient is a scaled version of the sum of the absolute differences in the consecutive ranks of \\(Y\\) when ordered by \\(X\\). It is perhaps best to think about Chatterjee‚Äôs method as a measure of dependence and not strictly a correlation coefficient.\nThere are some major difference comapred to the previous correlation measures. Chatterjee‚Äôs correlation coefficient lies in the \\([0,1]\\) interval. It is equal to zero if and only if \\(X\\) and \\(Y\\) are independent and to one if one of them is a function of the other. Unlike the coefficients descibed above, it is not symmetric in \\(X\\) and \\(Y\\), meaning \\(corr^{C}(X,Y) \\neq corr^{C}(Y,X)\\). This is understandable since we are interested in whether \\(X\\) is a function of \\(Y\\), which does not imply the opposite. The author also develops asymptotic theory for calculating p-values although some researchers have raised concerns about the coefficient‚Äôs power.\nHere is a sample R code to calculate its value:\nn &lt;- 1000 x &lt;- runif(n) y &lt;- 5 * sin(x) + rnorm(n)\ndata &lt;- data.frame(x=x, y=y) data\\(R &lt;- rank(data\\)y) data &lt;- data[order(data$x), ]\n1 - 3 * sum(abs(diff(data$R))) / (n^2-1)\nAlternatively, you can used the XICOR R package.\nThere you have it. You are now well-equipped to dive deeper into your datasets and find new exciting relationships."
  },
  {
    "objectID": "blog/TODO-chatterjee-correlation.html#bottom-line",
    "href": "blog/TODO-chatterjee-correlation.html#bottom-line",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThere are numerous ways of measuring association between two variables.\nThe most common methods measure only linear or monotonic relationships. These are often useful but do not capture more complex, non-linear associations.\nA new correlation measure, Chatterjee‚Äôs coeffient, is designed to go beyond monotonicty and assess more general bivariate relationships."
  },
  {
    "objectID": "blog/TODO-chatterjee-correlation.html#where-to-learn-more",
    "href": "blog/TODO-chatterjee-correlation.html#where-to-learn-more",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia has detailed entries on correlation, rank correlation, and Kendall‚Äôs coefficient which I found helpful. The R bloggers platform has articles exploring the Chatterjee‚Äôs correlation coefficient in detail. The more technically oriented folks will find Chatterjee‚Äôs original paper helpful."
  },
  {
    "objectID": "blog/TODO-chatterjee-correlation.html#references",
    "href": "blog/TODO-chatterjee-correlation.html#references",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "References",
    "text": "References\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\nDette, H., Siburg, K. F., & Stoimenov, P. A. (2013). A Copula‚ÄêBased Non‚Äêparametric Measure of Regression Dependence. Scandinavian Journal of Statistics, 40(1), 21-41.\nShi, H., Drton, M., & Han, F. (2022). On the power of Chatterjee‚Äôs rank correlation. Biometrika, 109(2), 317-333.\nhttps://www.r-bloggers.com/2021/12/exploring-the-xi-correlation-coefficient/"
  },
  {
    "objectID": "blog/TODO-causality-wo-experiments.html",
    "href": "blog/TODO-causality-wo-experiments.html",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "",
    "text": "Causality is central to many practical data-related questions. Conventional methods for isolating causal relationships rely on experimentation, assume unconfoundedness, or require instrumental variables. However, experimentation is often infeasible, costly, or ethically concerning; good instruments are notoriously difficult to find; and unconfoundedness can be an uncomfortable assumption in many settings.\nThis article highlights methods for measuring causality beyond these three paradigms. These underappreciated approaches exploit higher moments and heteroscedastic error structures (Lewbel 2012, Rigobon 2003), latent instrumental variables (IVs) (Ebbes et al.¬†2005), and copulas (Park and Gupta 2012). I will unite them in a common statistical framework and discuss the key assumptions underlying each one.\nThe focus will be on the ideas, intuition, and practical aspects of these methodologies, rather than technical details. Readers can find more in-depth information in the References section. This article assumes familiarity with econometric endogeneity and the basics of instrumental variables; without this background, some sections may be challenging to follow.\nNote: Regression discontinuity (RD) methods are excluded from this discussion, as they fall somewhere between instrument-based and instrument-free econometric methodologies. We know that in fuzzy RDs, the running variable can be viewed as an instrument."
  },
  {
    "objectID": "blog/TODO-causality-wo-experiments.html#background",
    "href": "blog/TODO-causality-wo-experiments.html#background",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "",
    "text": "Causality is central to many practical data-related questions. Conventional methods for isolating causal relationships rely on experimentation, assume unconfoundedness, or require instrumental variables. However, experimentation is often infeasible, costly, or ethically concerning; good instruments are notoriously difficult to find; and unconfoundedness can be an uncomfortable assumption in many settings.\nThis article highlights methods for measuring causality beyond these three paradigms. These underappreciated approaches exploit higher moments and heteroscedastic error structures (Lewbel 2012, Rigobon 2003), latent instrumental variables (IVs) (Ebbes et al.¬†2005), and copulas (Park and Gupta 2012). I will unite them in a common statistical framework and discuss the key assumptions underlying each one.\nThe focus will be on the ideas, intuition, and practical aspects of these methodologies, rather than technical details. Readers can find more in-depth information in the References section. This article assumes familiarity with econometric endogeneity and the basics of instrumental variables; without this background, some sections may be challenging to follow.\nNote: Regression discontinuity (RD) methods are excluded from this discussion, as they fall somewhere between instrument-based and instrument-free econometric methodologies. We know that in fuzzy RDs, the running variable can be viewed as an instrument."
  },
  {
    "objectID": "blog/TODO-causality-wo-experiments.html#notation-and-setup",
    "href": "blog/TODO-causality-wo-experiments.html#notation-and-setup",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Notation and Setup",
    "text": "Notation and Setup\nLet‚Äôs begin by establishing some basic notation. We aim to analyze the impact of a binary, endogenous treatment variable \\(X_1\\) on an outcome variable \\(Y\\), in a setting with exogenous variables \\(X_2\\). We have access to a well-behaved, representative iid sample of size \\(n\\) of \\(Y\\), and \\(X:=[X_1, X_2]\\). These variables are related as follows:\n\\[ Y = \\beta X_1 + \\gamma X_2 + \\epsilon, \\]\nwhere is a mean-zero error term. Our goal is to obtain a consistent estimate of \\(\\beta\\). For simplicity, we‚Äôre using the same notation for both single- and vector-valued quantities, as \\(X_2\\) can be in \\(\\mathbb{R}^p\\) with $ p&gt;1$.\nThe challenge arises because \\(X_1\\) and \\(\\epsilon\\) are correlated, rendering the standard OLS estimator inconsistent. Even in large samples, \\(\\hat{\\beta}_{OLS}\\) will be biased, and getting more data would not help. Specifically:\n\\[\\hat{\\beta}_{OLS} := (X'X)^{-1}X'Y \\nrightarrow \\beta. \\]\nStandard instrument-based methods rely on the existence of an instrumental variable \\(Z\\) which, conditional on \\(X_2\\), correlates with \\(X_1\\) but not with . Estimation then proceeds with 2SLS, LIML, or GMM, potentially yielding good estimates of \\(\\beta\\) given appropriate assumptions. Common issues with instrumental variables include implausibility of the exclusion restriction, weak correlation with \\(X_1\\), and challenges in interpreting \\(\\hat{\\beta}_{IV}\\). Formally:\n\\[\\hat{\\beta}_{IV} := (Z'X)^{-1}Z'Y \\rightarrow \\beta. \\]\nIn this article, we focus on obtaining correct estimates of \\(\\beta\\) in settings where we don‚Äôt have access to such an instrumental variable \\(Z\\)."
  },
  {
    "objectID": "blog/TODO-causality-wo-experiments.html#diving-deeper",
    "href": "blog/TODO-causality-wo-experiments.html#diving-deeper",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nLet‚Äôs start with the heteroskedasticity-based approach of Lewbel (2012).\n\nMethod 1: Heteroskedasticity & Higher Moments\nThe main idea here is to construct valid instruments for \\(X_1\\) by using information contained in the heteroskedasticity of \\(\\epsilon\\). Intuitively, if \\(\\epsilon\\) exhibits heteroskedasticity related to $ X_2$, we can use to create instruments‚Äîspecifically by interacting \\(X_2\\) with the residuals of the endogenous regressor‚Äôs reduced form equation. So, this is an IV-based method, but the instrument is ‚Äúinternal‚Äù to the model and does not rely on any external information.\nThe key assumptions are:\nThe error term in the structural equation (\\(\\epsilon\\)) is heteroskedastic. This means \\(var(\\epsilon|X_2)\\) is not constant and depends on \\(X_2\\). Moreover, we need \\(cov(X_2,\\epsilon^2) \\neq 0\\). This is an analogue of the first stage assumption in IV methods. The exogenous variable (\\(X_2\\)) are uncorrelated with the product of the endogeneous variable (\\(X_1\\)) and the error term (\\(\\epsilon\\)). That is, \\(cov(X_2, X_1\\epsilon) = 0\\). This is a form of the standard exogeneity assumption in IV estimation. The heteroskedasticity-based estimator of Lewbel (2012) proceeds in two steps:\nRegress X_1 on X_2 and save the estimated residuals; call them \\(\\hat{u}\\). Construct an instrument for \\(X_1\\) as \\(\\tilde{Z}=(X_2-\\bar{X}_2)\\hat{u}\\), where \\(\\bar{X}_2\\) is the mean of \\(X_2\\). Use \\(\\tilde{Z}\\) as an instrument in a standard 2SLS estimation: \\[\\hat{\\beta}_{LEWBEL} = (X'P_{\\tilde{Z}}X)^{-1}X'P_{\\tilde{Z}}Y,\\]\nwhere \\(P_{\\tilde{Z}}\\) ‚Äãis the projection matrix onto the instrument.\nThis line of thought can also be extended to use higher moments as an alternative or additional way to construct instrumental variables. The original approach uses the variance of the error term, but we can also rely on skewness, kurtosis, etc. The assumptions then must be modified such that these higher moments are correlated with the endogenous variable, etc.\nSoftware Packages: REndo, ivlewbel.\nLet‚Äôs now move to the second set of methods.\n\n\nMethod 2: Latent IVs\nThe latent IV approach imposes distributional assumptions on the exogenous part of the endogenous variable and employs likelihood-based methods to estimate \\(\\beta\\).\nLet‚Äôs simplify the model above, so that we have:\n\\[ Y=\\beta X + \\epsilon, \\]\nwhere \\(X\\) is endogenous. The key idea is to decompose \\(X\\) into two components:\n\\[ X= \\theta + \\nu, \\]\nwith \\(cov(\\theta, \\epsilon)=0\\), \\(cov(\\theta, \\nu) = 0\\), and \\(cov(\\epsilon, \\nu)\\neq0\\). The first condition states that \\(\\theta\\) is the exogenous part of \\(X\\), and the last one gives rise to the endogeneity problem.\nWe then proceed with adding distributional assumptions. Importantly, \\(\\theta\\) must follow some discrete distribution with a finite number of mass points. A common example imposes:\n\\[ \\theta \\sim \\text{Multinomial}(\\cdot) \\]\nand\n\\[ (\\epsilon, \\nu) \\sim \\text{Gaussian}(\\cdot). \\]\nThese set of assumptions lead to analytical solutions for the conditional and unconditional distributions of \\((Y,X)\\) and all parameters of the model are identified. Maximum likelihood estimation can then give us an estimate of \\(\\beta_{LIV}\\).\nSoftware Packages: REndo.\nFinally, we turn our attention to the final set of instrument-free methods to solve endogeneity.\n\n\nMethod 3: Copulas\nFirst, a word on copulas. A copula is a multivariate cumulative distribution function (CDF) with uniform marginals on \\([0,1]\\). An old theorem states that any multivariate CDF can be expressed with uniform marginals and a copula function that represents the relationship between the variables. Specifically, if \\(A\\) and \\(B\\) are two random variables with marginal CDFs \\(F_A\\) and \\(F_B\\) and joint CDF \\(H\\), then there exists a copula \\(C\\) such that \\(H(a,b)=C(F_A(a), F_B(b))\\).\nHow does this fit into our context and framework? Park and Gupta (2012) introduced two estimation methods for \\(\\beta\\) under the assumption that \\(\\epsilon \\sim Gaussian(\\cdot)\\). The key idea is positing a Gaussian copula to link the marginal distributions of X and and obtain their joint distribution. We can then estimate $$in one of two ways: either impose distributional assumptions on these marginals and derive and maximize the joint likelihood function of \\(X\\) and \\(\\epsilon\\), or use a generated regressor approach. We will focus on the latter.\nIn the linear model, endogeneity is tackled by creating a novel variable \\(\\tilde{X}\\) and adding that as a control (i.e., a generated regressor). Using our simplified model where \\(X\\) is single-valued and endogenous, we now have:\n\\[Y=\\beta X + \\mu \\tilde{X} + \\eta, \\]\nwhere \\(\\eta\\) is the error term in this augmented model.\nWe construct \\(\\tilde{X}\\) as follows:\n\\[\\tilde{X}=\\Phi^{-1}(\\hat{F}_X(X)).\\]\nHere \\(\\Phi^{-1}(\\cdot)\\) is the inverse CDF of the standard normal distribution and \\(F_X(\\cdot)\\) is the marginal CDF of \\(X\\). We can estimate the latter using the empirical CDF by sorting the observations in ascending order and calculating the proportion of rows with smaller values for each observation. As you can guess, this introduces further uncertainty into the model, so the standard errors should be estimated using bootstrap.\nSoftware Packages: REndo, copula.\n\n\nComparison\nEach statistical method has its strengths and limitations. While the methods described here circumvent the traditional unconfoundedness and external instruments-based assumptions, they do not provide a magical panacea to the endogeneity problem. Instead, they rely on their own, different assumptions. These methods are not universally superior, but should be considered when traditional approaches do not fit your context.\nThe heteroskedasticity-based approach, as the name suggests, requires a considerable degree of heteroskedasticity to perform well. Latent IVs may offer efficiency advantages but come at the cost of imposing distributional assumptions and requiring a group structure of \\(X_1\\). The copula-based approach, while simple to implement, also requires strong assumptions about the distributions of \\(X\\) and \\(Y\\), as well as their relationship.\nThat‚Äôs it. You are now equipped with a set of new methods designed to identify causal relationships in your data."
  },
  {
    "objectID": "blog/TODO-causality-wo-experiments.html#bottom-line",
    "href": "blog/TODO-causality-wo-experiments.html#bottom-line",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nConventional methods used to tease causality rely on experiments or ambitious assumptions such as unconfoudedness or the access to valid instrumental variables.\nResearchers have developed methods aimed at measuring causality without relying on these frameworks.\nNone of these are a panacea and they rely on their own assumptions that have to be checked on a case-by-case basis."
  },
  {
    "objectID": "blog/TODO-causality-wo-experiments.html#where-to-learn-more",
    "href": "blog/TODO-causality-wo-experiments.html#where-to-learn-more",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nEbbes, Wedel, and Bockenholt (2009), Park and Gupta (2012), Papies, Ebbes, and Heerde (2017), and Rutz and Watson (2019) provide detailed comparisons of these IV-free methods with alternative methods. Also, Qian et al.¬†(2024) and Papadopolous (2022) and Baum and Lewbel (2019) have a practical angle that many data scientist will find accessible and attractive."
  },
  {
    "objectID": "blog/TODO-causality-wo-experiments.html#references",
    "href": "blog/TODO-causality-wo-experiments.html#references",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "References",
    "text": "References\nBaum, C. F., & Lewbel, A. (2019). Advice on using heteroskedasticity-based identification. The Stata Journal, 19(4), 757-767.\nEbbes, P. (2004). Latent instrumental variables: a new approach to solve for endogeneity.\nEbbes, P., Wedel, M., & B√∂ckenholt, U. (2009). Frugal IV alternatives to identify the parameter for an endogenous regressor. Journal of Applied Econometrics, 24(3), 446-468.\nEbbes, P., Wedel, M., B√∂ckenholt, U., & Steerneman, T. (2005). Solving and testing for regressor-error (in) dependence when no IVs are available: With new evidence for the effect of education on income. Quantitative Marketing and Economics, 3, 365-392.\nErickson, T., & Whited, T. M. (2002). Two-step GMM estimation of the errors-in-variables model using high-order moments. Econometric Theory, 18(3), 776-799.\nGui, R., Meierer, M., Schilter, P., & Algesheimer, R. (2020). REndo: An R package to address endogeneity without external instrumental variables. Journal of Statistical Software.\nHueter, I. (2016). Latent instrumental variables: a critical review. Institute for New Economic Thinking Working Paper Series, (46).\nLewbel, A. (1997). Constructing instruments for regressions with measurement error when no additional data are available, with an application to patents and R&D. Econometrica, 1201-1213.\nLewbel, A. (2012). Using heteroscedasticity to identify and estimate mismeasured and endogenous regressor models. Journal of business & economic statistics, 30(1), 67-80.\nPapadopoulos, A. (2022). Accounting for endogeneity in regression models using Copulas: A step-by-step guide for empirical studies. Journal of Econometric Methods, 11(1), 127-154.\nPapies, D., Ebbes, P., & Van Heerde, H. J. (2017). Addressing endogeneity in marketing models. Advanced methods for modeling markets, 581-627.\nPark, S., & Gupta, S. (2012). Handling endogenous regressors by joint estimation using copulas. Marketing Science, 31(4), 567-586.\nRigobon, R. (2003). Identification through heteroskedasticity. Review of Economics and Statistics, 85(4), 777-792.\nQian, Y., Koschmann, A., & Xie, H. (2024). A Practical Guide to Endogeneity Correction Using Copulas (No.¬†w32231). National Bureau of Economic Research.\nRigobon, R. (2003). Identification through heteroskedasticity. Review of Economics and Statistics, 85(4), 777-792.\nRutz, O. J., & Watson, G. F. (2019). Endogeneity and marketing strategy research: An overview. Journal of the Academy of Marketing Science, 47, 479-498.\nTran, K. C., & Tsionas, E. G. (2015). Endogeneity in stochastic frontier models: Copula approach without external instruments. Economics Letters, 133, 85-88."
  },
  {
    "objectID": "blog/TODO-column-sampling-bootstrap.html#a-closer-look",
    "href": "blog/TODO-column-sampling-bootstrap.html#a-closer-look",
    "title": "Column-Sampling Bootstrap?",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nHere‚Äôs the basic algorithm:\n\nRandomly sample columns from your dataset with replacement to create fake dataset.\nCompute their correlation coefficient.\nRepeat this many times.\nCompare your observed correlation to the distribution of these synthetic correlations.\nDeclare statistical significance if the observed correlation appears as an ‚Äúoutlier‚Äù in this synthetic distribution.\n\nThis approach allows you to explore a large number of possible correlations in a computationally efficient way. But does it actually work? Let‚Äôs unpack the key considerations.\nThe column-sampling bootstrap is most valuable when a dataset has many columns but too few rows for traditional bootstrap methods. The abundance of columns provides a rich sampling landscape. The goal is determining whether a correlation is significantly stronger or weaker than what might occur by chance. Let‚Äôs simplify the problem and ignore any issues stemming from being unable to estimate the correlation coefficients well enough.\n\n\nProblems\nHowever, several critical challenges exist. The method assumes columns are independent and identically distributed (i.i.d.), which rarely holds in practice. Columns often represent related variables‚Äîlike gene measurements or interconnected phenomena‚Äîand these dependencies can bias resampled correlations. Moreover, by resampling columns, you ignore row-level relationships, such as connections in time series or grouped data (like patients from the same household).\nInterpreting the null distribution presents another significant challenge. Synthetic correlation coefficients generated through column resampling might not represent a meaningful null hypothesis. If your dataset contains highly correlated features, the null distribution could shift, potentially leading to misleading conclusions. Unlike traditional bootstrapping‚Äîwhere samples reflect a subpopulation‚Äîthis method lacks that fundamental connection.\n\n\nThe Verdict\nWhile the column-sampling bootstrap is an intriguing concept, it will likely prove useful only in very specific, carefully constrained settings."
  },
  {
    "objectID": "blog/DONE-limits-parametric-models.html",
    "href": "blog/DONE-limits-parametric-models.html",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "",
    "text": "Obtaining the lowest possible variance is a primary goal for anyone working with statistical models. Efficiency (or precision), as is the jargon, is a cornerstone of statistics and econometrics, guiding us toward estimators that extract the maximum possible information from the data. It can make or break a data project.\nThe Cram√©r-Rao lower bound (CRLB) plays a pivotal role in this context by establishing a theoretical limit on the variance of unbiased estimators. Unbiased estimators are those that yield the true answer (on average), rendering them a highly attractive class of methods. The CRLB highlights the best achievable precision for parameter estimation based on the Fisher information in the data. This article explores the theoretical foundation of the CRLB, its computation, and its implications for practical estimation.\nIn what follows, I am concerned with unbiased estimators, a common practice that should not be taken for granted. As a counterexample, consider the James-Stein estimator ‚Äîa biased but attractive technique."
  },
  {
    "objectID": "blog/DONE-limits-parametric-models.html#background",
    "href": "blog/DONE-limits-parametric-models.html#background",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "",
    "text": "Obtaining the lowest possible variance is a primary goal for anyone working with statistical models. Efficiency (or precision), as is the jargon, is a cornerstone of statistics and econometrics, guiding us toward estimators that extract the maximum possible information from the data. It can make or break a data project.\nThe Cram√©r-Rao lower bound (CRLB) plays a pivotal role in this context by establishing a theoretical limit on the variance of unbiased estimators. Unbiased estimators are those that yield the true answer (on average), rendering them a highly attractive class of methods. The CRLB highlights the best achievable precision for parameter estimation based on the Fisher information in the data. This article explores the theoretical foundation of the CRLB, its computation, and its implications for practical estimation.\nIn what follows, I am concerned with unbiased estimators, a common practice that should not be taken for granted. As a counterexample, consider the James-Stein estimator ‚Äîa biased but attractive technique."
  },
  {
    "objectID": "blog/DONE-limits-parametric-models.html#notation",
    "href": "blog/DONE-limits-parametric-models.html#notation",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Notation",
    "text": "Notation\nBefore diving in, let‚Äôs establish a unified notation to structure the mathematical discussion:\n\nLet X denote the observed data, with \\(X_1, X_2, \\dots, X_n\\) being n independent and identically distributed (i.i.d.) observations.\nThe model governing the data is characterized by a (finite-dimensional) parameter \\(\\theta \\in \\mathbb{R}^d\\) which we aim to estimate.\nThe likelihood of the data is \\(f(x; \\theta)\\), fully specified by the parameter \\(\\theta\\)."
  },
  {
    "objectID": "blog/DONE-limits-parametric-models.html#a-closer-look",
    "href": "blog/DONE-limits-parametric-models.html#a-closer-look",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "A Closer Look",
    "text": "A Closer Look\nThe Cram√©r-Rao lower bound provides a theoretical benchmark for how precise an unbiased estimator can be. It sets the minimum variance that any unbiased estimator of a parameter \\(\\theta\\) can achieve, given a specific data-generating process.\n\nThe CRLB Formula\nFor a parameter \\(\\theta\\) in a parametric model with likelihood \\(f(x; \\theta)\\), the CRLB is expressed as:\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{I(\\theta)},\\]\nwhere \\(I(\\theta)\\) is the Fisher information (FI), defined as:\n\\[I(\\theta) = \\mathbb{E}\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 \\right].\\]\n\n\nIntuition\nTo understand the CRLB, we must delve into the concept of Fisher information named after one of the modern fathers of statistics R.A. Fisher. Intuitively, FI quantifies how much information the observed data carries about the parameter \\(\\theta\\).\nThink of the likelihood function \\(f(x; \\theta)\\) as describing the probability of observing a given dataset \\(x\\) for a particular value of \\(\\theta\\). If the likelihood changes sharply with \\(\\theta\\) (i.e., \\(\\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta)\\) is large), small changes in \\(\\theta\\) lead to noticeable differences in the likelihood. This variability reflects high information: the data can ‚Äúpinpoint‚Äù \\(\\theta\\) with greater precision. Conversely, if the likelihood changes slowly with \\(\\theta\\), the data offers less information about its true value.\nMathematically, the Fisher information \\(I(\\theta)\\) is the variance of the the partial derivative\n\\[\\frac{\\partial}{\\partial \\theta} logf(x;\\theta),\\]\nwhich we refer to as the score function. This score measures how sensitive the likelihood function is to changes in \\(\\theta\\). Higher variance in the score corresponds to more precise information about \\(\\theta\\).\n\n\nPractical Application\nThe CRLB provides a benchmark for evaluating the performance of estimators. For example, if you propose an unbiased estimator \\(\\hat{\\theta}\\), you can compare its variance to the CRLB. If \\(\\text{Var}(\\hat{\\theta}) = \\frac{1}{I(\\theta)}\\), we say the estimator is efficient. However, if the variance is higher, there may be room to improve the estimation method.\nMoreover, the CRLB also offers insight into the difficulty of estimating a parameter. If \\(I(\\theta)\\) is ‚Äúsmall‚Äù, so that the bound on the variance is high, then no unbiased estimator can achieve high precision with the available data. It is possible to develop a biased estimator for \\(\\theta\\) with lower variance, but it is not clear why you would do that."
  },
  {
    "objectID": "blog/DONE-limits-parametric-models.html#an-example",
    "href": "blog/DONE-limits-parametric-models.html#an-example",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "An Example",
    "text": "An Example\nImagine you are estimating the mean of a normal distribution, where \\(X \\sim N(\\mu, \\sigma^2)\\), and \\(\\sigma^2\\) is known. The likelihood for a single observation \\(x_i\\) is:\n\\[f(x_i;\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2 \\sigma^2}}.\\]\nUsing the Fisher information defintion given above, taking the derivative and simplifying, we find:\n\\[I(\\mu)=  \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 = \\frac{1}{\\sigma^2}.\\]\nFor n independent observations, this expression becomes:\n\\[I(\\mu)=\\frac{n}{\\sigma^2}.\\]\nThe CRLB for the variance of any unbiased estimator of is:\n\\[\\text{Var}(\\hat{\\mu})\\geq \\frac{\\sigma^2}{n}\\]\nThis result aligns with our intuition: as n increases, the precision of our estimate improves. In other words, more data leads to more informative results."
  },
  {
    "objectID": "blog/DONE-limits-parametric-models.html#where-to-learn-more",
    "href": "blog/DONE-limits-parametric-models.html#where-to-learn-more",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAny graduate econometrics textbook will do. Personally, my grad school nightmares were induced by Greene‚Äôs textbook (cited below). It can be dry but certainly contains what you need to know."
  },
  {
    "objectID": "blog/DONE-limits-parametric-models.html#bottom-line",
    "href": "blog/DONE-limits-parametric-models.html#bottom-line",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe CRLB establishes a theoretical lower limit on the variance of unbiased estimators, serving as a benchmark for efficiency.\nFisher information measures the sensitivity of the likelihood to changes in the parameter \\(\\theta\\), linking the amount of information in the data to the precision of estimation.\nEfficient estimators achieve the CRLB and are optimal under the given model assumptions."
  },
  {
    "objectID": "blog/DONE-limits-parametric-models.html#references",
    "href": "blog/DONE-limits-parametric-models.html#references",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "References",
    "text": "References\nGreene, William H. ‚ÄúEconometric analysis‚Äù. New Jersey: Prentice Hall (2000): 201-215."
  },
  {
    "objectID": "blog/DONE-column-sampling-bootstrap.html",
    "href": "blog/DONE-column-sampling-bootstrap.html",
    "title": "Column-Sampling Bootstrap?",
    "section": "",
    "text": "The bootstrap is a versatile resampling technique traditionally focused on rows. Let‚Äôs add a twist to the plain vanilla bootstrap. Imagine you have a wide dataset‚Äîmany variables but few rows‚Äîand want to test the statistical significance of a correlation between two variables. An example is a genetic dataset with thousands of columns (genetic information and outcomes) but a limited number of rows (patients). Can you use the bootstrap to determine if the correlation between a specific gene-outcome pair is statistically significant?\nOne creative approach is resampling columns instead of rows, generating a distribution of correlation coefficients to assess the significance of your observed correlation."
  },
  {
    "objectID": "blog/DONE-column-sampling-bootstrap.html#background",
    "href": "blog/DONE-column-sampling-bootstrap.html#background",
    "title": "Column-Sampling Bootstrap?",
    "section": "",
    "text": "The bootstrap is a versatile resampling technique traditionally focused on rows. Let‚Äôs add a twist to the plain vanilla bootstrap. Imagine you have a wide dataset‚Äîmany variables but few rows‚Äîand want to test the statistical significance of a correlation between two variables. An example is a genetic dataset with thousands of columns (genetic information and outcomes) but a limited number of rows (patients). Can you use the bootstrap to determine if the correlation between a specific gene-outcome pair is statistically significant?\nOne creative approach is resampling columns instead of rows, generating a distribution of correlation coefficients to assess the significance of your observed correlation."
  },
  {
    "objectID": "blog/DONE-column-sampling-bootstrap.html#a-closer-look",
    "href": "blog/DONE-column-sampling-bootstrap.html#a-closer-look",
    "title": "Column-Sampling Bootstrap?",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nHere‚Äôs the basic algorithm:\n\nRandomly sample columns from your dataset with replacement to create fake dataset.\nCompute their correlation coefficient.\nRepeat this many times.\nCompare your observed correlation to the distribution of these synthetic correlations.\nDeclare statistical significance if the observed correlation appears as an ‚Äúoutlier‚Äù in this synthetic distribution.\n\nThis approach allows you to explore a large number of possible correlations in a computationally efficient way. But does it actually work? Let‚Äôs unpack the key considerations.\nThe column-sampling bootstrap is most valuable when a dataset has many columns but too few rows for traditional bootstrap methods. The abundance of columns provides a rich sampling landscape. The goal is determining whether a correlation is significantly stronger or weaker than what might occur by chance. Let‚Äôs simplify the problem and ignore any issues stemming from being unable to estimate the correlation coefficients well enough.\n\n\nProblems\nHowever, several critical challenges exist. The method assumes columns are independent and identically distributed (i.i.d.), which rarely holds in practice. Columns often represent related variables‚Äîlike gene measurements or interconnected phenomena‚Äîand these dependencies can bias resampled correlations. Moreover, by resampling columns, you ignore row-level relationships, such as connections in time series or grouped data (like patients from the same household).\nInterpreting the null distribution presents another significant challenge. Synthetic correlation coefficients generated through column resampling might not represent a meaningful null hypothesis. If your dataset contains highly correlated features, the null distribution could shift, potentially leading to misleading conclusions. Unlike traditional bootstrapping‚Äîwhere samples reflect a subpopulation‚Äîthis method lacks that fundamental connection.\n\n\nThe Verdict\nWhile the column-sampling bootstrap is an intriguing concept, it will likely prove useful only in very specific, carefully constrained settings."
  },
  {
    "objectID": "blog/DONE-column-sampling-bootstrap.html#an-example",
    "href": "blog/DONE-column-sampling-bootstrap.html#an-example",
    "title": "Column-Sampling Bootstrap?",
    "section": "An Example",
    "text": "An Example\nWhile we should be skeptical of the column-sampling bootstrap in practical applications, it can be instructive to see how we might implement it.\nBelow is a sample R and python code illustrating the main concept. We begin with setting up a synthetic dataset.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\ndata &lt;- as.data.frame(matrix(rnorm(1000), nrow = 50, ncol = 20))\nobserved_correlation &lt;- cor(data[[1]], data[[2]])\n\n# Perform the resampling\nn_bootstrap &lt;- 1000  # Number of bootstrap iterations\nn_columns &lt;- ncol(data)  # Total number of columns in the dataset\nbootstrap_correlations &lt;- numeric(n_bootstrap)\n\nfor (i in 1:n_bootstrap) {\n  resampled_columns &lt;- sample(1:n_columns, size = n_columns, replace = TRUE)\n  resampled_data &lt;- data[, resampled_columns]\n  bootstrap_correlations[i] &lt;- cor(resampled_data[[1]], resampled_data[[2]])\n}\n\n# Test the significance of the observed correlation\np_value &lt;- mean(abs(bootstrap_correlations) &gt;= abs(observed_correlation))\n\n# Print the results\ncat(\"Observed Correlation:\", observed_correlation, \"\\n\")\ncat(\"P-value:\", p_value, \"\\n\")\n\n\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(1988)\n\n# Generate synthetic dataset\ndata = np.random.normal(size=(50, 20))  # 50 rows, 20 columns\nobserved_correlation = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n\n# Perform the resampling\nn_bootstrap = 1000  # Number of bootstrap iterations\nn_columns = data.shape[1]  # Total number of columns in the dataset\nbootstrap_correlations = []\n\nfor _ in range(n_bootstrap):\n    # Resample columns with replacement\n    resampled_columns = np.random.choice(n_columns, size=n_columns, replace=True)\n    resampled_data = data[:, resampled_columns]\n    # Compute correlation between the first two columns of the resampled data\n    bootstrap_correlations.append(np.corrcoef(resampled_data[:, 0], resampled_data[:, 1])[0, 1])\n\n# Test the significance of the observed correlation\nbootstrap_correlations = np.array(bootstrap_correlations)\np_value = np.mean(np.abs(bootstrap_correlations) &gt;= np.abs(observed_correlation))\n\n# Print the results\nprint(\"Observed Correlation:\", observed_correlation)\nprint(\"P-value:\", p_value)\n\n\n\nThe observed correlation is quite low and equal to \\(0.58\\). Its associated p-value is \\(0.676\\), consistent with the value not being statistically significant."
  },
  {
    "objectID": "blog/DONE-column-sampling-bootstrap.html#bottom-line",
    "href": "blog/DONE-column-sampling-bootstrap.html#bottom-line",
    "title": "Column-Sampling Bootstrap?",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe column-sampling bootstrap is a thought-proviking twist on traditional resampling techniques that leverages the width of your dataset.\nWhile it offers computational efficiency and flexibility, its reliance on the i.i.d. assumption and potential to overlook row-level dependencies highlight the need for careful application.\nThe column-sampling bootrap should not be your go-to method to assess statistical significance."
  },
  {
    "objectID": "blog/DONE-weights-statistics.html",
    "href": "blog/DONE-weights-statistics.html",
    "title": "Weights in Statistical Analyses",
    "section": "",
    "text": "Weights in statistical analyses offer a way to assign varying importance to observations in a dataset. Although powerful, they can be quite confusing due to the various types of weights available. In this article, I will unpack the details behind the most common types of weights used in data science.\nMost types of statistical analyses can be performed with weights. These include calculating summary statistics, regression models, bootstrap, etc. Even maximum likelihood estimation is minimally affected. In this article, I will keep it simple and only discuss mean and variance estimation.\nThe two primary types of weights encountered in practice are sampling weights and frequency weights. I will explore each in turn and conclude with a comparative example.\nTo begin, let‚Äôs imagine a well-behaved random variable X of which we have an iid sample of size n.¬†I will use w to denote the relevant weighting variable."
  },
  {
    "objectID": "blog/DONE-weights-statistics.html#background",
    "href": "blog/DONE-weights-statistics.html#background",
    "title": "Weights in Statistical Analyses",
    "section": "",
    "text": "Weights in statistical analyses offer a way to assign varying importance to observations in a dataset. Although powerful, they can be quite confusing due to the various types of weights available. In this article, I will unpack the details behind the most common types of weights used in data science.\nMost types of statistical analyses can be performed with weights. These include calculating summary statistics, regression models, bootstrap, etc. Even maximum likelihood estimation is minimally affected. In this article, I will keep it simple and only discuss mean and variance estimation.\nThe two primary types of weights encountered in practice are sampling weights and frequency weights. I will explore each in turn and conclude with a comparative example.\nTo begin, let‚Äôs imagine a well-behaved random variable X of which we have an iid sample of size n.¬†I will use w to denote the relevant weighting variable."
  },
  {
    "objectID": "blog/DONE-weights-statistics.html#diving-deeper",
    "href": "blog/DONE-weights-statistics.html#diving-deeper",
    "title": "Weights in Statistical Analyses",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nMean estimation does not depend on the type of weights. We have:\n\\[ \\bar{X} = \\frac{\\sum_i wX}{\\sum_i w}. \\]\nVariance estimation depends on the weight type.\nRemember that we estimate the standard error of \\(\\bar{X}\\) as:\n\\[ SE(\\bar{X})=\\frac{s}{\\sqrt{n}}, \\hspace{.3cm} \\text{where} \\hspace{.3cm} s=\\sqrt{\\frac{1}{N-1}\\sum_i(X-\\bar{X})^2} \\]\nis an estimate of the standard deviation of \\(X\\). I will explain below how estimation of \\(SE(\\bar{X})\\) differs for sampling and frequency weights. Imporantly, s_w will denote the weighted version of this standard error.\n\nSampling Weights\nSampling weights measure the inverse probability of an observation entering the sample. They are particularly relevant in surveys when some units in the population are sampled more frequently than others. These weights adjust for sampling discrepancies to ensure that survey estimates are representative of the entire population. Sampling weights are also sometimes called probability weights.\nIntuitively, if we want to use the survey to accurately represent the population, we should assign higher importance to observations that are less likely to be sampled and lower importance to those more likely to appear in our data. This is exactly what sampling weights achieve. Sampling weights are, thus, inversely proportional to the probability of selection. Therefore, a smaller weight indicates a higher probability of being sampled, and vice versa.\nFor example, households in rural areas might be less likely to enter a survey due to the increased resources required to reach remote locations. Conversely, urban households are typically easier to sample and thus more likely to appear in the data.\nThis weighting approach is also common in causal inference analyses using propensity score methods. Similarly, in machine learning, weighting is often employed to adjust for unbalanced samples in the context of rare events (e.g., fraud detection, cancer diagnosis).\nLet‚Äôs now get back to the discussion on variance estimation. With sampling weights, we have:\n\\[ SE^{\\text{sampling}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{S}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\frac{(\\sum_i w)^2}{\\sum_i w^2}}}\\]\nThe numerator is the weighted version of the sum of squared deviations from the mean, emphasizing observations with higher weights. The denominator normalizes the standard error based on the total sum of the weights. When the weights differ greatly‚Äîhigh weights make some observations much more influential, increasing uncertainty about the population mean and so the sampling-weighted standard error is often larger.\nI will move on to describing frequency weights.\nSoftware Package: survey.\n\n\nFrequency Weights\nFrequency weights measure the number of times an observation should be counted in the analysis. They are most relevant when there are multiple identical observations or when duplicating observations is possible. Naturally, higher weights correspond to observations that appear more frequently. Frequency weights are common in aggregated datasets. For instance, with market- or city-level data, we might want to weight the rows by market size, thus assigning more importance to larger units.\nWe can gain intuition from a linear algebra perspective. In a regression context, the design matrix X must be of full rank (to ensure invertibility), which implies no two rows can be identical. We thus need to collapse X to keep only distinct rows and record the number of times each row appears in the original data. This record constitutes the frequency weights.\nThe formula for estimating the standard error of X with frequency weights is:\n\\[ SE^{\\text{frequency}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{F}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\sum_i w}}.\\]\nHere \\(n_{\\text{eff}}\\) is the effective sample size (i.e., the sum of all weights) and \\(s_w\\) is the weighted standard deviation of \\(X\\).\nThe frequency-weighted standard error is typically the smaller because it increases the effective sample size without introducing variability in the contribution of different observations. In contrast, the sampling-weighted standard error could be larger if some observations are given much higher weights, increasing the variability and lowering the precision.\nSoftware Package: survey\n\n\nOne More Thing\nThere is actually one more type of weights which are not so commonly used in practice, precision weights. They represent the precision (\\(1/\\text{variance}\\)) of observations. A weight of 5, for example, reflects 5 times the precision of a weight of 1, originally based on averaging 5 replicate observations. Precision weights often come up in statistical theory in places such as Generalized Least Squares where they promise efficiency gains (i.e., lower variance) relative to traditional Ordinary Least Squares estimation. In practice, precision weights are in fact frequency weights normalized to sum to n.¬†Lastly, Stata‚Äôs user guide refers to them as analytic weights."
  },
  {
    "objectID": "blog/DONE-weights-statistics.html#an-example",
    "href": "blog/DONE-weights-statistics.html#an-example",
    "title": "Weights in Statistical Analyses",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs see all of this in practice. We begin by creating a fake dataset of a variable \\(X\\) with both sampling and frequency weights. The weights are randomly created and hence have no underlying meaning, so think of this example as a tutorial, without a strong focus on the results.\n\nRPython\n\n\n# clear workspace and load libraries\nlibrary(survey)\nrm(list=ls())\nset.seed(681)\n\n# generate fake data\nn &lt;- 100000\ndata &lt;- data.frame(\n  x = rnorm(n),\n  prob_selection = runif(n, .1, .9),\n  freq_weight = rpois(n, 3)\n)\ndata$samp_weight &lt;- 1 / data$prob_selection\n\n# calculate the average value of $X$ using both types of weights.\ndesign_unweight &lt;- svydesign(ids = ~1, data = data, weights = ~1)\ndesign_samp &lt;- svydesign(ids = ~1, data = data, weights = ~samp_weight)\ndesign_freq &lt;- svydesign(ids = ~1, data = data, weights = ~freq_weight)\n\nmean_unweight &lt;- svymean(~x, design_unweight)\nmean_samp &lt;- svymean(~x, design_samp)\nmean_freq &lt;- svymean(~x, design_freq)\n\n# print results\nprint(round(mean_unweight, digits=3))\nprint(round(mean_samp, digits=3))\nprint(round(mean_freq, digits=3))\n\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.weightstats import DescrStatsW\n\n# Set seed for reproducibility\nnp.random.seed(681)\n\n# Generate fake data\nn = 100000\ndata = pd.DataFrame({\n    \"x\": np.random.normal(size=n),\n    \"prob_selection\": np.random.uniform(0.1, 0.9, size=n),\n    \"freq_weight\": np.random.poisson(3, size=n)\n})\ndata[\"samp_weight\"] = 1 / data[\"prob_selection\"]\n\n# Calculate the average value of X using both types of weights\n# Unweighted mean\nmean_unweight = DescrStatsW(data[\"x\"]).mean, DescrStatsW(data[\"x\"]).std_mean\n\n# Sampling-weighted mean\nmean_samp = DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"samp_weight\"]).std_mean\n\n# Frequency-weighted mean\nmean_freq = DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).mean, DescrStatsW(data[\"x\"], weights=data[\"freq_weight\"]).std_mean\n\n# Print results\nprint(\"Unweighted Mean and SE:\", np.round(mean_unweight, 3))\nprint(\"Sampling-Weighted Mean and SE:\", np.round(mean_samp, 3))\nprint(\"Frequency-Weighted Mean and SE:\", np.round(mean_freq, 3))\n\n\n\nThe unweighted and frequency-weighted means match exactly, (I am not sure why the sampling-weighted mean is slightly lower.) while the variances are different. The variance with frequency weights is lower than that of sampling weights."
  },
  {
    "objectID": "blog/DONE-weights-statistics.html#bottom-line",
    "href": "blog/DONE-weights-statistics.html#bottom-line",
    "title": "Weights in Statistical Analyses",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWeights are one of the most confusing aspects of working with data.\nSampling and frequency weights are the most common types of weights found in practice.\nThe former measure the inverse probability of being sampled, while the latter represent the number of times an observation enters the sample.\nWhile weighting usually does not impact point estimates (e.g., regression coefficients, means), incorrect usage of weights can lead to inaccurate confidence intervals and p-values.\nThis is relevant only if (i) your dataset contains weights, and (ii) you are interested in population-level statistics."
  },
  {
    "objectID": "blog/DONE-weights-statistics.html#where-to-learn-more",
    "href": "blog/DONE-weights-statistics.html#where-to-learn-more",
    "title": "Weights in Statistical Analyses",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nGoogle is a great starting place. Lumley‚Äôs blog post titled Weights in Statistics was incredibly helpful in preparing this article. Stata‚Äôs manuals which are publically available contain more detailed information on various types of weighting schemes. See also Solon et al.¬†(2015) for using weights in causal inference."
  },
  {
    "objectID": "blog/DONE-weights-statistics.html#references",
    "href": "blog/DONE-weights-statistics.html#references",
    "title": "Weights in Statistical Analyses",
    "section": "References",
    "text": "References\nDupraz, Y. (2013). Using Weights in Stata. Memo, 54(2)\nLumley, T. (2020), Weights in Statistics, Blog Post\nSolon, G., Haider, S. J., & Wooldridge, J. M. (2015). What are we weighting for?. Journal of Human resources, 50(2), 301-316.\nStata User‚Äôs Guide (2023)"
  },
  {
    "objectID": "blog/DONE-weights-statistics.html#a-closer-look",
    "href": "blog/DONE-weights-statistics.html#a-closer-look",
    "title": "Weights in Statistical Analyses",
    "section": "A Closer Look",
    "text": "A Closer Look\nMean estimation does not depend on the type of weights. We have:\n\\[ \\bar{X} = \\frac{\\sum_i wX}{\\sum_i w}. \\]\nVariance estimation depends on the weight type.\nRemember that we estimate the standard error of \\(\\bar{X}\\) as:\n\\[ SE(\\bar{X})=\\frac{s}{\\sqrt{n}}, \\hspace{.3cm} \\text{where} \\hspace{.3cm} s=\\sqrt{\\frac{1}{N-1}\\sum_i(X-\\bar{X})^2} \\]\nis an estimate of the standard deviation of \\(X\\). I will explain below how estimation of \\(SE(\\bar{X})\\) differs for sampling and frequency weights. Imporantly, s_w will denote the weighted version of this standard error.\n\nSampling Weights\nSampling weights measure the inverse probability of an observation entering the sample. They are particularly relevant in surveys when some units in the population are sampled more frequently than others. These weights adjust for sampling discrepancies to ensure that survey estimates are representative of the entire population. Sampling weights are also sometimes called probability weights.\nIntuitively, if we want to use the survey to accurately represent the population, we should assign higher importance to observations that are less likely to be sampled and lower importance to those more likely to appear in our data. This is exactly what sampling weights achieve. Sampling weights are, thus, inversely proportional to the probability of selection. Therefore, a smaller weight indicates a higher probability of being sampled, and vice versa.\nFor example, households in rural areas might be less likely to enter a survey due to the increased resources required to reach remote locations. Conversely, urban households are typically easier to sample and thus more likely to appear in the data.\nThis weighting approach is also common in causal inference analyses using propensity score methods. Similarly, in machine learning, weighting is often employed to adjust for unbalanced samples in the context of rare events (e.g., fraud detection, cancer diagnosis).\nLet‚Äôs now get back to the discussion on variance estimation. With sampling weights, we have:\n\\[ SE^{\\text{sampling}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{S}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\frac{(\\sum_i w)^2}{\\sum_i w^2}}}\\]\nThe numerator is the weighted version of the sum of squared deviations from the mean, emphasizing observations with higher weights. The denominator normalizes the standard error based on the total sum of the weights. When the weights differ greatly‚Äîhigh weights make some observations much more influential, increasing uncertainty about the population mean and so the sampling-weighted standard error is often larger.\nI will move on to describing frequency weights.\nSoftware Package: survey\n\n\nFrequency Weights\nFrequency weights measure the number of times an observation should be counted in the analysis. They are most relevant when there are multiple identical observations or when duplicating observations is possible. Naturally, higher weights correspond to observations that appear more frequently. Frequency weights are common in aggregated datasets. For instance, with market- or city-level data, we might want to weight the rows by market size, thus assigning more importance to larger units.\nWe can gain intuition from a linear algebra perspective. In a regression context, the design matrix X must be of full rank (to ensure invertibility), which implies no two rows can be identical. We thus need to collapse X to keep only distinct rows and record the number of times each row appears in the original data. This record constitutes the frequency weights.\nThe formula for estimating the standard error of X with frequency weights is:\n\\[ SE^{\\text{frequency}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{F}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\sum_i w}}.\\]\nHere \\(n_{\\text{eff}}\\) is the effective sample size (i.e., the sum of all weights) and \\(s_w\\) is the weighted standard deviation of \\(X\\).\nThe frequency-weighted standard error is typically the smaller because it increases the effective sample size without introducing variability in the contribution of different observations. In contrast, the sampling-weighted standard error could be larger if some observations are given much higher weights, increasing the variability and lowering the precision.\nSoftware Package: survey.\n\n\nOne More Thing\nThere is actually one more type of weights which are not so commonly used in practice, precision weights. They represent the precision (\\(1/\\text{variance}\\)) of observations. A weight of 5, for example, reflects 5 times the precision of a weight of 1, originally based on averaging 5 replicate observations. Precision weights often come up in statistical theory in places such as Generalized Least Squares where they promise efficiency gains (i.e., lower variance) relative to traditional Ordinary Least Squares estimation. In practice, precision weights are in fact frequency weights normalized to sum to n.¬†Lastly, Stata‚Äôs user guide refers to them as analytic weights."
  },
  {
    "objectID": "blog/TODO-causality-wo-experiments.html#notation",
    "href": "blog/TODO-causality-wo-experiments.html#notation",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs begin by establishing some basic notation. We aim to analyze the impact of a binary, endogenous treatment variable \\(X_1\\) on an outcome variable \\(Y\\), in a setting with exogenous variables \\(X_2\\). We have access to a well-behaved, representative iid sample of size \\(n\\) of \\(Y\\), and \\(X:=[X_1, X_2]\\). These variables are related as follows:\n\\[ Y = \\beta X_1 + \\gamma X_2 + \\epsilon, \\]\nwhere is a mean-zero error term. Our goal is to obtain a consistent estimate of \\(\\beta\\). For simplicity, we‚Äôre using the same notation for both single- and vector-valued quantities, as \\(X_2\\) can be in \\(\\mathbb{R}^p\\) with $ p&gt;1$.\nThe challenge arises because \\(X_1\\) and \\(\\epsilon\\) are correlated, rendering the standard OLS estimator inconsistent. Even in large samples, \\(\\hat{\\beta}_{OLS}\\) will be biased, and getting more data would not help. Specifically:\n\\[\\hat{\\beta}_{OLS} := (X'X)^{-1}X'Y \\nrightarrow \\beta. \\]\nStandard instrument-based methods rely on the existence of an instrumental variable \\(Z\\) which, conditional on \\(X_2\\), correlates with \\(X_1\\) but not with . Estimation then proceeds with 2SLS, LIML, or GMM, potentially yielding good estimates of \\(\\beta\\) given appropriate assumptions. Common issues with instrumental variables include implausibility of the exclusion restriction, weak correlation with \\(X_1\\), and challenges in interpreting \\(\\hat{\\beta}_{IV}\\). Formally:\n\\[\\hat{\\beta}_{IV} := (Z'X)^{-1}Z'Y \\rightarrow \\beta. \\]\nIn this article, we focus on obtaining correct estimates of \\(\\beta\\) in settings where we don‚Äôt have access to such an instrumental variable \\(Z\\)."
  },
  {
    "objectID": "blog/TODO-causality-wo-experiments.html#a-closer-look",
    "href": "blog/TODO-causality-wo-experiments.html#a-closer-look",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "A Closer Look",
    "text": "A Closer Look\nLet‚Äôs start with the heteroskedasticity-based approach of Lewbel (2012).\n\nHeteroskedasticity & Higher Moments\nThe main idea here is to construct valid instruments for \\(X_1\\) by using information contained in the heteroskedasticity of \\(\\epsilon\\). Intuitively, if \\(\\epsilon\\) exhibits heteroskedasticity related to $ X_2$, we can use to create instruments‚Äîspecifically by interacting \\(X_2\\) with the residuals of the endogenous regressor‚Äôs reduced form equation. So, this is an IV-based method, but the instrument is ‚Äúinternal‚Äù to the model and does not rely on any external information.\nThe key assumptions are:\nThe error term in the structural equation (\\(\\epsilon\\)) is heteroskedastic. This means \\(var(\\epsilon|X_2)\\) is not constant and depends on \\(X_2\\). Moreover, we need \\(cov(X_2,\\epsilon^2) \\neq 0\\). This is an analogue of the first stage assumption in IV methods. The exogenous variable (\\(X_2\\)) are uncorrelated with the product of the endogeneous variable (\\(X_1\\)) and the error term (\\(\\epsilon\\)). That is, \\(cov(X_2, X_1\\epsilon) = 0\\). This is a form of the standard exogeneity assumption in IV estimation. The heteroskedasticity-based estimator of Lewbel (2012) proceeds in two steps:\nRegress \\(X_1\\) on \\(X_2\\) and save the estimated residuals; call them \\(\\hat{u}\\). Construct an instrument for \\(X_1\\) as \\(\\tilde{Z}=(X_2-\\bar{X}_2)\\hat{u}\\), where \\(\\bar{X}_2\\) is the mean of \\(X_2\\). Use \\(\\tilde{Z}\\) as an instrument in a standard 2SLS estimation: \\[\\hat{\\beta}_{LEWBEL} = (X'P_{\\tilde{Z}}X)^{-1}X'P_{\\tilde{Z}}Y,\\]\nwhere \\(P_{\\tilde{Z}}\\) ‚Äãis the projection matrix onto the instrument.\nThis line of thought can also be extended to use higher moments as an alternative or additional way to construct instrumental variables. The original approach uses the variance of the error term, but we can also rely on skewness, kurtosis, etc. The assumptions then must be modified such that these higher moments are correlated with the endogenous variable, etc.\nSoftware Packages: REndo, ivlewbel.\nLet‚Äôs now move to the second set of methods.\n\n\nLatent IVs\nThe latent IV approach imposes distributional assumptions on the exogenous part of the endogenous variable and employs likelihood-based methods to estimate \\(\\beta\\).\nLet‚Äôs simplify the model above, so that we have:\n\\[ Y=\\beta X + \\epsilon, \\]\nwhere \\(X\\) is endogenous. The key idea is to decompose \\(X\\) into two components:\n\\[ X= \\theta + \\nu, \\]\nwith \\(cov(\\theta, \\epsilon)=0\\), \\(cov(\\theta, \\nu) = 0\\), and \\(cov(\\epsilon, \\nu)\\neq0\\). The first condition states that \\(\\theta\\) is the exogenous part of \\(X\\), and the last one gives rise to the endogeneity problem.\nWe then proceed with adding distributional assumptions. Importantly, \\(\\theta\\) must follow some discrete distribution with a finite number of mass points. A common example imposes:\n\\[ \\theta \\sim \\text{Multinomial}(\\cdot) \\]\nand\n\\[ (\\epsilon, \\nu) \\sim \\text{Gaussian}(\\cdot). \\]\nThese set of assumptions lead to analytical solutions for the conditional and unconditional distributions of \\((Y,X)\\) and all parameters of the model are identified. Maximum likelihood estimation can then give us an estimate of \\(\\beta_{LIV}\\).\nSoftware Packages: REndo.\nFinally, we turn our attention to the final set of instrument-free methods to solve endogeneity.\n\n\nCopulas\nFirst, a word on copulas. A copula is a multivariate cumulative distribution function (CDF) with uniform marginals on \\([0,1]\\). An old theorem states that any multivariate CDF can be expressed with uniform marginals and a copula function that represents the relationship between the variables. Specifically, if \\(A\\) and \\(B\\) are two random variables with marginal CDFs \\(F_A\\) and \\(F_B\\) and joint CDF \\(H\\), then there exists a copula \\(C\\) such that \\(H(a,b)=C(F_A(a), F_B(b))\\).\nHow does this fit into our context and framework? Park and Gupta (2012) introduced two estimation methods for \\(\\beta\\) under the assumption that \\(\\epsilon \\sim Gaussian(\\cdot)\\). The key idea is positing a Gaussian copula to link the marginal distributions of X and and obtain their joint distribution. We can then estimate $$in one of two ways: either impose distributional assumptions on these marginals and derive and maximize the joint likelihood function of \\(X\\) and \\(\\epsilon\\), or use a generated regressor approach. We will focus on the latter.\nIn the linear model, endogeneity is tackled by creating a novel variable \\(\\tilde{X}\\) and adding that as a control (i.e., a generated regressor). Using our simplified model where \\(X\\) is single-valued and endogenous, we now have:\n\\[Y=\\beta X + \\mu \\tilde{X} + \\eta, \\]\nwhere \\(\\eta\\) is the error term in this augmented model.\nWe construct \\(\\tilde{X}\\) as follows:\n\\[\\tilde{X}=\\Phi^{-1}(\\hat{F}_X(X)).\\]\nHere \\(\\Phi^{-1}(\\cdot)\\) is the inverse CDF of the standard normal distribution and \\(F_X(\\cdot)\\) is the marginal CDF of \\(X\\). We can estimate the latter using the empirical CDF by sorting the observations in ascending order and calculating the proportion of rows with smaller values for each observation. As you can guess, this introduces further uncertainty into the model, so the standard errors should be estimated using bootstrap.\nSoftware Packages: REndo, copula.\n\n\nComparison\nEach statistical method has its strengths and limitations. While the methods described here circumvent the traditional unconfoundedness and external instruments-based assumptions, they do not provide a magical panacea to the endogeneity problem. Instead, they rely on their own, different assumptions. These methods are not universally superior, but should be considered when traditional approaches do not fit your context.\nThe heteroskedasticity-based approach, as the name suggests, requires a considerable degree of heteroskedasticity to perform well. Latent IVs may offer efficiency advantages but come at the cost of imposing distributional assumptions and requiring a group structure of \\(X_1\\). The copula-based approach, while simple to implement, also requires strong assumptions about the distributions of \\(X\\) and \\(Y\\), as well as their relationship.\nThat‚Äôs it. You are now equipped with a set of new methods designed to identify causal relationships in your data."
  },
  {
    "objectID": "blog/DONE-causality-wo-experiments.html",
    "href": "blog/DONE-causality-wo-experiments.html",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "",
    "text": "Causality is central to many practical data-related questions. Conventional methods for isolating causal relationships rely on experimentation, assume unconfoundedness, or require instrumental variables. However, experimentation is often infeasible, costly, or ethically concerning; good instruments are notoriously difficult to find; and unconfoundedness can be an uncomfortable assumption in many settings.\nThis article highlights methods for measuring causality beyond these three paradigms. These underappreciated approaches exploit higher moments and heteroscedastic error structures (Lewbel 2012, Rigobon 2003), latent instrumental variables (IVs) (Ebbes et al.¬†2005), and copulas (Park and Gupta 2012). I will unite them in a common statistical framework and discuss the key assumptions underlying each one.\nThe focus will be on the ideas, intuition, and practical aspects of these methodologies, rather than technical details. Readers can find more in-depth information in the References section. This article assumes familiarity with econometric endogeneity and the basics of instrumental variables; without this background, some sections may be challenging to follow.\nNote: Regression discontinuity (RD) methods are excluded from this discussion, as they fall somewhere between instrument-based and instrument-free econometric methodologies. We know that in fuzzy RDs, the running variable can be viewed as an instrument."
  },
  {
    "objectID": "blog/DONE-causality-wo-experiments.html#background",
    "href": "blog/DONE-causality-wo-experiments.html#background",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "",
    "text": "Causality is central to many practical data-related questions. Conventional methods for isolating causal relationships rely on experimentation, assume unconfoundedness, or require instrumental variables. However, experimentation is often infeasible, costly, or ethically concerning; good instruments are notoriously difficult to find; and unconfoundedness can be an uncomfortable assumption in many settings.\nThis article highlights methods for measuring causality beyond these three paradigms. These underappreciated approaches exploit higher moments and heteroscedastic error structures (Lewbel 2012, Rigobon 2003), latent instrumental variables (IVs) (Ebbes et al.¬†2005), and copulas (Park and Gupta 2012). I will unite them in a common statistical framework and discuss the key assumptions underlying each one.\nThe focus will be on the ideas, intuition, and practical aspects of these methodologies, rather than technical details. Readers can find more in-depth information in the References section. This article assumes familiarity with econometric endogeneity and the basics of instrumental variables; without this background, some sections may be challenging to follow.\nNote: Regression discontinuity (RD) methods are excluded from this discussion, as they fall somewhere between instrument-based and instrument-free econometric methodologies. We know that in fuzzy RDs, the running variable can be viewed as an instrument."
  },
  {
    "objectID": "blog/DONE-causality-wo-experiments.html#notation",
    "href": "blog/DONE-causality-wo-experiments.html#notation",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs begin by establishing some basic notation. We aim to analyze the impact of a binary, endogenous treatment variable \\(X_1\\) on an outcome variable \\(Y\\), in a setting with exogenous variables \\(X_2\\). We have access to a well-behaved, representative iid sample of size \\(n\\) of \\(Y\\), and \\(X:=[X_1, X_2]\\). These variables are related as follows:\n\\[ Y = \\beta X_1 + \\gamma X_2 + \\epsilon, \\]\nwhere is a mean-zero error term. Our goal is to obtain a consistent estimate of \\(\\beta\\). For simplicity, we‚Äôre using the same notation for both single- and vector-valued quantities, as \\(X_2\\) can be in \\(\\mathbb{R}^p\\) with $ p&gt;1$.\nThe challenge arises because \\(X_1\\) and \\(\\epsilon\\) are correlated, rendering the standard OLS estimator inconsistent. Even in large samples, \\(\\hat{\\beta}_{OLS}\\) will be biased, and getting more data would not help. Specifically:\n\\[\\hat{\\beta}_{OLS} := (X'X)^{-1}X'Y \\nrightarrow \\beta. \\]\nStandard instrument-based methods rely on the existence of an instrumental variable \\(Z\\) which, conditional on \\(X_2\\), correlates with \\(X_1\\) but not with . Estimation then proceeds with 2SLS, LIML, or GMM, potentially yielding good estimates of \\(\\beta\\) given appropriate assumptions. Common issues with instrumental variables include implausibility of the exclusion restriction, weak correlation with \\(X_1\\), and challenges in interpreting \\(\\hat{\\beta}_{IV}\\). Formally:\n\\[\\hat{\\beta}_{IV} := (Z'X)^{-1}Z'Y \\rightarrow \\beta. \\]\nIn this article, we focus on obtaining correct estimates of \\(\\beta\\) in settings where we don‚Äôt have access to such an instrumental variable \\(Z\\)."
  },
  {
    "objectID": "blog/DONE-causality-wo-experiments.html#a-closer-look",
    "href": "blog/DONE-causality-wo-experiments.html#a-closer-look",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "A Closer Look",
    "text": "A Closer Look\nLet‚Äôs start with the heteroskedasticity-based approach of Lewbel (2012).\n\nHeteroskedasticity & Higher Moments\nThe main idea here is to construct valid instruments for \\(X_1\\) by using information contained in the heteroskedasticity of \\(\\epsilon\\). Intuitively, if \\(\\epsilon\\) exhibits heteroskedasticity related to $ X_2$, we can use to create instruments‚Äîspecifically by interacting \\(X_2\\) with the residuals of the endogenous regressor‚Äôs reduced form equation. So, this is an IV-based method, but the instrument is ‚Äúinternal‚Äù to the model and does not rely on any external information.\nThe key assumptions are:\n\nThe error term in the structural equation (\\(\\epsilon\\)) is heteroskedastic. This means \\(var(\\epsilon|X_2)\\) is not constant and depends on \\(X_2\\). Moreover, we need \\(cov(X_2,\\epsilon^2) \\neq 0\\). This is an analogue of the first stage assumption in IV methods.\nThe exogenous variable (\\(X_2\\)) are uncorrelated with the product of the endogeneous variable (\\(X_1\\)) and the error term (\\(\\epsilon\\)). That is, \\(cov(X_2, X_1\\epsilon) = 0\\). This is a form of the standard exogeneity assumption in IV estimation.\n\nThe heteroskedasticity-based estimator of Lewbel (2012) proceeds in two steps:\n\nRegress \\(X_1\\) on \\(X_2\\) and save the estimated residuals; call them \\(\\hat{u}\\). Construct an instrument for \\(X_1\\) as \\(\\tilde{Z}=(X_2-\\bar{X}_2)\\hat{u}\\), where \\(\\bar{X}_2\\) is the mean of \\(X_2\\).\nUse \\(\\tilde{Z}\\) as an instrument in a standard 2SLS estimation: \\[\\hat{\\beta}_{LEWBEL} = (X'P_{\\tilde{Z}}X)^{-1}X'P_{\\tilde{Z}}Y,\\]\n\nwhere \\(P_{\\tilde{Z}}\\) ‚Äãis the projection matrix onto the instrument.\nThis line of thought can also be extended to use higher moments as an alternative or additional way to construct instrumental variables. The original approach uses the variance of the error term, but we can also rely on skewness, kurtosis, etc. The assumptions then must be modified such that these higher moments are correlated with the endogenous variable, etc.\nSoftware Packages: REndo, ivlewbel.\n\n\nLatent IVs\nThe latent IV approach imposes distributional assumptions on the exogenous part of the endogenous variable and employs likelihood-based methods to estimate \\(\\beta\\).\nLet‚Äôs simplify the model above, so that we have:\n\\[ Y=\\beta X + \\epsilon, \\]\nwhere \\(X\\) is endogenous. The key idea is to decompose \\(X\\) into two components:\n\\[ X= \\theta + \\nu, \\]\nwith \\(cov(\\theta, \\epsilon)=0\\), \\(cov(\\theta, \\nu) = 0\\), and \\(cov(\\epsilon, \\nu)\\neq0\\). The first condition states that \\(\\theta\\) is the exogenous part of \\(X\\), and the last one gives rise to the endogeneity problem.\nWe then proceed with adding distributional assumptions. Importantly, \\(\\theta\\) must follow some discrete distribution with a finite number of mass points. A common example imposes:\n\\[ \\theta \\sim \\text{Multinomial}(\\cdot) \\]\nand\n\\[ (\\epsilon, \\nu) \\sim \\text{Gaussian}(\\cdot). \\]\nThese set of assumptions lead to analytical solutions for the conditional and unconditional distributions of \\((Y,X)\\) and all parameters of the model are identified. Maximum likelihood estimation can then give us an estimate of \\(\\beta_{LIV}\\).\nSoftware Packages: REndo.\n\n\nCopulas\nFirst, a word on copulas. A copula is a multivariate cumulative distribution function (CDF) with uniform marginals on \\([0,1]\\). An old theorem states that any multivariate CDF can be expressed with uniform marginals and a copula function that represents the relationship between the variables. Specifically, if \\(A\\) and \\(B\\) are two random variables with marginal CDFs \\(F_A\\) and \\(F_B\\) and joint CDF \\(H\\), then there exists a copula \\(C\\) such that \\(H(a,b)=C(F_A(a), F_B(b))\\).\nHow does this fit into our context and framework? Park and Gupta (2012) introduced two estimation methods for \\(\\beta\\) under the assumption that \\(\\epsilon \\sim Gaussian(\\cdot)\\). The key idea is positing a Gaussian copula to link the marginal distributions of \\(X\\) and \\(\\epsilon\\) and obtain their joint distribution. We can then estimate \\(\\beta\\) in one of two ways: either impose distributional assumptions on these marginals and derive and maximize the joint likelihood function of \\(X\\) and \\(\\epsilon\\), or use a generated regressor approach. We will focus on the latter.\nIn the linear model, endogeneity is tackled by creating a novel variable \\(\\tilde{X}\\) and adding that as a control (i.e., a generated regressor). Using our simplified model where \\(X\\) is single-valued and endogenous, we now have:\n\\[Y=\\beta X + \\mu \\tilde{X} + \\eta, \\]\nwhere \\(\\eta\\) is the error term in this augmented model.\nWe construct \\(\\tilde{X}\\) as follows:\n\\[\\tilde{X}=\\Phi^{-1}(\\hat{F}_X(X)).\\]\nHere \\(\\Phi^{-1}(\\cdot)\\) is the inverse CDF of the standard normal distribution and \\(F_X(\\cdot)\\) is the marginal CDF of \\(X\\). We can estimate the latter using the empirical CDF by sorting the observations in ascending order and calculating the proportion of rows with smaller values for each observation. As you can guess, this introduces further uncertainty into the model, so the standard errors should be estimated using bootstrap.\nSoftware Packages: REndo, copula.\n\n\nComparison\nEach statistical method has its strengths and limitations. While the methods described here circumvent the traditional unconfoundedness and external instruments-based assumptions, they do not provide a magical panacea to the endogeneity problem. Instead, they rely on their own, different assumptions. These methods are not universally superior, but should be considered when traditional approaches do not fit your context.\nThe heteroskedasticity-based approach, as the name suggests, requires a considerable degree of heteroskedasticity to perform well. Latent IVs may offer efficiency advantages but come at the cost of imposing distributional assumptions and requiring a group structure of \\(X_1\\). The copula-based approach, while simple to implement, also requires strong assumptions about the distributions of \\(X\\) and \\(Y\\), as well as their relationship.\nThat‚Äôs it. You are now equipped with a set of new methods designed to identify causal relationships in your data."
  },
  {
    "objectID": "blog/DONE-causality-wo-experiments.html#bottom-line",
    "href": "blog/DONE-causality-wo-experiments.html#bottom-line",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nConventional methods used to tease causality rely on experiments or ambitious assumptions such as unconfoudedness or the access to valid instrumental variables.\nResearchers have developed methods aimed at measuring causality without relying on these frameworks.\nNone of these are a panacea and they rely on their own assumptions that have to be checked on a case-by-case basis."
  },
  {
    "objectID": "blog/DONE-causality-wo-experiments.html#where-to-learn-more",
    "href": "blog/DONE-causality-wo-experiments.html#where-to-learn-more",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nEbbes, Wedel, and Bockenholt (2009), Park and Gupta (2012), Papies, Ebbes, and Heerde (2017), and Rutz and Watson (2019) provide detailed comparisons of these IV-free methods with alternative methods. Also, Qian et al.¬†(2024) and Papadopolous (2022) and Baum and Lewbel (2019) have a practical angle that many data scientist will find accessible and attractive."
  },
  {
    "objectID": "blog/DONE-causality-wo-experiments.html#references",
    "href": "blog/DONE-causality-wo-experiments.html#references",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "References",
    "text": "References\nBaum, C. F., & Lewbel, A. (2019). Advice on using heteroskedasticity-based identification. The Stata Journal, 19(4), 757-767.\nEbbes, P. (2004). Latent instrumental variables: a new approach to solve for endogeneity.\nEbbes, P., Wedel, M., & B√∂ckenholt, U. (2009). Frugal IV alternatives to identify the parameter for an endogenous regressor. Journal of Applied Econometrics, 24(3), 446-468.\nEbbes, P., Wedel, M., B√∂ckenholt, U., & Steerneman, T. (2005). Solving and testing for regressor-error (in) dependence when no IVs are available: With new evidence for the effect of education on income. Quantitative Marketing and Economics, 3, 365-392.\nErickson, T., & Whited, T. M. (2002). Two-step GMM estimation of the errors-in-variables model using high-order moments. Econometric Theory, 18(3), 776-799.\nGui, R., Meierer, M., Schilter, P., & Algesheimer, R. (2020). REndo: An R package to address endogeneity without external instrumental variables. Journal of Statistical Software.\nHueter, I. (2016). Latent instrumental variables: a critical review. Institute for New Economic Thinking Working Paper Series, (46).\nLewbel, A. (1997). Constructing instruments for regressions with measurement error when no additional data are available, with an application to patents and R&D. Econometrica, 1201-1213.\nLewbel, A. (2012). Using heteroscedasticity to identify and estimate mismeasured and endogenous regressor models. Journal of business & economic statistics, 30(1), 67-80.\nPapadopoulos, A. (2022). Accounting for endogeneity in regression models using Copulas: A step-by-step guide for empirical studies. Journal of Econometric Methods, 11(1), 127-154.\nPapies, D., Ebbes, P., & Van Heerde, H. J. (2017). Addressing endogeneity in marketing models. Advanced methods for modeling markets, 581-627.\nPark, S., & Gupta, S. (2012). Handling endogenous regressors by joint estimation using copulas. Marketing Science, 31(4), 567-586.\nRigobon, R. (2003). Identification through heteroskedasticity. Review of Economics and Statistics, 85(4), 777-792.\nQian, Y., Koschmann, A., & Xie, H. (2024). A Practical Guide to Endogeneity Correction Using Copulas (No.¬†w32231). National Bureau of Economic Research.\nRigobon, R. (2003). Identification through heteroskedasticity. Review of Economics and Statistics, 85(4), 777-792.\nRutz, O. J., & Watson, G. F. (2019). Endogeneity and marketing strategy research: An overview. Journal of the Academy of Marketing Science, 47, 479-498.\nTran, K. C., & Tsionas, E. G. (2015). Endogeneity in stochastic frontier models: Copula approach without external instruments. Economics Letters, 133, 85-88."
  },
  {
    "objectID": "blog/TODO-stein-paradox.html",
    "href": "blog/TODO-stein-paradox.html",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "[This blog post concludes a series of explorations of commonly observed ‚Äúmysteries‚Äù in data science ‚Äì see also my entries on Lord‚Äôs paradox, and Simpson‚Äôs paradox.]"
  },
  {
    "objectID": "blog/TODO-stein-paradox.html#background",
    "href": "blog/TODO-stein-paradox.html#background",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "Background",
    "text": "Background\nIn the realm of statistics, few findings are as counterintuitive and fascinating as Stein‚Äôs paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein‚Äôs paradox is not merely about memorizing an oddity‚Äîit‚Äôs about recognizing the delicate balance inherent in statistical decision-making.\nIn essence, Stein‚Äôs paradox asserts that when estimating the means of multiple variables simultaneously, it‚Äôs possible to achieve better results compared to relying solely on the sample averages.\nLet‚Äôs now delve deeper into this statement and unravel the underlying mechanisms."
  },
  {
    "objectID": "blog/TODO-stein-paradox.html#diving-deeper",
    "href": "blog/TODO-stein-paradox.html#diving-deeper",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "Diving Deeper",
    "text": "Diving Deeper"
  },
  {
    "objectID": "blog/TODO-stein-paradox.html#refresher-on-mean-squared-error-mse",
    "href": "blog/TODO-stein-paradox.html#refresher-on-mean-squared-error-mse",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "Refresher on Mean Squared Error (MSE)",
    "text": "Refresher on Mean Squared Error (MSE)\nTo understand the paradox, let‚Äôs begin by quantify what we mean by ‚Äúbetter‚Äù estimation. The MSE of an estimator \\(\\hat{\\mu}\\) is defined as its expected squared error (or loss):\n\\[\\text{MSE}(\\hat{\\mu}) = \\mathbb{E}\\left[ ( \\hat{\\mu} - \\mu )^2 \\right],\\]\nwhere \\(\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_p)\\) is the true vector of means. All else equal, the lower MSE the better.\n\nMathematical Formulation\nStein‚Äôs paradox arises in the context of estimating multiple parameters simultaneously. Suppose you‚Äôre estimating the mean \\(\\mu_i\\) of several independent normal distributions:\n\\[X_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, p,\\]\nwhere \\(X_i\\) are observed values, \\(\\mu_i\\) are the unknown means, and \\(\\sigma^2\\) is known. We assume non-zero covariance between the variables.\nA natural approach is to estimate each _i using its corresponding sample mean \\(X_i\\), which is the maximum likelihood estimator (MLE). However, in dimensions \\(p \\geq 3\\), this seemingly reasonable approach is dominated by an alternative method.\nThe surprise? Shrinking the individual estimates toward a common value‚Äîsuch as the overall mean‚Äîproduces an estimator with uniformly lower expected squared error.\nThe MSE of the MLE is equal to:\n\\[R(\\hat{\\mu}_\\text{MLE}) = p\\sigma^2.\\]\nNow consider the biased(!) James-Stein (JS) estimator:\n\\[\\hat{\\mu}_\\text{JS} = \\left( 1 - \\frac{(p - 2)\\sigma^2}{\\lVert X \\rVert^2} \\right) X,\\]\nwhere \\(\\lVert X \\rVert^2 = \\sum_i X_i^2\\) is the squared norm of the observed data. The shrinkage factor \\(1 - \\frac{(p - 2)\\sigma^2}{|X|^2}\\) pulls the estimates toward zero (or any other pre-specified point).\nRemarkably, the James-Stein estimator has lower MSE than the MLE for \\(p \\geq 3\\):\n\\[\\text{MSE}(\\hat{\\mu}_{\\text{JS}}) &lt; \\text{MSE}(\\hat{\\mu}_{\\text{MLE}}).\\]\nThis is the mystery at its core.\n\n\nAn Explanation\nThis result holds because the James-Stein estimator balances variance reduction and bias introduction in a way that minimizes overall MSE. The MLE, in contrast, does not account for the possibility of shared structure among the parameters. The counterintuitive nature of this result stems from the independence of the \\(X_i\\)‚Äôs. Intuition suggests that pooling information across independent observations should not improve estimation, yet the James-Stein estimator demonstrates otherwise."
  },
  {
    "objectID": "blog/TODO-stein-paradox.html#an-example",
    "href": "blog/TODO-stein-paradox.html#an-example",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs emulate this paradox in R and python in a setting with \\(p=5\\).\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\np &lt;- 5  # Number of means\nn &lt;- 1000  # Number of simulations\nsigma &lt;- 1\nmu &lt;- rnorm(p, mean = 5, sd = 2)  # True means\n\n# results storage\nmse_mle &lt;- numeric(n)  \nmse_js &lt;- numeric(n)\n\n# create a fake dataset and compute the MSEs of both the MLE and JS estimator. \n# repeat this 1,000 times and take the average loss for each estimator.\nfor (sim in 1:n) {\n  # Simulate observations\n  X &lt;- rnorm(p, mean = mu, sd = sigma)\n\n  # MLE estimator and its MSE\n  mle &lt;- X\n  mse_mle[sim] &lt;- sum((mle - mu)^2)\n\n  # James-Stein estimator and its MSE\n  shrinkage &lt;- max(0, 1 - ((p - 2) * sigma^2) / sum(X^2))\n  js &lt;- shrinkage * X\n  mse_js[sim] &lt;- sum((js - mu)^2)\n}\n\n# print the results.\ncat(\"Average MSE of MLE:\", mean(mse_mle), \"\\n\")\n&gt; Average MSE of MLE: 5.125081 \ncat(\"Average MSE of James-Stein:\", mean(mse_js), \"\\n\")\n&gt; Average MSE of James-Stein: 5.055019 \n\n\n\n\n\n\nIn this example, average MSE of the James-Stein estimator (\\(5.06\\)) is consistently lower than that of the MLE (\\(5.13\\)), illustrating the paradox in action."
  },
  {
    "objectID": "blog/TODO-stein-paradox.html#bottom-line",
    "href": "blog/TODO-stein-paradox.html#bottom-line",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nStein‚Äôs paradox shows that shrinkage estimators can outperform the MLE in dimensions \\(p \\geq 3\\), even when the underlying variables are independent.\nThe James-Stein estimator achieves lower MSE by balancing variance reduction and bias introduction.\nUnderstanding this result highlights the power of shrinkage techniques in high-dimensional statistics."
  },
  {
    "objectID": "blog/TODO-stein-paradox.html#references",
    "href": "blog/TODO-stein-paradox.html#references",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "References",
    "text": "References\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html",
    "href": "blog/TODO-two-types-weights-causality.html",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "",
    "text": "Causal inference fundamentally seeks to answer: What is the effect of a treatment or intervention? The challenge lies in ensuring that the comparison groups‚Äîtreated versus untreated‚Äîare balanced in terms of their characteristics, so differences in outcomes can be attributed solely to the intervention. This idea, often referred to as ‚Äúlike-with-like‚Äù or ‚Äúapples-to-apples‚Äù comparison, is as simple as it is intuitive.\nBalance is thus fundamental to causal inference. Weighting methods, central to achieving this balance, have evolved to include two primary types: inverse propensity score weights and covariate balancing weights. This article briefly describes these weights, their mathematical foundations, and the intuition behind them, with an example in R to bring it all together."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html#background",
    "href": "blog/TODO-two-types-weights-causality.html#background",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "",
    "text": "Causal inference fundamentally seeks to answer: What is the effect of a treatment or intervention? The challenge lies in ensuring that the comparison groups‚Äîtreated versus untreated‚Äîare balanced in terms of their characteristics, so differences in outcomes can be attributed solely to the intervention. This idea, often referred to as ‚Äúlike-with-like‚Äù or ‚Äúapples-to-apples‚Äù comparison, is as simple as it is intuitive.\nBalance is thus fundamental to causal inference. Weighting methods, central to achieving this balance, have evolved to include two primary types: inverse propensity score weights and covariate balancing weights. This article briefly describes these weights, their mathematical foundations, and the intuition behind them, with an example in R to bring it all together."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html#notation",
    "href": "blog/TODO-two-types-weights-causality.html#notation",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Notation",
    "text": "Notation\nTo set the stage, we begin by establishing the potential outcome framework and laying out the notation for the discussion:\n\n\\(Y(0), Y(1)\\): Potential outcomes under treatment and control.\n\\(W\\): Treatment indicator (\\(1\\) for treated, \\(0\\) for untreated).\n\\(X\\): Vector of covariates (i.e., control variables).\n\\(e(X)=P(W=1 \\mid X)\\): Propensity score, the probability of receiving treatment given covariates.\n\\(\\mu_1‚Äã=E[Y(1)]\\): Mean outcome under treatment.\n\\(\\tau=\\mu_1 - \\mu_0\\): Average treatment effect (ATE), the main object of interest.\n\\(n\\): number of observations in the sample.\n\\(n_T\\): number of observations in the treatment group.\n\nWe observe a random sample of size \\(n\\), ${,Y_i, W_i, X_i }$,, where i indexes units (e.g., individuals, firms, schools etc.). Under the assumptions of strong ignorability‚Äîunconfoundedness \\(W \\perp (Y(0), Y(1)) \\mid X\\) and overlap \\((0&lt;e(X)&lt;1)\\) ‚Äî the ATE can be identified and estimated."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html#diving-deeper",
    "href": "blog/TODO-two-types-weights-causality.html#diving-deeper",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nInverse Propensity Score Weights\nInverse propensity score (IPS) weights rely on the estimated propensity score (X). The weights for treated and untreated groups are defined as:\n\\[ \\gamma_{\\text{IPS}}(X) = \\begin{cases} 1 / \\hat{e}(X) & \\text{if } W = 1, \\\\ 1 / (1 - \\hat{e}(X)) & \\text{if } W = 0. \\end{cases} \\]\nIPS weights intuitively correct for the unequal probability of treatment assignment. If an individual has a low probability of treatment \\(e(X)\\approx 0\\) but is treated, their weight is large, amplifying their influence in the analysis. Conversely, individuals with high treatment probabilities are downweighted to prevent overrepresentation.\nA few notes. First, these weights adjust the observed data such that the distribution of covariates in the weighted treated and control groups resembles that of the overall population. This helps mitigate selection bias, ensuring that comparisons between treated and control groups reflect a treatment effect rather than confounded differences in covariates (e.g., the control group is older).\nA second key property is that the weighted average of the outcomes for the treated group is an unbiased estimator for the mean outcome under treatment, \\(\\mu_1\\). This can be expressed mathematically as:\n\\[ \\hat{\\mu}_1(X)=\\frac{\\sum_i W_i Y_i/ \\hat{e}(X)}{ \\sum_i W_i / \\hat{e}(X)}=\\frac{1}{n_T}\\sum_i  \\gamma_{\\text{IPS}}(X_i) W_i Y_i. \\]\nThis equality is particularly useful in the next step, when estimating the treatment effect \\(\\tau\\).\nThird, these IPS weights are generally unknown and have to be estimated from the data. A leading exception is controlled randomized experiments where the researcher determines the probability of treatment. This is, however, uncommon. Traditionally, practitioners rely on methods like logistic regression to first obtain \\(\\hat{e}(X)\\) and then take its reciprocal to construct the weights.\nUsing IPS weights entails two primary challenges. When \\(\\hat{e}(X)\\) is close to \\(0\\) or \\(1\\), weights can become excessively large, leading to high variance in estimates. Practitioners then turn to trim observations with ‚Äútoo large‚Äù or ‚Äútoo small‚Äù \\(\\hat{e}(X)\\) values. Moreover, model misspecification in \\(\\hat{e}(X)\\) can lead to poor covariate balance, introducing bias. More flexible methods such as machin learning models can alleviate this problem.\nSoftware Packages: MatchIt, Matching.\n\n\nCovariate Balancing Weights\nAn alternative approach seeks weights that directly balance the dataset at hand. Typically this is framed as a constrained optimization problem with placing restrictions on the maximum imbalance in X between the treatment and control groups.\nThe weights \\(\\gamma_{\\text{CB}}(X)\\) are obtained by solving:\n\\[ \\min_{\\gamma} \\text{Imbalance}(\\gamma) + \\lambda \\text{Penalty}(\\gamma),\\]\nwhere ‚ÄúImbalance‚Äù measures covariate discrepancies between groups, and ‚ÄúPenalty‚Äù controls for extreme weights. A common formulation balances means:\n\\[ \\frac{1}{n} \\sum_{i=1}^n W_i \\gamma(X_i) f(X_i) \\approx \\frac{1}{n} \\sum_{i=1}^n f(X_i), \\]\nwith \\(f(x_i)=x\\). The same idea can be applied to higher moments of \\(X\\) like variance or skewness. Examples methods relying on this type of weights include include Hainmueller (2012), Chan et al.¬†(2016), and Athey et al.¬†(2018), among many others.\nCovariate balancing weights bypass the need to explicitly estimate \\(e(X)\\). Instead, they solve for weights that ensure the treated and control groups are balanced across predefined covariates or their transformations. This method aligns closely with the goal of causal inference: achieving balance. The main challenge is selecting the right set of covariates to be balanced. Variance estimation can also be more involved, although modern statistical packages take care of it.\nSoftware Packages: MatchIt, Matching."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html#hybrid-approach",
    "href": "blog/TODO-two-types-weights-causality.html#hybrid-approach",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Hybrid Approach",
    "text": "Hybrid Approach\nImai and Ratkovic (2013) developed a hybrid method that combines elements of covariate balancing and inverse propensity score methods. Their method, known as Covariate Balancing Propensity Score (CBPS), aims to estimate propensity scores while simultaneously achieving covariate balance. Instead of relying on a tuning parameter, CBPS ensures balance by solving the following moment conditions:\n\\[ \\sum_{i}\\left[ W_i - e(X_i; \\gamma) \\right] X_i = 0. \\]\nAdditionally, CBPS estimates by solving the standard likelihood score equation:\n\\[ \\sum_{i} \\left[ W_i - e(X_i; \\gamma) \\right] \\frac{\\partial e(X_i; \\gamma)}{\\partial \\gamma} = 0. \\]\nThe CBPS is operationalized in the Genralized Method of Moments (GMM) framework from the econmetrics literature. This approach ensures that the estimated propensity scores lead to balanced covariates, potentially improving the robustness of causal inference. CBPS can be implemented using available GMM software packages and is particularly useful when traditional propensity score models fail to achieve adequate balance.\nSoftware Packages: CBPS."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html#an-example",
    "href": "blog/TODO-two-types-weights-causality.html#an-example",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate these methods in practice with R. Consider a dataset with treatment W, outcome Y, and covariates \\(X_1, X_2‚Äã\\). We estimate the ATE using both IPS, entropy balancing weights and CBPS. The exercise starts with generating some synthetic data.\nrm(list=ls())\nlibrary(MASS)\nlibrary(WeightIt)\nset.seed(1988)\n\nn &lt;- 1000\nX1 &lt;- rnorm(n)\nX2 &lt;- rnorm(n)\nW &lt;- rbinom(n, 1, plogis(0.5 * X1 - 0.25 * X2))\nY &lt;- 3 + 2 * W + X1 + X2 + rnorm(n)\ndata = data.frame(Y, W, X1, X2)\n\nWe define functions that will calculate the weights and the associated Average Treatment Effects.\n\ncompute_weights &lt;- function(method) {\n    weightit(W ~ X1 + X2, method = method, data = data)$weights\n}\n\ncompute_ate &lt;- function(weights) {\n     weighted.mean(Y[W == 1], weights = weights[W == 1]) - \n        weighted.mean(Y[W == 0], weights = weights[W == 0])\n}\n\nNext, we calcualte the three types of estimates.\n\nips_weights &lt;- compute_weights(\"glm\")\nebal_weights &lt;- compute_weights(\"ebal\")\ncbps_weights &lt;- compute_weights(\"cbps\")\nFinally, we estimate the average treatment effect and print the results.\n\nips_ate &lt;- compute_ate(ips_weights)\nebal_ate &lt;- compute_ate(ebal_weights)\ncbps_ate &lt;- compute_ate(cbps_weights)\n\ncat(\"ATE (IPS Weights):\", ips_ate, \"\\n\")\n&gt;ATE (IPS Weights): 2.287048 \ncat(\"ATE (Entropy Balance Weights):\", ebal_ate, \"\\n\")\n&gt;ATE (Entropy Balance Weights): 2.287048 \n\ncat(\"ATE (CBPS Weights):\", cbps_ate, \"\\n\")\n&gt;ATE (CBPS Weights): 2.287048 \nThe weights are all very highly correlated with each other (not shown above), so they yield nearly identical results. For simplicity, I have ignored variance estimation and confidence intervals.\nYou can download the code from this GitHub repo."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html#where-to-learn-more",
    "href": "blog/TODO-two-types-weights-causality.html#where-to-learn-more",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThis article was inspired by Ben-Michael et al.¬†(2021) and Chattopadhyay et al.¬†(2020). Both references are great starting points. There are plenty of accessible materials on the topic online. My favorite is Imbens (2015). For more in-depth content turn to Imbens and Rubin (2015)‚Äôs seminal textbook."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html#bottom-line",
    "href": "blog/TODO-two-types-weights-causality.html#bottom-line",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nCovariate balance between the treatment and control groups is at the core of causal inference.\nThere are two broad classes of weights that achieve such balance.\nIPS weights adjust for treatment probability but can be unstable.\nCovariate balancing weights directly target balance X, bypassing propensity score estimation."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html#references",
    "href": "blog/TODO-two-types-weights-causality.html#references",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "References",
    "text": "References\nAthey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing: debiased inference of average treatment effects in high dimensions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 80(4), 597-623.\nBen-Michael, E., Feller, A., Hirshberg, D. A., & Zubizarreta, J. R. (2021). The balancing act in causal inference. arXiv preprint arXiv:2110.14831.\nChan, K. C. G., Yam, S. C. P., & Zhang, Z. (2016). Globally efficient non-parametric inference of average treatment effects by empirical balancing calibration weighting. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(3), 673-700.\nChattopadhyay, A., Hase, C. H., & Zubizarreta, J. R. (2020). Balancing vs modeling approaches to weighting in practice. Statistics in Medicine, 39(24), 3227-3254.\nHainmueller, J. (2012). Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20(1), 25-46.\nHirshberg, D. A., & Wager, S. (2018). Augmented minimax linear estimation for treatment and policy evaluation.\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society Series B: Statistical Methodology, 76(1), 243-263.\nImbens, G. W., & Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical sciences. Cambridge university press.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nLi, F., Morgan, K. L., & Zaslavsky, A. M. (2018). Balancing covariates via propensity score weighting. Journal of the American Statistical Association, 113(521), 390-400.\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41-55."
  },
  {
    "objectID": "blog/TODO-stratified-sampling-cont-var.html",
    "href": "blog/TODO-stratified-sampling-cont-var.html",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "",
    "text": "Stratified sampling is a foundational technique in survey design, ensuring that observations capture key characteristics of a population. By dividing the data into distinct strata and sampling from each, stratified sampling often results in more efficient estimates than simple random sampling. Strata are typically defined by categorical variables such as classrooms, villages, or user types. This method is particularly advantageous when some strata are rare but carry critical information, as it ensures their representation in the sample. It is also often employed to tackle spillover effects or manage survey costs more effectively.\nWhile straightforward for categorical variables (like geographic region), continuous variables‚Äîsuch as income or churn score‚Äîpose greater challenges for stratified sampling. The primary issue lies in the curse of dimensionality: attempting to create strata across multiple continuous variables results in an explosion of possible combinations, making effective sampling impractical. For example, stratifying a population based on income at every possible dollar amount is absurd.\nIn this article, I present two solutions to the problem of stratified sampling with continuous variables."
  },
  {
    "objectID": "blog/TODO-stratified-sampling-cont-var.html#background",
    "href": "blog/TODO-stratified-sampling-cont-var.html#background",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "",
    "text": "Stratified sampling is a foundational technique in survey design, ensuring that observations capture key characteristics of a population. By dividing the data into distinct strata and sampling from each, stratified sampling often results in more efficient estimates than simple random sampling. Strata are typically defined by categorical variables such as classrooms, villages, or user types. This method is particularly advantageous when some strata are rare but carry critical information, as it ensures their representation in the sample. It is also often employed to tackle spillover effects or manage survey costs more effectively.\nWhile straightforward for categorical variables (like geographic region), continuous variables‚Äîsuch as income or churn score‚Äîpose greater challenges for stratified sampling. The primary issue lies in the curse of dimensionality: attempting to create strata across multiple continuous variables results in an explosion of possible combinations, making effective sampling impractical. For example, stratifying a population based on income at every possible dollar amount is absurd.\nIn this article, I present two solutions to the problem of stratified sampling with continuous variables."
  },
  {
    "objectID": "blog/TODO-stratified-sampling-cont-var.html#diving-deeper",
    "href": "blog/TODO-stratified-sampling-cont-var.html#diving-deeper",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nThe Traditional Method: Equal-Sized Binning\nThis approach involves dividing the continuous variable(s) into intervals or bins. For example, churn score, a single continuous variable \\(X\\), can be divided into quantiles (e.g., quartiles or deciles), ensuring each bin contains approximately the same number of observations/users.\nLet‚Äôs focus on the case of building ten equally-sized strata. Mathematically, for a continuous variable \\(X\\), the decile-based binning can be defined as:\n\\[\\text{Bin}_i = \\{x \\in X : Q_{10\\times (i-1)+1} \\leq x &lt; Q_{10\\times i}\\} \\text{ for } i \\in \\{1,\\dots,10\\}, \\]\nwhere \\(Q_k\\) represents the \\(k\\)-th quantile of \\(X\\). This approach splits in the first ten percentiles (i.e., minimum value to the \\(10\\)th percentile) into a single stratum, the next ten percentiles (\\(10\\)th to \\(20\\)th) into another stratum, and so on.\nWhen dealing with multiple variables, this method extends to either marginally stratify each variable or jointly stratify them. However, joint stratification across multiple variables can also fall prey to the curse of dimensionality.\n\n\nThe Modern Method: Unsupervised Clustering\nAn alternative approach uses unsupervised clustering algorithms, such as \\(k\\)-means or hierarchical clustering, to group observations into clusters, treating these clusters as strata. Unlike binning, clustering leverages the distribution of the data to form natural groupings.\nFormally, let \\(X\\) be a matrix of n observations across \\(p\\) continuous variables. One class of clustering algorithms aims to assign each observation \\(i\\) to one of \\(k\\) clusters:\n\\[\\text{minimize} \\quad \\sum_{j=1}^k \\sum_{i \\in \\mathcal{C}_j} \\text{distance}(X_i - \\mu_j), \\]\nwhere \\(\\mu_j=\\frac{1}{|S_j|}\\sum_{i\\in \\mathcal{C}_j} X\\)‚Äã is the centroid of cluster \\(\\mathcal{C}_j\\) of \\(size |S_j|\\).\nCommonly, \\(\\text{distance}(X_i, \\mu_j)=\\|X_i - \\mu_j\\|^2\\) which leads to \\(k\\)-means clustering. Unlike in the binning approach, here we are not restricting each strata to have the same number of observations."
  },
  {
    "objectID": "blog/TODO-stratified-sampling-cont-var.html#pros-and-cons",
    "href": "blog/TODO-stratified-sampling-cont-var.html#pros-and-cons",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nUnsurprisingly, each method comes with its trade-offs. Traditional binning is simple and interpretable but can struggle with multivariate dependencies. Clustering accounts for multivariate relationships between variables, avoids imposing arbitrary bin thresholds, and may results in more natural groupings. However, it can be computationally expensive and sensitive to the choice of algorithm and hyperparameters (e.g., \\(k\\) in \\(k\\)-means).\nOne can also imagine a hybrid approach. Begin with a dimensionality reduction method like PCA and then perform binning on the first few principal components."
  },
  {
    "objectID": "blog/TODO-stratified-sampling-cont-var.html#an-example",
    "href": "blog/TODO-stratified-sampling-cont-var.html#an-example",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "An Example",
    "text": "An Example\nHere is R and python code illustrating both types of approaches on the popular iris dataset. We are interested in creating strata based on the SepalLenght variable. We begin with the traditional binning approach.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\ndata(iris)\n\n#Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris$SepalLengthBin &lt;- cut(iris$Sepal.Length, \n                           breaks = quantile(iris$Sepal.Length, probs = seq(0, 1, 0.25)), include.lowest = TRUE)\n\n#Inspect the resulting strata\ntable(iris$SepalLengthBin)\n\n#We split the dataset into four roughly equal parts based on the SepalLenght values, each with about 38 observations.\n\n#Perform k-means clustering on two continuous variables\niris_cluster &lt;- kmeans(iris[, c(\"Sepal.Length\", \"Petal.Length\")], centers = 4)\n\n#Assign clusters as strata\niris$Cluster &lt;- as.factor(iris_cluster$cluster)\n\n#Inspect the resulting strata\ntable(iris$Cluster)\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Set seed for reproducibility\nnp.random.seed(1988)\n\n# Load the iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris['SepalLengthBin'] = pd.qcut(iris['Sepal.Length'], q=4, labels=False)\n\n# Inspect the resulting strata\nprint(\"Quantile Bins (Sepal.Length):\")\nprint(iris['SepalLengthBin'].value_counts())\n\n# Perform k-means clustering on two continuous variables\nkmeans = KMeans(n_clusters=4, random_state=1988)\niris['Cluster'] = kmeans.fit_predict(iris[['Sepal.Length', 'Petal.Length']])\n\n# Assign clusters as strata\niris['Cluster'] = iris['Cluster'].astype('category')\n\n# Inspect the resulting strata\nprint(\"\\nCluster Sizes:\")\nprint(iris['Cluster'].value_counts())\n\n\n\nHere we also have four clusters, but their size ranges from \\(25\\) to \\(50\\) observations each."
  },
  {
    "objectID": "blog/TODO-stratified-sampling-cont-var.html#bottom-line",
    "href": "blog/TODO-stratified-sampling-cont-var.html#bottom-line",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nStratified sampling with continuous variables requires balancing simplicity and sophistication.\nTraditional binning remains a practical choice for single continuous variables or very few categorical ones.\nClustering provides a robust alternative, enabling stratification with multiple continuous variables at the cost of adding complexity and tuning parameters."
  },
  {
    "objectID": "blog/TODO-limits-nonparametric-models.html",
    "href": "blog/TODO-limits-nonparametric-models.html",
    "title": "The Limits of Nonparametric Models",
    "section": "",
    "text": "Nonparametric statistics offers a powerful toolkit for data analysis when the underlying data-generating process is too complex or unknown to be captured by parametric models. Unlike parametric methods, which assume a specific form for the underlying distribution or functional relationship, nonparametric approaches let the data ‚Äúspeak for itself.‚Äù\nIn earlier articles, we explored parametric and semiparametric models, where the focus was on attaining the lowest possible variance. However, in nonparametric statistics, this goal is unsuitable, as prioritizing minimal variance often introduces significant bias. The spotlight falls instead on selecting the optimal tuning parameter: the bandwidth. This article explains the various considerations and mathematical details associated with selecting the optimal bandwidth value in Kernel Density (KD) and Kernel Regression (KR) estimation."
  },
  {
    "objectID": "blog/TODO-limits-nonparametric-models.html#background",
    "href": "blog/TODO-limits-nonparametric-models.html#background",
    "title": "The Limits of Nonparametric Models",
    "section": "",
    "text": "Nonparametric statistics offers a powerful toolkit for data analysis when the underlying data-generating process is too complex or unknown to be captured by parametric models. Unlike parametric methods, which assume a specific form for the underlying distribution or functional relationship, nonparametric approaches let the data ‚Äúspeak for itself.‚Äù\nIn earlier articles, we explored parametric and semiparametric models, where the focus was on attaining the lowest possible variance. However, in nonparametric statistics, this goal is unsuitable, as prioritizing minimal variance often introduces significant bias. The spotlight falls instead on selecting the optimal tuning parameter: the bandwidth. This article explains the various considerations and mathematical details associated with selecting the optimal bandwidth value in Kernel Density (KD) and Kernel Regression (KR) estimation."
  },
  {
    "objectID": "blog/TODO-limits-nonparametric-models.html#notation",
    "href": "blog/TODO-limits-nonparametric-models.html#notation",
    "title": "The Limits of Nonparametric Models",
    "section": "Notation",
    "text": "Notation\nWe begin by establishing the mathematical terminology and notations essential for our analysis.\n\nKernels\nA kernel function \\(K(\\cdot)\\) is a non-negative, symmetric function that integrates to \\(1\\) over its domain. It acts as a weighting function that determines how observations contribute to the estimate at any given point, effectively spreading the mass of each data point over a local neighborhood.\n\n\nKernel Density\nKD is sort of like creating a smooth histogram. The kernel density estimator for a sample of n observations \\(X_1, X_2, \\dots, X_n \\in \\mathbb{R}\\) is given by:\n\\[\\hat{f}_h (x) = \\frac{1}{n h} \\sum_i K\\left(\\frac{x - X_i}{h}\\right),\\]\nwhere \\(h &gt; 0\\) is the bandwidth, a positive scalar controlling the smoothness of the estimate. Its function is to determine the window length around x in which the observations get positive weight.\nThe value \\(\\hat{f}_h (x)\\) will be large when there are many data points around \\(x\\), and small otherwise. Some popular kernel functions like Epanechnikov, Gaussian or triangular assign higher weights to observations closer to \\(x\\) and decaying importance to data further away.\nDensity estimation can also be performed in higher dimensions where the curse of dimensionality lurks in the background. In two dimensions, these estimates typically take the form of heat map-type figures.\n\n\nKernel Regression\nKernel regression, also known as local regression, is much like fitting a flexible smooth curve through data points. In this context, we have access to \\(n\\) observations of an outcome variable \\(Y\\). The objective is to estimate the conditional mean function at some point X=x:\n\\[m(x) = \\mathbb{E}[Y \\mid X = x].\\]\nAmong various kernel regression methods, the Nadaraya-Watson (NW) estimator is particularly popular. It is defined as:\n\\[\\hat{m}_h (x) = \\frac{\\sum_i K\\left(\\frac{x - X_i}{h}\\right) Y_i}{\\sum_i K\\left(\\frac{x - X_i}{h}\\right)}.\\]\nThe NW method fits a local constant around \\(x\\) equal to the average \\(Y\\) in that region (determined by the bandwidth). This approach is also known as local constant regression. More sophisticated variants include local linear and quadratic methods, which extend this fundamental idea but require more data. Across all these approaches, the bandwidth \\(h\\) serves as the key parameter that determines the trade-off between bias and variance, a relationship we will explore in detail below."
  },
  {
    "objectID": "blog/TODO-limits-nonparametric-models.html#diving-deeper",
    "href": "blog/TODO-limits-nonparametric-models.html#diving-deeper",
    "title": "The Limits of Nonparametric Models",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nIn practice, the choice of kernel is not as consequential. The primary focus of nonparametric statistics is selecting the optimal bandwidth \\(h\\).\n\nThe Bias-Variance Tradeoff\nAt the heart of bandwidth selection is the bias-variance tradeoff. Intuitively:\n\nSmall bandwidth (‚Äúoverfitting‚Äù): Captures fine details but also noise, leading to high variance and low bias.\nLarge bandwidth (‚Äúunderfitting‚Äù): Smoothes over the noise but can miss important structure, leading to high bias and low variance.\nFor nonparmetric curve estimation, the mean integrated squared error (MISE) is a common loss function for evaluating performance:\n\n\\[\\text{MISE}(h) = \\mathbb{E}\\left[ \\int \\left( \\hat{f}_h(x) - f(x) \\right)^2 dx \\right].\\]\nThis decomposes into bias and variance terms:\n\\[\\text{MISE}(h) =  \\int \\text{Bias}^2(\\hat{f}_h(x)) dx + \\int \\text{Var}(\\hat{f}_h(x)) dx.\\]\nMISE is analogous to the mean squared error (MSE) commonly used in machine learning, but integrated, or summed, across the support of the data. Minimizing the MISE yields the optimal bandwidth by balancing the bias-variance tradeoff, a fundamental concept in statistical estimation theory.\n\n\nOptimal \\(h\\) for Kernel Density\nSilverman‚Äôs rule of thumb provides a practical, closed-form approximation for \\(h\\) in KDE, assuming the data is roughly Gaussian. The expression is:\n\\[h^* = \\left( \\frac{4\\sigma^5}{3n} \\right) ^{\\frac{1}{5}}  = 1.06 \\sigma n^{-1/5},\\]\nwhere \\(\\sigma\\) is the standard deviation of the data. This formula emerges from theoretical analysis of the bias-variance tradeoff and serves as an effective initial bandwidth choice. While it performs well for many datasets, particularly those with approximately normal distributions, it may not be optimal for multimodal or heavily skewed distributions.\n\n\nOptimal \\(h\\) for Kernel Regression\nFor Nadaraya-Watson regression, several methods exist for selecting the bandwidth h. Cross-validation provides reliable results but can be computationally demanding. Theoretical analysis based on MISE minimization for common kernel functions suggests that the optimal bandwidth follows the relationship:\n\\[h^* \\propto n^{-1/5}.\\]\nThis expressiom highlights a fundamental characteristic of nonparametric methods: their convergence rate is slower than that of parametric methods. This represents one manifestation of the curse of dimensionality in nonparametric estimation. Furthermore, this relationship only specifies the rate at which the optimal bandwidth should decrease with sample size (proportionality to \\(n^{-1/5}\\)), rather than providing an exact value. Without the proportionality constant, its direct practical application is limited."
  },
  {
    "objectID": "blog/TODO-limits-nonparametric-models.html#an-example",
    "href": "blog/TODO-limits-nonparametric-models.html#an-example",
    "title": "The Limits of Nonparametric Models",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate bandwidth selection with a simple example.\nrequire(graphics)\nwith(cars, {\n    plot(speed, dist)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 2), col = 2)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 5), col = 3)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 10), col = 4)\n})\nThis yields a simplified version of the following figure:\n[ADD IMAGE]\nThe green line corresponds to the Nadaraya-Watson regression with an optimal bandwidth and features some degree of curveture. The red line has a smaller bandwidth (higher variance, lower bias), and the blue line is undersmoothed (higher bandwidth)."
  },
  {
    "objectID": "blog/TODO-limits-nonparametric-models.html#bottom-line",
    "href": "blog/TODO-limits-nonparametric-models.html#bottom-line",
    "title": "The Limits of Nonparametric Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBandwidth selection is critical for nonparametric methods to balance bias and variance.\nSilverman‚Äôs rule of thumb offers a simple yet effective starting point for KD.\nFor commonly used second-order kernels, the optimal bandwidth in KR scales as \\(n^{-1/5}\\).\nPractical implementation often relies on cross-validation or plug-in methods and is limited by the curse of dimensionality."
  },
  {
    "objectID": "blog/TODO-limits-nonparametric-models.html#references",
    "href": "blog/TODO-limits-nonparametric-models.html#references",
    "title": "The Limits of Nonparametric Models",
    "section": "References",
    "text": "References\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.\nLi, Q., & Racine, J. S. (2023). Nonparametric econometrics: theory and practice. Princeton University Press.\nSilverman, B. W. (1986). Density Estimation for Statistics and Data Analysis. Wand, M. P., & Jones, M. C. (1995). Kernel Smoothing.\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media."
  },
  {
    "objectID": "blog/TODO-mutual-information.html",
    "href": "blog/TODO-mutual-information.html",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "",
    "text": "When exploring dependencies between variables, the data scientist‚Äôs toolbox often relies on correlation measures to reveal relationships and potential patterns. But what if we‚Äôre looking to capture relationships beyond linear correlations? Mutual Information (MI) quantifies the ‚Äúamount of information‚Äù shared between variables in a more general sense. It measures how much we know about Y by observing X. This approach goes beyond linear patterns and can help us uncover more complex relationships within data.\nMI can be especially helpful for applications such as feature selection and unsupervised learning. For readers familiar with various types of correlation metrics (as discussed in my earlier post), Mutual Information provides an additional lens to interpret relationships within data.\nIn this article, I‚Äôll guide you through the concept of Mutual Information, its definition, properties, and usage, as well as comparisons with other dependency measures. We‚Äôll explore MI‚Äôs mathematical foundations, its advantages and limitations, and its applications, concluding with an example demonstrating MI‚Äôs behavior alongside more traditional measures like Pearson‚Äôs correlation."
  },
  {
    "objectID": "blog/TODO-mutual-information.html#introduction",
    "href": "blog/TODO-mutual-information.html#introduction",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "",
    "text": "When exploring dependencies between variables, the data scientist‚Äôs toolbox often relies on correlation measures to reveal relationships and potential patterns. But what if we‚Äôre looking to capture relationships beyond linear correlations? Mutual Information (MI) quantifies the ‚Äúamount of information‚Äù shared between variables in a more general sense. It measures how much we know about Y by observing X. This approach goes beyond linear patterns and can help us uncover more complex relationships within data.\nMI can be especially helpful for applications such as feature selection and unsupervised learning. For readers familiar with various types of correlation metrics (as discussed in my earlier post), Mutual Information provides an additional lens to interpret relationships within data.\nIn this article, I‚Äôll guide you through the concept of Mutual Information, its definition, properties, and usage, as well as comparisons with other dependency measures. We‚Äôll explore MI‚Äôs mathematical foundations, its advantages and limitations, and its applications, concluding with an example demonstrating MI‚Äôs behavior alongside more traditional measures like Pearson‚Äôs correlation."
  },
  {
    "objectID": "blog/TODO-mutual-information.html#notation",
    "href": "blog/TODO-mutual-information.html#notation",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Notation",
    "text": "Notation\nBefore diving deeper, let‚Äôs establish our notation:\n\nRandom variables will be denoted by capital letters (\\(X\\), \\(Y\\)).\nLowercase letters (\\(x\\),\\(y\\)) represent specific values of these variables.\n\\(p(x)\\) denotes the probability mass/density function of \\(X\\).\n\\(p(x,y)\\) represents the joint probability mass/density function of \\(X\\) and \\(Y\\).\n\\(p(x\\mid y)\\) is the conditional probability of \\(X\\) given \\(Y\\).\n\\(H(X)\\) represents the entropy of random variable \\(X\\).\n\nThese should not surprise anyone, as they are standard conventions in most statistics textbooks."
  },
  {
    "objectID": "blog/TODO-mutual-information.html#diving-deeper",
    "href": "blog/TODO-mutual-information.html#diving-deeper",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nRefresher on Entropy\nMutual Information is related to the notion of entropy, a measure of uncertainty or randomness in a random variable. In less formal terms, entropy quantifies how ‚Äúsurprising‚Äù or ‚Äúunpredictable‚Äù the outcomes of \\(X\\) are.\nFormally, for a discrete random variable X with possible outcomes \\(x_1, x_2, \\dots, x_n\\) and associated probabilities \\(p(x_1), p(x_2), \\dots, p(x_n)\\), entropy \\(H(X)\\) is defined as:\n[H(X) = -_{i=1}^n p(x_i) p(x_i).]\nFor continuous variables we switch the summation with an integral.\nEntropy equals \\(0\\) when there‚Äôs no uncertainty, such as when \\(X\\) always takes a single outcome. High entropy means greater uncertainty (many possible outcomes, all equally likely), while low entropy indicates less uncertainty, as some outcomes are much more likely than others. In essence, entropy tells us how much ‚Äúinformation‚Äù is gained on average when observing the variable‚Äôs realization.\n\n\nMathematical Definitions of MI\nMI can be defined in mulitple ways. Perhaps the most intiuitive definition of MI between two random variables \\(X\\) and $ Y$ is that it measures the difference between the joint distribution and the product of their marginals. If two random variables are independent, their joint distribution is the product of their marginals \\(p(x,y)=p(x)p(y)\\). In some sense, the discrepancy between these two objects (i.e., the two sides of the equality) measures the strength of association between \\(X\\) and \\(Y\\) (i.e., their ‚Äúindependence‚Äù).\nFormally, MI is the Kullback-Leibler divergence between the joint distribution and the product of the two marginals:\n\\[ MI(X,Y) = D_{KL}(p(X,Y) || p(X)p(Y). \\]\nLet‚Äôs now examine MI from a different angle. We can express mutual information as follows:\n\\[ MI(X,Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\left(\\frac{p(x,y)}{p(x)p(y)}\\right)\\]\nAgain, for continuous variables, the sums become integrals. This is a more standard definition since it does not rely on\nAlternatively, MI can also be expressed in terms of entropy. It equals the sum of the entropy of \\(X\\) and \\(Y\\) taking away their joint entropy:\n\\[ \\begin{align*}MI(X,Y) & = H(X) - H(X|Y) \\\\ & = H(Y) - H(Y|X) \\\\ & = H(X) + H(Y) - H(X,Y)\\end{align*} \\]\n. You can compute MI in R with the infotheo package.\nSoftware Package: infotheo.\n\n\nProperties\nMI has several intriguing properties:\n\nNon-negativity: \\(MI(X;Y) \\geq 0\\). Mutual Information is always non-negative, as it measures the amount of information one variable provides about the other. Higher values correspond to stronger association.\nSymmetry: \\(MI(X,Y) = MI(Y,X)\\). This symmetry implies that the information \\(X\\) provides about \\(Y\\) is the same as what Y provides about X.\nIndependence: Similarly to Chatterjee‚Äôs correlation coefficient, \\(MI(X,Y) = 0\\) if and only if \\(X\\) and \\(Y\\) are independent.\nScale-invarance: Mutual information is scale invariant. If you apply a scaling transformation to the variables, their MI will not be affected.\n\n\n\nConditional MI\nConditional Mutual Information (CMI) extends the concept of MI to measure dependency between \\(X\\) and \\(Y\\) given a third variable \\(Z\\). CMI is useful for investigating how much information \\(X\\) and \\(Y\\) share independently of \\(Z\\). It is defined as:\n\\[MI(X,Y|Z) = \\sum_{x,y,z} p(x,y,z) \\log \\left(\\frac{p(x,y|z)}{p(x|z)p(y|z)}\\right). \\]\nThis can be valuable in causal inference, where understanding dependencies conditioned on specific variables aids in interpreting relationships within complex models. CMI is also particularly useful for feature selection when accounting for redundancy among already included features. Let‚Äôs explore this idea in greater detail.\n\n\nFeature Selection with MI\nIn machine learning, MI serves as a useful metric for feature selection (Brown et al.¬†2012, Vergara and Est√©vez 2014). Consider an outcome variable \\(Y\\) and a set of features \\(X\\in\\mathbb{R}^p\\) with n i.i.d. observations. By evaluating the MI between each feature and the target variable, one can retain features with the highest information content, passing a certain threshold. This is somewhat primitive since it assumes independence across features. More sophisticated approaches take feature dependency into acount.\nFor instance, methods like Minimum Redundancy Maximum Relevance (mRMR, Peng et al.¬†2005) aim to maximize the relevance of features to the target while minimizing redundancy among features. Here is a concise version of the mRMR algorithm:\n\nCalculate MI between each feature and the target (relevance).\nCalculate MI between features (redundancy).\nSelect features that maximize relevance while minimizing redundancy: \\[ \\text{mRMR} = \\max_{X_i} \\left[MI(X_i;Y) - \\frac{1}{|S|} \\sum_{X_j \\in S} MI(X_i;X_j)\\right], \\]\n\nwhere \\(S\\) is the set of already selected features."
  },
  {
    "objectID": "blog/TODO-mutual-information.html#pros-and-cons",
    "href": "blog/TODO-mutual-information.html#pros-and-cons",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nLike any other statistical tool, mutual Information has several advantages and limitations. On the positive side, it captures both linear and nonlinear relationships, is scale-invariant, and works with both continuous and discrete variables, making it a theoretically well-founded measure. However, it requires density estimation for continuous variables, can be computationally intensive for large datasets, and its results can be sensitive to binning choices for continuous variables. Additionally, there is no standard normalization for MI."
  },
  {
    "objectID": "blog/TODO-mutual-information.html#an-example",
    "href": "blog/TODO-mutual-information.html#an-example",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs implement MI calculation in R and python and compare it with traditional correlation measures using the iris dataset.\n\nRPython\n\n\nrm(list=ls())\nlibrary(infotheo)\nlibrary(corrplot)\nlibrary(dplyr)\ndata(iris)\n\n# Calculate mutual information matrix\nmi_matrix &lt;- mutinformation(discretize(iris[,1:4]))\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(iris[,1:4])\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value &lt;- mi_matrix[1,3]\ncor_value &lt;- cor_matrix[1,3]\n\nprint(paste(\"Mutual Information:\", round(mi_value, 3)))\nprint(paste(\"Pearson Correlation:\", round(cor_value, 3)))\nWe can now print the results.\n\n[1] \"Mutual Information: 0.585\"\n[1] \"Pearson Correlation: 0.872\"\n\n\n\n\n\n\n[ADD IMAGE]\nThe left matrix displays the MI results and the right one shows the standard (Pearson) correlation values. The default scales are different, so one should compare the values and not the colors. Indeed, the variable pairs with negative linear correlation also have the lowest MI values.\nThis example demonstrates how MI can capture nonlinear relationships that might be missed by traditional correlation measures.\nYou can download the code from this GitHub repo."
  },
  {
    "objectID": "blog/TODO-mutual-information.html#bottom-line",
    "href": "blog/TODO-mutual-information.html#bottom-line",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMutual Information provides a comprehensive measure of statistical dependence, capturing both linear and nonlinear relationships.\nUnlike correlation coefficients, MI works naturally with both continuous and categorical variables.\nMI serves as the foundation for sophisticated feature selection algorithms like mRMR."
  },
  {
    "objectID": "blog/TODO-mutual-information.html#where-to-learn-more",
    "href": "blog/TODO-mutual-information.html#where-to-learn-more",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great place to start and learng the basics. Brown et al.¬†(2012) and Vergara and Est√©vez (2014) are the go-to resources for conditional MI and using MI for feature selection."
  },
  {
    "objectID": "blog/TODO-mutual-information.html#references",
    "href": "blog/TODO-mutual-information.html#references",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "References",
    "text": "References\nBrown, G., Pocock, A., Zhao, M. J., & Luj√°n, M. (2012). Conditional likelihood maximisation: a unifying framework for information theoretic feature selection. JMLR. Cover, T. M., & Thomas, J. A. (2006). Elements of information theory (2nd ed.). Wiley-Interscience. Kraskov, A., St√∂gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical Review E, 69(6). Peng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE TPAMI. Ross, B. C. (2014). Mutual information between discrete and continuous data sets. PloS one, 9(2). Vergara, J. R., & Est√©vez, P. A. (2014). A review of feature selection methods based on mutual information. Neural Computing and Applications, 24(1)."
  },
  {
    "objectID": "blog/TODO-chatterjee-correlation.html#a-closer-look",
    "href": "blog/TODO-chatterjee-correlation.html#a-closer-look",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLinear Relationships\nWhen we simply say ‚Äúcorrelation‚Äù we refer to the so-called Pearson correlation coefficient. Virtually everyone working with data is familiar with it. Given a random sample \\((X_1,Y_1),\\dots,(X_n, Y_n)\\) of two random variables \\(X\\) and \\(Y\\) it is computed by:\n\\[corr^{P}(X,Y) = \\frac{ \\sum_i(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}},\\]\nwhere an upper bar denotes a sample mean.\nThis coefficient lives in the \\([-1,1]\\) interval. Larger values indicate stronger relationship between \\(X\\) and \\(Y\\), be it positive or negative. At the extreme, the Pearson coefficient will equal \\(1\\) when all observations can be perfectly lined up on a upward sloping line. Yes, you guessed it ‚Äì when it equals \\(-1\\) the line is sloping down.\nYou can easily calculate the Pearson correlation in R and python:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'pearson', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr\n\npearson_corr, pearson_pval = pearsonr(x, y)\n\n\n\nThis measure, while widely popular, suffers from a few shortcomings. First, outliers have an outsized impact in skewing its value. Sample means vulnerable to outliers are a key ingredient in the calculation, rendering the measure sensitive to data anomalies. Second, it is designed to detect only linear relationships. Two variables might have a strong but non-linear relationship which this measure will not detect. Lastly, it is not transformation-invariant, meaning that applying a monotone transformation to either of the variables will change the correlation value.\nLet‚Äôs discuss some improvements to the Pearson correlation measure. Enter Spearman correlation.\n\n\nMonotone Relationships\nSpearman correlation is the Pearson correlation among the ranks of \\(X\\) and \\(Y\\):\n\\[corr^{S}(X,Y) = corr^{P}(R(X),R(Y)),\\]\nwhere \\(R(\\cdot)\\) denotes an observation‚Äôs rank (or order) in the sample.\nSpearman correlation is thus a rank correlation measure. As such, it quantifies how well the relationship between \\(X\\) and \\(Y\\) can be described using a monotone (and not necessarily a linear) function. It is therefore a more flexible measure of association. Again, intuitively, Spearman correlation will take on a large positive value when the \\(X\\) and \\(Y\\) observations have similar ranks. This value will be negative when the ranks tend to go in opposite directions.\nSpearman correlation addresses some of the shortcomings associated with Pearson correlation. It is not easily influenced by outliers, and it does not change if we apply a monotone transformation of \\(X\\) and/or \\(Y\\). These benefits come at the expense of a loss in interpretation and potential issues when tied ranks are common, a scenario I ignore here.\nCalculating it is just as simple:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import spearmanr\n\nspearman_corr, spearman_pval = spearmanr(x, y)\n\n\n\nSpearman correlation is not the only rank correlation coefficient out there. –ê popular alternative is the Kendall rank coefficient which is computed slightly differently. Let‚Äôs define a pair of observations \\((X_i, Y_i)\\) and \\((X_j, Y_j)\\) to be agreeing (the technical term is concordant) if the differences \\((X_i - X_j)\\) and \\((Y_i - Y_j)\\) have the same sign (i.e., either both \\(X_i &gt; X_j\\) and $Y_i &gt; Y_j $or both \\(X_i &lt; X_j\\) and \\(Y_i &lt; Y_j\\) ).\nThen, Kendall‚Äôs coefficient is expressed as:\n\\[corr^{K} = \\frac{\\text{number of agreeing pairs} - \\text{number of disagreeing pairs}} {\\text{total number of pairs}}.\\]\nSo, it quantifies the degree of agreement between the ranks of \\(X\\) and \\(Y\\). Like the other coeffients described above, its range is \\([-1, 1]\\) and values away from zero indicate stronger relationship.\nAgain, this coefficient is similarly computed in R and python:\n\nRPython\n\n\ncor(x,y, method = 'kendall')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\nkendall_corr, kendall_pval = kendalltau(x, y)\n\n\n\nKendall‚Äôs measure improves on some of the shortcomings baked in the Spearman‚Äôs coefficients ‚Äì it has a clearer interpretation and it is less sensitive to rank ties.\nHowever, none of these rank correlation coefficients can detect non-monotonic relationships. For instance, \\(X\\) and \\(Y\\) can have a parabola- or wave-like pattern when plotted against each other. We would like a correlation measure flexible enough to capture such non-linear relationships.\nThis is where Chatterjee‚Äôs coefficient comes in.\n\n\nMore General Relationships\nChatterjee recently proposed a new correlation coefficient designed to detect non-monotonic relationships. He discovered a novel estimator of a population quantity first proposed by Dette et al.¬†(2013).\nLet‚Äôs start with the formula. The Chatterjee correlation coefficient is calculated as follows:\n\\[corr^{C}(X,Y) = 1-\\frac{3\\sum_{i=1}^{n-1}|R(Y_{k:R(X_k)=i+1}) - R(Y_{k:R(X_k)=i})|}{n^2-1},\\]\nwhere n is the sample size. This looks complicated, so let‚Äôs try to simplify the numerator. Let‚Äôs sort the data in an ascending order of \\(X\\) so that we have \\((X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})\\), where \\(X_{(1)}&lt;X_{(2)}&lt;\\dots &lt;X_{(n)}\\). Also, denote \\(R(Y_i)\\) be the rank of \\(Y_{(i)}\\). Then:\n\\[corr^{C}(X,Y) = 1 - \\frac{3\\sum_{i=1}^{n-1}|R(Y_{i+1})-R(Y_i)|}{n^2-1}.\\]\nSo, this new coefficient is a scaled version of the sum of the absolute differences in the consecutive ranks of \\(Y\\) when ordered by \\(X\\). It is perhaps best to think about Chatterjee‚Äôs method as a measure of dependence and not strictly a correlation coefficient.\nThere are some major difference comapred to the previous correlation measures. Chatterjee‚Äôs correlation coefficient lies in the \\([0,1]\\) interval. It is equal to zero if and only if \\(X\\) and \\(Y\\) are independent and to one if one of them is a function of the other. Unlike the coefficients descibed above, it is not symmetric in \\(X\\) and \\(Y\\), meaning \\(corr^{C}(X,Y) \\neq corr^{C}(Y,X)\\). This is understandable since we are interested in whether \\(X\\) is a function of \\(Y\\), which does not imply the opposite. The author also develops asymptotic theory for calculating p-values although some researchers have raised concerns about the coefficient‚Äôs power.\nHere is a sample code to calculate its value:\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\nn &lt;- 1000\nx &lt;- runif(n) \ny &lt;- 5 * sin(x) + rnorm(n)\n\ndata &lt;- data.frame(x=x, y=y)\ndata$R &lt;- rank(data$y)\ndata &lt;- data[order(data$x), ]\n\n1 - 3 * sum(abs(diff(data$R))) / (n^2-1)\n\n\nimport numpy as np\n\nnp.random.seed(1988)\nn = 1000\nx = np.random.uniform(size=n)\ny = 5 * np.sin(x) + np.random.normal(size=n)\n\ndata = np.array(sorted(zip(x, y), key=lambda pair: pair[0]))\nranks = np.argsort(np.argsort(data[:, 1]))  # Rank of y\nchatterjee_corr = 1 - 3 * np.sum(np.abs(np.diff(ranks))) / (n**2 - 1)\n\n\n\nSoftware Package: XICOR.\nThere you have it. You are now well-equipped to dive deeper into your datasets and find new exciting relationships."
  },
  {
    "objectID": "blog/DONE-chatterjee-correlation.html",
    "href": "blog/DONE-chatterjee-correlation.html",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "",
    "text": "Much of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford published a paper outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\nIn this article I will go over the basics of Chatterjee‚Äôs correlation measure. Before we get there, let‚Äôs first review some of the more traditional approaches in assessing bivariate relationship strength.\nFor simplicity, let‚Äôs assume away ties. Let‚Äôs also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant."
  },
  {
    "objectID": "blog/DONE-chatterjee-correlation.html#background",
    "href": "blog/DONE-chatterjee-correlation.html#background",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "",
    "text": "Much of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford published a paper outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\nIn this article I will go over the basics of Chatterjee‚Äôs correlation measure. Before we get there, let‚Äôs first review some of the more traditional approaches in assessing bivariate relationship strength.\nFor simplicity, let‚Äôs assume away ties. Let‚Äôs also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant."
  },
  {
    "objectID": "blog/DONE-chatterjee-correlation.html#a-closer-look",
    "href": "blog/DONE-chatterjee-correlation.html#a-closer-look",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLinear Relationships\nWhen we simply say ‚Äúcorrelation‚Äù we refer to the so-called Pearson correlation coefficient. Virtually everyone working with data is familiar with it. Given a random sample \\((X_1,Y_1),\\dots,(X_n, Y_n)\\) of two random variables \\(X\\) and \\(Y\\) it is computed by:\n\\[corr^{P}(X,Y) = \\frac{ \\sum_i(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}},\\]\nwhere an upper bar denotes a sample mean.\nThis coefficient lives in the \\([-1,1]\\) interval. Larger values indicate stronger relationship between \\(X\\) and \\(Y\\), be it positive or negative. At the extreme, the Pearson coefficient will equal \\(1\\) when all observations can be perfectly lined up on a upward sloping line. Yes, you guessed it ‚Äì when it equals \\(-1\\) the line is sloping down.\nYou can easily calculate the Pearson correlation in R and python:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'pearson', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr\n\npearson_corr, pearson_pval = pearsonr(x, y)\n\n\n\nThis measure, while widely popular, suffers from a few shortcomings. First, outliers have an outsized impact in skewing its value. Sample means vulnerable to outliers are a key ingredient in the calculation, rendering the measure sensitive to data anomalies. Second, it is designed to detect only linear relationships. Two variables might have a strong but non-linear relationship which this measure will not detect. Lastly, it is not transformation-invariant, meaning that applying a monotone transformation to either of the variables will change the correlation value.\nLet‚Äôs discuss some improvements to the Pearson correlation measure. Enter Spearman correlation.\n\n\nMonotone Relationships\nSpearman correlation is the Pearson correlation among the ranks of \\(X\\) and \\(Y\\):\n\\[corr^{S}(X,Y) = corr^{P}(R(X),R(Y)),\\]\nwhere \\(R(\\cdot)\\) denotes an observation‚Äôs rank (or order) in the sample.\nSpearman correlation is thus a rank correlation measure. As such, it quantifies how well the relationship between \\(X\\) and \\(Y\\) can be described using a monotone (and not necessarily a linear) function. It is therefore a more flexible measure of association. Again, intuitively, Spearman correlation will take on a large positive value when the \\(X\\) and \\(Y\\) observations have similar ranks. This value will be negative when the ranks tend to go in opposite directions.\nSpearman correlation addresses some of the shortcomings associated with Pearson correlation. It is not easily influenced by outliers, and it does not change if we apply a monotone transformation of \\(X\\) and/or \\(Y\\). These benefits come at the expense of a loss in interpretation and potential issues when tied ranks are common, a scenario I ignore here.\nCalculating it is just as simple:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import spearmanr\n\nspearman_corr, spearman_pval = spearmanr(x, y)\n\n\n\nSpearman correlation is not the only rank correlation coefficient out there. –ê popular alternative is the Kendall rank coefficient which is computed slightly differently. Let‚Äôs define a pair of observations \\((X_i, Y_i)\\) and \\((X_j, Y_j)\\) to be agreeing (the technical term is concordant) if the differences \\((X_i - X_j)\\) and \\((Y_i - Y_j)\\) have the same sign (i.e., either both \\(X_i &gt; X_j\\) and $Y_i &gt; Y_j $or both \\(X_i &lt; X_j\\) and \\(Y_i &lt; Y_j\\) ).\nThen, Kendall‚Äôs coefficient is expressed as:\n\\[corr^{K} = \\frac{\\text{number of agreeing pairs} - \\text{number of disagreeing pairs}} {\\text{total number of pairs}}.\\]\nSo, it quantifies the degree of agreement between the ranks of \\(X\\) and \\(Y\\). Like the other coeffients described above, its range is \\([-1, 1]\\) and values away from zero indicate stronger relationship.\nAgain, this coefficient is similarly computed in R and python:\n\nRPython\n\n\ncor(x,y, method = 'kendall')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\nkendall_corr, kendall_pval = kendalltau(x, y)\n\n\n\nKendall‚Äôs measure improves on some of the shortcomings baked in the Spearman‚Äôs coefficients ‚Äì it has a clearer interpretation and it is less sensitive to rank ties.\nHowever, none of these rank correlation coefficients can detect non-monotonic relationships. For instance, \\(X\\) and \\(Y\\) can have a parabola- or wave-like pattern when plotted against each other. We would like a correlation measure flexible enough to capture such non-linear relationships.\nThis is where Chatterjee‚Äôs coefficient comes in.\n\n\nMore General Relationships\nChatterjee recently proposed a new correlation coefficient designed to detect non-monotonic relationships. He discovered a novel estimator of a population quantity first proposed by Dette et al.¬†(2013).\nLet‚Äôs start with the formula. The Chatterjee correlation coefficient is calculated as follows:\n\\[corr^{C}(X,Y) = 1-\\frac{3\\sum_{i=1}^{n-1}|R(Y_{k:R(X_k)=i+1}) - R(Y_{k:R(X_k)=i})|}{n^2-1},\\]\nwhere n is the sample size. This looks complicated, so let‚Äôs try to simplify the numerator. Let‚Äôs sort the data in an ascending order of \\(X\\) so that we have \\((X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})\\), where \\(X_{(1)}&lt;X_{(2)}&lt;\\dots &lt;X_{(n)}\\). Also, denote \\(R(Y_i)\\) be the rank of \\(Y_{(i)}\\). Then:\n\\[corr^{C}(X,Y) = 1 - \\frac{3\\sum_{i=1}^{n-1}|R(Y_{i+1})-R(Y_i)|}{n^2-1}.\\]\nSo, this new coefficient is a scaled version of the sum of the absolute differences in the consecutive ranks of \\(Y\\) when ordered by \\(X\\). It is perhaps best to think about Chatterjee‚Äôs method as a measure of dependence and not strictly a correlation coefficient.\nThere are some major difference comapred to the previous correlation measures. Chatterjee‚Äôs correlation coefficient lies in the \\([0,1]\\) interval. It is equal to zero if and only if \\(X\\) and \\(Y\\) are independent and to one if one of them is a function of the other. Unlike the coefficients descibed above, it is not symmetric in \\(X\\) and \\(Y\\), meaning \\(corr^{C}(X,Y) \\neq corr^{C}(Y,X)\\). This is understandable since we are interested in whether \\(X\\) is a function of \\(Y\\), which does not imply the opposite. The author also develops asymptotic theory for calculating p-values although some researchers have raised concerns about the coefficient‚Äôs power.\nHere is a sample code to calculate its value:\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\nn &lt;- 1000\nx &lt;- runif(n) \ny &lt;- 5 * sin(x) + rnorm(n)\n\ndata &lt;- data.frame(x=x, y=y)\ndata$R &lt;- rank(data$y)\ndata &lt;- data[order(data$x), ]\n\n1 - 3 * sum(abs(diff(data$R))) / (n^2-1)\n\n\nimport numpy as np\n\nnp.random.seed(1988)\nn = 1000\nx = np.random.uniform(size=n)\ny = 5 * np.sin(x) + np.random.normal(size=n)\n\ndata = np.array(sorted(zip(x, y), key=lambda pair: pair[0]))\nranks = np.argsort(np.argsort(data[:, 1]))  # Rank of y\nchatterjee_corr = 1 - 3 * np.sum(np.abs(np.diff(ranks))) / (n**2 - 1)\n\n\n\nSoftware Package: XICOR.\nThere you have it. You are now well-equipped to dive deeper into your datasets and find new exciting relationships."
  },
  {
    "objectID": "blog/DONE-chatterjee-correlation.html#bottom-line",
    "href": "blog/DONE-chatterjee-correlation.html#bottom-line",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThere are numerous ways of measuring association between two variables.\nThe most common methods measure only linear or monotonic relationships. These are often useful but do not capture more complex, non-linear associations.\nA new correlation measure, Chatterjee‚Äôs coeffient, is designed to go beyond monotonicty and assess more general bivariate relationships."
  },
  {
    "objectID": "blog/DONE-chatterjee-correlation.html#where-to-learn-more",
    "href": "blog/DONE-chatterjee-correlation.html#where-to-learn-more",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia has detailed entries on correlation, rank correlation, and Kendall‚Äôs coefficient which I found helpful. The R bloggers platform has articles exploring the Chatterjee‚Äôs correlation coefficient in detail. The more technically oriented folks will find Chatterjee‚Äôs original paper helpful."
  },
  {
    "objectID": "blog/DONE-chatterjee-correlation.html#references",
    "href": "blog/DONE-chatterjee-correlation.html#references",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "References",
    "text": "References\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\nDette, H., Siburg, K. F., & Stoimenov, P. A. (2013). A Copula‚ÄêBased Non‚Äêparametric Measure of Regression Dependence. Scandinavian Journal of Statistics, 40(1), 21-41.\nShi, H., Drton, M., & Han, F. (2022). On the power of Chatterjee‚Äôs rank correlation. Biometrika, 109(2), 317-333.\nhttps://www.r-bloggers.com/2021/12/exploring-the-xi-correlation-coefficient/"
  },
  {
    "objectID": "blog/TODO-limits-nonparametric-models.html#a-closer-look",
    "href": "blog/TODO-limits-nonparametric-models.html#a-closer-look",
    "title": "The Limits of Nonparametric Models",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn practice, the choice of kernel is not as consequential. The primary focus of nonparametric statistics is selecting the optimal bandwidth \\(h\\).\n\nThe Bias-Variance Tradeoff\nAt the heart of bandwidth selection is the bias-variance tradeoff. Intuitively:\n\nSmall bandwidth (‚Äúoverfitting‚Äù): Captures fine details but also noise, leading to high variance and low bias.\nLarge bandwidth (‚Äúunderfitting‚Äù): Smoothes over the noise but can miss important structure, leading to high bias and low variance.\nFor nonparmetric curve estimation, the mean integrated squared error (MISE) is a common loss function for evaluating performance:\n\n\\[\\text{MISE}(h) = \\mathbb{E}\\left[ \\int \\left( \\hat{f}_h(x) - f(x) \\right)^2 dx \\right].\\]\nThis decomposes into bias and variance terms:\n\\[\\text{MISE}(h) =  \\int \\text{Bias}^2(\\hat{f}_h(x)) dx + \\int \\text{Var}(\\hat{f}_h(x)) dx.\\]\nMISE is analogous to the mean squared error (MSE) commonly used in machine learning, but integrated, or summed, across the support of the data. Minimizing the MISE yields the optimal bandwidth by balancing the bias-variance tradeoff, a fundamental concept in statistical estimation theory.\n\n\nOptimal \\(h\\) for Kernel Density\nSilverman‚Äôs rule of thumb provides a practical, closed-form approximation for \\(h\\) in KDE, assuming the data is roughly Gaussian. The expression is:\n\\[h^* = \\left( \\frac{4\\sigma^5}{3n} \\right) ^{\\frac{1}{5}}  = 1.06 \\sigma n^{-1/5},\\]\nwhere \\(\\sigma\\) is the standard deviation of the data. This formula emerges from theoretical analysis of the bias-variance tradeoff and serves as an effective initial bandwidth choice. While it performs well for many datasets, particularly those with approximately normal distributions, it may not be optimal for multimodal or heavily skewed distributions.\n\n\nOptimal \\(h\\) for Kernel Regression\nFor Nadaraya-Watson regression, several methods exist for selecting the bandwidth h. Cross-validation provides reliable results but can be computationally demanding. Theoretical analysis based on MISE minimization for common kernel functions suggests that the optimal bandwidth follows the relationship:\n\\[h^* \\propto n^{-1/5}.\\]\nThis expressiom highlights a fundamental characteristic of nonparametric methods: their convergence rate is slower than that of parametric methods. This represents one manifestation of the curse of dimensionality in nonparametric estimation. Furthermore, this relationship only specifies the rate at which the optimal bandwidth should decrease with sample size (proportionality to \\(n^{-1/5}\\)), rather than providing an exact value. Without the proportionality constant, its direct practical application is limited."
  },
  {
    "objectID": "blog/DONE-limits-nonparametric-models.html",
    "href": "blog/DONE-limits-nonparametric-models.html",
    "title": "The Limits of Nonparametric Models",
    "section": "",
    "text": "Nonparametric statistics offers a powerful toolkit for data analysis when the underlying data-generating process is too complex or unknown to be captured by parametric models. Unlike parametric methods, which assume a specific form for the underlying distribution or functional relationship, nonparametric approaches let the data ‚Äúspeak for itself.‚Äù\nIn earlier articles, we explored parametric and semiparametric models, where the focus was on attaining the lowest possible variance. However, in nonparametric statistics, this goal is unsuitable, as prioritizing minimal variance often introduces significant bias. The spotlight falls instead on selecting the optimal tuning parameter: the bandwidth. This article explains the various considerations and mathematical details associated with selecting the optimal bandwidth value in Kernel Density (KD) and Kernel Regression (KR) estimation."
  },
  {
    "objectID": "blog/DONE-limits-nonparametric-models.html#background",
    "href": "blog/DONE-limits-nonparametric-models.html#background",
    "title": "The Limits of Nonparametric Models",
    "section": "",
    "text": "Nonparametric statistics offers a powerful toolkit for data analysis when the underlying data-generating process is too complex or unknown to be captured by parametric models. Unlike parametric methods, which assume a specific form for the underlying distribution or functional relationship, nonparametric approaches let the data ‚Äúspeak for itself.‚Äù\nIn earlier articles, we explored parametric and semiparametric models, where the focus was on attaining the lowest possible variance. However, in nonparametric statistics, this goal is unsuitable, as prioritizing minimal variance often introduces significant bias. The spotlight falls instead on selecting the optimal tuning parameter: the bandwidth. This article explains the various considerations and mathematical details associated with selecting the optimal bandwidth value in Kernel Density (KD) and Kernel Regression (KR) estimation."
  },
  {
    "objectID": "blog/DONE-limits-nonparametric-models.html#notation",
    "href": "blog/DONE-limits-nonparametric-models.html#notation",
    "title": "The Limits of Nonparametric Models",
    "section": "Notation",
    "text": "Notation\nWe begin by establishing the mathematical terminology and notations essential for our analysis.\n\nKernels\nA kernel function \\(K(\\cdot)\\) is a non-negative, symmetric function that integrates to \\(1\\) over its domain. It acts as a weighting function that determines how observations contribute to the estimate at any given point, effectively spreading the mass of each data point over a local neighborhood.\n\n\nKernel Density\nKD is sort of like creating a smooth histogram. The kernel density estimator for a sample of n observations \\(X_1, X_2, \\dots, X_n \\in \\mathbb{R}\\) is given by:\n\\[\\hat{f}_h (x) = \\frac{1}{n h} \\sum_i K\\left(\\frac{x - X_i}{h}\\right),\\]\nwhere \\(h &gt; 0\\) is the bandwidth, a positive scalar controlling the smoothness of the estimate. Its function is to determine the window length around x in which the observations get positive weight.\nThe value \\(\\hat{f}_h (x)\\) will be large when there are many data points around \\(x\\), and small otherwise. Some popular kernel functions like Epanechnikov, Gaussian or triangular assign higher weights to observations closer to \\(x\\) and decaying importance to data further away.\nDensity estimation can also be performed in higher dimensions where the curse of dimensionality lurks in the background. In two dimensions, these estimates typically take the form of heat map-type figures.\n\n\nKernel Regression\nKernel regression, also known as local regression, is much like fitting a flexible smooth curve through data points. In this context, we have access to \\(n\\) observations of an outcome variable \\(Y\\). The objective is to estimate the conditional mean function at some point X=x:\n\\[m(x) = \\mathbb{E}[Y \\mid X = x].\\]\nAmong various kernel regression methods, the Nadaraya-Watson (NW) estimator is particularly popular. It is defined as:\n\\[\\hat{m}_h (x) = \\frac{\\sum_i K\\left(\\frac{x - X_i}{h}\\right) Y_i}{\\sum_i K\\left(\\frac{x - X_i}{h}\\right)}.\\]\nThe NW method fits a local constant around \\(x\\) equal to the average \\(Y\\) in that region (determined by the bandwidth). This approach is also known as local constant regression. More sophisticated variants include local linear and quadratic methods, which extend this fundamental idea but require more data. Across all these approaches, the bandwidth \\(h\\) serves as the key parameter that determines the trade-off between bias and variance, a relationship we will explore in detail below."
  },
  {
    "objectID": "blog/DONE-limits-nonparametric-models.html#a-closer-look",
    "href": "blog/DONE-limits-nonparametric-models.html#a-closer-look",
    "title": "The Limits of Nonparametric Models",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn practice, the choice of kernel is not as consequential. The primary focus of nonparametric statistics is selecting the optimal bandwidth \\(h\\).\n\nThe Bias-Variance Tradeoff\nAt the heart of bandwidth selection is the bias-variance tradeoff. Intuitively:\n\nSmall bandwidth (‚Äúoverfitting‚Äù): Captures fine details but also noise, leading to high variance and low bias.\nLarge bandwidth (‚Äúunderfitting‚Äù): Smoothes over the noise but can miss important structure, leading to high bias and low variance.\nFor nonparmetric curve estimation, the mean integrated squared error (MISE) is a common loss function for evaluating performance:\n\n\\[\\text{MISE}(h) = \\mathbb{E}\\left[ \\int \\left( \\hat{f}_h(x) - f(x) \\right)^2 dx \\right].\\]\nThis decomposes into bias and variance terms:\n\\[\\text{MISE}(h) =  \\int \\text{Bias}^2(\\hat{f}_h(x)) dx + \\int \\text{Var}(\\hat{f}_h(x)) dx.\\]\nMISE is analogous to the mean squared error (MSE) commonly used in machine learning, but integrated, or summed, across the support of the data. Minimizing the MISE yields the optimal bandwidth by balancing the bias-variance tradeoff, a fundamental concept in statistical estimation theory.\n\n\nOptimal \\(h\\) for Kernel Density\nSilverman‚Äôs rule of thumb provides a practical, closed-form approximation for \\(h\\) in KDE, assuming the data is roughly Gaussian. The expression is:\n\\[h^* = \\left( \\frac{4\\sigma^5}{3n} \\right) ^{\\frac{1}{5}}  = 1.06 \\sigma n^{-1/5},\\]\nwhere \\(\\sigma\\) is the standard deviation of the data. This formula emerges from theoretical analysis of the bias-variance tradeoff and serves as an effective initial bandwidth choice. While it performs well for many datasets, particularly those with approximately normal distributions, it may not be optimal for multimodal or heavily skewed distributions.\n\n\nOptimal \\(h\\) for Kernel Regression\nFor Nadaraya-Watson regression, several methods exist for selecting the bandwidth h. Cross-validation provides reliable results but can be computationally demanding. Theoretical analysis based on MISE minimization for common kernel functions suggests that the optimal bandwidth follows the relationship:\n\\[h^* \\propto n^{-1/5}.\\]\nThis expressiom highlights a fundamental characteristic of nonparametric methods: their convergence rate is slower than that of parametric methods. This represents one manifestation of the curse of dimensionality in nonparametric estimation. Furthermore, this relationship only specifies the rate at which the optimal bandwidth should decrease with sample size (proportionality to \\(n^{-1/5}\\)), rather than providing an exact value. Without the proportionality constant, its direct practical application is limited."
  },
  {
    "objectID": "blog/DONE-limits-nonparametric-models.html#an-example",
    "href": "blog/DONE-limits-nonparametric-models.html#an-example",
    "title": "The Limits of Nonparametric Models",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate bandwidth selection with a simple example.\nrequire(graphics)\nwith(cars, {\n    plot(speed, dist)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 2), col = 2)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 5), col = 3)\n    lines(ksmooth(speed, dist, \"normal\", bandwidth = 10), col = 4)\n})\nThis yields a simplified version of the following figure:\n\nThe green line corresponds to the Nadaraya-Watson regression with an optimal bandwidth and features some degree of curveture. The red line has a smaller bandwidth (higher variance, lower bias), and the blue line is undersmoothed (higher bandwidth)."
  },
  {
    "objectID": "blog/DONE-limits-nonparametric-models.html#bottom-line",
    "href": "blog/DONE-limits-nonparametric-models.html#bottom-line",
    "title": "The Limits of Nonparametric Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBandwidth selection is critical for nonparametric methods to balance bias and variance.\nSilverman‚Äôs rule of thumb offers a simple yet effective starting point for KD.\nFor commonly used second-order kernels, the optimal bandwidth in KR scales as \\(n^{-1/5}\\).\nPractical implementation often relies on cross-validation or plug-in methods and is limited by the curse of dimensionality."
  },
  {
    "objectID": "blog/DONE-limits-nonparametric-models.html#references",
    "href": "blog/DONE-limits-nonparametric-models.html#references",
    "title": "The Limits of Nonparametric Models",
    "section": "References",
    "text": "References\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.\nLi, Q., & Racine, J. S. (2023). Nonparametric econometrics: theory and practice. Princeton University Press.\nSilverman, B. W. (1986). Density Estimation for Statistics and Data Analysis. Wand, M. P., & Jones, M. C. (1995). Kernel Smoothing.\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media."
  },
  {
    "objectID": "blog/TODO-stratified-sampling-cont-var.html#a-closer-look",
    "href": "blog/TODO-stratified-sampling-cont-var.html#a-closer-look",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Traditional Method: Equal-Sized Binning\nThis approach involves dividing the continuous variable(s) into intervals or bins. For example, churn score, a single continuous variable \\(X\\), can be divided into quantiles (e.g., quartiles or deciles), ensuring each bin contains approximately the same number of observations/users.\nLet‚Äôs focus on the case of building ten equally-sized strata. Mathematically, for a continuous variable \\(X\\), the decile-based binning can be defined as:\n\\[\\text{Bin}_i = \\{x \\in X : Q_{10\\times (i-1)+1} \\leq x &lt; Q_{10\\times i}\\} \\text{ for } i \\in \\{1,\\dots,10\\}, \\]\nwhere \\(Q_k\\) represents the \\(k\\)-th quantile of \\(X\\). This approach splits in the first ten percentiles (i.e., minimum value to the \\(10\\)th percentile) into a single stratum, the next ten percentiles (\\(10\\)th to \\(20\\)th) into another stratum, and so on.\nWhen dealing with multiple variables, this method extends to either marginally stratify each variable or jointly stratify them. However, joint stratification across multiple variables can also fall prey to the curse of dimensionality.\n\n\nThe Modern Method: Unsupervised Clustering\nAn alternative approach uses unsupervised clustering algorithms, such as \\(k\\)-means or hierarchical clustering, to group observations into clusters, treating these clusters as strata. Unlike binning, clustering leverages the distribution of the data to form natural groupings.\nFormally, let \\(X\\) be a matrix of n observations across \\(p\\) continuous variables. One class of clustering algorithms aims to assign each observation \\(i\\) to one of \\(k\\) clusters:\n\\[\\text{minimize} \\quad \\sum_{j=1}^k \\sum_{i \\in \\mathcal{C}_j} \\text{distance}(X_i - \\mu_j), \\]\nwhere \\(\\mu_j=\\frac{1}{|S_j|}\\sum_{i\\in \\mathcal{C}_j} X\\)‚Äã is the centroid of cluster \\(\\mathcal{C}_j\\) of \\(size |S_j|\\).\nCommonly, \\(\\text{distance}(X_i, \\mu_j)=\\|X_i - \\mu_j\\|^2\\) which leads to \\(k\\)-means clustering. Unlike in the binning approach, here we are not restricting each strata to have the same number of observations."
  },
  {
    "objectID": "blog/DONE-stratified-sampling-cont-var.html",
    "href": "blog/DONE-stratified-sampling-cont-var.html",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "",
    "text": "Stratified sampling is a foundational technique in survey design, ensuring that observations capture key characteristics of a population. By dividing the data into distinct strata and sampling from each, stratified sampling often results in more efficient estimates than simple random sampling. Strata are typically defined by categorical variables such as classrooms, villages, or user types. This method is particularly advantageous when some strata are rare but carry critical information, as it ensures their representation in the sample. It is also often employed to tackle spillover effects or manage survey costs more effectively.\nWhile straightforward for categorical variables (like geographic region), continuous variables‚Äîsuch as income or churn score‚Äîpose greater challenges for stratified sampling. The primary issue lies in the curse of dimensionality: attempting to create strata across multiple continuous variables results in an explosion of possible combinations, making effective sampling impractical. For example, stratifying a population based on income at every possible dollar amount is absurd.\nIn this article, I present two solutions to the problem of stratified sampling with continuous variables."
  },
  {
    "objectID": "blog/DONE-stratified-sampling-cont-var.html#background",
    "href": "blog/DONE-stratified-sampling-cont-var.html#background",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "",
    "text": "Stratified sampling is a foundational technique in survey design, ensuring that observations capture key characteristics of a population. By dividing the data into distinct strata and sampling from each, stratified sampling often results in more efficient estimates than simple random sampling. Strata are typically defined by categorical variables such as classrooms, villages, or user types. This method is particularly advantageous when some strata are rare but carry critical information, as it ensures their representation in the sample. It is also often employed to tackle spillover effects or manage survey costs more effectively.\nWhile straightforward for categorical variables (like geographic region), continuous variables‚Äîsuch as income or churn score‚Äîpose greater challenges for stratified sampling. The primary issue lies in the curse of dimensionality: attempting to create strata across multiple continuous variables results in an explosion of possible combinations, making effective sampling impractical. For example, stratifying a population based on income at every possible dollar amount is absurd.\nIn this article, I present two solutions to the problem of stratified sampling with continuous variables."
  },
  {
    "objectID": "blog/DONE-stratified-sampling-cont-var.html#a-closer-look",
    "href": "blog/DONE-stratified-sampling-cont-var.html#a-closer-look",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Traditional Method: Equal-Sized Binning\nThis approach involves dividing the continuous variable(s) into intervals or bins. For example, churn score, a single continuous variable \\(X\\), can be divided into quantiles (e.g., quartiles or deciles), ensuring each bin contains approximately the same number of observations/users.\nLet‚Äôs focus on the case of building ten equally-sized strata. Mathematically, for a continuous variable \\(X\\), the decile-based binning can be defined as:\n\\[\\text{Bin}_i = \\{x \\in X : Q_{10\\times (i-1)+1} \\leq x &lt; Q_{10\\times i}\\} \\text{ for } i \\in \\{1,\\dots,10\\}, \\]\nwhere \\(Q_k\\) represents the \\(k\\)-th quantile of \\(X\\). This approach splits in the first ten percentiles (i.e., minimum value to the \\(10\\)th percentile) into a single stratum, the next ten percentiles (\\(10\\)th to \\(20\\)th) into another stratum, and so on.\nWhen dealing with multiple variables, this method extends to either marginally stratify each variable or jointly stratify them. However, joint stratification across multiple variables can also fall prey to the curse of dimensionality.\n\n\nThe Modern Method: Unsupervised Clustering\nAn alternative approach uses unsupervised clustering algorithms, such as \\(k\\)-means or hierarchical clustering, to group observations into clusters, treating these clusters as strata. Unlike binning, clustering leverages the distribution of the data to form natural groupings.\nFormally, let \\(X\\) be a matrix of n observations across \\(p\\) continuous variables. One class of clustering algorithms aims to assign each observation \\(i\\) to one of \\(k\\) clusters:\n\\[\\text{minimize} \\quad \\sum_{j=1}^k \\sum_{i \\in \\mathcal{C}_j} \\text{distance}(X_i - \\mu_j), \\]\nwhere \\(\\mu_j=\\frac{1}{|S_j|}\\sum_{i\\in \\mathcal{C}_j} X\\)‚Äã is the centroid of cluster \\(\\mathcal{C}_j\\) of \\(size |S_j|\\).\nCommonly, \\(\\text{distance}(X_i, \\mu_j)=\\|X_i - \\mu_j\\|^2\\) which leads to \\(k\\)-means clustering. Unlike in the binning approach, here we are not restricting each strata to have the same number of observations."
  },
  {
    "objectID": "blog/DONE-stratified-sampling-cont-var.html#pros-and-cons",
    "href": "blog/DONE-stratified-sampling-cont-var.html#pros-and-cons",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nUnsurprisingly, each method comes with its trade-offs. Traditional binning is simple and interpretable but can struggle with multivariate dependencies. Clustering accounts for multivariate relationships between variables, avoids imposing arbitrary bin thresholds, and may results in more natural groupings. However, it can be computationally expensive and sensitive to the choice of algorithm and hyperparameters (e.g., \\(k\\) in \\(k\\)-means).\nOne can also imagine a hybrid approach. Begin with a dimensionality reduction method like PCA and then perform binning on the first few principal components."
  },
  {
    "objectID": "blog/DONE-stratified-sampling-cont-var.html#an-example",
    "href": "blog/DONE-stratified-sampling-cont-var.html#an-example",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "An Example",
    "text": "An Example\nHere is R and python code illustrating both types of approaches on the popular iris dataset. We are interested in creating strata based on the SepalLenght variable. We begin with the traditional binning approach.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\ndata(iris)\n\n#Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris$SepalLengthBin &lt;- cut(iris$Sepal.Length, \n                           breaks = quantile(iris$Sepal.Length, probs = seq(0, 1, 0.25)), include.lowest = TRUE)\n\n#Inspect the resulting strata\ntable(iris$SepalLengthBin)\n\n# We split the dataset into four roughly equal parts based on SepalLenght values, each with about 38 observations.\n\n#Perform k-means clustering on two continuous variables\niris_cluster &lt;- kmeans(iris[, c(\"Sepal.Length\", \"Petal.Length\")], centers = 4)\n\n#Assign clusters as strata\niris$Cluster &lt;- as.factor(iris_cluster$cluster)\n\n#Inspect the resulting strata\ntable(iris$Cluster)\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Set seed for reproducibility\nnp.random.seed(1988)\n\n# Load the iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Divide the continuous variable \"Sepal.Length\" into 4 quantile bins\niris['SepalLengthBin'] = pd.qcut(iris['Sepal.Length'], q=4, labels=False)\n\n# Inspect the resulting strata\nprint(\"Quantile Bins (Sepal.Length):\")\nprint(iris['SepalLengthBin'].value_counts())\n\n# Perform k-means clustering on two continuous variables\nkmeans = KMeans(n_clusters=4, random_state=1988)\niris['Cluster'] = kmeans.fit_predict(iris[['Sepal.Length', 'Petal.Length']])\n\n# Assign clusters as strata\niris['Cluster'] = iris['Cluster'].astype('category')\n\n# Inspect the resulting strata\nprint(\"\\nCluster Sizes:\")\nprint(iris['Cluster'].value_counts())\n\n\n\nHere we also have four clusters, but their size ranges from \\(25\\) to \\(50\\) observations each."
  },
  {
    "objectID": "blog/DONE-stratified-sampling-cont-var.html#bottom-line",
    "href": "blog/DONE-stratified-sampling-cont-var.html#bottom-line",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nStratified sampling with continuous variables requires balancing simplicity and sophistication.\nTraditional binning remains a practical choice for single continuous variables or very few categorical ones.\nClustering provides a robust alternative, enabling stratification with multiple continuous variables at the cost of adding complexity and tuning parameters."
  },
  {
    "objectID": "blog/DONE-stein-paradox.html",
    "href": "blog/DONE-stein-paradox.html",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "In the realm of statistics, few findings are as counterintuitive and fascinating as Stein‚Äôs paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein‚Äôs paradox is not merely about memorizing an oddity‚Äîit‚Äôs about recognizing the delicate balance inherent in statistical decision-making.\nIn essence, Stein‚Äôs paradox asserts that when estimating the means of multiple variables simultaneously, it‚Äôs possible to achieve better results compared to relying solely on the sample averages.\nLet‚Äôs now delve deeper into this statement and unravel the underlying mechanisms."
  },
  {
    "objectID": "blog/DONE-stein-paradox.html#background",
    "href": "blog/DONE-stein-paradox.html#background",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "In the realm of statistics, few findings are as counterintuitive and fascinating as Stein‚Äôs paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein‚Äôs paradox is not merely about memorizing an oddity‚Äîit‚Äôs about recognizing the delicate balance inherent in statistical decision-making.\nIn essence, Stein‚Äôs paradox asserts that when estimating the means of multiple variables simultaneously, it‚Äôs possible to achieve better results compared to relying solely on the sample averages.\nLet‚Äôs now delve deeper into this statement and unravel the underlying mechanisms."
  },
  {
    "objectID": "blog/DONE-stein-paradox.html#a-closer-look",
    "href": "blog/DONE-stein-paradox.html#a-closer-look",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Mean Squared Error (MSE)\nTo understand the paradox, let‚Äôs begin by quantify what we mean by ‚Äúbetter‚Äù estimation. The MSE of an estimator \\(\\hat{\\mu}\\) is defined as its expected squared error (or loss):\n\\[\\text{MSE}(\\hat{\\mu}) = \\mathbb{E}\\left[ ( \\hat{\\mu} - \\mu )^2 \\right],\\]\nwhere \\(\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_p)\\) is the true vector of means. All else equal, the lower MSE the better.\n\n\nMathematical Formulation\nStein‚Äôs paradox arises in the context of estimating multiple parameters simultaneously. Suppose you‚Äôre estimating the mean \\(\\mu_i\\) of several independent normal distributions:\n\\[X_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, p,\\]\nwhere \\(X_i\\) are observed values, \\(\\mu_i\\) are the unknown means, and \\(\\sigma^2\\) is known. We assume non-zero covariance between the variables.\nA natural approach is to estimate each _i using its corresponding sample mean \\(X_i\\), which is the maximum likelihood estimator (MLE). However, in dimensions \\(p \\geq 3\\), this seemingly reasonable approach is dominated by an alternative method.\nThe surprise? Shrinking the individual estimates toward a common value‚Äîsuch as the overall mean‚Äîproduces an estimator with uniformly lower expected squared error.\nThe MSE of the MLE is equal to:\n\\[R(\\hat{\\mu}_\\text{MLE}) = p\\sigma^2.\\]\nNow consider the biased(!) James-Stein (JS) estimator:\n\\[\\hat{\\mu}_\\text{JS} = \\left( 1 - \\frac{(p - 2)\\sigma^2}{\\lVert X \\rVert^2} \\right) X,\\]\nwhere \\(\\lVert X \\rVert^2 = \\sum_i X_i^2\\) is the squared norm of the observed data. The shrinkage factor \\(1 - \\frac{(p - 2)\\sigma^2}{|X|^2}\\) pulls the estimates toward zero (or any other pre-specified point).\nRemarkably, the James-Stein estimator has lower MSE than the MLE for \\(p \\geq 3\\):\n\\[\\text{MSE}(\\hat{\\mu}_{\\text{JS}}) &lt; \\text{MSE}(\\hat{\\mu}_{\\text{MLE}}).\\]\nThis is the mystery at its core.\n\n\nExplanation\nThis result holds because the James-Stein estimator balances variance reduction and bias introduction in a way that minimizes overall MSE. The MLE, in contrast, does not account for the possibility of shared structure among the parameters. The counterintuitive nature of this result stems from the independence of the \\(X_i\\)‚Äôs. Intuition suggests that pooling information across independent observations should not improve estimation, yet the James-Stein estimator demonstrates otherwise."
  },
  {
    "objectID": "blog/DONE-stein-paradox.html#an-example",
    "href": "blog/DONE-stein-paradox.html#an-example",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs emulate this paradox in R and python in a setting with \\(p=5\\).\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\np &lt;- 5  # Number of means\nn &lt;- 1000  # Number of simulations\nsigma &lt;- 1\nmu &lt;- rnorm(p, mean = 5, sd = 2)  # True means\n\n# results storage\nmse_mle &lt;- numeric(n)  \nmse_js &lt;- numeric(n)\n\n# create a fake dataset and compute the MSEs of both the MLE and JS estimator. \n# repeat this 1,000 times and take the average loss for each estimator.\nfor (sim in 1:n) {\n  # Simulate observations\n  X &lt;- rnorm(p, mean = mu, sd = sigma)\n\n  # MLE estimator and its MSE\n  mle &lt;- X\n  mse_mle[sim] &lt;- sum((mle - mu)^2)\n\n  # James-Stein estimator and its MSE\n  shrinkage &lt;- max(0, 1 - ((p - 2) * sigma^2) / sum(X^2))\n  js &lt;- shrinkage * X\n  mse_js[sim] &lt;- sum((js - mu)^2)\n}\n\n# print the results.\ncat(\"Average MSE of MLE:\", mean(mse_mle), \"\\n\")\n&gt; Average MSE of MLE: 5.125081 \ncat(\"Average MSE of James-Stein:\", mean(mse_js), \"\\n\")\n&gt; Average MSE of James-Stein: 5.055019 \n\n\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(1988)\n\n# Parameters\np = 5  # Number of means\nn = 1000  # Number of simulations\nsigma = 1\nmu = np.random.normal(loc=5, scale=2, size=p)  # True means\n\n# Results storage\nmse_mle = np.zeros(n)\nmse_js = np.zeros(n)\n\n# Simulate data and compute MSEs for MLE and James-Stein estimator\nfor sim in range(n):\n    # Simulate observations\n    X = np.random.normal(loc=mu, scale=sigma, size=p)\n\n    # MLE estimator and its MSE\n    mle = X\n    mse_mle[sim] = np.sum((mle - mu) ** 2)\n\n    # James-Stein estimator and its MSE\n    shrinkage = max(0, 1 - ((p - 2) * sigma**2) / np.sum(X**2))\n    js = shrinkage * X\n    mse_js[sim] = np.sum((js - mu) ** 2)\n\n# Print the results\nprint(\"Average MSE of MLE:\", np.mean(mse_mle))\nprint(\"Average MSE of James-Stein:\", np.mean(mse_js))\n\n\n\nIn this example, average MSE of the James-Stein estimator (\\(5.06\\)) is consistently lower than that of the MLE (\\(5.13\\)), illustrating the paradox in action."
  },
  {
    "objectID": "blog/DONE-stein-paradox.html#bottom-line",
    "href": "blog/DONE-stein-paradox.html#bottom-line",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nStein‚Äôs paradox shows that shrinkage estimators can outperform the MLE in dimensions \\(p \\geq 3\\), even when the underlying variables are independent.\nThe James-Stein estimator achieves lower MSE by balancing variance reduction and bias introduction.\nUnderstanding this result highlights the power of shrinkage techniques in high-dimensional statistics."
  },
  {
    "objectID": "blog/DONE-stein-paradox.html#references",
    "href": "blog/DONE-stein-paradox.html#references",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "References",
    "text": "References\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press."
  },
  {
    "objectID": "blog/TODO-mutual-information.html#a-closer-look",
    "href": "blog/TODO-mutual-information.html#a-closer-look",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Entropy\nMutual Information is related to the notion of entropy, a measure of uncertainty or randomness in a random variable. In less formal terms, entropy quantifies how ‚Äúsurprising‚Äù or ‚Äúunpredictable‚Äù the outcomes of \\(X\\) are.\nFormally, for a discrete random variable \\(X\\) with possible outcomes \\(x_1, x_2, \\dots, x_n\\) and associated probabilities \\(p(x_1), p(x_2), \\dots, p(x_n)\\), entropy \\(H(X)\\) is defined as:\n\\[H(X) = -\\sum_{i=1}^n p(x_i) \\log p(x_i).\\]\nFor continuous variables we switch the summation with an integral.\nEntropy equals \\(0\\) when there‚Äôs no uncertainty, such as when \\(X\\) always takes a single outcome. High entropy means greater uncertainty (many possible outcomes, all equally likely), while low entropy indicates less uncertainty, as some outcomes are much more likely than others. In essence, entropy tells us how much ‚Äúinformation‚Äù is gained on average when observing the variable‚Äôs realization.\n\n\nMathematical Definitions of MI\nMI can be defined in mulitple ways. Perhaps the most intiuitive definition of MI between two random variables \\(X\\) and \\(Y\\) is that it measures the difference between the joint distribution and the product of their marginals. If two random variables are independent, their joint distribution is the product of their marginals \\(p(x,y)=p(x)p(y)\\). In some sense, the discrepancy between these two objects (i.e., the two sides of the equality) measures the strength of association between \\(X\\) and \\(Y\\) (i.e., their ‚Äúindependence‚Äù).\nFormally, MI is the Kullback-Leibler divergence between the joint distribution and the product of the two marginals:\n\\[ MI(X,Y) = D_{KL}(p(X,Y) || p(X)p(Y). \\]\nLet‚Äôs now examine MI from a different angle. We can express mutual information as follows:\n\\[ MI(X,Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\left(\\frac{p(x,y)}{p(x)p(y)}\\right)\\]\nAgain, for continuous variables, the sums become integrals.\nAlternatively, MI can also be expressed in terms of entropy. It equals the sum of the entropy of \\(X\\) and \\(Y\\) taking away their joint entropy:\n\\[ \\begin{align*}MI(X,Y) & = H(X) - H(X|Y) \\\\ & = H(Y) - H(Y|X) \\\\ & = H(X) + H(Y) - H(X,Y).\\end{align*} \\]\nYou can compute MI in R with the infotheo package.\nSoftware Package: infotheo.\n\n\nProperties\nMI has several intriguing properties:\n\nNon-negativity: \\(MI(X;Y) \\geq 0\\). Mutual Information is always non-negative, as it measures the amount of information one variable provides about the other. Higher values correspond to stronger association.\nSymmetry: \\(MI(X,Y) = MI(Y,X)\\). This symmetry implies that the information \\(X\\) provides about \\(Y\\) is the same as what Y provides about X.\nIndependence: Similarly to Chatterjee‚Äôs correlation coefficient, \\(MI(X,Y) = 0\\) if and only if \\(X\\) and \\(Y\\) are independent.\nScale-invarance: Mutual information is scale invariant. If you apply a scaling transformation to the variables, their MI will not be affected.\n\n\n\nConditional MI\nConditional Mutual Information (CMI) extends the concept of MI to measure dependency between \\(X\\) and \\(Y\\) given a third variable \\(Z\\). CMI is useful for investigating how much information \\(X\\) and \\(Y\\) share independently of \\(Z\\). It is defined as:\n\\[MI(X,Y|Z) = \\sum_{x,y,z} p(x,y,z) \\log \\left(\\frac{p(x,y|z)}{p(x|z)p(y|z)}\\right). \\]\nThis can be valuable in causal inference, where understanding dependencies conditioned on specific variables aids in interpreting relationships within complex models. CMI is also particularly useful for feature selection when accounting for redundancy among already included features. Let‚Äôs explore this idea in greater detail.\n\n\nFeature Selection with MI\nIn machine learning, MI serves as a useful metric for feature selection (Brown et al.¬†2012, Vergara and Est√©vez 2014). Consider an outcome variable \\(Y\\) and a set of features \\(X\\in\\mathbb{R}^p\\) with n i.i.d. observations. By evaluating the MI between each feature and the target variable, one can retain features with the highest information content, passing a certain threshold. This is somewhat primitive since it assumes independence across features. More sophisticated approaches take feature dependency into acount.\nFor instance, methods like Minimum Redundancy Maximum Relevance (mRMR, Peng et al.¬†2005) aim to maximize the relevance of features to the target while minimizing redundancy among features. Here is a concise version of the mRMR algorithm:\n\nCalculate MI between each feature and the target (relevance).\nCalculate MI between features (redundancy).\nSelect features that maximize relevance while minimizing redundancy: \\[ \\text{mRMR} = \\max_{X_i} \\left[MI(X_i;Y) - \\frac{1}{|S|} \\sum_{X_j \\in S} MI(X_i;X_j)\\right], \\]\n\nwhere \\(S\\) is the set of already selected features.\n\n\nPros and Cons\nLike any other statistical tool, mutual Information has several advantages and limitations. On the positive side, it captures both linear and nonlinear relationships, is scale-invariant, and works with both continuous and discrete variables, making it a theoretically well-founded measure. However, it requires density estimation for continuous variables, can be computationally intensive for large datasets, and its results can be sensitive to binning choices for continuous variables. Additionally, there is no standard normalization for MI."
  },
  {
    "objectID": "blog/DONE-mutual-information.html",
    "href": "blog/DONE-mutual-information.html",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "",
    "text": "When exploring dependencies between variables, the data scientist‚Äôs toolbox often relies on correlation measures to reveal relationships and potential patterns. But what if we‚Äôre looking to capture relationships beyond linear correlations? Mutual Information (MI) quantifies the ‚Äúamount of information‚Äù shared between variables in a more general sense. It measures how much we know about \\(Y\\) by observing \\(X\\). This approach goes beyond linear patterns and can help us uncover more complex relationships within data.\nMI can be especially helpful for applications such as feature selection and unsupervised learning. For readers familiar with various types of correlation metrics, Mutual Information provides an additional lens to interpret relationships within data.\nIn this article, I‚Äôll guide you through the concept of Mutual Information, its definition, properties, and usage, as well as comparisons with other dependency measures. We‚Äôll explore MI‚Äôs mathematical foundations, its advantages and limitations, and its applications, concluding with an example demonstrating MI‚Äôs behavior alongside more traditional measures like Pearson‚Äôs correlation."
  },
  {
    "objectID": "blog/DONE-mutual-information.html#introduction",
    "href": "blog/DONE-mutual-information.html#introduction",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "",
    "text": "When exploring dependencies between variables, the data scientist‚Äôs toolbox often relies on correlation measures to reveal relationships and potential patterns. But what if we‚Äôre looking to capture relationships beyond linear correlations? Mutual Information (MI) quantifies the ‚Äúamount of information‚Äù shared between variables in a more general sense. It measures how much we know about \\(Y\\) by observing \\(X\\). This approach goes beyond linear patterns and can help us uncover more complex relationships within data.\nMI can be especially helpful for applications such as feature selection and unsupervised learning. For readers familiar with various types of correlation metrics, Mutual Information provides an additional lens to interpret relationships within data.\nIn this article, I‚Äôll guide you through the concept of Mutual Information, its definition, properties, and usage, as well as comparisons with other dependency measures. We‚Äôll explore MI‚Äôs mathematical foundations, its advantages and limitations, and its applications, concluding with an example demonstrating MI‚Äôs behavior alongside more traditional measures like Pearson‚Äôs correlation."
  },
  {
    "objectID": "blog/DONE-mutual-information.html#notation",
    "href": "blog/DONE-mutual-information.html#notation",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Notation",
    "text": "Notation\nBefore diving deeper, let‚Äôs establish our notation:\n\nRandom variables will be denoted by capital letters (\\(X\\), \\(Y\\)).\nLowercase letters (\\(x\\),\\(y\\)) represent specific values of these variables.\n\\(p(x)\\) denotes the probability mass/density function of \\(X\\).\n\\(p(x,y)\\) represents the joint probability mass/density function of \\(X\\) and \\(Y\\).\n\\(p(x\\mid y)\\) is the conditional probability of \\(X\\) given \\(Y\\).\n\\(H(X)\\) represents the entropy of random variable \\(X\\).\n\nThese should not surprise anyone, as they are standard conventions in most statistics textbooks."
  },
  {
    "objectID": "blog/DONE-mutual-information.html#a-closer-look",
    "href": "blog/DONE-mutual-information.html#a-closer-look",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Entropy\nMutual Information is related to the notion of entropy, a measure of uncertainty or randomness in a random variable. In less formal terms, entropy quantifies how ‚Äúsurprising‚Äù or ‚Äúunpredictable‚Äù the outcomes of \\(X\\) are.\nFormally, for a discrete random variable \\(X\\) with possible outcomes \\(x_1, x_2, \\dots, x_n\\) and associated probabilities \\(p(x_1), p(x_2), \\dots, p(x_n)\\), entropy \\(H(X)\\) is defined as:\n\\[H(X) = -\\sum_{i=1}^n p(x_i) \\log p(x_i).\\]\nFor continuous variables we switch the summation with an integral.\nEntropy equals \\(0\\) when there‚Äôs no uncertainty, such as when \\(X\\) always takes a single outcome. High entropy means greater uncertainty (many possible outcomes, all equally likely), while low entropy indicates less uncertainty, as some outcomes are much more likely than others. In essence, entropy tells us how much ‚Äúinformation‚Äù is gained on average when observing the variable‚Äôs realization.\n\n\nMathematical Definitions of MI\nMI can be defined in mulitple ways. Perhaps the most intiuitive definition of MI between two random variables \\(X\\) and \\(Y\\) is that it measures the difference between the joint distribution and the product of their marginals. If two random variables are independent, their joint distribution is the product of their marginals \\(p(x,y)=p(x)p(y)\\). In some sense, the discrepancy between these two objects (i.e., the two sides of the equality) measures the strength of association between \\(X\\) and \\(Y\\) (i.e., their ‚Äúindependence‚Äù).\nFormally, MI is the Kullback-Leibler divergence between the joint distribution and the product of the two marginals:\n\\[ MI(X,Y) = D_{KL}(p(X,Y) || p(X)p(Y). \\]\nLet‚Äôs now examine MI from a different angle. We can express mutual information as follows:\n\\[ MI(X,Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\left(\\frac{p(x,y)}{p(x)p(y)}\\right)\\]\nAgain, for continuous variables, the sums become integrals.\nAlternatively, MI can also be expressed in terms of entropy. It equals the sum of the entropy of \\(X\\) and \\(Y\\) taking away their joint entropy:\n\\[ \\begin{align*}MI(X,Y) & = H(X) - H(X|Y) \\\\ & = H(Y) - H(Y|X) \\\\ & = H(X) + H(Y) - H(X,Y).\\end{align*} \\]\nYou can compute MI in R with the infotheo package.\nSoftware Package: infotheo.\n\n\nProperties\nMI has several intriguing properties:\n\nNon-negativity: \\(MI(X;Y) \\geq 0\\). Mutual Information is always non-negative, as it measures the amount of information one variable provides about the other. Higher values correspond to stronger association.\nSymmetry: \\(MI(X,Y) = MI(Y,X)\\). This symmetry implies that the information \\(X\\) provides about \\(Y\\) is the same as what Y provides about X.\nIndependence: Similarly to Chatterjee‚Äôs correlation coefficient, \\(MI(X,Y) = 0\\) if and only if \\(X\\) and \\(Y\\) are independent.\nScale-invarance: Mutual information is scale invariant. If you apply a scaling transformation to the variables, their MI will not be affected.\n\n\n\nConditional MI\nConditional Mutual Information (CMI) extends the concept of MI to measure dependency between \\(X\\) and \\(Y\\) given a third variable \\(Z\\). CMI is useful for investigating how much information \\(X\\) and \\(Y\\) share independently of \\(Z\\). It is defined as:\n\\[MI(X,Y|Z) = \\sum_{x,y,z} p(x,y,z) \\log \\left(\\frac{p(x,y|z)}{p(x|z)p(y|z)}\\right). \\]\nThis can be valuable in causal inference, where understanding dependencies conditioned on specific variables aids in interpreting relationships within complex models. CMI is also particularly useful for feature selection when accounting for redundancy among already included features. Let‚Äôs explore this idea in greater detail.\n\n\nFeature Selection with MI\nIn machine learning, MI serves as a useful metric for feature selection (Brown et al.¬†2012, Vergara and Est√©vez 2014). Consider an outcome variable \\(Y\\) and a set of features \\(X\\in\\mathbb{R}^p\\) with n i.i.d. observations. By evaluating the MI between each feature and the target variable, one can retain features with the highest information content, passing a certain threshold. This is somewhat primitive since it assumes independence across features. More sophisticated approaches take feature dependency into acount.\nFor instance, methods like Minimum Redundancy Maximum Relevance (mRMR, Peng et al.¬†2005) aim to maximize the relevance of features to the target while minimizing redundancy among features. Here is a concise version of the mRMR algorithm:\n\nCalculate MI between each feature and the target (relevance).\nCalculate MI between features (redundancy).\nSelect features that maximize relevance while minimizing redundancy: \\[ \\text{mRMR} = \\max_{X_i} \\left[MI(X_i;Y) - \\frac{1}{|S|} \\sum_{X_j \\in S} MI(X_i;X_j)\\right], \\]\n\nwhere \\(S\\) is the set of already selected features.\n\n\nPros and Cons\nLike any other statistical tool, mutual Information has several advantages and limitations. On the positive side, it captures both linear and nonlinear relationships, is scale-invariant, and works with both continuous and discrete variables, making it a theoretically well-founded measure. However, it requires density estimation for continuous variables, can be computationally intensive for large datasets, and its results can be sensitive to binning choices for continuous variables. Additionally, there is no standard normalization for MI."
  },
  {
    "objectID": "blog/DONE-mutual-information.html#an-example",
    "href": "blog/DONE-mutual-information.html#an-example",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs implement MI calculation in R and python and compare it with traditional correlation measures using the iris dataset.\n\nRPython\n\n\nrm(list=ls())\nlibrary(infotheo)\nlibrary(corrplot)\nlibrary(dplyr)\ndata(iris)\n\n# Calculate mutual information matrix\nmi_matrix &lt;- mutinformation(discretize(iris[,1:4]))\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(iris[,1:4])\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value &lt;- mi_matrix[1,3]\ncor_value &lt;- cor_matrix[1,3]\n\n# Print results\nprint(paste(\"Mutual Information:\", round(mi_value, 3)))\nprint(paste(\"Pearson Correlation:\", round(cor_value, 3)))\n\n[1] \"Mutual Information: 0.585\"\n[1] \"Pearson Correlation: 0.872\"\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.metrics import mutual_info_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load dataset\niris = sns.load_dataset('iris')\n\n# Discretize the dataset (except the target variable)\nX = iris.iloc[:, :-1]\nest = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\nX_discretized = est.fit_transform(X)\n\n# Calculate mutual information matrix\nmi_matrix = np.zeros((X.shape[1], X.shape[1]))\nfor i in range(X.shape[1]):\n    for j in range(X.shape[1]):\n        mi_matrix[i, j] = mutual_info_score(X_discretized[:, i], X_discretized[:, j])\n\n# Calculate correlation matrix\ncor_matrix = X.corr()\n\n# Compare MI vs Correlation for Sepal.Length and Petal.Length\nmi_value = mi_matrix[0, 2]  # Sepal.Length vs Petal.Length\ncor_value = cor_matrix.iloc[0, 2]  # Sepal.Length vs Petal.Length\n\n# Print results\nprint(f\"Mutual Information: {mi_value:.3f}\")\nprint(f\"Pearson Correlation: {cor_value:.3f}\")\n\n\n\n\nThe left matrix displays the MI results and the right one shows the standard (Pearson) correlation values. The default scales are different, so one should compare the values and not the colors. Indeed, the variable pairs with negative linear correlation also have the lowest MI values.\nThis example demonstrates how MI can capture nonlinear relationships that might be missed by traditional correlation measures."
  },
  {
    "objectID": "blog/DONE-mutual-information.html#bottom-line",
    "href": "blog/DONE-mutual-information.html#bottom-line",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMutual Information provides a comprehensive measure of statistical dependence, capturing both linear and nonlinear relationships.\nUnlike correlation coefficients, MI works naturally with both continuous and categorical variables.\nMI serves as the foundation for sophisticated feature selection algorithms like mRMR."
  },
  {
    "objectID": "blog/DONE-mutual-information.html#where-to-learn-more",
    "href": "blog/DONE-mutual-information.html#where-to-learn-more",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great place to start and learng the basics. Brown et al.¬†(2012) and Vergara and Est√©vez (2014) are the go-to resources for conditional MI and using MI for feature selection."
  },
  {
    "objectID": "blog/DONE-mutual-information.html#references",
    "href": "blog/DONE-mutual-information.html#references",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "References",
    "text": "References\nBrown, G., Pocock, A., Zhao, M. J., & Luj√°n, M. (2012). Conditional likelihood maximisation: a unifying framework for information theoretic feature selection. JMLR.\nCover, T. M., & Thomas, J. A. (2006). Elements of information theory (2nd ed.). Wiley-Interscience.\nKraskov, A., St√∂gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical Review E, 69(6).\nPeng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE TPAMI.\nRoss, B. C. (2014). Mutual information between discrete and continuous data sets. PloS one, 9(2).\nVergara, J. R., & Est√©vez, P. A. (2014). A review of feature selection methods based on mutual information. Neural Computing and Applications, 24(1)."
  },
  {
    "objectID": "blog/TODO-two-types-weights-causality.html#a-closer-look",
    "href": "blog/TODO-two-types-weights-causality.html#a-closer-look",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nInverse Propensity Score Weights\nInverse propensity score (IPS) weights rely on the estimated propensity score (X). The weights for treated and untreated groups are defined as:\n\\[ \\gamma_{\\text{IPS}}(X) = \\begin{cases} 1 / \\hat{e}(X) & \\text{if } W = 1, \\\\ 1 / (1 - \\hat{e}(X)) & \\text{if } W = 0. \\end{cases} \\]\nIPS weights intuitively correct for the unequal probability of treatment assignment. If an individual has a low probability of treatment \\(e(X)\\approx 0\\) but is treated, their weight is large, amplifying their influence in the analysis. Conversely, individuals with high treatment probabilities are downweighted to prevent overrepresentation.\nA few notes. First, these weights adjust the observed data such that the distribution of covariates in the weighted treated and control groups resembles that of the overall population. This helps mitigate selection bias, ensuring that comparisons between treated and control groups reflect a treatment effect rather than confounded differences in covariates (e.g., the control group is older).\nA second key property is that the weighted average of the outcomes for the treated group is an unbiased estimator for the mean outcome under treatment, \\(\\mu_1\\). This can be expressed mathematically as:\n\\[ \\hat{\\mu}_1(X)=\\frac{\\sum_i W_i Y_i/ \\hat{e}(X)}{ \\sum_i W_i / \\hat{e}(X)}=\\frac{1}{n_T}\\sum_i  \\gamma_{\\text{IPS}}(X_i) W_i Y_i. \\]\nThis equality is particularly useful in the next step, when estimating the treatment effect \\(\\tau\\).\nThird, these IPS weights are generally unknown and have to be estimated from the data. A leading exception is controlled randomized experiments where the researcher determines the probability of treatment. This is, however, uncommon. Traditionally, practitioners rely on methods like logistic regression to first obtain \\(\\hat{e}(X)\\) and then take its reciprocal to construct the weights.\nUsing IPS weights entails two primary challenges. When \\(\\hat{e}(X)\\) is close to \\(0\\) or \\(1\\), weights can become excessively large, leading to high variance in estimates. Practitioners then turn to trim observations with ‚Äútoo large‚Äù or ‚Äútoo small‚Äù \\(\\hat{e}(X)\\) values. Moreover, model misspecification in \\(\\hat{e}(X)\\) can lead to poor covariate balance, introducing bias. More flexible methods such as machin learning models can alleviate this problem.\nSoftware Packages: MatchIt, Matching.\n\n\nCovariate Balancing Weights\nAn alternative approach seeks weights that directly balance the dataset at hand. Typically this is framed as a constrained optimization problem with placing restrictions on the maximum imbalance in X between the treatment and control groups.\nThe weights \\(\\gamma_{\\text{CB}}(X)\\) are obtained by solving:\n\\[ \\min_{\\gamma} \\text{Imbalance}(\\gamma) + \\lambda \\text{Penalty}(\\gamma),\\]\nwhere ‚ÄúImbalance‚Äù measures covariate discrepancies between groups, and ‚ÄúPenalty‚Äù controls for extreme weights. A common formulation balances means:\n\\[ \\frac{1}{n} \\sum_{i=1}^n W_i \\gamma(X_i) f(X_i) \\approx \\frac{1}{n} \\sum_{i=1}^n f(X_i), \\]\nwith \\(f(x_i)=x\\). The same idea can be applied to higher moments of \\(X\\) like variance or skewness. Examples methods relying on this type of weights include include Hainmueller (2012), Chan et al.¬†(2016), and Athey et al.¬†(2018), among many others.\nCovariate balancing weights bypass the need to explicitly estimate \\(e(X)\\). Instead, they solve for weights that ensure the treated and control groups are balanced across predefined covariates or their transformations. This method aligns closely with the goal of causal inference: achieving balance. The main challenge is selecting the right set of covariates to be balanced. Variance estimation can also be more involved, although modern statistical packages take care of it.\nSoftware Packages: MatchIt, Matching."
  },
  {
    "objectID": "blog/DONE-two-types-weights-causality.html",
    "href": "blog/DONE-two-types-weights-causality.html",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "",
    "text": "Causal inference fundamentally seeks to answer: What is the effect of a treatment or intervention? The challenge lies in ensuring that the comparison groups‚Äîtreated versus untreated‚Äîare balanced in terms of their characteristics, so differences in outcomes can be attributed solely to the intervention. This idea, often referred to as ‚Äúlike-with-like‚Äù or ‚Äúapples-to-apples‚Äù comparison, is as simple as it is intuitive.\nBalance is thus fundamental to causal inference. Weighting methods, central to achieving this balance, have evolved to include two primary types: inverse propensity score weights and covariate balancing weights. This article briefly describes these weights, their mathematical foundations, and the intuition behind them, with an example in R to bring it all together."
  },
  {
    "objectID": "blog/DONE-two-types-weights-causality.html#background",
    "href": "blog/DONE-two-types-weights-causality.html#background",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "",
    "text": "Causal inference fundamentally seeks to answer: What is the effect of a treatment or intervention? The challenge lies in ensuring that the comparison groups‚Äîtreated versus untreated‚Äîare balanced in terms of their characteristics, so differences in outcomes can be attributed solely to the intervention. This idea, often referred to as ‚Äúlike-with-like‚Äù or ‚Äúapples-to-apples‚Äù comparison, is as simple as it is intuitive.\nBalance is thus fundamental to causal inference. Weighting methods, central to achieving this balance, have evolved to include two primary types: inverse propensity score weights and covariate balancing weights. This article briefly describes these weights, their mathematical foundations, and the intuition behind them, with an example in R to bring it all together."
  },
  {
    "objectID": "blog/DONE-two-types-weights-causality.html#notation",
    "href": "blog/DONE-two-types-weights-causality.html#notation",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Notation",
    "text": "Notation\nTo set the stage, we begin by establishing the potential outcome framework and laying out the notation for the discussion:\n\n\\(Y(0), Y(1)\\): Potential outcomes under treatment and control.\n\\(W\\): Treatment indicator (\\(1\\) for treated, \\(0\\) for untreated).\n\\(X\\): Vector of covariates (i.e., control variables).\n\\(e(X)=P(W=1 \\mid X)\\): Propensity score, the probability of receiving treatment given covariates.\n\\(\\mu_1‚Äã=E[Y(1)]\\): Mean outcome under treatment.\n\\(\\tau=\\mu_1 - \\mu_0\\): Average treatment effect (ATE), the main object of interest.\n\\(n\\): number of observations in the sample.\n\\(n_T\\): number of observations in the treatment group.\n\nWe observe a random sample of size \\(n\\), of \\(\\{Y_i, W_i, X_i \\}\\), where \\(i\\) indexes units (e.g., individuals, firms, schools etc.). Under the assumptions of strong ignorability‚Äîunconfoundedness \\(W \\perp (Y(0), Y(1)) \\mid X\\) and overlap \\((0&lt;e(X)&lt;1)\\) ‚Äî the ATE can be identified and estimated."
  },
  {
    "objectID": "blog/DONE-two-types-weights-causality.html#a-closer-look",
    "href": "blog/DONE-two-types-weights-causality.html#a-closer-look",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nInverse Propensity Score Weights\nInverse propensity score (IPS) weights rely on the estimated propensity score (X). The weights for treated and untreated groups are defined as:\n\\[ \\gamma_{\\text{IPS}}(X) = \\begin{cases} 1 / \\hat{e}(X) & \\text{if } W = 1, \\\\ 1 / (1 - \\hat{e}(X)) & \\text{if } W = 0. \\end{cases} \\]\nIPS weights intuitively correct for the unequal probability of treatment assignment. If an individual has a low probability of treatment \\(e(X)\\approx 0\\) but is treated, their weight is large, amplifying their influence in the analysis. Conversely, individuals with high treatment probabilities are downweighted to prevent overrepresentation.\nA few notes. First, these weights adjust the observed data such that the distribution of covariates in the weighted treated and control groups resembles that of the overall population. This helps mitigate selection bias, ensuring that comparisons between treated and control groups reflect a treatment effect rather than confounded differences in covariates (e.g., the control group is older).\nA second key property is that the weighted average of the outcomes for the treated group is an unbiased estimator for the mean outcome under treatment, \\(\\mu_1\\). This can be expressed mathematically as:\n\\[ \\hat{\\mu}_1(X)=\\frac{\\sum_i W_i Y_i/ \\hat{e}(X)}{ \\sum_i W_i / \\hat{e}(X)}=\\frac{1}{n_T}\\sum_i  \\gamma_{\\text{IPS}}(X_i) W_i Y_i. \\]\nThis equality is particularly useful in the next step, when estimating the treatment effect \\(\\tau\\).\nThird, these IPS weights are generally unknown and have to be estimated from the data. A leading exception is controlled randomized experiments where the researcher determines the probability of treatment. This is, however, uncommon. Traditionally, practitioners rely on methods like logistic regression to first obtain \\(\\hat{e}(X)\\) and then take its reciprocal to construct the weights.\nUsing IPS weights entails two primary challenges. When \\(\\hat{e}(X)\\) is close to \\(0\\) or \\(1\\), weights can become excessively large, leading to high variance in estimates. Practitioners then turn to trim observations with ‚Äútoo large‚Äù or ‚Äútoo small‚Äù \\(\\hat{e}(X)\\) values. Moreover, model misspecification in \\(\\hat{e}(X)\\) can lead to poor covariate balance, introducing bias. More flexible methods such as machin learning models can alleviate this problem.\nSoftware Packages: MatchIt, Matching.\n\n\nCovariate Balancing Weights\nAn alternative approach seeks weights that directly balance the dataset at hand. Typically this is framed as a constrained optimization problem with placing restrictions on the maximum imbalance in X between the treatment and control groups.\nThe weights \\(\\gamma_{\\text{CB}}(X)\\) are obtained by solving:\n\\[ \\min_{\\gamma} \\text{Imbalance}(\\gamma) + \\lambda \\text{Penalty}(\\gamma),\\]\nwhere ‚ÄúImbalance‚Äù measures covariate discrepancies between groups, and ‚ÄúPenalty‚Äù controls for extreme weights. A common formulation balances means:\n\\[ \\frac{1}{n} \\sum_{i=1}^n W_i \\gamma(X_i) f(X_i) \\approx \\frac{1}{n} \\sum_{i=1}^n f(X_i), \\]\nwith \\(f(x_i)=x\\). The same idea can be applied to higher moments of \\(X\\) like variance or skewness. Examples methods relying on this type of weights include include Hainmueller (2012), Chan et al.¬†(2016), and Athey et al.¬†(2018), among many others.\nCovariate balancing weights bypass the need to explicitly estimate \\(e(X)\\). Instead, they solve for weights that ensure the treated and control groups are balanced across predefined covariates or their transformations. This method aligns closely with the goal of causal inference: achieving balance. The main challenge is selecting the right set of covariates to be balanced. Variance estimation can also be more involved, although modern statistical packages take care of it.\nSoftware Packages: MatchIt, Matching.\n\n\nHybrid Approach\nImai and Ratkovic (2013) developed a hybrid method that combines elements of covariate balancing and inverse propensity score methods. Their method, known as Covariate Balancing Propensity Score (CBPS), aims to estimate propensity scores while simultaneously achieving covariate balance. Instead of relying on a tuning parameter, CBPS ensures balance by solving the following moment conditions:\n\\[ \\sum_{i}\\left[ W_i - e(X_i; \\gamma) \\right] X_i = 0. \\]\nAdditionally, CBPS estimates by solving the standard likelihood score equation:\n\\[ \\sum_{i} \\left[ W_i - e(X_i; \\gamma) \\right] \\frac{\\partial e(X_i; \\gamma)}{\\partial \\gamma} = 0. \\]\nThe CBPS is operationalized in the Genralized Method of Moments (GMM) framework from the econmetrics literature. This approach ensures that the estimated propensity scores lead to balanced covariates, potentially improving the robustness of causal inference. CBPS can be implemented using available GMM software packages and is particularly useful when traditional propensity score models fail to achieve adequate balance.\nSoftware Packages: CBPS."
  },
  {
    "objectID": "blog/DONE-two-types-weights-causality.html#hybrid-approach",
    "href": "blog/DONE-two-types-weights-causality.html#hybrid-approach",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Hybrid Approach",
    "text": "Hybrid Approach\nImai and Ratkovic (2013) developed a hybrid method that combines elements of covariate balancing and inverse propensity score methods. Their method, known as Covariate Balancing Propensity Score (CBPS), aims to estimate propensity scores while simultaneously achieving covariate balance. Instead of relying on a tuning parameter, CBPS ensures balance by solving the following moment conditions:\n\\[ \\sum_{i}\\left[ W_i - e(X_i; \\gamma) \\right] X_i = 0. \\]\nAdditionally, CBPS estimates by solving the standard likelihood score equation:\n\\[ \\sum_{i} \\left[ W_i - e(X_i; \\gamma) \\right] \\frac{\\partial e(X_i; \\gamma)}{\\partial \\gamma} = 0. \\]\nThe CBPS is operationalized in the Genralized Method of Moments (GMM) framework from the econmetrics literature. This approach ensures that the estimated propensity scores lead to balanced covariates, potentially improving the robustness of causal inference. CBPS can be implemented using available GMM software packages and is particularly useful when traditional propensity score models fail to achieve adequate balance.\nSoftware Packages: CBPS."
  },
  {
    "objectID": "blog/DONE-two-types-weights-causality.html#an-example",
    "href": "blog/DONE-two-types-weights-causality.html#an-example",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate these methods in practice with R. Consider a dataset with treatment \\(W\\), outcome \\(Y\\), and covariates \\(X_1, X_2‚Äã\\). We estimate the ATE using both IPS, entropy balancing weights and CBPS. The exercise starts with generating some synthetic data.\nrm(list=ls())\nlibrary(MASS)\nlibrary(WeightIt)\nset.seed(1988)\n\n# generate fake data\nn &lt;- 1000\nX1 &lt;- rnorm(n)\nX2 &lt;- rnorm(n)\nW &lt;- rbinom(n, 1, plogis(0.5 * X1 - 0.25 * X2))\nY &lt;- 3 + 2 * W + X1 + X2 + rnorm(n)\ndata = data.frame(Y, W, X1, X2)\n\n# define functions that will calculate the weights and the associated Average Treatment Effects.\n\ncompute_weights &lt;- function(method) {\n    weightit(W ~ X1 + X2, method = method, data = data)$weights\n}\n\ncompute_ate &lt;- function(weights) {\n     weighted.mean(Y[W == 1], weights = weights[W == 1]) - \n        weighted.mean(Y[W == 0], weights = weights[W == 0])\n}\n\n# calcualte the three types of estimates.\nips_weights &lt;- compute_weights(\"glm\")\nebal_weights &lt;- compute_weights(\"ebal\")\ncbps_weights &lt;- compute_weights(\"cbps\")\n\n# we estimate the average treatment effect and print the results.\nips_ate &lt;- compute_ate(ips_weights)\nebal_ate &lt;- compute_ate(ebal_weights)\ncbps_ate &lt;- compute_ate(cbps_weights)\n\ncat(\"ATE (IPS Weights):\", ips_ate, \"\\n\")\n&gt;ATE (IPS Weights): 2.287048 \ncat(\"ATE (Entropy Balance Weights):\", ebal_ate, \"\\n\")\n&gt;ATE (Entropy Balance Weights): 2.287048 \n\ncat(\"ATE (CBPS Weights):\", cbps_ate, \"\\n\")\n&gt;ATE (CBPS Weights): 2.287048 \nThe weights are all very highly correlated with each other (not shown above), so they yield nearly identical results. For simplicity, I have ignored variance estimation and confidence intervals."
  },
  {
    "objectID": "blog/DONE-two-types-weights-causality.html#where-to-learn-more",
    "href": "blog/DONE-two-types-weights-causality.html#where-to-learn-more",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThis article was inspired by Ben-Michael et al.¬†(2021) and Chattopadhyay et al.¬†(2020). Both references are great starting points. There are plenty of accessible materials on the topic online. My favorite is Imbens (2015). For more in-depth content turn to Imbens and Rubin (2015)‚Äôs seminal textbook."
  },
  {
    "objectID": "blog/DONE-two-types-weights-causality.html#bottom-line",
    "href": "blog/DONE-two-types-weights-causality.html#bottom-line",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nCovariate balance between the treatment and control groups is at the core of causal inference.\nThere are two broad classes of weights that achieve such balance.\nIPS weights adjust for treatment probability but can be unstable.\nCovariate balancing weights directly target balance X, bypassing propensity score estimation."
  },
  {
    "objectID": "blog/DONE-two-types-weights-causality.html#references",
    "href": "blog/DONE-two-types-weights-causality.html#references",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "References",
    "text": "References\nAthey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing: debiased inference of average treatment effects in high dimensions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 80(4), 597-623.\nBen-Michael, E., Feller, A., Hirshberg, D. A., & Zubizarreta, J. R. (2021). The balancing act in causal inference. arXiv preprint arXiv:2110.14831.\nChan, K. C. G., Yam, S. C. P., & Zhang, Z. (2016). Globally efficient non-parametric inference of average treatment effects by empirical balancing calibration weighting. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(3), 673-700.\nChattopadhyay, A., Hase, C. H., & Zubizarreta, J. R. (2020). Balancing vs modeling approaches to weighting in practice. Statistics in Medicine, 39(24), 3227-3254.\nHainmueller, J. (2012). Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20(1), 25-46.\nHirshberg, D. A., & Wager, S. (2018). Augmented minimax linear estimation for treatment and policy evaluation.\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society Series B: Statistical Methodology, 76(1), 243-263.\nImbens, G. W., & Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical sciences. Cambridge university press.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nLi, F., Morgan, K. L., & Zaslavsky, A. M. (2018). Balancing covariates via propensity score weighting. Journal of the American Statistical Association, 113(521), 390-400.\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41-55."
  },
  {
    "objectID": "blog/limits-semiparam-models.html#a-closer-look",
    "href": "blog/limits-semiparam-models.html#a-closer-look",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn semiparametric models, the presence of \\(\\eta\\) complicates the estimation in that it can obscure the relationship between \\(\\theta\\) and the observed data. The semiparametric efficiency bound generalizes the CRLB by accounting for the nuisance parameter \\(\\eta\\) and isolating the information relevant to \\(\\theta\\).\n\nParametric Submodels\nLet‚Äôs take a sidestep for a minute. A parametric submodel, say \\(f(\\theta)\\), that contains \\(\\theta\\) alone, represents a subset of distributions that satisfy semiparametric assumptions and contains the true distribution \\(f(X; \\theta, \\eta)\\). For any semiparametric estimator that is consistent and asymptotically normal, its asymptotic variance can be compared to the CRLB of the parametric submodel. Since this relationship holds for all possible parametric submodels, the semiparametric estimator‚Äôs variance cannot be smaller than any submodel‚Äôs bound. In other words, the asymptotic variance of any semiparametric estimator is at least as large as the largest CRLB across all parametric submodels.\nInformally,\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\max_{\\text{{param. submodel}}} \\text{CRLB}.\\]\nThis is our first insight on the semiparametric efficiency bound, which admittedly is more of theoretical than practical significance.\n\n\nEfficient Influence Functions\nThe semiparametric efficiency bound depends on the interplay between \\(\\theta\\) and \\(\\eta\\), captured through the something called the Efficient Influence Function (EIF). Remember that the score function for \\(\\theta\\),\n\\[S_\\theta(X)=\\frac{\\partial}{\\partial \\theta} \\log f(X; \\theta, \\eta),\\]\nmeasures the sensitivity of the log-likelihood to changes in \\(\\theta\\). We can similarly define the score with respect to \\(\\eta\\):\n\\[S_\\eta(X)=\\frac{\\partial}{\\partial \\eta} \\log f(X; \\theta, \\eta).\\]\nNow enter the EIF \\(\\psi^*(X)\\) which captures the variation in \\(\\theta\\) while adjusting for the nuisance parameter \\(\\eta\\). It satisfies the orthogonality condition:\n\\[\\mathbb{E}\\left[ \\psi^*(X) \\cdot S_\\eta(X) \\right] = 0,\\]\nensuring that the influence of \\(\\eta\\) is removed from \\(\\psi(X)\\). In other words, \\(\\psi^*(X)\\) captures only information about \\(\\theta\\), uncontaminated by nuisance parameters. It is the influence function with the lowest possible variance.\n\n\nEfficient Score\nThe next piece of the puzzle is the Efficient Score \\(S^*_{\\theta}\\), the projection of \\(S_\\theta(X)\\) onto the space orthogonal to the nuisance tangent space \\(\\mathcal{T}_\\eta\\):\n\\[S^*_{\\theta}(X) = S_\\theta(X)  - \\Pi( S_\\theta(X) \\mid \\mathcal{T}_\\eta),\\]\nwhere \\(\\Pi(\\cdot)\\) is the projection operator. Here \\(\\mathcal{T}_\\eta\\) is simply the linear subspace spanned by \\(S_\\eta(X)\\). The Efficient Score is the part of the score vector that is ‚Äúfree‚Äù from the influence of nuisance parameters. It represents the best possible score function for estimating \\(\\theta\\) in the presence of \\(\\eta\\). (A similar technique underlies the so-called Neyman orthogonality principle in dobule/debiased machine learning.) We can construct the efficient influence function by appropriately scaling the efficient score to get to the optimal EIF.\n\n\nThe Semiparametric Efficiency Bound\nWe are, at last, ready to state the main result. The semiparametric efficiency bound is determined by the variance of the Efficient Score:\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{\\mathbb{E}[S_\\theta^*(X)^2]}.\\]\n‚Äã This generalizes the Cram√©r-Rao lower bound for semiparametric models by incorporating the complexity introduced by the nuisance parameter \\(\\eta\\). In parametric models, this bound collapses to and is determined by the Fisher Information, while in here, it is governed by the efficient score.\nTo achieve the bound in practice, nuisance parameters are often removed through methods like regression residuals, inverse probability weighting, or targeted maximum likelihood estimation (TMLE). These techniques isolate the information about \\(\\theta\\) from \\(\\eta\\), enabling efficient estimation."
  },
  {
    "objectID": "blog/DONE-conformal-inference.html",
    "href": "blog/DONE-conformal-inference.html",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "",
    "text": "Traditional confidence intervals estimate the range in which a population parameter, such as a mean or regression coefficient, is likely to fall with a specified level of confidence (e.g., 95%). They reflect the uncertainty in estimating a fixed but unknown parameter based on observing only a sample of the population. But what if we are interested in measuring uncertainty in the context of making predictions? Can we use traditional methods, or do we need a new framework?\nPrediction intervals provide a range within which a future individual observation is expected to fall with a given probability. Since they account for both the uncertainty in estimating the mean and the inherent variability of individual observations, they are typically wider than confidence intervals. While a confidence interval narrows as sample size increases, a prediction interval remains relatively wide due to the irreducible noise in individual data points. For simplicity, I will use both terms interchangeably.\nConformal inference offers a formalized framework for building such prediction intervals in machine learning models. This article provides a gentle introduction to the main idea behind conformal inference, presenting a new way of thinking about uncertainty in the context of machine learning (i.e., prediction) models.\nAs a teaser, the basic idea behind the method rests on a simple result about sample quantiles. Let me explain."
  },
  {
    "objectID": "blog/DONE-conformal-inference.html#background",
    "href": "blog/DONE-conformal-inference.html#background",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "",
    "text": "Traditional confidence intervals estimate the range in which a population parameter, such as a mean or regression coefficient, is likely to fall with a specified level of confidence (e.g., 95%). They reflect the uncertainty in estimating a fixed but unknown parameter based on observing only a sample of the population. But what if we are interested in measuring uncertainty in the context of making predictions? Can we use traditional methods, or do we need a new framework?\nPrediction intervals provide a range within which a future individual observation is expected to fall with a given probability. Since they account for both the uncertainty in estimating the mean and the inherent variability of individual observations, they are typically wider than confidence intervals. While a confidence interval narrows as sample size increases, a prediction interval remains relatively wide due to the irreducible noise in individual data points. For simplicity, I will use both terms interchangeably.\nConformal inference offers a formalized framework for building such prediction intervals in machine learning models. This article provides a gentle introduction to the main idea behind conformal inference, presenting a new way of thinking about uncertainty in the context of machine learning (i.e., prediction) models.\nAs a teaser, the basic idea behind the method rests on a simple result about sample quantiles. Let me explain."
  },
  {
    "objectID": "blog/DONE-conformal-inference.html#notation",
    "href": "blog/DONE-conformal-inference.html#notation",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs imagine a size n i.i.d. sample of an outcome variable \\(Y\\) and a covariate vector \\(X\\), \\((X_1, Y_1) \\dots (X_n, Y_n)\\). Conformal inference is concerned with building a ‚Äúconfidence interval‚Äù for a new outcome observation \\(Y_{n+1}\\) from a new feature realization \\(X_{n+1}\\).\nImportantly, this interval should be valid:\n\nin finite samples (i.e., non-asymptotically),\nwithout assumptions on the data generating process, and\nfor any estimator of the regression function, \\(\\mu(x)=E[Y \\mid X=x]\\).\n\nIn mathematical notation, given a significance level , we want to construct a confidence interval \\(CI(X_{n+1})\\) satisfying the above properties and such that:\n\\[P(Y_{n+1} \\in CI(X_{n+1})) \\geq 1-\\alpha.\\]"
  },
  {
    "objectID": "blog/DONE-conformal-inference.html#a-closer-look",
    "href": "blog/DONE-conformal-inference.html#a-closer-look",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Sample Quantiles\nI will start with reviewing sample quantiles. Given an i.i.d. sample, \\(U_1, \\dots, U_n\\), the (\\(1-\\alpha\\))th quantile is the value \\(\\hat{q}_{1-\\alpha}\\) such that approximately \\((1-\\alpha)\\times100\\%\\) of the data is smaller than it. For instance, the \\(95\\)th quantile (sometimes also called percentile) is the value for which \\(95\\%\\) of the observations are at least as small.\nSo, given a new observation \\(U_{n+1}\\), we know that:\n\\[P(U_{n+1}\\leq \\hat{q}_{1-\\alpha})\\geq 1-\\alpha.\\]\n\n\nThe Na√Øve Approach\nLet‚Äôs turn back to the regression example with Y and X. We are given a new observation \\(X_{n+1}\\) and our focus is on \\(Y_{n+1}\\). Following the fact described above, a na√Øve way to construct a confidence interval for \\(Y_{n+1}\\) is as follows:\n\\[CI^{\\text{na√Øve}}_{1-\\alpha}(X_{n+1}) = \\left[ \\hat{\\mu}(X_{n+1}) \\pm \\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha} \\right].\\]\nHere \\(\\mu(\\cdot)\\) is an estimate of the regression function \\(E[Y \\mid X]\\), and \\(\\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha}\\) is the \\((1-\\alpha)\\)th quantile of empirical distribution function of the fitted residuals \\(\\mid Y-\\hat{\\mu}(X) \\mid\\).\nPut simply, we can look at an interval around our best prediction for \\(Y_{n+1}\\) (i.e., \\(\\hat{\\mu}(X_{n+1})\\)) defined by the residuals estimated on the original data.\nIt turns out this interval is too narrow. In a series of papers Vladimir Vovk and co-authors show that the empirical distribution function of the fitted residuals is often biased downward and hence this interval is invalid. This is where conformal inference comes in.\n\n\nConformal Inference\nConsider the following strategy. For each \\(y\\) we fit a regression $_y $ on the sample \\((Y_1, X_1),\\dots (Y_n, X_n), (y, X_{n+1})\\). We calculate the residuals \\(R^y_i\\) for \\(i=1,\\dots,n\\) and \\(R^y_{n+1}\\) and count the proportion of \\(R^y_i\\)‚Äôs smaller than \\(R^y_{n+1}\\). Let‚Äôs call this number \\(\\sigma(y)\\). That is,\n\\[\\sigma(y) = \\frac{1}{n+1}\\sum_{i=1}^{n+1} I (R^y_i \\leq R^y_{n+1}),\\]\nwhere \\(I(\\cdot)\\) is the indicator function equal to one when the statement in the parenthesis is true and 0 if when it is not.\nThe test statistic \\(\\sigma({Y_{n+1}})\\) is uniformly distributed over the set \\(\\{ \\frac{1}{n+1}, \\frac{2}{n+1},\\dots, 1\\}\\), implying we can use \\(1-\\sigma({Y_{n+1}})\\) as a valid p-value for testing the null that \\(Y_{n+1}=y.\\) Then, using the sample quantiles logic outlined above we arrive at the following confidence interval for \\(Y_{n+1}\\):\n\\[ CI^{\\text{conformal}}_{1-\\alpha}(X_{n+1}) \\approx \\{ y\\in \\mathbb{R} : \\sigma(y)\\leq 1-\\alpha \\}.\\]\nThis is summarized in the following procedure:\n\nFor each value \\(y\\):\n\n\nFit the regression function \\(\\mu(\\cdot)\\) on \\((X_1, Y_1), \\dots, (X_n, Y_n), (X_{n+1}, y)\\) using your favorite estimator/learner.\nCalculate the \\(n+1\\) residuals.\nCalculate the proportion \\(\\sigma(y)\\).\n\n\nConstruct \\(CI = \\{y: \\sigma(y) \\leq (1-\\alpha)\\}\\).\n\nSoftware Package: conformalInference\nTwo notes. First, conformal inference guarantees unconditional coverage. This is conceptually different and should not be confused with the conditional statement \\(P(Y_{n+1}\\in CI(x) \\mid X_{n+1}=x)\\geq 1-\\alpha\\). The latter is stronger and more difficult to assert, requiring additional assumptions such as consistency of our estimator of \\(\\mu(\\cdot)\\).\nSecond, this procedure can be computationally expensive. For a given value \\(X_{n+1}\\) we need to fit a regression model and compute residuals for every \\(y\\) which we consider including in the confidence interval. This is where split conformal inference comes in.\n\n\nSplit Conformal Inference\nSplit conformal inference is a modification of the original algorithm that requires significantly less computation power. The idea is to split the fitting and ranking steps, so that the former is done only once. Here is the algorithm.\nAlgorithm:\n\nRandomly split the data in two equal-sized bins.\nGet \\(\\hat{\\mu}\\) on the first bin.\nCalculate the residuals for each observation in the second bin.\nLet \\(d\\) be the \\(s\\)-th smallest residual, where \\(s=(\\frac{n}{2}+1)(1-\\alpha)\\).\nConstruct \\(CI^{\\text{split}}=[\\hat{\\mu}-d,\\hat{\\mu}+d]\\).\n\nA downside of this splitting approach is the introduction of extra randomness. One way to mitigate this is to perform the split multiple times and construct a final confidence interval by taking the intersection of all intervals. The aggregation decreases the variability from a single data split and, as this paper shows, still remains valid. Similar random split aggregation has also been used in the context of statistical significance in high-dimensional models."
  },
  {
    "objectID": "blog/DONE-conformal-inference.html#an-example",
    "href": "blog/DONE-conformal-inference.html#an-example",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "An Example",
    "text": "An Example\nI used the popular iris dataset to try out the R package conformalInference. Like most of my data demos, this is meant to be a mere illustration and you should not take the results seriously.\nThe outcome variable was Sepal.Length, and the matrix \\(X\\) included sepal.width, petal.length, petal.width, species_setosa, species_versicolor, and species_virginica. Some of these were categorical in which case I converted them to a bunch of binary variables. I used the first \\(148\\) observations to estimate the regression function \\(\\mu(X)\\) using lasso and the \\(149\\)th row to form the prediction (i.e., the test set).\nHere is the code.\n# clear workspace and load packages\nrm(list=ls())\nlibrary(conformalInference)\nlibrary(glmnet)\nlibrary(tidyverse)\n\n# load the iris dataset\ndata &lt;- iris\n\n# clean data\ncolnames(data) &lt;- tolower(colnames(data))\n\n# one-hot encode the species variable\ndata &lt;- data %&gt;%\n  mutate(species_setosa = as.integer(species == 'setosa'),\n         species_versicolor = as.integer(species == 'versicolor'),\n         species_virginica = as.integer(species == 'virginica')) %&gt;%\n  dplyr::select(-species)\n\n# check for missing values (none in iris, but included for completeness)\ndata &lt;- na.omit(data)\n\n# split training/test data\ndata0 &lt;- data %&gt;% filter(row_number() == nrow(data))  # last row as test set\ndata &lt;- data %&gt;% filter(row_number() &lt; nrow(data))   # remaining rows as training set\n\n# select variables X, Y\ny &lt;- data$sepal.length  # target variable\nx &lt;- data %&gt;% dplyr::select(-sepal.length)  # predictors\nx &lt;- as.matrix(x)\nx0 &lt;- data0 %&gt;% dplyr::select(-sepal.length)  # test predictors\nx0 &lt;- as.matrix(x0)\nn &lt;- nrow(x)\n\n# use lasso to estimate mu\nout.gnet = glmnet(x, y, nlambda=100, lambda.min.ratio=1e-3)\nlambda = min(out.gnet$lambda)\nfuns = lasso.funs(lambda=lambda)\n\n# run conformal inference\nout.conf = conformal.pred(x, y, x0, \n                          alpha=0.1,\n                          train.fun=funs$train, \n                          predict.fun=funs$predict, \n                          verb=TRUE)\n\n# run split conformal inference\nout.split = conformal.pred.split(x, y, x0, \n                                 alpha=0.1,\n                                 train.fun=funs$train, \n                                 predict.fun=funs$predict, \n                                 verb=TRUE)\n\n# print results\npaste('The lower bound is', out.conf$lo, 'and the upper bound is', out.conf$up)\n&gt; [1] \"The lower bound is 5.89 and the upper bound is 6.68\"\nout.conf$pred\n&gt;         [,1]\n&gt; [1,] 6.316882\n\n# print results for split conformal inference\npaste('The lower bound is', out.split$lo, 'and the upper bound is', out.split$up)\n&gt; [1] \"The lower bound is 5.74 and the upper bound is 6.93\"\nout.split$pred\n&gt;          [,1]\n&gt; [1,] 6.33556\nThe actual age value in the test set was \\(6.2\\) while the conformal inference approach computed a confidence interval (\\(5.88, 6.68\\)). The splitting algorithm gave similar results."
  },
  {
    "objectID": "blog/DONE-conformal-inference.html#bottom-line",
    "href": "blog/DONE-conformal-inference.html#bottom-line",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nConformal inference offers a novel approach for constructing valid finite-sample prediction intervals in machine learning models."
  },
  {
    "objectID": "blog/DONE-conformal-inference.html#where-to-learn-more",
    "href": "blog/DONE-conformal-inference.html#where-to-learn-more",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nConformal inference in machine learning is an ongoing research topic and I do not know of any review papers or textbook treatments of the subject. If you are interested in learning more, check the paper referenced below."
  },
  {
    "objectID": "blog/DONE-conformal-inference.html#references",
    "href": "blog/DONE-conformal-inference.html#references",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "References",
    "text": "References\nLei, J., G‚ÄôSell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094-1111.\nLei, J., Rinaldo, A., & Wasserman, L. (2015). A conformal prediction approach to explore functional data. Annals of Mathematics and Artificial Intelligence, 74, 29-43.\nShafer, G., & Vovk, V. (2008). A Tutorial on Conformal Prediction. Journal of Machine Learning Research, 9(3).\nVovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world (Vol. 29). New York: Springer."
  },
  {
    "objectID": "blog/DONE-limits-semiparam-models.html",
    "href": "blog/DONE-limits-semiparam-models.html",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "",
    "text": "The efficiency bound is a cornerstone of the academic literature on semiparametric models, and it‚Äôs easy to see why. This bound quantifies the potential loss in efficiency (i.e., increase in variance) that arises when opting for a semiparametric model over a fully parametric one. In doing so, it offers a rigorous benchmark for evaluating the asymptotic variance of any estimator. By providing insights into the trade-offs between model flexibility and statistical precision, the efficiency bound occupies a critical role in understanding the theoretical limits of estimation. Despite its importance, this concept and the broader class of semiparametric models remain underappreciated within much of the data science community.\nThis article aims to demystify the notion of the semiparametric efficiency bound and its relevance to practical applications. It unpacks the mathematical foundations underlying this concept, shedding light on its relationship with the Cram√©r-Rao lower bound (CRLB). I will also touch on the bound‚Äôs implications for real-world data analysis, where balancing flexibility and efficiency is often a key concern."
  },
  {
    "objectID": "blog/DONE-limits-semiparam-models.html#background",
    "href": "blog/DONE-limits-semiparam-models.html#background",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "",
    "text": "The efficiency bound is a cornerstone of the academic literature on semiparametric models, and it‚Äôs easy to see why. This bound quantifies the potential loss in efficiency (i.e., increase in variance) that arises when opting for a semiparametric model over a fully parametric one. In doing so, it offers a rigorous benchmark for evaluating the asymptotic variance of any estimator. By providing insights into the trade-offs between model flexibility and statistical precision, the efficiency bound occupies a critical role in understanding the theoretical limits of estimation. Despite its importance, this concept and the broader class of semiparametric models remain underappreciated within much of the data science community.\nThis article aims to demystify the notion of the semiparametric efficiency bound and its relevance to practical applications. It unpacks the mathematical foundations underlying this concept, shedding light on its relationship with the Cram√©r-Rao lower bound (CRLB). I will also touch on the bound‚Äôs implications for real-world data analysis, where balancing flexibility and efficiency is often a key concern."
  },
  {
    "objectID": "blog/DONE-limits-semiparam-models.html#notation",
    "href": "blog/DONE-limits-semiparam-models.html#notation",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "Notation",
    "text": "Notation\nBefore diving in, let‚Äôs establish the necessary notation to guide our technical discussion. The model governing the data is characterized by parameters \\(\\theta\\) and \\(\\eta\\) with likelihood \\(f(X; \\theta, \\eta)\\). Moreover:\n\n\\(\\theta \\in \\mathbb{R}^d\\) is the parameter of interest, a finite-dimensional vector we want to estimate. Often \\(\\theta\\) is a scalar. It represents the parametric component of the model.\n\\(\\eta\\) is a nuisance parameter, which is infinite-dimensional (e.g., a nonparametric density or function). It is a nuisance in the sense that it is part of the model, but we are not interested in it for its own sake. It represents the nonparametric component of the model.\n\nA leading example is the partially linear model:\n\\[Y= \\theta X+g(Z)+\\epsilon,\\]\nwhere \\(Y\\) is the outcome variable, \\(Z\\) represents a vector of covariates, \\(g(\\cdot)\\) is a function characterized by \\(\\eta\\), while \\(\\epsilon\\) is an error term. To fit this model in the likelihood notation above, think of \\(Z\\) as a component of \\(X\\). We assume we have a random i.i.d. sample of all necessary variables."
  },
  {
    "objectID": "blog/DONE-limits-semiparam-models.html#a-closer-look",
    "href": "blog/DONE-limits-semiparam-models.html#a-closer-look",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn semiparametric models, the presence of \\(\\eta\\) complicates the estimation in that it can obscure the relationship between \\(\\theta\\) and the observed data. The semiparametric efficiency bound generalizes the CRLB by accounting for the nuisance parameter \\(\\eta\\) and isolating the information relevant to \\(\\theta\\).\n\nParametric Submodels\nLet‚Äôs take a sidestep for a minute. A parametric submodel, say \\(f(\\theta)\\), that contains \\(\\theta\\) alone, represents a subset of distributions that satisfy semiparametric assumptions and contains the true distribution \\(f(X; \\theta, \\eta)\\). For any semiparametric estimator that is consistent and asymptotically normal, its asymptotic variance can be compared to the CRLB of the parametric submodel. Since this relationship holds for all possible parametric submodels, the semiparametric estimator‚Äôs variance cannot be smaller than any submodel‚Äôs bound. In other words, the asymptotic variance of any semiparametric estimator is at least as large as the largest CRLB across all parametric submodels.\nInformally,\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\max_{\\text{{param. submodel}}} \\text{CRLB}.\\]\nThis is our first insight on the semiparametric efficiency bound, which admittedly is more of theoretical than practical significance.\n\n\nEfficient Influence Functions\nThe semiparametric efficiency bound depends on the interplay between \\(\\theta\\) and \\(\\eta\\), captured through the something called the Efficient Influence Function (EIF). Remember that the score function for \\(\\theta\\),\n\\[S_\\theta(X)=\\frac{\\partial}{\\partial \\theta} \\log f(X; \\theta, \\eta),\\]\nmeasures the sensitivity of the log-likelihood to changes in \\(\\theta\\). We can similarly define the score with respect to \\(\\eta\\):\n\\[S_\\eta(X)=\\frac{\\partial}{\\partial \\eta} \\log f(X; \\theta, \\eta).\\]\nNow enter the EIF \\(\\psi^*(X)\\) which captures the variation in \\(\\theta\\) while adjusting for the nuisance parameter \\(\\eta\\). It satisfies the orthogonality condition:\n\\[\\mathbb{E}\\left[ \\psi^*(X) \\cdot S_\\eta(X) \\right] = 0,\\]\nensuring that the influence of \\(\\eta\\) is removed from \\(\\psi(X)\\). In other words, \\(\\psi^*(X)\\) captures only information about \\(\\theta\\), uncontaminated by nuisance parameters. It is the influence function with the lowest possible variance.\n\n\nEfficient Score\nThe next piece of the puzzle is the Efficient Score \\(S^*_{\\theta}\\), the projection of \\(S_\\theta(X)\\) onto the space orthogonal to the nuisance tangent space \\(\\mathcal{T}_\\eta\\):\n\\[S^*_{\\theta}(X) = S_\\theta(X)  - \\Pi( S_\\theta(X) \\mid \\mathcal{T}_\\eta),\\]\nwhere \\(\\Pi(\\cdot)\\) is the projection operator. Here \\(\\mathcal{T}_\\eta\\) is simply the linear subspace spanned by \\(S_\\eta(X)\\). The Efficient Score is the part of the score vector that is ‚Äúfree‚Äù from the influence of nuisance parameters. It represents the best possible score function for estimating \\(\\theta\\) in the presence of \\(\\eta\\). (A similar technique underlies the so-called Neyman orthogonality principle in dobule/debiased machine learning.) We can construct the efficient influence function by appropriately scaling the efficient score to get to the optimal EIF.\n\n\nThe Semiparametric Efficiency Bound\nWe are, at last, ready to state the main result. The semiparametric efficiency bound is determined by the variance of the Efficient Score:\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{\\mathbb{E}[S_\\theta^*(X)^2]}.\\]\n‚Äã This generalizes the Cram√©r-Rao lower bound for semiparametric models by incorporating the complexity introduced by the nuisance parameter \\(\\eta\\). In parametric models, this bound collapses to and is determined by the Fisher Information, while in here, it is governed by the efficient score.\nTo achieve the bound in practice, nuisance parameters are often removed through methods like regression residuals, inverse probability weighting, or targeted maximum likelihood estimation (TMLE). These techniques isolate the information about \\(\\theta\\) from \\(\\eta\\), enabling efficient estimation."
  },
  {
    "objectID": "blog/DONE-limits-semiparam-models.html#bottom-line",
    "href": "blog/DONE-limits-semiparam-models.html#bottom-line",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nSemiparametric models blend parametric assumptions (related to a parameter of interest) with nonparametric flexibility (related to nuisance parameters).\nThe efficient influence function isolates the information about the parameter of interest, removing the impact of nuisance parameters.\nThe semiparametric efficiency bound generalizes CRLB to this class of models. It is determined by the variance of the efficient score vector.\nPractical estimation achieving this bound often involves removing nuisance effects through residualization or other adjustment techniques."
  },
  {
    "objectID": "blog/DONE-limits-semiparam-models.html#references",
    "href": "blog/DONE-limits-semiparam-models.html#references",
    "title": "The Limits of Semiparametric Models: The Efficiency Bound",
    "section": "References",
    "text": "References\nBickel, P. J., Klaassen, C. A., Ritov, Y., & Wellner, J. A. (1993)\nEfficient and Adaptive Estimation for Semiparametric Models. Johns Hopkins University Press.\nGreene, William H. ‚ÄúEconometric analysis‚Äù. New Jersey: Prentice Hall (2000): 201-215.\nHines, O., Dukes, O., Diaz-Ordaz, K., & Vansteelandt, S. (2022). Demystifying statistical learning based on efficient influence functions. The American Statistician, 76(3), 292-304.\nIchimura, H., & Todd, P. (2007) Implementing Nonparametric and Semiparametric Estimators. In Heckman, J. & Leamer, E. (Eds.), Handbook of Econometrics (Vol. 6B).\nNewey, W. K. (1990) Semiparametric Efficiency Bounds. Journal of Applied Econometrics, 5(2), 99‚Äì135.\nTsiatis, A. (2007). Semiparametric theory and missing data. Springer Science & Business Media\nVan der Vaart, A. W. (2000) Asymptotic Statistics. Cambridge University Press."
  },
  {
    "objectID": "blog/DONE-lord-paradox.html",
    "href": "blog/DONE-lord-paradox.html",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Lord‚Äôs paradox presents a fascinating challenge in causal inference and statistics. It highlights how different statistical methods applied to the same data can lead to contradictory conclusions. The paradox typically arises when comparing changes over time between two groups in the context of adjusting for baseline characteristics. Despite its theoretical nature, the phenomena is deeply relevant to practical data analysis, especially in observational studies where causation is challenging to establish. Let‚Äôs look at an example."
  },
  {
    "objectID": "blog/DONE-lord-paradox.html#background",
    "href": "blog/DONE-lord-paradox.html#background",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Lord‚Äôs paradox presents a fascinating challenge in causal inference and statistics. It highlights how different statistical methods applied to the same data can lead to contradictory conclusions. The paradox typically arises when comparing changes over time between two groups in the context of adjusting for baseline characteristics. Despite its theoretical nature, the phenomena is deeply relevant to practical data analysis, especially in observational studies where causation is challenging to establish. Let‚Äôs look at an example."
  },
  {
    "objectID": "blog/DONE-lord-paradox.html#a-closer-look",
    "href": "blog/DONE-lord-paradox.html#a-closer-look",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nMean Differences Over Time\nTo explore Lord‚Äôs paradox, consider the following scenario: Suppose we have two groups of individuals‚Äî\\(A\\) and \\(B\\)‚Äîwith their body weights measured at two time points: before and after a specific intervention. Let the weight at the initial time point be denoted as \\(W_{\\text{pre}}\\), and the weight at the final time point be \\(W_{\\text{post}}\\). We are interested in whether the intervention caused a change in weight between the two groups.\nOne approach to analyze this is to compute the (unadjusted) mean weight change for each group over time:\n\\[\\Delta = \\Delta^A - \\Delta^B.\\]\nIf this quantity is statistically significant, we might conclude that the intervention had a differential effect.\n\n\nControlling for Baseline Characteristics\nAn alternative approach involves adjusting for baseline weight \\(W_{\\text{pre}}\\) using, for example, a regression model:\n\\[W_{\\text{post}}=\\beta_1 + \\beta_2 G_A + \\beta_3 W_{\\text{pre}} + \\epsilon,\\]\nwhere \\(G\\) is a binary indicator for group \\(A\\) membership and \\(\\epsilon\\) is an error term. Here, \\(\\beta_2\\) captures the group difference in \\(W_{\\text{post}}\\), linearly controlling for baseline body weight.\nSurprisingly, these two approaches can yield conflicting results. For example, the former method might suggest no difference, while the regression adjustment indicates a significant group effect.\n\n\nExplanation\nThis contradiction arises because the two methods implicitly address different causal questions.\n\nMethod 1 asks: ‚ÄúDo Groups \\(A\\) and \\(B\\) gain/lose different amounts of weight?‚Äù\nMethod 2 asks: ‚ÄúGiven the same initial weight, does any of the groups end up at different final weights?‚Äù The regression approach adjusts for baseline differences, assuming \\(W_{\\text{pre}}\\) is a confounder.\n\nThe key insight is that the choice of analysis reflects underlying assumptions about the causal structure of the data. If \\(W_{\\text{pre}}\\) is affected by group membership (e.g., due to selection bias), then adjusting for it may introduce bias rather than remove it. However, in practice we most often should control for baseline characteristics as they are critical in balancing the treatment and control groups.\n\n\nThe Simpson‚Äôs Paradox Once Again\nI recently illustrated the more commonly discussed Simpson‚Äôs paradox. Interestingly, a 2008 paper claims that two phenomena are closely related, with the Lord‚Äôs paradox being a ‚Äúcontinuous version‚Äù of Simpson‚Äôs paradox."
  },
  {
    "objectID": "blog/DONE-lord-paradox.html#an-example",
    "href": "blog/DONE-lord-paradox.html#an-example",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs look at some code illustrating Lord‚Äôs paradox in R and python. We start with simulating a dataset where two groups have identical distributions of \\(W_{\\text{pre}}\\) and \\(W_{\\text{post}}\\), yet differing relationships between the two variables.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\nn &lt;- 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup &lt;- factor(rep(c(\"A\", \"B\"), each = n/2))\n\n# Initial weight (pre). # Group A starts with higher average weight\nweight_pre &lt;- numeric(n)\nweight_pre[group == \"A\"] &lt;- rnorm(n/2, mean = 75, sd = 10)\nweight_pre[group == \"B\"] &lt;- rnorm(n/2, mean = 65, sd = 10)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain &lt;- rnorm(n, mean = 10, sd = 5)\nweight_post &lt;- weight_pre + gain\n\n# Create data frame\ndata &lt;- data.frame(group = group, pre = weight_pre, post = weight_post)\n\n# Analysis 1: Compare change scores between groups`\nt.test(post - pre ~ group, data = data)\n&gt; p-value = 0.6107\n\n# Analysis 2: Regression Adjustment`\nmodel &lt;- lm(post ~ group + pre, data = data)\nsummary(model)\n&gt; p-value = 0.08428742\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Set seed for reproducibility\nnp.random.seed(1988)\nn = 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup = np.array([\"A\"] * (n // 2) + [\"B\"] * (n // 2))\n\n# Initial weight (pre). Group A starts with higher average weight\nweight_pre = np.zeros(n)\nweight_pre[group == \"A\"] = np.random.normal(loc=75, scale=10, size=n // 2)\nweight_pre[group == \"B\"] = np.random.normal(loc=65, scale=10, size=n // 2)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain = np.random.normal(loc=10, scale=5, size=n)\nweight_post = weight_pre + gain\n\n# Create DataFrame\ndata = pd.DataFrame({\"group\": group, \"pre\": weight_pre, \"post\": weight_post})\n\n# Analysis 1: Compare change scores between groups\ndata[\"change\"] = data[\"post\"] - data[\"pre\"]\ngroup_a_change = data[data[\"group\"] == \"A\"][\"change\"]\ngroup_b_change = data[data[\"group\"] == \"B\"][\"change\"]\nt_stat, p_value = ttest_ind(group_a_change, group_b_change)\nprint(f\"Analysis 1: p-value = {p_value:.4f}\")\n\n# Analysis 2: Regression Adjustment\nmodel = smf.ols(\"post ~ group + pre\", data=data).fit()\nprint(model.summary())\n\n\n\nThe former approach shows lack of statistically significant differences in the body weight after the intervention between the two groups (\\(p\\)-value =$ 0.6107\\(). The results from the latter method do show meaningful differences (\\)p$-value = \\(0.0842\\)).\nThis illustrates the core of Lord‚Äôs paradox ‚Äì the statistical approach chosen can lead to different interpretations of the same underlying phenomenon."
  },
  {
    "objectID": "blog/DONE-lord-paradox.html#bottom-line",
    "href": "blog/DONE-lord-paradox.html#bottom-line",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLord‚Äôs paradox underscores the importance of aligning statistical methods with causal assumptions.\nDifferent methods answer different questions and may yield contradictory results if applied blindly.\nCareful consideration of the data-generating process and the role of potential confounders is crucial in choosing the appropriate analytical approach."
  },
  {
    "objectID": "blog/DONE-lord-paradox.html#references",
    "href": "blog/DONE-lord-paradox.html#references",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "References",
    "text": "References\nLord, E. M. (1967). A paradox in the interpretation of group comparisons. Psychological Bulletin, 68, 304‚Äì305. doi:10.1037/h0025105\nLord, F. M. (1969). Statistical adjustments when comparing preexisting groups. Psychological Bulletin, 72, 336‚Äì337. doi:10.1037/h0028108\nLord, E. M. (1975). Lord‚Äôs paradox. In S. B. Anderson, S. Ball, R. T. Murphy, & Associates, Encyclopedia of Educational Evaluation (pp.¬†232‚Äì236). San Francisco, CA: Jossey-Bass.\nTu, Y. K., Gunnell, D., & Gilthorpe, M. S. (2008). Simpson‚Äôs Paradox, Lord‚Äôs Paradox, and Suppression Effects are the same phenomenon‚Äìthe reversal paradox. Emerging themes in epidemiology, 5, 1-9."
  },
  {
    "objectID": "blog/DONE-conformal-inference-var-selection.html",
    "href": "blog/DONE-conformal-inference-var-selection.html",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "",
    "text": "Many machine learning (ML) methods operate as opaque systems, generating predictions when given a dataset as input. Identifying which variables have the greatest impact on these predictions is often crucial. This adds a touch of interpretability and transparency and aids stakeholders in better understanding the relevant context. Examples abound. For instance, identifying the house attributes most important for predicting home prices, the school or hospital characteristics most strongly associated with better students‚Äô and patients‚Äô outcomes, etc.\nConformal inference offers a novel way of measuring variable importance in ML. In an earlier article I introduced conformal inference as a tool for generating confidence intervals when making predictions for new observations, and here I will describe how we can adapt it to the context of feature importance. The approach is thus similar in spirit to the Gini Importance-based methods mentioned above."
  },
  {
    "objectID": "blog/DONE-conformal-inference-var-selection.html#background",
    "href": "blog/DONE-conformal-inference-var-selection.html#background",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "",
    "text": "Many machine learning (ML) methods operate as opaque systems, generating predictions when given a dataset as input. Identifying which variables have the greatest impact on these predictions is often crucial. This adds a touch of interpretability and transparency and aids stakeholders in better understanding the relevant context. Examples abound. For instance, identifying the house attributes most important for predicting home prices, the school or hospital characteristics most strongly associated with better students‚Äô and patients‚Äô outcomes, etc.\nConformal inference offers a novel way of measuring variable importance in ML. In an earlier article I introduced conformal inference as a tool for generating confidence intervals when making predictions for new observations, and here I will describe how we can adapt it to the context of feature importance. The approach is thus similar in spirit to the Gini Importance-based methods mentioned above."
  },
  {
    "objectID": "blog/DONE-conformal-inference-var-selection.html#notation",
    "href": "blog/DONE-conformal-inference-var-selection.html#notation",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs begin by setting up some notation. We have a size \\(n\\) i.i.d. random sample of a feature vector \\(X\\) and an outcome \\(Y\\). The focus of conformal inference is on constructing a ‚Äúconfidence interval‚Äù for predicting a new observation \\(Y_{n+1}\\) given a new feature realization \\(X_{n+1}\\). I denote the estimate of the mean function by \\(\\hat{\\mu}\\) and the same estimate when removing feature \\(j\\) from $X $ by \\(\\hat{\\mu}_{-j}\\).\nPlease refer to my previous article for more details on the conformal inference framework, methodology and its properties."
  },
  {
    "objectID": "blog/DONE-conformal-inference-var-selection.html#a-closer-look",
    "href": "blog/DONE-conformal-inference-var-selection.html#a-closer-look",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Variable Importance\nThe idea of measuring which variables contribute most to a prediction model is not new. The data scientist‚Äôs toolbox contains some useful techniques designed to measure variable importance in ML models. Popular choices include:\n\nGini Importance and Information Gain in tree-based models (e.g., random forest, gradient boosting) measure the decrease in various within-leaf impurity indexes caused by excluding a certain variable. The larger the loss, the more important the variable.\nSHAP Values use a cooperative game-theoretic approach to measure each variable‚Äôs contribution to the final model‚Äôs prediction.\nPermutation Importance assesses a variable‚Äôs significance by randomly shuffling its values and comparing the change in the model‚Äôs performance. The larger the drop, the more important the variable.\nVariable Coefficients in linear ML models (e.g., Lasso, Ridge) can directly signal importance. This requires an appropriate standardization before fitting the model (to make sure all features are on a level playing field).\n\n\n\nVariable Importance with Conformal Inference\nWe can measure the prediction error associated with dropping a feature \\(j\\) when predicting a new observation \\(Y_{n+1}\\) by:\n\\[\\Delta_j^{n+1} = |Y_{n+1} - \\hat{\\mu}_{-j}(X_{n+1})| - |Y_{n+1}-\\hat{\\mu}(X_{n+1})|.\\]\nThe main idea is to use conformal inference ideas to construct a confidence interval for this prediction loss, \\(\\Delta_j^{n+1}\\), as a signal whether that variable is relevant in predicting the outcome.\nSpecifically, let \\(CI(\\cdot)\\) denote the conformal inference interval for \\(Y_{n+1}\\) given \\(X_{n+1}\\). Then, the interval\n\\[S_j(x)=\\{ |y-\\hat{\\mu}_{-j}(x)|-|y-\\hat{\\mu}(x)| : y \\in CI(x) \\}.\\]\nhas a valid finite-sample coverage in the sense that:\n\\[ P(\\Delta_j^{n+1} \\in S_j(X_{n+1})) \\geq 1-\\alpha, \\]\nwhere \\(\\alpha\\) is a pre-specified significance level. This holds for all \\(j\\).\nWe can plot the confidence intervals \\(S_j(X_i)\\) for \\(i=1 \\dots n\\) and roughly interpret them as measuring variable importance. The closer the intervals are to zero, the less important the variable is for predicting new outcomes. The opposite is true as well. The further and more often it is away from zero, the more important the variable.\nAnother, more global, approach to using conformal inference for variable importance focuses on the distribution of \\(\\Delta_j(X_{n+1}, Y_{n+1})\\) and conducts hypothesis testing on its median or mean. Intuitively, failing to reject a hyptohesis that these statistics are non-zero is evidence that variable \\(j\\) does not play a significant role in predicting \\(Y\\)."
  },
  {
    "objectID": "blog/DONE-conformal-inference-var-selection.html#bottom-line",
    "href": "blog/DONE-conformal-inference-var-selection.html#bottom-line",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWhile many ML methods act as black boxes, attention often falls on measuring individual variable importance.\nConformal inference offers a new way for data scientists to quantify the influence of each variable to the model performance.\nThe main idea is to use conformal inference to construct a confidence interval for the loss in prediction accuracy associated with removing a feature from the dataset."
  },
  {
    "objectID": "blog/DONE-conformal-inference-var-selection.html#where-to-learn-more",
    "href": "blog/DONE-conformal-inference-var-selection.html#where-to-learn-more",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nSee Section 6 in Lei et al.¬†(2018) and the references therein."
  },
  {
    "objectID": "blog/DONE-conformal-inference-var-selection.html#references",
    "href": "blog/DONE-conformal-inference-var-selection.html#references",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "References",
    "text": "References\nLei, J., G‚ÄôSell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094-1111.\nLei, J., Rinaldo, A., & Wasserman, L. (2015). A conformal prediction approach to explore functional data. Annals of Mathematics and Artificial Intelligence, 74, 29-43.\nShafer, G., & Vovk, V. (2008). A Tutorial on Conformal Prediction. Journal of Machine Learning Research, 9(3).\nVovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world (Vol. 29). New York: Springer."
  },
  {
    "objectID": "blog/DONE-filling-missing-data-mcmc.html",
    "href": "blog/DONE-filling-missing-data-mcmc.html",
    "title": "Filling in Missing Data with MCMC",
    "section": "",
    "text": "Every dataset inevitably contains missing or incomplete values. Practitioners then face the dilemma of how to address these missing observations. A common approach, though potentially problematic, is to simply ignore them. I am guilty of doing this all too often. While convenient, ignoring missing data can introduce bias into analyses, particularly if the missingness is not entirely random. Moreover, throwing away data usually results in loss of statistical precision. Traditional methods for handling missing data, such as mean or median imputation, usually oversimplify the underlying data-generating process. Regression-based adjustments offer some improvement, but they rely on the linearity assumption.\nThis article introduces Markov Chain Monte Carlo (MCMC) as a robust and theoretically sound methodology for addressing missing data. Unlike arbitrary imputation methods, MCMC leverages the inherent information within the dataset to generate plausible values for the missing observations. The core principle of MCMC involves treating missing data as random variables and employing the algorithm to sample from their posterior distribution, thereby capturing the uncertainty and built-in structure within the data. The use of MCMC to draw observations repeatedly in the context of missing data is often referred to as Multiple Imputation (MI).\nLet‚Äôs break this down step by step, explore the underlying intuition, as well as an illustrative example, assuming a basic understanding of probability theory and Bayesian methods."
  },
  {
    "objectID": "blog/DONE-filling-missing-data-mcmc.html#background",
    "href": "blog/DONE-filling-missing-data-mcmc.html#background",
    "title": "Filling in Missing Data with MCMC",
    "section": "",
    "text": "Every dataset inevitably contains missing or incomplete values. Practitioners then face the dilemma of how to address these missing observations. A common approach, though potentially problematic, is to simply ignore them. I am guilty of doing this all too often. While convenient, ignoring missing data can introduce bias into analyses, particularly if the missingness is not entirely random. Moreover, throwing away data usually results in loss of statistical precision. Traditional methods for handling missing data, such as mean or median imputation, usually oversimplify the underlying data-generating process. Regression-based adjustments offer some improvement, but they rely on the linearity assumption.\nThis article introduces Markov Chain Monte Carlo (MCMC) as a robust and theoretically sound methodology for addressing missing data. Unlike arbitrary imputation methods, MCMC leverages the inherent information within the dataset to generate plausible values for the missing observations. The core principle of MCMC involves treating missing data as random variables and employing the algorithm to sample from their posterior distribution, thereby capturing the uncertainty and built-in structure within the data. The use of MCMC to draw observations repeatedly in the context of missing data is often referred to as Multiple Imputation (MI).\nLet‚Äôs break this down step by step, explore the underlying intuition, as well as an illustrative example, assuming a basic understanding of probability theory and Bayesian methods."
  },
  {
    "objectID": "blog/DONE-filling-missing-data-mcmc.html#notation",
    "href": "blog/DONE-filling-missing-data-mcmc.html#notation",
    "title": "Filling in Missing Data with MCMC",
    "section": "Notation",
    "text": "Notation\nTo keep things precise, let‚Äôs set up some notation. Let Y be the complete dataset, which we wish we had. We observe some, but not all observations of Y. Let‚Äôs split it into observed data \\(Y_{\\text{obs}}\\), and missing (or incomplete) data \\(Y_{\\text{miss}}\\), so that \\(Y = \\left( Y_{\\text{obs}}, Y_{\\text{miss}} \\right)\\).\nAssume a model for the data parameterized by \\(\\theta\\), that is \\(Y \\sim f(Y \\mid \\theta)\\). A simple example would be that a univaraite \\(Y\\) is Gaussian with some unspecified mean and variance. Our goal is to estimate and fill in \\(Y_{\\text{miss}}\\) by sampling from the posterior distribution\n\\[P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta).\\]\nThe results and intuition hold also conditional on some covariates \\(X\\), but for simplicity‚Äôs sake, I will keep that out of the notation for now."
  },
  {
    "objectID": "blog/DONE-filling-missing-data-mcmc.html#a-closer-look",
    "href": "blog/DONE-filling-missing-data-mcmc.html#a-closer-look",
    "title": "Filling in Missing Data with MCMC",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nMarkov Chain Monte Carlo is a powerful computational technique designed to draw observations from complex probability distributions that are difficult to directly sample from. This might happen because they do not have a nice closed-form analytical expression, or they do, but it‚Äôs too messy. Such distributions often arise in Bayesian statistics, where we aim to estimate the posterior distribution of parameters given observed data.\nThe ‚Äúmagic‚Äù of MCMC lies in its iterative nature. It begins with an initial guess for the parameter \\(\\theta\\). Then, a sophisticated sampling algorithm, such as the Metropolis-Hastings or Gibbs sampler, is employed to generate a sequence of observations. These observations are not independent but are related to each other in a specific way, forming a Markov chain. Crucially, under certain conditions, this Markov chain will eventually converge to the true target distribution.\nIn the context of missing data, MCMC iteratively alternates between the \\(I\\)- and the \\(P\\)-steps. At the \\(t\\)-th iteration with current guess for \\(\\theta\\) denoted \\(\\theta^t\\), these steps are:\n\nThe \\(I\\)-step (imputation): Draw \\(Y_{\\text{miss}}\\) from \\(P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta^t)\\). That is, from its conditional distribution given the observed data and current parameter estimates.\nThe \\(P\\)-step (posterior): Draw \\(\\theta^{t+1}\\) from \\(P(\\theta \\mid Y_{\\text{obs}}, Y_{\\text{miss}}^{t+1})\\). This is its posterior distribution given the observed data and the newly imputed \\(Y_{\\text{miss}}\\).\n\nThis back-and-forth dance ensures that the imputed values reflect the uncertainty and structure of the data. And with enough iterations (large \\(t\\)) the chain will converge to our target, \\(P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta)\\).\nSoftware Packages: mcmc, MCMCPack, mice.\n\n\nPractical Considerations\nConvergence diagnostics are crucial to ensure the MCMC chains have reached a stable equilibrium, as the initial values can significantly influence the results. In simple words, the chain should run long enough so that the posterior distribution does not change significantly after each additional iteration. It is also common to discard (or ‚Äúburn‚Äù) an initial batch of values since they do not come from the final, stable posterior distribution. Additionally, computational costs can be a significant factor, especially for large datasets or complex models but efficient algorithms and parallel processing can help. Lastly, model specification is critical, as the choice of imputation model directly impacts the quality of the imputed values."
  },
  {
    "objectID": "blog/DONE-filling-missing-data-mcmc.html#an-example",
    "href": "blog/DONE-filling-missing-data-mcmc.html#an-example",
    "title": "Filling in Missing Data with MCMC",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs walk through an example using a simple dataset with missing values. Suppose you have a dataset with two variables, \\(X\\) and \\(Y\\), where \\(Y \\sim N(\\beta_0 + \\beta_1 X, \\sigma^2)\\), and some values of \\(Y\\) are missing. We assume the following relationship:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\nwhere \\(\\epsilon\\) is an error term. We impose priors on \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) (which collectively comprise \\(\\theta\\) in this exampe).\nWe begin with generating some fake data and introduce missingness in \\(Y\\).\nrm(list=ls())\nset.seed(1988)\nlibrary(mice)\n\n# generate fake data\nn &lt;- 100                  \nc &lt;- 0.2                 \nX &lt;- rnorm(n, mean = 5, sd = 2)\nbeta0 &lt;- 2                 \nbeta1 &lt;- 1.5              \nepsilon &lt;- rnorm(n, mean = 0, sd = 1)  \nY &lt;- beta0 + beta1 * X + epsilon      \n\n# introduce missingness in Y\nmissing_indices &lt;- sample(1:n, size = n * c, replace = FALSE)\nY[missing_indices] &lt;- NA  \n\n# combine data into a data frame\ndata &lt;- data.frame(X = X, Y = Y)\nhead(data)\n\n# perform imputation\nimputed_data &lt;- mice(data, \n                    m = 5, \n                    method = \"norm\", \n                    seed = 1988)\nmodels &lt;- with(imputed_data, lm(Y ~ X))\n\n# print results\nsummary(pool(models))\n\n         term estimate std.error statistic       df      p.value\n1 (Intercept) 1.730583 0.3008046  5.753179 49.51038 5.435802e-07\n2           X 1.546689 0.0544164 28.423207 52.02756 2.313802e-33\nThis is clearly a silly example since \\(Y\\) is missing at random, suggesting that missing data does not result in bias. Anyway, for illustration purposes we run the Bayesian Regression algorithm to fill in the missing \\(Y\\) and proceed with a linear regression of \\(Y\\) on \\(X\\).\nSpecifically, the code below uses normal (linear regression) imputation to fill in the missing values. For each missing point the algorithm fits a linear regression model predicting \\(Y\\) from \\(X\\) using the complete data and then use this model to predict or impute the missing \\(Y\\). This process is repeated \\(m\\) times (hence the name multiple imputation), creating \\(m\\) different versions of the dataset with the missing values filled in.\nBoth coefficients fall in the expected respective regions."
  },
  {
    "objectID": "blog/DONE-filling-missing-data-mcmc.html#where-to-learn-more",
    "href": "blog/DONE-filling-missing-data-mcmc.html#where-to-learn-more",
    "title": "Filling in Missing Data with MCMC",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFollowing some computational innovations, Bayesian methods have experienced somewhat of a revival in the last fiveteen years. Consequently, there are plenty of high-quality materials online. Takahashi (2017) is an accessible resource on MCMC and Multiple Imputation which I used extensively."
  },
  {
    "objectID": "blog/DONE-filling-missing-data-mcmc.html#bottom-line",
    "href": "blog/DONE-filling-missing-data-mcmc.html#bottom-line",
    "title": "Filling in Missing Data with MCMC",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMissing data is an ever-present issue in practice.\nStandard approaches to dealing with missing information include ignoring it or imputing it with mean or predicted values.\nMCMC leverages the full joint distribution of the data, making it a robust imputation method.\nBy alternating between imputing missing values and updating parameters, MCMC aligns imputations with the observed data‚Äôs structure."
  },
  {
    "objectID": "blog/DONE-filling-missing-data-mcmc.html#references",
    "href": "blog/DONE-filling-missing-data-mcmc.html#references",
    "title": "Filling in Missing Data with MCMC",
    "section": "References",
    "text": "References\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman and Hall/CRC.\nRubin, D B 1987 Multiple Imputation for Nonresponse in Surveys. New York, NY: John Wiley & Sons. DOI: https://doi.org/10.1002/9780470316696\nSchafer, J L 1997 Analysis of Incomplete Multivariate Data. Boca Raton, FL: Chapman & Hall/CRC. DOI: https://doi.org/10.1201/9781439821862\nScheuren, F 2005 Multiple imputation: How it began and continues. The American Statistician, 59(4): 315‚Äì319.\nTakahashi, M. (2017). Statistical inference in missing data by MCMC and non-MCMC multiple imputation algorithms: Assessing the effects of between-imputation iterations. Data Science Journal, 16, 37-37."
  },
  {
    "objectID": "blog/DONE-ml-based-adjustments.html",
    "href": "blog/DONE-ml-based-adjustments.html",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "",
    "text": "Randomized experiments are the gold standard when interested in measuring causal relationships with data. In settings with small treatment effects or underpowered designs, a major focus falls on decreasing the variance. In simple low-dimensional settings a common attempt to do that is to include a bunch of covariates and their interaction with the treatment variable in an OLS regression. Under standard assumptions, the coefficient on the treatment variable is still asymptotically unbiased (albeit not in finite samples) and including the interactions guarantees that this estimator does not have higher asymptotic variance than the simple difference-in-means.\nIn high-dimensional settings, however, this can easily lead to overfitting and new tools for variance reduction are needed. In this article, I will focus on two ways Machine Learning (ML) can be helpful with this problem when we have access to a bunch of covariates. In the first set of methods, we use a ML algorithm (such as the lasso) to directly estimate the treatment effect. Alternatively, we can first use ML to predict the outcome and then feed that prediction in an OLS regression.\nA helpful benchmark with which to compare these methods is the simple (non-parametric) difference-in-means estimator. Under certain conditions, both approaches guarantee smaller or equal variance."
  },
  {
    "objectID": "blog/DONE-ml-based-adjustments.html#background",
    "href": "blog/DONE-ml-based-adjustments.html#background",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "",
    "text": "Randomized experiments are the gold standard when interested in measuring causal relationships with data. In settings with small treatment effects or underpowered designs, a major focus falls on decreasing the variance. In simple low-dimensional settings a common attempt to do that is to include a bunch of covariates and their interaction with the treatment variable in an OLS regression. Under standard assumptions, the coefficient on the treatment variable is still asymptotically unbiased (albeit not in finite samples) and including the interactions guarantees that this estimator does not have higher asymptotic variance than the simple difference-in-means.\nIn high-dimensional settings, however, this can easily lead to overfitting and new tools for variance reduction are needed. In this article, I will focus on two ways Machine Learning (ML) can be helpful with this problem when we have access to a bunch of covariates. In the first set of methods, we use a ML algorithm (such as the lasso) to directly estimate the treatment effect. Alternatively, we can first use ML to predict the outcome and then feed that prediction in an OLS regression.\nA helpful benchmark with which to compare these methods is the simple (non-parametric) difference-in-means estimator. Under certain conditions, both approaches guarantee smaller or equal variance."
  },
  {
    "objectID": "blog/DONE-ml-based-adjustments.html#notation",
    "href": "blog/DONE-ml-based-adjustments.html#notation",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "Notation",
    "text": "Notation\nI use \\(\\bar{Y}^T\\) and \\(\\bar{Y}^C\\) to denote the sample average outcomes for the treatment and control groups respectively. \\(X\\) is the covariate vector and its deviations from the average are \\(\\tilde{X}\\). The benchmark estimator can be expressed as:\n\\[\\hat{ATE}^{simple} = \\bar{Y}^T - \\bar{Y}^C.\\]"
  },
  {
    "objectID": "blog/DONE-ml-based-adjustments.html#a-closer-look",
    "href": "blog/DONE-ml-based-adjustments.html#a-closer-look",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "A Closer Look",
    "text": "A Closer Look\nBroadly speaking, there are two ML Methods for Variance Reduction.\n\nUsing ML Regression Directly\nThe simplest and most natural way to incorporate covariates is to add them to a linear model (along with the treatment variable and their interactions with the treatment variable). Bloniarz et al.¬†(2015) show we can directly use Lasso regression instead of OLS.\nTo guarantee that the lasso does not omit the treatment variable, we can run two separate regressions, one for each (treatment) group. Then the estimator can be formulated as:\n\\[\\hat{ATE}^{lasso} = (\\bar{Y}^T-\\tilde{X}^T\\beta^{T}_{lasso}) - (\\bar{Y}^C-\\tilde{X}^C\\beta^{C}_{lasso}),\\]\nwhere \\(\\beta^{i}_{lasso}\\) is the coefficient vector from the lasso regressions on observations in group \\(i\\in\\{T,C\\}\\). The authors also give a conservative formula for computing the variance of \\(\\hat{ATE}^{lasso}\\). When the two lasso regressions select different sets of covariates (which is probably common in practice), this is no longer guaranteed to yield equal or lower asymptotic variance compared to the benchmark.\nAlgorithm:\n\nFor the treatment and control groups separately, run lasso regression of \\(Y\\) on \\(\\tilde{X}\\) go get \\(\\hat{\\beta}^T_{lasso}\\) and \\(\\hat{\\beta}^C_{lasso}\\).\nCalculate the treatment effect estimate \\(\\hat{ATE}^{lasso}\\) using the above formula.\nCalculate the estimate of the variance of \\(\\hat{ATE}^{lasso}\\) using the formula in Blonarz et al.¬†(2015).\n\nThe authors also propose the lasso+OLS estimator which first uses \\(L1\\) regularization as above to select the covariates and then plugs those in OLS to get the treatment effect estimate.\nA similar idea has also been studied by Wager et al (2016). They show that when additionally, assuming Gaussian data (along with a bunch of regularity assumptions), we can use any ‚Äúrisk consistent‚Äù ML estimator such as ridge, elastic net, etc. ‚ÄúRisk consistent‚Äù here means as we give the algorithm more data, it gets closer to the truth. The lower the risk the higher the variance reduction gains compared to the simple difference-in-means estimator. The authors also propose a simple cross-fitting approach to calculate confidence intervals.\nAlgorithm:\n\nSplit the data into \\(k\\) equal sized folds.\nFor each fold \\(k\\):\n\n\ncalculate \\(\\bar{Y}^k, \\tilde{X}^k\\).\nget the coefficients \\(\\hat{\\beta}_{lasso}^{-k}\\) based on regressions on all other \\(k-1\\) folds.\ncombine both quantities and calculate \\(\\hat{ATE}^{lasso}\\).\ncalculate its standard error.\n\n\nGet the final estimates \\(\\hat{ATE}^{lasso}\\) and its standard error by taking weighted averages across all \\(k\\) folds.\n\nThis concludes the discussion of using a ML-type linear regression model to reduce the variance in A/B tests. Let‚Äôs now move on to the second method.\n\n\nUsing ML Regression Indirectly\nAn alternative approach first uses ML to predict \\(Y\\) and then plugs that prediction into an OLS regression of the outcome on the treatment variable. One can then use cross-fitting to do the prediction which ensures the ‚Äúna√Øve‚Äù OLS confidence intervals remain valid. The authors call this procedure MLRATE (machine learning regression-adjusted treatment effect estimator).\nHere is a rough version of the algorithm:\n\nSplit the data in \\(k\\) equal-sized folds.\nFor each fold \\(k\\):\n\n\nPredict Y by applying a ML algorithm to all other \\(k-1\\) folds. Call this prediction \\(\\bar{Y}_k\\).\n\n\nGet a final prediction \\(\\bar{Y}=\\sum_k\\bar{Y}_k\\).\nRun OLS of \\(Y\\) on \\(T\\),$ {Y}_k$ and \\((\\bar{Y}_k-\\bar{Y})\\timesT\\) and use the associated standard errors and \\(p\\)-values."
  },
  {
    "objectID": "blog/DONE-ml-based-adjustments.html#bottom-line",
    "href": "blog/DONE-ml-based-adjustments.html#bottom-line",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nRegression adjustments are a commonly used tool to reduce variance in A/B tests.\nThe machine learning toolbox offers possibly more powerful algorithms in this space.\nThere are two main ML approaches. Both can be shown under certain conditions to be at least as good as the simple difference-in-means estimator.\nThe first approach uses ML regression algorithms directly.\nThe second method, instead, uses ML to predict the outcome and adds that in an OLS regression."
  },
  {
    "objectID": "blog/DONE-ml-based-adjustments.html#references",
    "href": "blog/DONE-ml-based-adjustments.html#references",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "References",
    "text": "References\nBelloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse models. Bernoulli 19(2): 521-547\nBloniarz, A., Liu, H., Zhang, C. H., Sekhon, J. S., & Yu, B. (2016). Lasso adjustments of treatment effect estimates in randomized experiments. Proceedings of the National Academy of Sciences, 113(27), 7383-7390.\nGuo, Y., Coey, D., Konutgan, M., Li, W., Schoener, C., & Goldman, M. (2021). Machine learning for variance reduction in online experiments. Advances in Neural Information Processing Systems, 34, 8637-8648.\nLin, W. (2013). Agnostic notes on regression adjustments to experimental data: Reexamining Freedman‚Äôs critique. Ann. Appl. Stat. 7(1): 295-318\nList, J. A., Muir, I., & Sun, G. K. (2022). Using Machine Learning for Efficient Flexible Regression Adjustment in Economic Experiments (No.¬†w30756). National Bureau of Economic Research.\nNegi, A., & Wooldridge, J. M. (2021). Revisiting regression adjustment in experiments with heterogeneous treatment effects. Econometric Reviews, 40(5), 504-534.\nPoyarkov, A., Drutsa, A., Khalyavin, A., Gusev, G., & Serdyukov, P. (2016, August). Boosted decision tree regression adjustment for variance reduction in online controlled experiments. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.¬†235-244).\nWager, S., Du, W., Taylor, J., & Tibshirani, R. J. (2016). High-dimensional regression adjustments in randomized experiments. Proceedings of the National Academy of Sciences, 113(45), 12673-12678."
  },
  {
    "objectID": "blog/DONE-simpson-paradox.html",
    "href": "blog/DONE-simpson-paradox.html",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Simpson‚Äôs paradox is one of the most counterintuitive phenomena in data analysis. It describes situations where a trend observed within groups disappears‚Äîor even reverses‚Äîwhen the data is aggregated. The underlying cause is often a confounding variable that distorts the overall trend. Let‚Äôs examine a concrete example."
  },
  {
    "objectID": "blog/DONE-simpson-paradox.html#background",
    "href": "blog/DONE-simpson-paradox.html#background",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Simpson‚Äôs paradox is one of the most counterintuitive phenomena in data analysis. It describes situations where a trend observed within groups disappears‚Äîor even reverses‚Äîwhen the data is aggregated. The underlying cause is often a confounding variable that distorts the overall trend. Let‚Äôs examine a concrete example."
  },
  {
    "objectID": "blog/DONE-simpson-paradox.html#a-closer-look",
    "href": "blog/DONE-simpson-paradox.html#a-closer-look",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nAn Example\nImagine two hospitals, \\(A\\) and \\(B\\), treating patients for a particular condition with two treatment options, \\(T_1\\) and \\(T_2\\). Hospital \\(A\\), located in a higher-income neighborhood, primarily receives healthier patients, while Hospital \\(B\\), in a lower-income neighborhood, tends to treat sicker patients. The effectiveness of the treatments is measured as improvement in a continuous health score.\nWe are interested in examining whether one of the treatment options leads to better health outcomes. Consider the following data gathered across both hospitals.\n\n\nHealth improvement by hospital and treatment type. Both treatments \\(T_1\\) and \\(T_2\\) are equally effective within each hospital.\n\n\nHospital\nTreatment\nHealth Improvement\nN\n\n\n\n\nA\n\\(T_1\\)\n20\n90\n\n\nA\n\\(T_2\\)\n20\n10\n\n\nB\n\\(T_1\\)\n10\n10\n\n\nB\n\\(T_2\\)\n10\n90\n\n\n\n\nLet‚Äôs now look at what happens when we combine the data from both hospitals.\n\n\nHealth improvement by treatment type. Treatment \\(T_1\\) is more effective overall.\n\n\nTreatment\nN\nHealth Improvement\n\n\n\n\n\\(T_1\\)\n100\n19 = 20 * .9 + 10 * .1\n\n\n\\(T_2\\)\n100\n11 = 10 * .9 + 20 * .1\n\n\n\n\nWithin each hospital, the data shows that both treatments are equally effective. However, combining the data across both hospitals reveals that treatment \\(T_1\\) appears to be significantly more effective overall. Why does this happen?\nThe confounding variable here is the underlying health status of patients. Hospital \\(A\\) treats mostly healthier patients, while Hospital \\(B\\) handles more severe cases. This difference in patient distribution influences the overall success rates of the treatments, even though both treatments perform identically within each hospital.\n\n\nA Visualization\nTo illustrate, imagine a scatter plot where each dot represents a patient. The color of the dot indicates the treatment they received, and the horizontal lines represent the average health improvement for each group. The vertical axis depicts the outcome variable (health improvement).\n\nIn the aggregated data, \\(T_1\\) shows a higher average improvement, creating the illusion of greater effectiveness. But when disaggregated by hospital, the averages for \\(T_1\\) and \\(T_2\\) are identical.\nYou can find the code to reproduce the figure in this GitHub repo."
  },
  {
    "objectID": "blog/DONE-simpson-paradox.html#a-visualization",
    "href": "blog/DONE-simpson-paradox.html#a-visualization",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "A Visualization",
    "text": "A Visualization\nTo illustrate, imagine a scatter plot where each dot represents a patient. The color of the dot indicates the treatment they received, and the horizontal lines represent the average health improvement for each group. The vertical axis depicts the outcome variable (health improvement).\n[ADD IMAGE]\nIn the aggregated data, \\(T_1\\) shows a higher average improvement, creating the illusion of greater effectiveness. But when disaggregated by hospital, the averages for \\(T_1\\) and \\(T_2\\) are identical.\nYou can find the code to reproduce the figure in this GitHub repo."
  },
  {
    "objectID": "blog/DONE-simpson-paradox.html#where-to-learn-more",
    "href": "blog/DONE-simpson-paradox.html#where-to-learn-more",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nStart with the Wikipedia entry, where you will find all necessary additional resources."
  },
  {
    "objectID": "blog/DONE-simpson-paradox.html#bottom-line",
    "href": "blog/DONE-simpson-paradox.html#bottom-line",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nSimpson‚Äôs paradox manifests when an observable pattern within groups disappears if the data is aggregated.\nIt is a reminder of the critical role confounding variables play in data analysis.\nIt underscores the importance of stratifying data by meaningful subgroups and carefully considering the context before drawing conclusions from aggregated statistics."
  },
  {
    "objectID": "blog/DONE-new-dev-fdr.html",
    "href": "blog/DONE-new-dev-fdr.html",
    "title": "New Developments in False Discovery Rate",
    "section": "",
    "text": "A while back I wrote an article summarizing various approaches to correcting for multiple hypothesis testing. The dominant framework, False Discovery Rate (FDR), controls the share of hypotheses that are incorrectly rejected at a pre-specified level . Its foundations were laid out in 1995 by Benjamini and Hochberg (BH) and to date, their method remains the most popular approach for controlling FDR. Since then, the literature has gone in a few directions.\nOne strand of research generalizes the BH procedure to accommodate cases in which there is a dependency (i.e., correlation) among the hypotheses being tested. Another group of papers makes use of covariates that carry information about whether a given hypothesis is likely to be false. While intuitive in theory, in practice this idea is of limited use as such covariates are not often available.\nFinally, a relatively new class of methods builds on the notion of ‚Äúknockoff‚Äù (or fake) variables and performs variable selection while controlling FDR. The underlying idea is based on creating a fake variable and comparing its test statistics to that of the original variable. Since the fake one is, by definition, null, a small discrepancy between the two test statistics signals the original variable does not belong in the model. The baseline model-X knockoff method requires knowledge of the joint distribution of all covariates. Recent simulations show if this distribution is unknown and misspecified (which in practice it almost always is) there is a loss of statistical power and FDR increase.\nIn this article I will discuss a few new papers which aim to build on and improve the knockoff method. Like knockoffs, they are based on ‚Äúmirror statistics‚Äù, but unlike them they do not require exact knowledge or consistent estimation of any distribution. Specifically, I will discuss Gaussian Mirrors and Data Splitting for FDR control."
  },
  {
    "objectID": "blog/DONE-new-dev-fdr.html#background",
    "href": "blog/DONE-new-dev-fdr.html#background",
    "title": "New Developments in False Discovery Rate",
    "section": "",
    "text": "A while back I wrote an article summarizing various approaches to correcting for multiple hypothesis testing. The dominant framework, False Discovery Rate (FDR), controls the share of hypotheses that are incorrectly rejected at a pre-specified level . Its foundations were laid out in 1995 by Benjamini and Hochberg (BH) and to date, their method remains the most popular approach for controlling FDR. Since then, the literature has gone in a few directions.\nOne strand of research generalizes the BH procedure to accommodate cases in which there is a dependency (i.e., correlation) among the hypotheses being tested. Another group of papers makes use of covariates that carry information about whether a given hypothesis is likely to be false. While intuitive in theory, in practice this idea is of limited use as such covariates are not often available.\nFinally, a relatively new class of methods builds on the notion of ‚Äúknockoff‚Äù (or fake) variables and performs variable selection while controlling FDR. The underlying idea is based on creating a fake variable and comparing its test statistics to that of the original variable. Since the fake one is, by definition, null, a small discrepancy between the two test statistics signals the original variable does not belong in the model. The baseline model-X knockoff method requires knowledge of the joint distribution of all covariates. Recent simulations show if this distribution is unknown and misspecified (which in practice it almost always is) there is a loss of statistical power and FDR increase.\nIn this article I will discuss a few new papers which aim to build on and improve the knockoff method. Like knockoffs, they are based on ‚Äúmirror statistics‚Äù, but unlike them they do not require exact knowledge or consistent estimation of any distribution. Specifically, I will discuss Gaussian Mirrors and Data Splitting for FDR control."
  },
  {
    "objectID": "blog/DONE-new-dev-fdr.html#notation",
    "href": "blog/DONE-new-dev-fdr.html#notation",
    "title": "New Developments in False Discovery Rate",
    "section": "Notation",
    "text": "Notation\nAlthough many of the results generalize to more complex settings, I will work with the simple linear model:\n\\[Y = X\\beta + \\epsilon.\\]\nWe have n observations of an outcome \\(Y\\), and a covariate vector \\(X\\in \\mathbb{R}^p\\) with \\(p &lt; n\\). (Again, some of these results generalize to high-dimensional settings, but let‚Äôs keep it simple here.) I will index the variables in \\(X\\) by \\(j\\). My goal is to find a subset of relevant features from $ X$ while controlling the FDR at some level \\(\\alpha\\). In other words, I wil be testing the series of \\(p\\) null hypotheses of the kind \\(\\beta_j=0\\)."
  },
  {
    "objectID": "blog/DONE-new-dev-fdr.html#a-closer-look",
    "href": "blog/DONE-new-dev-fdr.html#a-closer-look",
    "title": "New Developments in False Discovery Rate",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFDR Control with Mirror Statistics\nThe building block of these methods are the so-called mirror statistics, \\(M_j\\). They have the following two properties:\n\nVariables with larger mirror statistics (\\(M_j\\)‚Äôs) are more likely to be relevant.\nTheir distribution under the null hypothesis is (asymptotically) symmetric around \\(0\\).\n\nThese properties are simple and intuitive. For instance, the commonly used t-statistic for hypothesis testing in the linear model satisfies both. The first property suggests we can order the features and select ones with a mirror statistic exceeding some pre-defined threshold. The second one leads to an approximate upper bound on the number of false positives for any cutoff \\(t\\):\n\\[FDP(t) = \\frac{\\#\\{j: \\text{j is irrelevant, but } M_j &gt; t\\}}{\\# \\{j:M_j &gt; t\\}} \\leq \\frac{\\# \\{j:M_j &lt;- t\\}}{\\# \\{j:M_j &gt; t\\}}. \\]\nNow that we know the mirror statistics‚Äô properties, I will discuss various ways of calculating them.\n\n\nConstructing the Mirror Statistics\nThe mirror statistics \\(M_j\\) take the following general form:\n\\[M_j = sign(\\tilde{\\beta}_j^1, \\tilde{\\beta}_j^2) f(|\\tilde{\\beta}_j^1|, |\\tilde{\\beta}_j^2|),\\]\nwhere the \\(\\tilde{\\beta}\\) denote (standardized) estimates of the true coefficient \\(\\beta\\) and \\(f(\\cdot)\\) is a nonnegative, exchangeable and monotically increasing function. For instance, convenient choices for \\(f(\\cdot)\\) include \\(f(a,b) = 2min(a,b)\\) (Xing et al.¬†2019), \\(f(a,b) = ab\\), and \\(f(a,b) = a+b\\) (Dai et al.¬†2022).\nLet‚Äôs now turn to calculating the \\(\\tilde{\\beta}\\)‚Äôs.\n\n\nConstructing the Regression Coefficients\nThe coefficients \\(\\tilde{\\beta}\\) ought to satisfy the following two conditions:\n\nIndependence ‚Äì The two regression coefficients are (asymptotically) independent.\nSymmetry ‚Äì Under the null hypothesis, the (marginal) distribution of either of the two coefficients is (asymptotically) symmetric around zero.\n\nI will now describe two approaches in constructing them.\n\nMethod #1 ‚Äì Gaussian Mirrors\nSoftware Package: GM.\nThe main idea is to create a set of two perturbed mirror features for each variable \\(X_j\\). Namely,\n\\[X_j^+ = X_j + a_jZ_j, \\text{      and      } X_j^-=X_j -a_jZ_j,\\]\nwhere \\(a_j\\) is a scalar and \\(Z_j \\approx N(0,1)\\). The authors provide some guidance on how to select \\(a_j\\), but I will not get into that here.\nWhile it is possible to generate the mirror features for all columns in \\(X\\) simultaneously, the one-fit-per-feature approach shows better performance in simulations. So, the \\(\\tilde{\\beta}\\) are the estimates of \\(\\beta\\) in the following model:\n\\[ y = \\frac{\\beta_j}{2}X_j^+ +\\frac{\\beta_j}{2}X_j^- + X_{\\text{non-j}}\\beta_{\\text{non-j}} + \\epsilon. \\]\n\n\nMethod #2 ‚Äì Data Splitting\nAn alternative approach for getting two independent coefficient estimates \\(\\tilde{\\beta}\\) is through data splitting. When estimating the linear model, we can get \\(\\tilde{\\beta}^1\\) from one half of the data and $ ^2$ from the other half of the data. While this is simple and intuitive it can result in loss of statistical power. To alleviate this concern, we can do repeated data splitting and aggregate the results in the end. This is reminiscent of the procedure suggested by Meinheusen et al.¬†(2012) in the context of hypothesis testing in for high-dimensional regression. We can then determine the feature importance based on the share of data splits in which it ends up being included. I will omit the technical details here.\nThere is a technical wrinkle I have omitted ‚Äì the regression coefficients have to be standardized so that the \\(M_j\\)‚Äôs have comparable variances across variables. Check the original papers for details on exactly how to do that. Instead, I will now turn to the final algorithm for variable selection with FDR control using the approaches outlined above.\n\n\n\nPutting it All Together\nThis framework sets the stage for the following general algorithm for variable selection with FDR control:\nCalculate the \\(j\\) mirror statistics, \\(M_j\\). Given a FDR level \\(\\alpha\\), set a threshold \\(\\tau(\\alpha)\\) such that \\[\\tau(\\alpha)= min\\{t &gt; 0 : \\hat{FDP}(t) \\leq \\alpha\\}\\]\nSelect the features \\(\\{ j : M_j &gt;  \\tau(\\alpha) \\}\\). In words, given \\(\\alpha\\) calculate the \\(M_j\\)‚Äôs, find the magical threshold \\(\\tau(\\alpha)\\) and include the variables with \\(M_j &gt; \\tau(\\alpha)\\)."
  },
  {
    "objectID": "blog/DONE-new-dev-fdr.html#bottom-line",
    "href": "blog/DONE-new-dev-fdr.html#bottom-line",
    "title": "New Developments in False Discovery Rate",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe Benjamini-Hochberg approach is still the most popular way to control FDR.\nI discuss two novel approaches for variable selection and FDR control aimed at improving the knockoff filter."
  },
  {
    "objectID": "blog/DONE-new-dev-fdr.html#references",
    "href": "blog/DONE-new-dev-fdr.html#references",
    "title": "New Developments in False Discovery Rate",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2018). Controlling the false discovery rate via knockoffs. Annals of Statistics\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300.\nBenjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of statistics, 1165-1188.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2022). False discovery rate control via data splitting. Journal of the American Statistical Association, 1-18.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2023). A scale-free approach for false discovery rate control in generalized linear models. Journal of the American Statistical Association, 1-15.\nIgnatiadis, N., Klaus, B., Zaugg, J. B., & Huber, W. (2016). Data-driven hypothesis weighting increases detection power in genome-scale multiple testing. Nature methods, 13(7), 577-580.\nKorthauer, K., Kimes, P. K., Duvallet, C., Reyes, A., Subramanian, A., Teng, M., ‚Ä¶ & Hicks, S. C. (2019). A practical guide to methods controlling false discoveries in computational biology. Genome biology, 20(1), 1-21.\nScott, J. G., Kelly, R. C., Smith, M. A., Zhou, P., & Kass, R. E. (2015). False discovery rate regression: an application to neural synchrony detection in primary visual cortex. Journal of the American Statistical Association, 110(510), 459-471.\nXing, X., Zhao, Z., & Liu, J. S. (2023). Controlling false discovery rate using gaussian mirrors. Journal of the American Statistical Association, 118(541), 222-241."
  },
  {
    "objectID": "blog/TODO-hypo-testing-linear-ml.html",
    "href": "blog/TODO-hypo-testing-linear-ml.html",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "",
    "text": "Machine learning models are an indispensable part of data science. They are incredibly good at what they are designed for ‚Äì making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, p-values, or anything else related to statistical significance. Why?\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have already selected the most strongly correlated variables.\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The \\(t\\)-stats and \\(p\\)-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models ‚Äì Ridge, Elastic net, SCAD, etc."
  },
  {
    "objectID": "blog/TODO-hypo-testing-linear-ml.html#background",
    "href": "blog/TODO-hypo-testing-linear-ml.html#background",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "",
    "text": "Machine learning models are an indispensable part of data science. They are incredibly good at what they are designed for ‚Äì making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, p-values, or anything else related to statistical significance. Why?\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have already selected the most strongly correlated variables.\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The \\(t\\)-stats and \\(p\\)-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models ‚Äì Ridge, Elastic net, SCAD, etc."
  },
  {
    "objectID": "blog/TODO-hypo-testing-linear-ml.html#notation",
    "href": "blog/TODO-hypo-testing-linear-ml.html#notation",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Notation",
    "text": "Notation\nAs a reminder, \\(\\beta^{lasso}\\) is the solution to:\n\\[\\min_{\\beta} \\frac{1}{2} || Y-x\\beta|| ^2_2 + \\lambda ||\\beta||_1. \\]\nWe are trying to predict a vector \\(Y\\in \\mathbb{R}\\) with a set of features \\(X\\in \\mathbb{R}^{pxn}\\) with \\(p\\leq n\\), and \\(\\lambda\\) is a tuning parameter. When needed, I will use \\(j\\) to index individual columns (i.e., variables) of \\(X\\)."
  },
  {
    "objectID": "blog/TODO-hypo-testing-linear-ml.html#a-closer-look",
    "href": "blog/TODO-hypo-testing-linear-ml.html#a-closer-look",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nTwo Types of Models and Parameters\nWe first need to discuss an important subtlety. There are two distinct ways of thinking about performing hypothesis testing in ML models. The traditional view is that we have a true linear model which includes all variables:\n\\[\\begin{equation} Y=X\\beta_0+\\epsilon. \\end{equation}\\]\nWe are interested in testing whether \\(\\beta_0=0\\) ‚Äì that is, inference on the full model. This is certainly an intuitive target. The interpretation is that this model encapsulates all relevant causal variables for \\(Y\\). Importantly, even if a given variable $X_j $ is not selected, it still belongs to the model and has a meaningful interpretation.\nThere is an alternative way to think about the problem. Imagine we run a variable selection algorithm (e.g., lasso), which selects a subset \\(M=\\{1,\\dots,p\\}\\) of all available predictors (\\(X\\)), \\(M&lt;p\\) leading to the alternative model:\n\\[\\begin{equation} Y=X_M\\beta_M+u. \\end{equation}\\]\nNow we are interested in testing whether \\(\\beta_M=0\\) ‚Äì that is, inference on the selected model. Unlike the scenario above, here \\(\\beta_{Mj}\\) is interpreted as the change in \\(Y\\) for a unit change in \\(X_j\\) when all other variables in \\(X_M\\) (as opposed to all of \\(X\\)) are kept constant.\nWhich of the two targets is more intuitive?\nStatisticians argue vehemently about this. Some claim that the full model interpretation is inherently problematic. It is too na√Øve and perhaps even arrogant to think that (i) mother nature can be explained by a linear equation, (ii) we can measure and include the full set of relevant predictors. On top of this, there are also technical issues with this interpretation beyond the scope of this post.\nTo overcome these challenges, relatively recently, statisticians developed the idea of inference on the selected model. This introduces major technical challenges, however.\n\n\nThe Na√Øve Approach: What Not to Do\nFirst things first ‚Äì here is what we should not do.\n\nRun a Lasso regression.\nRun OLS regression on the subset of selected variables.\nPerform statistical inference with the estimated t-stats, confidence intervals, and p-values.\n\nThis is bad practice. Can you see why?\nIt is simply because we ignored the fact that we already peeked at the data when we ran the Lasso regression. Lasso already chose the variables that are strongly correlated with the outcome. Intuitively, we will need to inflate the \\(p\\)-values to account for the data exploration in the first step.\nIt turns out that, in general, both the finite- and large-sample distributions of these parameters are non-Gaussian and depend on unknown parameters in weird ways. Consequently, the calculated t-stats and p-values are all wrong, and there is little hope that anything simple can be done to save this approach. And no, the standard bootstrap cannot help us either.\nBut are there special cases when this approach might work? A recent paper titled ‚ÄúIn Defense of the Indefensible: A Very Na√Øve Approach to High-Dimensional Inference‚Äù argues that under very strict assumptions on \\(X\\) and \\(\\lambda\\), this method is actually kosher. The reason it works is that in the magical world of those assumptions, the set of variables that Lasso chooses is deterministic, and not random (hence circumventing the issue described above). The resulting estimator is unbiased and asymptotically normal ‚Äì hence hypothesis testing is trivial.\nHere is what we should do instead.\n\n\nThe Classical Approach: Inference on the Full Model\nRoughly speaking, there are at least four ways we can go about doing hypothesis testing for \\(\\beta\\) in equation (1).\n\nData Split\n\nSplit our data into two equal parts.\nRun a Lasso regression on the first part.\nRun OLS on the second part with the selected variables from Step 2.\nPerform inference using the computed \\(t\\)-stats, \\(p\\)-values, and confidence intervals.\n\nThis is simple and intuitive. The problem is that in small samples, the \\(p\\)-values can be quite sensitive to how we split the data in the first step. This is clearly undesirable, as we will be getting different results every time we run this algorithm for no apparent reason.\n\n\nMulti Split\nThis is a modification of the Data Split approach designed to solve the sensitivity issue and increase power.\n\nRepeat \\(B\\) times:\n\n\nReshuffle data.\nRun the Data Split method.\nSave the p-values.\n\n\nAggregate the B \\(p\\)-values into a single final one for each variable.\n\nInstead of splitting the data into two parts only once, we can do it many times, and each time, we get a \\(p\\)-value for every variable. The aggregation goes a long way to solving the instability of the simple data split approach. There is a lot of clever mathematics behind it. For example, there is a complicated expression for aggregating the \\(p\\)-values rather than taking a simple average.\nSoftware Package: hdi.\n\n\nBias Correction\nThis approach tackles the problem from a very different angle. The idea is to directly remove the bias from the na√Øve Lasso procedure without any subsampling or data splitting. Somewhat magically, the resulting estimator is unbiased and asymptotically normally distributed ‚Äì statistical inference is then straightforward.\nThere are multiple versions of this idea, but the general form of these estimators is:\n\\[\\hat{\\beta}^{\\text{bias cor}} = \\hat{\\beta}^{lasso} + \\hat{\\Theta} X'\\epsilon^{lasso},\\]\nWhere \\(\\hat{\\beta}^{lasso}\\) is the lasso estimator and \\(\\epsilon^{lasso}\\) are the residuals. The missing piece is the \\(\\hat{\\Theta}\\) matrix; there are several ways to estimate it depending on the setting. In its simplest form, \\(\\hat{\\Theta}\\) is the inverse of the sample variance-covariance matrix. Other examples include the matrix, which minimizes an error term related to the bias as well as the variance of its Gaussian component. Similar bias-correction methods have been developed for Ridge regression as well.\nSoftware Package: hdi.\n\n\nBootstrap\nAs in many other complicated settings for statistical inference, the bootstrap can come to the rescue. Still, the plain vanilla bootstrap will not do. Instead, here is the general idea of the leading version of the bootstrap estimator for Lasso:\n\nRun a Lasso regression.\nKeep only \\(\\beta^{lasso}\\)‚Äôs larger than some magical threshold.\nCompute the associated residuals and center them around \\(0\\).\nRepeat B times:\n\n\ndraw random samples of these centered residuals,\ncompute new responses \\(\\dot{Y}\\) by adding them to the predictions \\(X'\\beta^{lasso}\\), and\nobtain \\(\\beta^{lasso}\\) coefficients from Lasso regressions on these new responses \\(\\dot{Y}\\).\n\n\nUse the distribution of the obtained coefficients to conduct statistical inference.\n\nThis idea can be generalized to other settings and, for instance, be combined with the bias-corrected estimator.\nThis wraps up our discussion of methods for performing hypothesis testing on equation (1) (i.e., the full model). We now move on to a more challenging topic ‚Äì inference on equation (2) (i.e., the selected model).\n\n\n\nThe Novel Approach: Inference on the Selected Model\n\nPoSI (Post Selection Inference)\nThe goal of the PoSI method is to construct confidence intervals that are valid regardless of the variable selection method and the selected submodel. The benefit is that we would be reaching the correct conclusion even if we did not select the true model. This luxury comes at the expense of often being too conservative (i.e., confidence intervals are ‚Äútoo wide‚Äù). Let me explain how this is done.\nTo take a step back, most confidence intervals in statistics take the form:\n\\[\\hat{\\beta} \\pm m \\times \\hat{SE}(\\hat{\\beta}).\\]\nEvery data scientist has seen a similar formula before. The question is usually one about choosing the constant \\(m\\). When we work with two-sided hypotheses tests and large samples, we often use \\(m = 1.96\\) because this is roughly the \\(97.5\\)th percentile of the \\(t\\)-distribution with many degrees of freedom. This gives a \\(2.5\\%\\) false positive error on both tails of the distribution (\\(5\\%\\) in total) and hence the associated 95% confidence intervals. The larger m, the wider or more conservative the confidence interval.\nThere are a few ways to choose the constant m in the PoSI world. Vaguely speaking, PoSI says we should select this constant to equal the \\(97.5\\)th percentile of a distribution related to the largest \\(t\\)-statistic among all possible models. This is usually approximated with Monte Carlo simulations. Interestingly, we do not need the response variable to approximate the value.\n\\[m = \\max_{\\text{ \\{all models and vars\\} }} |t|.\\]\nAnother and even more conservative choice for \\(m\\) is the Scheffe constant.\n\\[m^{scheffe} = \\sqrt{rank(X) \\times F(rank(X),  n-p)},\\]\nwhere \\(F(\\dot)\\) denotes the \\(95\\)th percentile of the \\(F\\) distribution with the respective degrees of freedom.\nUnfortunately, you guessed correctly that this method does not scale well. In some sense, it is a ‚Äúbrute force‚Äù method that scans through all possible model combinations and all variables within each model and picks the most conservative value. The authors recommend this procedure for datasets with roughly \\(p&lt;20\\). This rules out many practical applications where machine learning is most useful.\nSoftware Package: PoSI\n\n\nEPoSI (Exact PoSI)\nOk, the name of this approach is not super original. The ‚ÄúE‚Äù here stands for ‚Äúexact.‚Äù Unlike its cousin, this approach is valid only in the selected submodel. Because we cover fewer scenarios, the intervals will generally be narrower than the PoSI ones. EPoSI produces valid finite sample (as opposed to asymptotic) confidence intervals and \\(p\\)-values. Like all methods described here, the math behind this is extremely technical. So, I will give you only a high-level description of how this works.\nThe idea is first to get the conditional distribution of \\(\\beta\\) given the selected model. A bit magically, it turns out it is a truncated normal distribution. Really, who would have guessed this? Do you even remember truncated probability distributions (hint: they are just like regular PDFs but bounded from at least one side. This requires further scaling so that the density area sums to 1.)?\nTo dig one layer deeper, the authors show that the selection of Lasso predictors can be recast as a ‚Äúpolyhedral region‚Äù of the form:\n\\[ AY\\leq b. \\]\nIn English, for fixed \\(X\\) and \\(\\lambda\\), the set of alternative outcome values \\(Y^*\\) which yields the same set of selected predictors, can be expressed by the simple inequality above. In it, \\(A\\) and \\(b\\) depend do not depend on \\(Y\\). Under this new result, the distribution of \\(\\hat{\\beta}_M^{\\text{EPoSI}}\\) is now well-understood and tractable, thus enabling valid hypothesis testing.\nThen, we can use the conditional distribution function to construct a whimsical test statistic that is uniformly distributed on the \\([0,1]\\) interval. And we can finally build confidence intervals based on that statistic.\nSelective inference is currently among the hottest topic in all of statistics. There have been a myriad of extensions and improvements on the original paper. Still, this literature is painstakingly technical. What is the polyhedral selection property or a union of polytopes?\nSoftware Package: selectiveInference."
  },
  {
    "objectID": "blog/TODO-hypo-testing-linear-ml.html#an-example",
    "href": "blog/TODO-hypo-testing-linear-ml.html#an-example",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Refer to the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\nUnlike in Monte Carlo simulations, I do not know the ground truth here, so this exercise is not informative about which approaches work well and which do not. Rather, it just serves as an illustrative example.\nHere is a table displaying the number of statistically significant variables with \\(p &lt; .05\\) for various inference methods.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Number of Statistically Significant Predictors (p &lt; .05)\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n# vars p &lt; .05\n7\n5\n2\n3\n2\n2\n\n\n\nAs expected, the naive method results in the smallest \\(p\\)-values and hence the highest number of significant predictors ‚Äì seven. Data Split knocks down two of those seven variables, resulting in five significant ones. The rest are more conservative, leaving only two or three features with \\(p &lt; .05\\).\nBelow is the table with \\(p\\)-values for all variables and each method.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: p-values\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.01\n1.00\n\n\nage\n0.00\n0.04\n1.00\n0.01\n1.00\n1.00\n\n\nsibsp\n0.02\n0.03\n1.00\n0.21\n1.00\n1.00\n\n\nparch\n0.32\n0.64\n1.00\n1.00\n1.00\n1.00\n\n\nfare\n0.20\n0.21\n1.00\n1.00\n1.00\n1.00\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n\n\nembarkedS\n0.00\n0.01\n1.00\n0.06\n-\n1.00\n\n\ncabinA\n0.39\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinB\n0.35\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinD\n0.05\n0.27\n1.00\n0.44\n1.00\n-\n\n\ncabinE\n0.00\n0.64\n1.00\n0.06\n1.00\n1.00\n\n\ncabinF\n0.02\n0.52\n1.00\n0.21\n1.00\n1.00\n\n\nembarkedC\n-\n-\n1.00\n0.11\n1.00\n1.00\n\n\nembarkedQ\n-\n-\n1.00\n1.00\n1.00\n0.00\n\n\ncabinC\n-\n-\n1.00\n1.00\n1.00\n-\n\n\n\nYou can find the code for this exercise in this GitHub repo."
  },
  {
    "objectID": "blog/TODO-hypo-testing-linear-ml.html#bottom-line",
    "href": "blog/TODO-hypo-testing-linear-ml.html#bottom-line",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods mine through datasets to find strong correlations between the response and the features. Quantifying this strength is an open and challenging problem.\nThe naive approach to hypothesis testing is usually invalid.\nThere are two main approaches that work ‚Äì inference on the full model or on the selected model. The latter poses more technical challenges than the former.\nIf we are interested in the full model, the Multi Split approach is general enough to cover a wide range of models and settings.\nIf we believe you have to focus on the selected model, EPoSI is the state-of-the-art.\nSimulation exercises usually show no clear winner, as none of the methods consistently outperforms the rest."
  },
  {
    "objectID": "blog/TODO-hypo-testing-linear-ml.html#where-to-learn-more",
    "href": "blog/TODO-hypo-testing-linear-ml.html#where-to-learn-more",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nTaylor and Tibshirani (2015) give a non-technical introduction to the problem space along with a description of the POSI method ‚Äì a great read but focused on a single approach. Other studies both provide a relatively accessible overview of the various methods for statistical inference on the full model. For technical readers, Zhang et al.¬†(2022) provide an excellent up-to-date review of the literature, which I used extensively."
  },
  {
    "objectID": "blog/TODO-hypo-testing-linear-ml.html#references",
    "href": "blog/TODO-hypo-testing-linear-ml.html#references",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "References",
    "text": "References\nBerk, R., Brown, L., Buja, A., Zhang, K., & Zhao, L. (2013). Valid post-selection inference. The Annals of Statistics, 802-837.\nB√ºhlmann, P. (2013). Statistical significance in high-dimensional linear models. Bernoulli, 19(4), 1212-1242.\nDezeure, R., B√ºhlmann, P., Meier, L., & Meinshausen, N. (2015). High-dimensional inference: confidence intervals, p-values and R-software hdi. Statistical Science, 533-558.\nJavanmard, A., & Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15(1), 2869-2909.\nLee, J. D., Sun, D. L., Sun, Y., & Taylor, J. E. (2016). Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3), 907-927.\nLeeb, H., & P√∂tscher, B. M. (2005). Model selection and inference: Facts and fiction. Econometric Theory, 21(1), 21-59.\nLeeb, H., P√∂tscher, B. M., & Ewald, K. (2015). On various confidence intervals post-model-selection. Statistical Science, 30(2), 216-227.\nMeinshausen, N., Meier, L., & B√ºhlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\nTaylor, J., & Tibshirani, R. J. (2015). Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112(25), 7629-7634.\nVan de Geer, S., B√ºhlmann, P., Ritov, Y. A., & Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. The Annals of Statistics, 42(3), 1166-1202.\nWasserman, L., & Roeder, K. (2009). High dimensional variable selection. Annals of Statistics, 37(5A), 2178.\nZhang, D., Khalili, A., & Asgharian, M. (2022). Post-model-selection inference in linear regression models: An integrated review. Statistics Surveys, 16, 86-136.\nZhang, C. H., & Zhang, S. S. (2014). Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 217-242.\nZhao, S., Witten, D., & Shojaie, A. (2021). In defense of the indefensible: A very naive approach to high-dimensional inference. Statistical Science, 36(4), 562-577."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html",
    "href": "blog/TODO-multiple-testing-overview.html",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "",
    "text": "The abundance of data around us is a major factor making the data science field so attractive. It enables all kinds of impactful, interesting, or fun analyses. I admit this is what got me into statistics and causal inference in the first place. However, this luxury has downsides ‚Äì it might require deeper methodological knowledge and attention.\nIf we are interested in measuring statistical significance, the standard frequentist framework relies on testing a single hypothesis on any given dataset. Once we start using the same data multiple times ‚Äì i.e., testing several hypotheses at once ‚Äì as we often do, we might have to make some additional adjustments.\nIn this article, I will walk you through the most popular multiple hypotheses (MH) testing corrections. Common examples of when these come to play include estimating the impact of several variables on a single outcome or estimating the effect of an intervention on several subgroups of users to mine for heterogeneous treatment effects. Both scenarios are ubiquitous in most data analysis projects.\nTo illustrate the problem, imagine we are testing seven hypotheses at the \\(5\\%\\) significance level. Then, the probability that we incorrectly reject at least one of these hypotheses is:\n\\[ 1 - (1-.05)^7 = .30. \\]\nThat is, roughly one in three times, we will reach a wrong conclusion.\nVaguely speaking, MH adjustments can be viewed as inflating the calculated p-values and hence decreasing the probability that we reject any given hypothesis. In other words, we are accounting for using the data multiple times by making the definition of statistical significance more stringent."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#background",
    "href": "blog/TODO-multiple-testing-overview.html#background",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "",
    "text": "The abundance of data around us is a major factor making the data science field so attractive. It enables all kinds of impactful, interesting, or fun analyses. I admit this is what got me into statistics and causal inference in the first place. However, this luxury has downsides ‚Äì it might require deeper methodological knowledge and attention.\nIf we are interested in measuring statistical significance, the standard frequentist framework relies on testing a single hypothesis on any given dataset. Once we start using the same data multiple times ‚Äì i.e., testing several hypotheses at once ‚Äì as we often do, we might have to make some additional adjustments.\nIn this article, I will walk you through the most popular multiple hypotheses (MH) testing corrections. Common examples of when these come to play include estimating the impact of several variables on a single outcome or estimating the effect of an intervention on several subgroups of users to mine for heterogeneous treatment effects. Both scenarios are ubiquitous in most data analysis projects.\nTo illustrate the problem, imagine we are testing seven hypotheses at the \\(5\\%\\) significance level. Then, the probability that we incorrectly reject at least one of these hypotheses is:\n\\[ 1 - (1-.05)^7 = .30. \\]\nThat is, roughly one in three times, we will reach a wrong conclusion.\nVaguely speaking, MH adjustments can be viewed as inflating the calculated p-values and hence decreasing the probability that we reject any given hypothesis. In other words, we are accounting for using the data multiple times by making the definition of statistical significance more stringent."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#when-are-multiple-hypothesis-adjustments-not-necessary",
    "href": "blog/TODO-multiple-testing-overview.html#when-are-multiple-hypothesis-adjustments-not-necessary",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "When are Multiple Hypothesis Adjustments NOT Necessary",
    "text": "When are Multiple Hypothesis Adjustments NOT Necessary\nMultiple hypothesis adjustments are not required when:\n\nwe are interested in a purely predictive model, such as in machine learning settings,\nwe have an independent dataset for each correlated hypothesis,\nthe dependent variables we are testing are not correlated. An example is testing the impact of a new feature or intervention on a set of outcomes unrelated to each other ‚Äì e.g., health and education variables. In such cases, it is common to adjust for MH separately within the health- and education-related hypotheses.\n\nBefore we dive into the methods, let‚Äôs establish some basic terminology and notation."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#notation",
    "href": "blog/TODO-multiple-testing-overview.html#notation",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Notation",
    "text": "Notation\nWe are interested in testing a bunch of m null hypotheses about a population parameter \\(\\beta\\).\nAs a running example, you can think of \\(\\beta\\) as the causal impact of a new product feature on user engagement and \\(m\\) as indexing some geographical regions such as cities. We are interested in whether the new feature is more impactful in some cities than others. We will denote these hypotheses with \\(H_1, H_2, \\dots, H_m\\) and refer to their associated p-values with \\(p_1, p_2, \\dots, p_m\\).\nWe use \\(\\alpha\\) to denote the probability of a Type 1 error ‚Äì rejecting a true null (i.e., a false positive). In technical jargon, we refer to \\(\\alpha\\) as test size. We often choose \\(\\alpha=.05\\), meaning that we are allowing a 5% chance that we will make such an error. This corresponds to the 95% confidence intervals that we often see.\nNext, statistical power is the probability of correctly rejecting a false null hypothesis (i.e., a true positive). This is a desirable property ‚Äì the higher it is, the better. While this terminology does not describe the entire hypothesis testing framework, it does cover what is necessary to continue reading the article."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#two-types-of-multiple-hypotheses-adjustments",
    "href": "blog/TODO-multiple-testing-overview.html#two-types-of-multiple-hypotheses-adjustments",
    "title": "Multiple Testing: Methods Overview",
    "section": "Two Types of Multiple Hypotheses Adjustments",
    "text": "Two Types of Multiple Hypotheses Adjustments\nIn the MH world, there are two distinct types of targets that data scientists are after.\nThe first one was introduced in the 1950s by the famous statistician John Tukey and is called Familywise Error Rate (FWER). FWER is the probability of making at least one type 1 error:\n\\[ FWER = \\mathbb{P} (\\text{At Least 1 Type 1 Error}). \\]\nIn the example from the introduction, \\(FWER = 0.3\\).\nControlling the FWER is good, but sometimes it is not great. It does what it is supposed to do ‚Äì limit the number of false positives, but it is often too conservative. In practice, too few variables remain significant after such FWER adjustments, especially when testing many (as opposed to just a few) hypotheses.\nTo correct this, in the 1990s, Yoav Benjamini and Yosef Hochberg, conceptualized controlling the so-called False Discovery Rate (FDR). FDR is the expected proportion of false positives:\n\\[ FDR = \\mathbb{E} \\left[ \\frac{\\text{\\# False Positives}}{\\text{\\# False Positives + \\# True Positives} } \\right]. \\]\nTake a minute to notice the significant difference between these two approaches. FWER controls the probability of having at least one false positive at \\(\\alpha\\). At the same time, FDR is okay with having several false positives as long as they are (on average) less than \\(\\alpha\\) of all hypotheses.\nFWER is more conservative than FDR, resulting in fewer significant variables. In our example, applying an FWER adjustment as opposed to an FDR one will lead to fewer statistically significant differences in the impact of the new feature across various cities.\nLet‚Äôs go over the most popular ways to control the FWER."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#the-classic-approach-fwer-control",
    "href": "blog/TODO-multiple-testing-overview.html#the-classic-approach-fwer-control",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "The Classic Approach: FWER Control",
    "text": "The Classic Approach: FWER Control\n\nBonferroni‚Äôs Procedure\nYou might have heard of this one. The procedure is extremely simple ‚Äì it goes like this:\n\nreject \\(H_i\\) if \\(p_i \\times m  \\leq \\alpha\\) for all \\(i\\).\n\nIn words, we multiply all \\(p\\)-values by m and reject the hypotheses with adjusted \\(p\\)-values that are smaller than or equal to \\(\\alpha\\).\nThe Bonferroni adjustment is often considered to control the FWER, but it actually controls an even more conservative target ‚Äì the per-family error rate (PFER). PFER is the sum of probabilities of Type 1 error for all the hypotheses.\nYou correctly guessed that PFER is considered too strict. Although it is common in practice, there are better alternatives. While the Bonferroni correction is simple and elegant, it is always dominated by the Holm-Bonferroni procedure, so there is really no reason ever to use it.\n\n\nHolm & Bonferroni‚Äôs Procedure\nThis adjustment offers greater statistical power than the Bonferroni method regardless of the test size without significantly compromising on simplicity. In algorithmic language:\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m.\\)\nReject the null hypothesis \\(H_1,\\dots H_{k-1}\\), where \\(k\\) is the smallest index for which \\(p_i \\times (m+1-k) \\leq \\alpha\\). Much like the Bonferroni method with a multiplication factor of (\\(m+1-k\\)) instead of \\(m\\).\n\nThese two procedures control the FWER by assuming a kind of a ‚Äòworst-case‚Äô scenario in which the \\(p\\)-values are close to being independent of each other. To see why this is the case, imagine the extreme opposite scenario in which there is perfect positive dependence. Then, there is effectively one test statistic, and FWER adjustment is unnecessary.\nIn practice, however, the \\(p\\)-values are often correlated, and exploiting this dependence can result in even more powerful FWER corrections. This can be achieved with various resampling methods, such as the bootstrap or permutation procedures.\n\n\nWestfall & Young‚Äòs Procedure\nThe Westfall & Young method, developed in the 1990s, was the first resampling procedure controlling the FWER. It uses the bootstrap to account for the dependence structure among the \\(p\\)-values and improves on Holm-Bonferroni‚Äôs method.\nIt goes roughly like this:\n\nOrder the \\(m\\) \\(p\\)-values in ascending order ‚Äì \\(p_1, \\dots, p_M\\).\nBegin with a \\(COUNTER_i = 0\\) for each hypothesis \\(i\\).\nTake a bootstrap sample and compute the \\(m\\) \\(p\\)-values (\\(p^*_1, \\dots, p^*_m\\))\nDefine the successive minima by starting with the largest \\(p\\)-value (\\(\\tilde{p}_M=p^*_M\\)) and for each subsequent hypothesis \\(i\\) taking the minimum between \\(\\tilde{p}_{i+1}\\) and \\(p^*_i\\).\n\n\nThis gives a list of \\(p\\)-values \\(\\tilde{p}_1, \\dots, \\tilde{p}_m\\).\n\n\nIf \\(\\tilde{p}_i \\leq p_i\\) add 1 unit to \\(COUNTER_i\\) for each \\(i\\).\nRepeat steps 3-5 above \\(B\\) times and compute the fraction \\(\\hat{p}_i = COUNT_i/B\\).\n\n\nThis gives a list of \\(p\\)-values \\(\\hat{p}_1, \\dots, \\hat{p}_m\\).\n\n\nEnforce monotonicity by starting with \\(\\hat{p}_1\\) and for each subsequent hypothesis \\(i\\) taking the maximum between \\(\\hat{p}_{i-1}\\) and \\(\\hat{p}_i\\).\n\n\nThis final vector presents the FWER-adjusted \\(p\\)-values.\n\n\nReject all null hypotheses \\(i\\) for which \\(\\hat{p}\\leq \\alpha\\).\n\nYes, this is a complicated procedure. But it does offer substantial improvements in power, especially if the hypotheses are positively correlated.\nImportantly, Westfall and Young‚Äôs method rests on a critical assumption called ‚Äúsubset pivotality.‚Äù Roughly speaking, this condition requires that the joint distribution of any subset of test statistics does not depend on whether the remaining hypotheses are true or not.\nSoftware Package: multtest.\n\n\nRomano & Wolf‚Äòs Procedure(s)\nThis procedure improves on Westfall and Young‚Äôs method by not requiring a pivotality assumption. Here I am describing the method from Romano and Wolf‚Äôs 2005 Econometrica paper, although the authors have developed a few closely related FWER adjustments. This is the rough algorithm for it:\n\nOrder the test statistics in descending order, \\(t_1 \\geq t_2 \\geq, \\dots, t_m\\).\nTake B bootstrap samples, and for each hypothesis i compute the empirical distribution function of the successive maximum test statistic random variable.\n\n\nThat is, the random variable of the maximum test statistic between \\(t_i, t_{i+1}, \\dots, t_m\\).\nCall this distribution \\(c(i)\\).\n\n\nReject \\(H_i\\) if \\(t_i\\) is greater than the 1-quantile of \\(c(1)\\).\nDenote by h the number of rejected hypotheses. If \\(h=0\\) stop. Otherwise, for \\(H_{h+1},\\dots H_s\\):\nReject \\(H_i\\) if \\(t_i &gt; c(h+1)\\)\nIterate until no further hypotheses are rejected.\n\nAnd you thought the Westfall and Young correction was tricky. The good news is that there are software packages for all these adjustments, so you won‚Äôt have to program them yourself. Arguably, this one of the best ways to control FWER; does not rest on pivotality assumptions. However, it can be difficult to explain to non-technical audiences; difficult to explain to anyone, really.\nSoftware Package: hdm.\nThis wraps up the descriptions of the methods controlling the FWER."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#the-modern-approach-fdr-control",
    "href": "blog/TODO-multiple-testing-overview.html#the-modern-approach-fdr-control",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "The Modern Approach: FDR Control",
    "text": "The Modern Approach: FDR Control\nAs a reminder, FDR procedures have greater power at the cost of increased rates of type 1 errors compared to FWER adjustments. This is the dominant framework in the modern literature on MH adjustment.\n\nBenjamini & Hochberg‚Äòs Procedure\nThe Benjamini-Hochberg (BH) method is simple:\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m}{k} \\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\nBH is valid when the tests are independent and in some, but not all, dependence scenarios. In general, FDR control is tricky with arbitrary dependence.\nThis adjustment has an interesting geometric interpretation:\n\nplot \\(p\\) versus \\(k\\),\ndraw a line through the origin with a slope equal to \\(frac{\\alpha}{m}\\),\nreject all hypotheses associated with the \\(p\\)-values on the left up to the last point below this line.\n\n\n\nBenjamini & Yekutieli‚Äòs Procedure\nBenjamini and Yekutieli (BY) developed a refinement of the BH procedure, which adds a magical constant \\(c(m)\\):\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m.c(m)}{k}\\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\nThe obvious question is in choosing the optimal value for \\(c(m)\\). You can view BH as a special case of BY with \\(c(m)=1\\). The authors show that the following choice works under any arbitrary dependence assumption:\n\\[ c(m) = \\sum_{i=1}^m \\frac{1}{i} \\approx log(m) + 0.57721 + \\frac{1}{2m}. \\]\nFascinating. For most applications, this simple procedure provides a credible way of taming the false positives rate. There are improvements to this method, but it feels like we are entering a world in which we get theoretical gains with only limited practical implications.\nThe main way to improve on BH/BY is to know (or estimate) the share of false hypotheses among the ones we are testing. This, however, goes beyond the scope of the article.\n\n\nKnockoffs\nWhat if we would like to combine variable selection and control the FDR? BH and BY do not really tell us how to do that. Enter knockoffs. The knockoffs approach is the kid on the block, and it does more than just FDR control.\nThe idea behind the knockoffs is novel and unrelated to any of the methods I discussed above. Knockoffs are copies of the variables with specific properties. In particular, for each feature/covariate variable \\(X\\), we create a knockoff copy \\(\\tilde{X}\\) such that the correlation structure between any two knockoff variables is the same as between their original counterparts. These knockoffs are constructed without looking at the outcome variable; hence, they are unrelated to it by design. Consequently, we can gauge whether each variable is statistically significant by comparing its test statistic to that of the one for its knockoff copy.\nKnockoffs are among the most active fields of research in statistics, with new papers coming out daily (ok, maybe weekly). These methods come in two flavors.\nThe fixed-design knockoff filter controls FDR for low-dimensional linear models regardless of the dependency structure of the features. This is great, but it does not work in high dimensions where variable selection is usually most needed.\nThe model-X knockoff filter solves this issue but requires exact knowledge of the joint distribution of the features. This is often infeasible in practice; even estimating that distribution in high dimensions can be really challenging as the curse of dimensionality lays its hammer on the data scientist.\nThe knockoff idea presents a big step forward because it solves two statistical problems at the same time ‚Äì selecting among a wide set of predictors and controlling the FDR. I am really excited to see where the current research will take us next.\nSoftware Package: knockoff.\n\n\nOther Modern Methods\nThere is certainly no shortage of statistical methods in this field. Each week, there is at least one new paper dealing with some aspects of FDR or FWER control. Knockoffs, in particular, have gotten a lot of attention recently, with researchers extending the idea to all kinds of settings and models.\nI will briefly mention some of the newer methods that have caught my eye in recent months. Many of these control the FDR asymptotically, and some are valid under general dependency structures among the test statistics.\nOne recent variation of the knockoffs idea uses Gaussian Mirrors; while others rely on subsampling approaches ‚Äì Data Splitting (based on estimating two independent regression coefficients after splitting the dataset in half) and Data Aggregation. Clever ideas to improve power include using covariate information to optimally weigh hypotheses in the BH framework or incorporating them into FDR regressions.\nTime will show whether any of these newcomers will come to dominate in practice."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#an-example",
    "href": "blog/TODO-multiple-testing-overview.html#an-example",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Check the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\n\n\n\nNumber of Statistically Significant Variables\n\n\nHere is a table of the \\(p\\)-values for each feature and various MH adjustments.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaw\nBonferroni\nHolm\nRom.-Wolf\nBenj.-Hoch.\nBenj.-Yek.\n\n\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nage\n0.00\n0.02\n0.01\n0.01\n0.00\n0.01\n\n\nsibsp\n0.02\n0.34\n0.18\n0.16\n0.04\n0.14\n\n\nparch\n0.32\n1.00\n1.00\n0.77\n0.40\n1.00\n\n\nfare\n0.22\n1.00\n1.00\n0.66\n0.30\n0.98\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nembarkedC\n0.01\n0.23\n0.15\n0.14\n0.04\n0.13\n\n\nembarkedQ\n0.05\n0.76\n0.35\n0.29\n0.09\n0.28\n\n\ncabinA\n0.39\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinB\n0.37\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinC\n0.92\n1.00\n1.00\n0.92\n0.92\n1.00\n\n\ncabinD\n0.06\n0.89\n0.36\n0.29\n0.09\n0.30\n\n\ncabinE\n0.01\n0.07\n0.05\n0.05\n0.01\n0.05\n\n\ncabinF\n0.02\n0.31\n0.18\n0.16\n0.04\n0.14\n\n\n\nEight variables were statistically significant without any MH adjustment. As expected, the FWER adjustments lead to fewer significant variables than the FDR ones. Interestingly, there is a noticeable difference between the two FDR methods ‚Äì BH and BY, with the latter being more conservative.\nYou can find the code for this analysis in this GitHub repository."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#bottom-line",
    "href": "blog/TODO-multiple-testing-overview.html#bottom-line",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMultiple testing corrections are likely necessary when you are using a single dataset multiple times (i.e., testing multiple hypotheses).\nTwo major frameworks exist ‚Äì FWER and FDR control. The former is often considered too conservative, while the latter is the dominant way researchers and practitioners think about correcting for MH testing.\nIn most settings, the Benjamini-Yekuiteli approach offers a great balance between statistical power and technical simplicity.\nKnockoffs are a novel and exciting approach to FDR control."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#where-to-learn-more",
    "href": "blog/TODO-multiple-testing-overview.html#where-to-learn-more",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great resource on the classic approaches to tackling MH issues (both for FDR and FWER control), but it lacks material on modern methodologies. Emmanuel Cand√®s‚Äô website features an accessible introduction to the world of knockoffs. Clarke et al.¬†(2020) strip down many technicalities and provide accessible descriptions of both Westfall and Young‚Äôs, as well as Romano and Wolf‚Äôs methods. Korthauer et al.¬†(2019) compare some of the more recent approaches to controlling the FDR, which are beyond the scope of this blog post."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#references",
    "href": "blog/TODO-multiple-testing-overview.html#references",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics, 43(5), 2055-2085.\nBenjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of Statistics, 1165-1188.\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300.\nCandes, E., Fan, Y., Janson, L., & Lv, J. (2018). Panning for gold:‚Äômodel‚ÄêX‚Äôknockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3), 551-577.\nClarke, D., Romano, J. P., & Wolf, M. (2020). The Romano‚ÄìWolf multiple-hypothesis correction in Stata. The Stata Journal, 20(4), 812-843.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2022). False discovery rate control via data splitting. Journal of the American Statistical Association, 1-38.\nKorthauer, K., Kimes, P. K., Duvallet, C., Reyes, A., Subramanian, A., Teng, M., ‚Ä¶ & Hicks, S. C. (2019). A practical guide to methods controlling false discoveries in computational biology. Genome biology, 20(1), 1-21.\nRomano, J. P., & Wolf, M. (2005). Stepwise multiple testing as formalized data snooping. Econometrica, 73(4), 1237-1282.\nRomano, J. P., & Wolf, M. (2005). Exact and approximate stepdown methods for multiple hypothesis testing. Journal of the American Statistical Association, 100(469), 94-108.\nWestfall, P. H., & Young, S. S. (1993). Resampling-based multiple testing: Examples and methods for p-value adjustment (Vol. 279). John Wiley & Sons.\nXing, X., Zhao, Z., & Liu, J. S. (2021). Controlling false discovery rate using gaussian mirrors. Journal of the American Statistical Association, 1-20."
  },
  {
    "objectID": "blog/TODO-alphabet-het-treatment-effects.html",
    "href": "blog/TODO-alphabet-het-treatment-effects.html",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "",
    "text": "Numerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods ‚Äì the so-called S, T, X and R learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as the lasso, gradient boosting, or even neural networks. In some clever sense, STXR is the ABC of heterogeneous treatment effect estimation.\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The CausalML Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them."
  },
  {
    "objectID": "blog/TODO-alphabet-het-treatment-effects.html#background",
    "href": "blog/TODO-alphabet-het-treatment-effects.html#background",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "",
    "text": "Numerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods ‚Äì the so-called S, T, X and R learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as the lasso, gradient boosting, or even neural networks. In some clever sense, STXR is the ABC of heterogeneous treatment effect estimation.\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The CausalML Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them."
  },
  {
    "objectID": "blog/TODO-alphabet-het-treatment-effects.html#notation",
    "href": "blog/TODO-alphabet-het-treatment-effects.html#notation",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Notation",
    "text": "Notation\nAs usual, let‚Äôs begin by setting some mathematical notation. I use D to denote a binary treatment indicator, \\(Y\\) is the observed outcome and \\(X\\) is a covariate of interest. The potential outcomes under each treatment state are \\(Y(0)\\) and \\(Y(1)\\), and \\(p\\) is the (conditional) probability of assignment into the treatment (i.e., the propensity score).\nThe average treatment effect is then the mean difference in potential outcomes across all units:\n\\[ATE = E[Y(1)-E(0)].\\]\nInterest is, instead, in the ATE for units with values \\(X=x\\) which I refer to as the heterogeneous treatment effect, \\(\\text{HTE}(X)\\):\n\\[\\text{HTE}(X) = E[Y(1)-E(0)|X].\\]\nIt is also helpful to define the conditional outcome functions under each treatment state:\n\\[\\mu(X,d) = E[Y(d)|X].\\]\nIt then follows that \\(\\text{HTE}(X)\\) can also be expressed as:\n\\[\\text{HTE}(X) = \\mu(X,1) - \\mu(X,0).\\]"
  },
  {
    "objectID": "blog/TODO-alphabet-het-treatment-effects.html#diving-deeper",
    "href": "blog/TODO-alphabet-het-treatment-effects.html#diving-deeper",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nI will now briefly describe the four learners for heterogeneous treatment effects in ascending order of complexity.\n\n\\(S\\)(ingle) Learner\nThe idea behind the S learner is to estimate a single outcome function (X,D) and then calculate (X) by taking the difference in the predicted values between the units in the treatment and control groups.\nAlgorithm:\n\nUse the entire sample to estimate \\(\\hat{\\mu}(X,D)\\).\nCompute \\(\\hat{\\text{HTE}}(X)=\\hat{\\mu}(X,1)-\\hat{\\mu}(X,0)\\).\n\nThis is intuitive and fine. But the problem is that the treatment variable D might be excluded in step 1 when it is not highly correlated with the outcome. (Note that in observational data this does not necessarily imply a null treatment effect.) In this case, we cannot even move to step 2.\n\n\n\\(T\\)(wo) Learner\nThe T learner solves the above problem by forcing the response models to include D. The idea is to first estimate two separate (conditional) outcome functions ‚Äì one for the treatment and one for the control and proceed similarly.\nAlgorithm:\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for \\(\\hat{\\mu}(X,1)\\).\nThen \\(\\hat{\\text{HTE}}(X)=\\hat{\\mu}(X,1)- \\hat{\\mu}(X,0)\\)\n\nThis is better, but still not great. A potential problem is when there are different number of observations in the treatment and control groups. For instance, in the common case where the treatment group is much smaller, their \\(\\text{HTE}(X)\\) will be estimated much less precisely than the that of the control group. When combining the two to arrive at a final estimate of \\(\\text{HTE}(X)\\) the former should get a smaller weight than the latter. This is because the ML algorithms optimize for learning the \\(\\mu(\\cdot)\\) functions, and not the \\(\\text{HTE}(X)\\) function directly.\n\n\n\\(X\\) Learner\nThe \\(X\\) learner is designed to overcome the above concern. The procedure starts similarly to the T learner but then weighs differently the \\(\\text{HTE}(X)\\)‚Äôs for the treatment and control groups.\nAlgorithm:\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for $ (X,1)$.\nEstimate the unit-level treatment effect for the observations in the control group, \\(\\hat{\\mu}(X,1)-Y\\), and for the treatment group, \\(Y-\\hat{\\mu}(X,0)\\).\nCombine both estimates by weighing them using the predicted propensity score, \\(\\hat{p}\\): \\[\\text{HTE}(X)=\\hat{p} \\times HTE(X|D=1) + (1-\\hat{p})\\times HTE(X|D=0).\\]\n\nHere \\(\\hat{p}\\) balances the uncertainty associated with the \\(\\text{HTE}(X)\\)‚Äôs in each group and hence, this approach is particularly effective when there is a significant difference in the number of units between the two groups.\n\n\n\\(R\\)(obinson) Learner\nTo avoid getting too deep into technical details, I will not be describing the entire algorithm. The R learner models both the outcome, and the propensity score and begins by computing unit-level predicted outcomes and treatment probabilities using cross-fitting and leave-one-out estimation. The key innovation is plugging these into a novel loss function featuring squared deviations of these predictions as well as a regularization term. This ensures ‚Äúoptimality‚Äù in learning the treatment effect heterogeneity."
  },
  {
    "objectID": "blog/TODO-alphabet-het-treatment-effects.html#bottom-line",
    "href": "blog/TODO-alphabet-het-treatment-effects.html#bottom-line",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nML methods offer a promising way of determining which groups of units experience differential response to treatments.\nI summarized four such model-agnostic methods ‚Äì the \\(S\\), \\(T\\), \\(X\\), and \\(R\\) learners.\nCompared to the simpler \\(S\\) and \\(T\\) learners, the $X $ and $R $ learners solve some common issues and are more attractive options in most settings."
  },
  {
    "objectID": "blog/TODO-alphabet-het-treatment-effects.html#where-to-learn-more",
    "href": "blog/TODO-alphabet-het-treatment-effects.html#where-to-learn-more",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nI have previously written on how ML can be useful in causal inference, more generally and how we can use the Lasso to estimate heterogeneous treatment effects. Hu (2022) offers a detailed summary of a bunch of ML methods for HTE estimation. A Statistical Odds and Ends blog post describes the S, T and X learners and contains useful advice on when each of them is preferrable. Chapter 21 of Causal Inference for the Brave and True also discusses this material and provides useful examples."
  },
  {
    "objectID": "blog/TODO-alphabet-het-treatment-effects.html#references",
    "href": "blog/TODO-alphabet-het-treatment-effects.html#references",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "References",
    "text": "References\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\nK√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165. Nie,\nXie, N., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319."
  },
  {
    "objectID": "blog/TODO-gradient-boosting.html",
    "href": "blog/TODO-gradient-boosting.html",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "",
    "text": "Gradient boosting has emerged as one of the most powerful techniques for predictive modeling. In its simplest form, we can think of gradient boosting like having a team of detectives working in sequence, where each new detective specifically focuses on solving the cases where their predecessors stumbled. Each detective contributes their findings to the investigation, with earlier ones catching obvious clues and later ones piecing together the subtle evidence that was initially missed. The final solution emerges from combining all their work.\nWhile the term ‚Äúgradient boosting‚Äù is often used generically, there are notable differences among its implementations‚Äîeach with unique strengths, weaknesses, and practical applications. Understanding these nuances is essential for advanced data scientists aiming to choose the best method for their specific datasets and computational constraints.\nThis article aims to provide a brief overview of the most popular gradient boosting methods, delving into their mathematical foundations and unique characteristics. By the end, you will have a clearer understanding of when and how to use each method to achieve the best results in your predictive modeling tasks. At the end of the article, I‚Äôll provide a step-by-step Python example that implements these algorithms.\nI assume a mathematical familiarity with machine learning (ML) basics and some minimal previous exposure to gradient boosting. If you need a refresher, grab any introductory ML textbook."
  },
  {
    "objectID": "blog/TODO-gradient-boosting.html#background",
    "href": "blog/TODO-gradient-boosting.html#background",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "",
    "text": "Gradient boosting has emerged as one of the most powerful techniques for predictive modeling. In its simplest form, we can think of gradient boosting like having a team of detectives working in sequence, where each new detective specifically focuses on solving the cases where their predecessors stumbled. Each detective contributes their findings to the investigation, with earlier ones catching obvious clues and later ones piecing together the subtle evidence that was initially missed. The final solution emerges from combining all their work.\nWhile the term ‚Äúgradient boosting‚Äù is often used generically, there are notable differences among its implementations‚Äîeach with unique strengths, weaknesses, and practical applications. Understanding these nuances is essential for advanced data scientists aiming to choose the best method for their specific datasets and computational constraints.\nThis article aims to provide a brief overview of the most popular gradient boosting methods, delving into their mathematical foundations and unique characteristics. By the end, you will have a clearer understanding of when and how to use each method to achieve the best results in your predictive modeling tasks. At the end of the article, I‚Äôll provide a step-by-step Python example that implements these algorithms.\nI assume a mathematical familiarity with machine learning (ML) basics and some minimal previous exposure to gradient boosting. If you need a refresher, grab any introductory ML textbook."
  },
  {
    "objectID": "blog/TODO-gradient-boosting.html#notation",
    "href": "blog/TODO-gradient-boosting.html#notation",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Notation",
    "text": "Notation\nBefore diving into the specifics of each method, let‚Äôs establish some common notation that will be used throughout this article:\n\n\\(\\mathbf{X}\\): Covariates/features matrix\n\\(\\mathbf{y}\\): Outcome/target variable\n\\(f(x)\\): Predictive model\n\\(L(y, \\hat{y})\\): Loss function\n\\(\\hat{y}\\): Predicted outcome/target value\n\\(\\gamma\\): Learning rate\n\\(n\\): Number of observations/instances\n\\(M\\): Number of algorithm iterations."
  },
  {
    "objectID": "blog/TODO-gradient-boosting.html#diving-deeper",
    "href": "blog/TODO-gradient-boosting.html#diving-deeper",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nRefresher on Gradient Boosting\nGradient boosting is an ensemble machine learning technique that builds models sequentially, each new model attempting to correct the errors of the previous ones. The general idea is to minimize a loss function by adding weak learners (typically short decision trees) in a stage-wise manner to arrive at a single strong learner. This iterative process allows the model to improve its performance gradually, making it highly effective for complex datasets. Gradient boosting methods are versatile in that they can be used both for regression and classifications problems. In the most common case when the weak learners are decision trees, the algorithm is known as gradient tree boosting.\nMathematically, the model is built as follows:\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m = 1\\) to \\(M\\):\n\n\nCompute the pseudo-residuals: \\(r_{im} = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}\\) for \\(i=1,\\dots,n\\). In regression tasks this is simply \\(y-\\hat{y}\\).\nFit a weak learner such as a tree, \\(h_m(x)\\), on \\(\\{(x_i, r_{im}) \\}_{i=1}^n.\\)\nCompute \\(\\gamma\\) by solving: \\(\\gamma=\\arg\\min\\sum_i L(y_i, f_{m-1}(x_i)+\\gamma h_m(x_i)).\\)\nUpdate the model: \\(f_m(x) = f_{m-1}(x) + \\gamma h_m(x).\\)\n\n\nThe final model is \\(f_M(x).\\)\n\nThis is the most generic recipe for a gradient boosting algorithm. Let‚Äôs now focus on more specific implementations of this idea.\n\n\nAdaBoost\nAdaBoost, short for Adaptive Boosting, is one of the earliest boosting algorithms. It adjusts the weights of incorrectly classified observations so that subsequent learners focus more on difficult ones. In other words, it works by learning from its mistakes ‚Äì after each round of predictions, it identifies which observartions it got wrong and pays special attention to them (i.e., gives them a higher weight) in the next round. Thus, AdaBoost is not strictly speaking a gradient boosting algorithm, in the modern sense of the term.\nLet‚Äôs consider a binary classification problem where the outcome variable takes values in \\(\\{ -1, 1\\}\\). Here is the general AdaBoost algorithm:\n\nInitialize weights \\(w_i = \\frac{1}{n}\\) for \\(i = 1, \\ldots, n.\\)\nFor \\(m = 1 \\dots M\\):\n\n\nTrain a weak learner \\(h_m(x)\\) using weights \\(w_i\\).\nCompute the weighted error: \\(\\epsilon_m = \\frac{\\sum_{i=1}^{n} w_i \\mathbb{I}(y_i \\neq h_m(x_i))}{\\sum_{i=1}^{n} w_i}\\), where \\(\\mathbb{I}(\\cdot)\\) is the indicator function.\nCompute the model weight:$ _m = ()$.\nUpdate and normalize the weights:$ w_i^{m+1}= w_i^{m} (-_m y_i h_m(x_i)))$, and \\(w_i^{m+1} = \\frac{ w_i^{m+1}}{\\sum_j  w_j^{m+1}} for i=1,\\dots,n\\).\n\n\nThe final model is \\(f(x)=sign \\left( \\sum_m \\alpha_m h_m(x) \\right)\\).\n\nAdaBoost is simple to implement while being relatively resistant to overfitting, making it especially effective for problems with clean, well-structured data. Its adaptive nature means it can identify and focus on the most challenging observations. However, AdaBoost has notable weaknesses: it‚Äôs highly sensitive to noisy data and outliers (since it increases weights on misclassified examples), can perform poorly when working with insufficient training data, and tends to be computationally intensive compared to newer, simpler algorithms.\nSoftware Packages. R: adabag, gbm. Python: scikit-learn.\n\n\nXGBoost\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting designed for speed and performance. It revolutionizes the method by using the Newton-Raphson method in function space, setting it apart from traditional gradient boosting‚Äôs simpler gradient descent approach. At its core, XGBoost leverages both the first and second derivatives of the loss function through a second-order Taylor approximation. This sophisticated approach enables faster convergence and more accurate predictions by making better-informed decisions about how to improve the model at each step. When building decision trees, XGBoost considers both the expected improvement and the uncertainty of potential splits, while simultaneously applying regularization to prevent overfitting.\nWith some simplifications, the regression version of the XGBoost is implemented as follows:\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m=1, \\dots M\\):\n\n\nCompute the gradients \\(grad_m(x_i)=\\left( \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right)_{f(x)=f_{m-1}(x)}\\) and hessians \\(hess_m(x_i)=\\left( \\frac{\\partial^2 L(y_i, f(x_i))}{\\partial^2 f(x_i)^2}\\right)_{f(x)=f_{m-1}(x)}\\) of the loss function for \\(i=1,\\dots,n\\).\nFit a weak learner such as a tree, \\(h_m(x)\\) on$ {(x_i, ) }_{i=1}^n$.\nUpdate the model \\(f_m(x)=f_{(m-1)} - \\alpha h_m(x).\\)\n\n\nThe final model is \\(f_M(x)=\\sum_m f_m(x)\\).\n\nXGBoost is ideal for large datasets and competitions where model performance is critical. It is also a good choice when you need a highly optimized and scalable solution as it is highly efficient, supports parallel and distributed computing. Nevertheless, this algorithm can be complex to tune, and may require significant computational resources for very large datasets.\nNext up is CatBoost.\nSoftware Packages: R: xgboost, Python: xgboost.\n\n\nCatBoost\nCatBoost, short for Categorical Boosting, is a gradient boosting library that handles categorical features efficiently. It uses ordered boosting to reduce overfitting and supports GPU training, making it both powerful and versatile. CatBoost modifies the standard gradient boosting algorithm by incorporating two novel techniques ‚Äì ordered boosting and target statistics for categorical features. Let‚Äôs examine each one in turn.\nRather than requiring preprocessing like one-hot encoding, CatBoost encodes categorical values based on the distribution of the target variable without introducing data leakage. This is achieved by encoding each data point as if it were unseen, preventing overfitting. Additionally, ordered boosting is a method that builds each new tree while treating each data point as ‚Äúout-of-fold‚Äù for itself. This helps reduce overfitting, particularly in small datasets, by preventing over-reliance on individual observations during training.\nWe now move on to a popular, faster gradient boosting implementation.\nSoftware Packages: R: catboost, Python: catboost.\n\n\nLightGBM\nLightGBM shares many of XGBoost‚Äôs benefits, such as support for sparse data, parallel training, multiple loss functions, regularization, and early stopping, but it also introduces a bunc of new features and improvements.\nFirst, rather than growing trees level-wise (row by row) as in most implementations, LightGBM grows trees leaf-wise, selecting the leaf that provides the greatest reduction in loss. Second, it does not use the typical sorted-based approach for finding split points, which sorts feature values to locate the best split. Instead, it relies on an efficient histogram-based method that significantly improves both speed and memory efficiency. Third, LightGBM incorporates the so-called Gradient-Based One-Side Sampling, which speeds up training by focusing on the most informative samples. Lastly, the algorithm uses Exclusive Feature Bundling which can group features to reduce dimensionality, enabling faster and more accurate model training.\nSoftware Packages: R: lightgmb, Python: lightgbm.\n\n\nChallenges\nGradinet boosting methods comes with a few common practical challenges. When a predicitve model learns the training data too precisely, it may perform poorly on new, unseen data ‚Äì a problem called overfitting. To combat this, various regularization methods are used to add helpful constraints to the learning process. A main parameter that regulates the model‚Äôs learning precision is the number of boosting rounds (M), which determines how many base models are created. While using more rounds reduces training errors, it also increases overfitting risk. To find the optimal M value, it‚Äôs common to use cross validation. The depth of decision trees is another important control parameter in tree boosting. Deeper trees can capture more complex patterns but are more prone to memorizing the training data rather than learning generalizable patterns.\nThe improved predictive performance of gradient boosting relative to simpler models comes at a cost of reduced model transparency. While a single decision tree‚Äôs reasoning can be easily traced and understood, tracking the decision-making process across numerous trees becomes extremely complex and challenging. Gradient boosting is an excellent example of the inherent tradeoff between model simplicity and performance.\nLet‚Äôs now look at how can we use these methods in practice."
  },
  {
    "objectID": "blog/TODO-gradient-boosting.html#an-example",
    "href": "blog/TODO-gradient-boosting.html#an-example",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "An Example",
    "text": "An Example\nI used GitHub copilot to create code illustrating the implementation of each algorithm described above on a common dataset. Let‚Äôs look at it in detail.\nWe start with loading the necessary libraries.\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\nNext, we load the data and split it into training and test parts.\n\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nNow it‚Äôs time to define and implement the boosting algorithms. Do not forget to save the results.\n\nclassifiers = {\n    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n    \"XGBoost\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42),\n    \"LightGBM\": LGBMClassifier(n_estimators=100, random_state=42),\n    \"CatBoost\": CatBoostClassifier(n_estimators=100, verbose=0, random_state=42)\n}\n\nresults = {}\nfor name, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = accuracy\nFinally, we print the accuracy results:\n\nfor name, accuracy in results.items():\n    print(f\"{name}: {accuracy:.4f}\")\n\nAdaBoost: 0.9737\nXGBoost: 0.9561\nLightGBM: 0.9649\nCatBoost: 0.9649\nOverall each method performed reasonably well, with acuracy ranging from \\(95.6\\)% to \\(97.4\\)%. Interestingly, Adaboost outperformed the other more complex algorithms, at least in the in terms of accuracy.\nYou can find the entire code in this GitHub repo.\nAnd that‚Äôs it. You are now familiar with the most popular implementations of gradient boosting along with their advantages and weaknesses. You also know how to employ them in practice. Have fun incorporating XGboost and the like into your predictive modeling tasks."
  },
  {
    "objectID": "blog/TODO-gradient-boosting.html#bottom-line",
    "href": "blog/TODO-gradient-boosting.html#bottom-line",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nGradient boosting is a powerful ensemble technique for predictive modeling that comes in a variety of flavors.\nAdaBoost focuses on misclassified instances by adjusting weights.\nXGBoost introduces regularization and optimization for speed and performance.\nCatBoost efficiently handles categorical features and reduces overfitting.\nLightGBM enjoys many of XGBoost‚Äôs strenghts while introducing a few novelties including a different way of building the underlying weak learners.\nCommon practical challenges when implementing gradient boosting include overfitting, decreased interpretability and computational costs."
  },
  {
    "objectID": "blog/TODO-gradient-boosting.html#where-to-learn-more",
    "href": "blog/TODO-gradient-boosting.html#where-to-learn-more",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great startint point, and it‚Äôs a resource I used extensively when preparing this article. ‚ÄúThe Elements of Statistical Learning‚Äù by Hastie, Tibshirani, and Friedman is a comprehensive guide that covers the theoretical foundations of machine learning, including gradient boosting. It is the de facto bible for statistical ML. While this book is phenomenal, it can be challenging for less technical practitioners for which I recommend its lighther versions, ‚ÄúAn Introduction to Statistical Learning‚Äù with R and Python code. All these books are available for free online. Lastly, if you want to dive even deeper into any of the algortihms descibe above, consider studying the papers in the References section below."
  },
  {
    "objectID": "blog/TODO-gradient-boosting.html#references",
    "href": "blog/TODO-gradient-boosting.html#references",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "References",
    "text": "References\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.¬†785-794).\nDorogush, A. V., Ershov, V., & Gulin, A. (2018). CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363.\nFreund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139.\nFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The elements of statistical learning: data mining, inference, and prediction.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An introduction to statistical learning: With applications in python. Springer Nature.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2013). An introduction to statistical learning: With applications in R. Springer Nature.\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ‚Ä¶ & Liu, T. Y. (2017). Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30.\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: unbiased boosting with categorical features. Advances in neural information processing systems, 31."
  },
  {
    "objectID": "blog/TODO-overlapping-conf-intervals.html",
    "href": "blog/TODO-overlapping-conf-intervals.html",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "",
    "text": "This is a mistake I make with data all the time. Even more, I have seen many well-known professors and data masters also fall for it. It goes like this.\nI see a graph with two bars depicting two sample means placed right next to each other. The presenter, being a good data scientist, has added error bars representing 95% confidence intervals. The side-to-side placement usually implies that the two quantities are about to be compared. My eyes immediately check whether the two confidence intervals overlap. When they do not, I quickly conclude that the two means are statistically significantly different from each other and, thus, the presenter has uncovered an exciting truth about the world.\nThis na√Øve but all too common approach to judging statistical significance is wrong. Here is why."
  },
  {
    "objectID": "blog/TODO-overlapping-conf-intervals.html#background",
    "href": "blog/TODO-overlapping-conf-intervals.html#background",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "",
    "text": "This is a mistake I make with data all the time. Even more, I have seen many well-known professors and data masters also fall for it. It goes like this.\nI see a graph with two bars depicting two sample means placed right next to each other. The presenter, being a good data scientist, has added error bars representing 95% confidence intervals. The side-to-side placement usually implies that the two quantities are about to be compared. My eyes immediately check whether the two confidence intervals overlap. When they do not, I quickly conclude that the two means are statistically significantly different from each other and, thus, the presenter has uncovered an exciting truth about the world.\nThis na√Øve but all too common approach to judging statistical significance is wrong. Here is why."
  },
  {
    "objectID": "blog/TODO-overlapping-conf-intervals.html#diving-deeper",
    "href": "blog/TODO-overlapping-conf-intervals.html#diving-deeper",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nThe Basics of Confidence Intervals\nLet‚Äôs set up a toy example following Schenker and Gentleman (2001). Imagine we have two quantities ‚Äì \\(Y_1\\) and \\(Y_2\\) ‚Äì and we are interested in testing whether they are statistically different from each other. These can stand for US and UK sales or user engagement on Android and iOS devices. For simplicity, we assume all friendly statistical properties (e.g., large and random samples, well-behaved distributions, consistent estimators for all quantities, etc.).\nWe are interested in whether the population values for \\(Y_1\\) and \\(Y_2\\) are equal. The null hypothesis (\\(H_0\\)) states that they indeed are:\n\\[ $H_0: Y_1 = Y_2$. \\]\nWe will denote our estimates of \\(Y_1\\) and \\(Y_2\\) with \\(\\hat{Y_1}\\) and \\(\\hat{Y_2}\\) and refer to their estimated standard errors as \\(\\hat{SE}(Y_1)\\) and \\(\\hat{SE}(Y_2)\\). The corresponding confidence intervals for \\(Y_1\\) and \\(Y_2\\) are given by the following:\n\\[ \\hat{Y_1} \\pm 1.96 \\times \\hat{SE}(Y_1) \\]\nand \\[ \\hat{Y_2} \\pm 1.96 \\times \\hat{SE}(Y_2). \\]\nSo far, so good. Importantly, note that we can also construct a confidence interval for the difference (\\(Y_1 - Y_2\\)):\n\\[ (\\hat{Y_1} - \\hat{Y_2}) \\pm 1.96 \\times \\sqrt{ \\hat{SE}(Y_1)^2+ \\hat{SE}(Y_2)^2}. \\]\nNow, let me describe the two approaches to determining statistical significance based on these intervals.\n\n\nTwo Ways to Determine Significance\nThe na√Øve method ‚Äì the mistake I make all the time:\n\nExamine whether the two confidence intervals (for \\(Y_1\\) and \\(Y_2\\)) overlap.\nReject the null hypothesis if they do, and do not reject it otherwise.\n\nThe correct method:\n\nExamine the confidence interval for the difference between the two quantities \\((Y_1 - Y_2)\\).\nReject the null hypothesis if it does not contain 0, and do not reject it otherwise.\n\n\n\nAn Explanation\nTo understand why the na√Øve approach is wrong, let‚Äôs examine the length of these confidence intervals.\nUnder the na√Øve approach, the two confidence intervals overlap only if the following interval contains 0:\n\n\\[ (\\hat{Y_1} - \\hat{Y_2}) \\pm 1.96 \\times (\\hat{SE}(Y_1) + \\hat{SE}(Y_2)). \\]\n\nThis is the equation on which the decision under the na√Øve approach is based.\nLet‚Äôs compare this expression with the confidence interval for the difference \\((Y_1-Y_2)\\) on which the decision under the correct approach is based.\nTheir ratio is equal to:\n\n\\[ \\frac{\\hat{SE}(Y_1)+ \\hat{SE}(Y_2)}{\\sqrt{\\hat{SE}(Y_1)^2 + \\hat{SE}(Y_2)^2}}} \\]\n\nIt is easy to see that this ratio is greater than one. In other words, the confidence interval in play under the na√Øve approach is wider than that under the correct approach.\nHence, when the two quantities are equal to each other (i.e., \\(H_0\\) is true), the na√Øve method is more conservative (rejects less often; under rejects). When they are different (\\(H_0\\) is false), the na√Øve approach is less conservative (rejects too often; over rejects).\nThe discrepancy between the two methods will be largest when the ratio above is large. This happens when \\(\\hat{SE}(Y_1)\\) and \\(\\hat{SE}(Y_2)\\) are of equal value. The opposite is true as well ‚Äì when one of \\(\\hat{SE}(Y_1)\\) and \\(\\hat{SE}(Y_2)\\) is much greater than the other, the ratio will roughly equal one, and hence the two methods will yield very similar results.\nAn Example Schenker and Gentleman (2001) give a numerical example where \\(Y_1\\) and \\(Y_2\\) measure proportions. They set \\(\\hat{Y}_1=.56\\), \\(\\hat{Y}_2 = .44\\), \\(\\hat{SE}(Y_1)=\\hat{SE}(Y_2)=.0351.\\)\nIn this case, the two confidence intervals for \\(Y_1\\) and \\(Y_2\\) are \\([.49, .63]\\) and \\([.37, .51]\\), respectively. The two intervals overlap, so under the na√Øve approach, we conclude the two population proportions are not significantly different.\nHowever, the confidence interval for the difference \\((Y_1-Y_2)\\) is \\([.02, .22]\\). Clearly, it does contain \\(0\\), and thus, we cannot conclude statistical significance under the correct approach."
  },
  {
    "objectID": "blog/TODO-overlapping-conf-intervals.html#bottom-line",
    "href": "blog/TODO-overlapping-conf-intervals.html#bottom-line",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nExamining overlap in confidence intervals is an intuitive and natural way to judge statistical significance between two quantities.\nWhile, in some cases, this na√Øve approach might give you the right answer, it is certainly not the correct way.\nInstead, remember to analyze the confidence interval for the difference between the two quantities."
  },
  {
    "objectID": "blog/TODO-overlapping-conf-intervals.html#where-to-learn-more",
    "href": "blog/TODO-overlapping-conf-intervals.html#where-to-learn-more",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nCheck the paper by Schenker and Gentleman (2001) on which this post is based. The authors go deeper into this question with simulations and discussions on Type 1 errors and power."
  },
  {
    "objectID": "blog/TODO-overlapping-conf-intervals.html#references",
    "href": "blog/TODO-overlapping-conf-intervals.html#references",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "References",
    "text": "References\nCole, S. R., & Blair, R. C. (1999). Overlapping confidence intervals. Journal of the American Academy of Dermatology, 41(6), 1051-1052.\nSchenker, N., & Gentleman, J. F. (2001). On judging the significance of differences by examining the overlap between confidence intervals. The American Statistician, 55(3), 182-186."
  },
  {
    "objectID": "blog/TODO-lasso-heterogeneous-effects.html",
    "href": "blog/TODO-lasso-heterogeneous-effects.html",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "",
    "text": "Lasso is one of my favorite machine learning algorithms. It is so simple, elegant, and powerful. My feelings aside, Lasso indeed has a lot to offer. While, admittedly it is outperformed by more complex, black-box type of methods (e.g., boosting or neural networks), it has several advantages: interpretability, computational efficiency, and flexibility. Even when it comes to accuracy, theory tells us that under appropriate assumptions, Lasso can uncover the true submodel and we can even derive bounds on its prediction loss.\nIn this article I will briefly describe two ways researchers use Lasso to detect heterogeneous treatment effects. The underlying idea is to throw a bunch of covariates in the model and let the \\(L1\\) regularization do the difficult job of identifying which ones are important for treatment effect heterogeneity."
  },
  {
    "objectID": "blog/TODO-lasso-heterogeneous-effects.html#background",
    "href": "blog/TODO-lasso-heterogeneous-effects.html#background",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "",
    "text": "Lasso is one of my favorite machine learning algorithms. It is so simple, elegant, and powerful. My feelings aside, Lasso indeed has a lot to offer. While, admittedly it is outperformed by more complex, black-box type of methods (e.g., boosting or neural networks), it has several advantages: interpretability, computational efficiency, and flexibility. Even when it comes to accuracy, theory tells us that under appropriate assumptions, Lasso can uncover the true submodel and we can even derive bounds on its prediction loss.\nIn this article I will briefly describe two ways researchers use Lasso to detect heterogeneous treatment effects. The underlying idea is to throw a bunch of covariates in the model and let the \\(L1\\) regularization do the difficult job of identifying which ones are important for treatment effect heterogeneity."
  },
  {
    "objectID": "blog/TODO-lasso-heterogeneous-effects.html#notation",
    "href": "blog/TODO-lasso-heterogeneous-effects.html#notation",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Notation",
    "text": "Notation\nAs always, let‚Äôs start with some notation. Let \\(T\\) denote a binary treatment indicator, \\(Y(0), Y(1)\\) be the potential outcomes under each treatment state (\\(Y\\) is the observed one), and \\(X\\) be a covariate vector. Lastly, \\(p\\) is the share of units in the treatment group, \\(p=\\frac{1}{N}\\sum T\\), where \\(N\\) is the sample size.\nThe lasso coefficient vector is commonly expressed as the solution to the following problem:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-X\\beta||_2^2 + \\lambda||\\beta||_1 \\},\\]\nwhere \\(\\lambda\\) is the regularization parameter governing the variance-bias trade-off.\nWe are interested in the heterogeneous treatment effect given \\(X (HTE(X))\\):\n\\[HTE(X) = E[Y(1)-Y(0)|X=x].\\]\nThat is, \\(HTE(X)\\) is the average treatment effect for units with covariate levels \\(X=x\\).\nMore precisely, our goal is identifying which variables in \\(X\\) divide the population of interest such that there are meaningful treatment effect differences in units across these groups. For instance, in the case of estimating the impact of school quality on test scores X might be students‚Äô gender (e.g., girls benefit more than boys), or in the context of online A/B testing \\(X\\) might denote previous product engagement (e.g., tenured users benefit more from a new feature than inexperienced users).\nBroadly speaking there are two main approaches of using Lasso to solve this problem ‚Äì (i) a linear model with interactions between $T $ and \\(X\\), and (ii) directly regressing the imputed unit-level treatment effects on \\(X\\)."
  },
  {
    "objectID": "blog/TODO-lasso-heterogeneous-effects.html#diving-deeper",
    "href": "blog/TODO-lasso-heterogeneous-effects.html#diving-deeper",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nLasso with Treatment Variable Interactions\nIn a low-dimensional world where regularization is not necessary and researchers are interested in \\(HTE\\)s, they often use a linear model in which the treatment variable is interacted with the covariates. Statistically significant interaction coefficients identify the \\(X\\) variables for which the treatment has differential impact.\nMathematically, when all variables are properly interacted, this is analogous to splitting the sample into subgroups based on \\(X\\) and running OLS on each group separately. This is feasible and convenient when \\(X\\) is binary or categorical, but not when it is continuous. The advantage of this approach, however, is that the linear regression spits out p-values which can be (mis)used to determine statistical significance of these interaction variables.\nThe OLS model is then:\n\\[Y = \\beta_1 T + \\beta_2 X + \\beta_3 X \\times T + \\epsilon,\\]\nwhere \\(\\epsilon\\) is the error term. The attention here falls on the coefficient vector $_3 $ which identifies whether the treatment has had a differential impact on units with a particular characteristic \\(X\\).\nIn high-dimensional settings when we have access to a wide vector \\(X\\), this is again not feasible. Instead, we might want to use an algorithm to pick out the variables in \\(X\\) which are important for the treatment effect heterogeneity.\nImai and Ratkovic (2013) show us how to adapt the Lasso to this setting. It turns out you should not simply throw this model into the Lasso loss function. Can you guess why? Some variables might be predictive of the baseline outcome while others only of the treatment effect heterogeneity. The trick is to have two separate lasso constraints ‚Äì \\(\\lambda_1\\) and \\(\\lambda_2\\).\nSo, the loss function looks something like this:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-f(X,T)\\beta_1 - X\\times T \\beta_2 ||_2^2 + \\lambda_1||\\beta_1||_1 + \\lambda_2||\\beta_2||_1 \\}.\\]\nA simplified version of the algorithm is as follows:\n\nGenerate interaction variables, \\(\\tilde{X}(T)\\).\nRun Lasso of \\(Y\\) on \\(T\\), \\(X\\) and \\(\\tilde{X}(T)\\).\nIdentify statistically significant variables determining treatment effect heterogeneity.\n\nTwo notes. First, this requires some assurance that we are not overfitting, so that some form of sample splitting or cross validation is necessary. On top of this, Athey and Imbens (2017) suggest comparing these results with post-lasso OLS estimates to further guard against overfitting. Second, multiple testing is an issue as is the case with ML algorithms more generally. (You can check my earlier post on multiple hypothesis testing in linear machine learning models.) Options to take care of this include sample splitting and bootstrap, among others.\nSoftware Package: FindIt.\n\n\nLasso with Knockoffs\nAn alternative approach directly regresses the unit-level treatment effects on \\(X\\). To get there, we first have to model the outcome function and impute the missing potential outcome for each unit. See my previous post on using Machine Learning tools for causal inference for more information on how we might do that.\nThis approach was developed by Xie et al.¬†(2018). They recognized the multiple hypothesis issue and suggested using knockoffs to control the False Discovery Rate. This is still not completely kosher as it does not account for the fact that the outcome variable is estimated in an earlier step. Oh well. My guess and hope are that this is more of a theoretical concern, and empirically the inference we get is still ‚Äúcorrect.‚Äù\nHere is a simplified version of their algorithm:\n\nTransform the outcome variable \\(\\tilde{Y}=Y\\times\\frac{(T-p)}{p(1-p)}\\).\nCalculate unit-level treatment effects, \\(\\hat{Y}(1)-\\hat{Y}(0)\\).\nGenerate the difference \\(Y^*=\\tilde{Y}-\\frac{1}{N}\\sum \\hat{Y}(1)-\\hat{Y}(0)\\).\nRun Lasso of \\(Y^*\\) on \\(X\\) and \\(X^*\\) (the knockoff counterparts of \\(X\\)).\nFollow the knockoff method to obtain the set of significant variables.\n\nRemember that \\(\\tilde{Y}\\) has the special property that \\(E[\\tilde{Y} \\mid X=x]=\\text{HTE}(X)\\) under the unconfoundedness assumption.\nInterestingly, in the special case when \\(p=1/2\\) the algebra reduces further which provides computation scaling advantages. Tian et al.¬†(2014) showed this result first."
  },
  {
    "objectID": "blog/TODO-lasso-heterogeneous-effects.html#an-example",
    "href": "blog/TODO-lasso-heterogeneous-effects.html#an-example",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate the latter method. As such, it is a mere depiction of the approach, and its results should certainly not be taken seriously.\nThe outcome variable was survived, and the treatment variable was male. As is well known, women were much more likely to survive than men (\\(74\\%\\) vs \\(19\\%\\) in this sample). So, I analyzed whether the gender difference in survival was impacted by other factors. I included the following covariates ‚Äì pclass (ticket class), age, sibsp (number of siblings aboard), parch (number of parents aboard), fare, embarked (port of Embarkation), and cabin. Some of these were categorical in which case I converted them to a bunch of binary variables.\nThe knockoff filter identified a single variable as having a significant impact on the treatment effect ‚Äì pclass. For the more affluent passengers (i.e., those in the higher ticket classes), this gender difference in survival was much smaller.\nYou can find the code in this GitHub repository."
  },
  {
    "objectID": "blog/TODO-lasso-heterogeneous-effects.html#bottom-line",
    "href": "blog/TODO-lasso-heterogeneous-effects.html#bottom-line",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe underlying idea behind using Lasso in HTEs estimation is to rely on L1 regularization to select which covariates are important for explaining differences in treatment responses. There are two main ways researchers use Lasso to estimate HTEs.\nFirst, we can feed a linear model in which all covariates are interacted with the treatment indicator to the Lasso loss function with two separate regularization constraints.\nSecond, we can directly regress the unit-level treatment effects on the covariates.\nI am not aware of any simulation studies comparing both approaches.\nWhile Lasso is among the simplest and most popular machine learning algorithms, there might be other, more suitable methods for estimating HTEs."
  },
  {
    "objectID": "blog/TODO-lasso-heterogeneous-effects.html#where-to-learn-more",
    "href": "blog/TODO-lasso-heterogeneous-effects.html#where-to-learn-more",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nHu (2022) is an excellent summary of the several ways researchers use Machine Learning to uncover heterogeneity in treatment effect estimation."
  },
  {
    "objectID": "blog/TODO-lasso-heterogeneous-effects.html#references",
    "href": "blog/TODO-lasso-heterogeneous-effects.html#references",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics: 2055-2085.\nBarber, R. F., & Cand√®s, E. J. (2019). A knockoff filter for high-dimensional selective inference. The Annals of Statistics, 47(5), 2504-2537.\nChatterjee, A., & Lahiri, S. N. (2011). Bootstrapping lasso estimators. Journal of the American Statistical Association, 106(494), 608-625.\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. The Annals of Applied Statistics (2013): 443-470.\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\nMeinshausen, N., Meier, L., & B√ºhlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nXie, Y., Chen, N., & Shi, X. (2018). False discovery rate controlled heterogeneous treatment effect detection for online controlled experiments. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp.¬†876-885)."
  },
  {
    "objectID": "blog/TODO-bayesian-ab-tests.html",
    "href": "blog/TODO-bayesian-ab-tests.html",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "",
    "text": "Imagine you‚Äôre a data scientist evaluating an A/B test of a new recommendation algorithm. The results show a modest but promising 0.5% lift in conversion rate‚Äîup from \\(8\\%\\) to \\(8.5\\%\\) across \\(20,000\\) users‚Äîalong with a p-value of \\(0.06\\). Given historical data, the improvement seems realistic, the cost to implement is low, and the projected revenue impact is substantial. Yet, the conventional analysis compels you to ‚Äúfail to reject the null hypothesis‚Äù simply because the p-value doesn‚Äôt meet an arbitrary \\(0.05\\) threshold.\nTraditional A/B testing approaches (randomized controlled trials or RCTs) rely primarily on frequentist methods like t-tests or chi-squared tests to detect differences between treatment and control groups. For more precision, analysts often adjust for covariates in linear models. After weeks or months of testing, however, these methods often boil down to a single outcome: the \\(p\\)-value. This binary interpretation‚Äîsignificant or not‚Äîcan obscure valuable insights, as seen in the example above.\nBayesian statistics, by contrast, offers a more nuanced and potentially more informative alternative. In this article, we‚Äôll walk through a practical example, discuss the implementation details, and dive into the Bayesian framework‚Äôs mathematical foundations for experimentation. One of the core advantages of the Bayesian approach is its ability to produce a full posterior distribution, offering a richer view of the experiment‚Äôs outcomes beyond just point estimates.\nThis article assumes a familiarity with Bayesian fundamentals like priors, posteriors, and conjugate distributions. If you‚Äôd like a refresher, see the References section below."
  },
  {
    "objectID": "blog/TODO-bayesian-ab-tests.html#background",
    "href": "blog/TODO-bayesian-ab-tests.html#background",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "",
    "text": "Imagine you‚Äôre a data scientist evaluating an A/B test of a new recommendation algorithm. The results show a modest but promising 0.5% lift in conversion rate‚Äîup from \\(8\\%\\) to \\(8.5\\%\\) across \\(20,000\\) users‚Äîalong with a p-value of \\(0.06\\). Given historical data, the improvement seems realistic, the cost to implement is low, and the projected revenue impact is substantial. Yet, the conventional analysis compels you to ‚Äúfail to reject the null hypothesis‚Äù simply because the p-value doesn‚Äôt meet an arbitrary \\(0.05\\) threshold.\nTraditional A/B testing approaches (randomized controlled trials or RCTs) rely primarily on frequentist methods like t-tests or chi-squared tests to detect differences between treatment and control groups. For more precision, analysts often adjust for covariates in linear models. After weeks or months of testing, however, these methods often boil down to a single outcome: the \\(p\\)-value. This binary interpretation‚Äîsignificant or not‚Äîcan obscure valuable insights, as seen in the example above.\nBayesian statistics, by contrast, offers a more nuanced and potentially more informative alternative. In this article, we‚Äôll walk through a practical example, discuss the implementation details, and dive into the Bayesian framework‚Äôs mathematical foundations for experimentation. One of the core advantages of the Bayesian approach is its ability to produce a full posterior distribution, offering a richer view of the experiment‚Äôs outcomes beyond just point estimates.\nThis article assumes a familiarity with Bayesian fundamentals like priors, posteriors, and conjugate distributions. If you‚Äôd like a refresher, see the References section below."
  },
  {
    "objectID": "blog/TODO-bayesian-ab-tests.html#notation",
    "href": "blog/TODO-bayesian-ab-tests.html#notation",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs establish our notation for a binary intervention randomized experiment:\n\n\\(T \\in {0,1}\\): Treatment indicator\n\\(N_T\\): Number of units in treatment group\n\\(N_C\\): Number of units in control group\n\\(N = N_T + N_C\\): Total sample size\n\\(X_T\\): Number of ‚Äúsuccesses‚Äù in treatment group\n\\(X_C\\): Number of ‚Äúsuccesses‚Äù in control group\n\\(Y\\): Success rate (e.g., conversion rate, employment status)."
  },
  {
    "objectID": "blog/TODO-bayesian-ab-tests.html#diving-deeper",
    "href": "blog/TODO-bayesian-ab-tests.html#diving-deeper",
    "title": "Bayesian Analysis of A/B Tests: A Modern Approach",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nWe are interested in making inferences about the treatment effect,$ $, of the intervention \\(T\\) on the success rate \\(Y\\). In Bayesian terms, this means we seek the posterior distribution of \\(\\tau\\). Once we obtain this distribution or can draw observations from it, we can calculate various statistics to summarize the impact of the intervention, potentially providing a richer understanding of its effects.\nThe Bayesian methodology of analyzing experiments proceeds in four steps.\n\nStep 1: Specify the Prior Distribution\nWe begin by choosing the prior distributions for Y. The prior distribution represents our beliefs about the parameter before seeing the data. It is usually informed by previous A/B testing experience combined with deep context knowledge. For example, we might believe that the treatment effect is around two percentage points (\\(\\hat{\\tau}=0.02\\)) with some uncertainty around it.\nGiven our binary outcome, the Beta distribution serves as a natural conjugate prior:\n\\[ Y_i \\sim \\text{Beta} (\\alpha_i, \\beta_i), \\hspace{.5cm} i \\in \\{T,C\\}.  \\]\n\n\nStep 2: Specify the Likelihood Function\nFor binary outcomes, we model the data using a Binomial distribution:\n\\[X_i|Y_i \\sim \\text{Binomial}(N_i, Y_i), \\hspace{.5cm} i\\in\\{T,C\\}.  \\]\nThis choice reflects the inherent structure of our data: counting successes in a fixed number of trials. We are now ready to use the Bayes rule to arrive at the posterior distributions.\n\n\nStep 3: Posterior Distributions Derivation\nThanks to conjugacy between Beta and Binomial distributions, we obtain closed-form posterior distributions:\n\\[ Y_i | X_i \\sim \\text{Beta}(\\alpha_i+X_i, \\beta_i+N_i-X_i), \\hspace{.5cm} i\\in\\{T,C\\}  .\\]\nThis mathematical convenience allows us to avoid more complex numerical methods like MCMC sampling. We can now even plot the two posterior distributions and visually asses their differences.\n\n\nStep 4: Inference and Decision Making\nThe Bayesian framework enables rich analysis beyond traditional frequentist-style hypothesis testing. Here are some examples:\n\nExample 1: Probability of Positive Impact\nWe can calculate the probability that the outcomes for the treatment group are higher than those of the control group. In closed form, we have:\n\\[ P(Y_T&gt;Y_C|X_T,X_C) = \\frac{1}{N}\\sum_i\\mathbf{1}(Y_{T,i}&gt;Y_{C,i}),\\]\nwhere \\(\\mathbf{1}(\\cdot)\\) is the indicator function. This is simply share of observations for which \\(Y_T &gt;Y_C\\).\n\n\nExample 2: Average Treatment Effect\nAnother potential example of an object of interest is the Average Treatment Effect (ATE):\n\\[ ATE = \\frac{1}{N_T}\\sum_{i \\in T}Y_{T,i} -  \\frac{1}{N_C}\\sum_{i \\in C}Y_{C,i} \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\n\\[ ATE \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\nAnd that‚Äôs it. You see how with the cost of additional assumptions we get much more than a single \\(p\\)-value, and that‚Äôs where much of the appeal of this approach lies.\n\n\n\nAdvantages\nOverall, Bayesian thinking entails some compelling advantages:\n\nIncorporation of Prior Information. It allows you to incorporate prior knowledge or expert opinion into the analysis. This is particularly useful when historical data or domain expertise is available.\nProbabilistic Interpretation: It provides direct probabilistic interpretations of the results, such as the probability that one variant is better than another. This may be more intuitive than frequentist p-values, which are often misinterpreted.\nHandling of Small Sample Sizes: It tends to perform better with small sample sizes because they incorporate prior distributions, which can regularize estimates and prevent overfitting.\nContinuous Learning: As new data comes in, Bayesian methods provide a natural way to update the posterior distribution, leading to continuous learning and adaptation.\n\nThe downsides are mostly related to the choice of prior distribution. In settings where there is a lack of expert knowledge, this Bayesian approach to experimentation might not be very attractive. Computation can also be an issue in more complex settings."
  },
  {
    "objectID": "blog/TODO-bayesian-ab-tests.html#an-example",
    "href": "blog/TODO-bayesian-ab-tests.html#an-example",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs code an example in R. We start with generating some fake data and selecting parameters for the prior distributions.\nrm(list=ls())\nset.seed(681)\nlibrary(ggplot2)\n\n# Data: Number of successes and total observations for T and C\nn_T &lt;- 1000\nx_T &lt;- 70\nn_C &lt;- 900\nx_C &lt;- 50\n\n# Prior parameters for the Beta distribution\nalpha_A &lt;- 1\nbeta_A &lt;- 1\nalpha_B &lt;- 1\nbeta_B &lt;- 1\nWe are now ready to get to and sample from the posteriors.\n\n# Posterior parameters\nposterior_alpha_T &lt;- alpha_T + x_T\nposterior_beta_T &lt;- beta_T + n_T - x_T\nposterior_alpha_C &lt;- alpha_C + x_C\nposterior_beta_C &lt;- beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T &lt;- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C &lt;- rbeta(10000, posterior_alpha_C, posterior_beta_C)\nWe are already at the final step, calculating the statistics of interest.\n\n# Sample from the posterior distributions\nposterior_obs_T &lt;- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C &lt;- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Estimate the probability that T is better than C\nprob_T_better &lt;- mean(posterior_obs_T &gt; posterior_obs_C)\nprint(paste(\"Probability that T is better than C:\", round(prob_T_better, digit=3)))\n\n# Estimate the average treatment effect\ntreatment_effect &lt;- mean(posterior_obs_T - posterior_obs_C)\nprint(paste(\"Average change in Y b/w T and C:\", round(treatment_effect, digit=3)))\n\ntreatment_effect_limit &lt;- (alpha_T + x_T)/(alpha_T + beta_T + n_T) - (alpha_C + x_C)/(alpha_C + beta_C + n_C)\nprint(treatment_effect_limit)\n\n# we can even plot both posteriors distributions.\ndf &lt;- data.frame(\n  y = c(posterior_obs_T, posterior_obs_C),\n  group = factor(rep(c(\"T\", \"C\"), each = 10000))\n)\n\nggplot(df, aes(x = y, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Posterior Distributions of Outcomes\",\n       x = \"Y = 1\", y = \"Density\") +\n  theme_minimal()\nYou can find the code in this GitHub repo. There is also a specalized bayesAB package in R. It produces some cool charts, so I definitely recommend giving it a try.\nSoftware Package: bayesAB"
  },
  {
    "objectID": "blog/TODO-bayesian-ab-tests.html#bottom-line",
    "href": "blog/TODO-bayesian-ab-tests.html#bottom-line",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBayesian inference offers a compelling alternative to the traditional methods based on frequentist statistics.\nThe main idea rests on incorporating prior information on the success rates in the treatment and control group as a starting point.\nAdvantages of bayesian methods include incorporating prior information, providing probabilistic interpretations, handling small sample sizes better, and enabling continuous learning.\nThe main challenge is the choice of prior distribution, which can be difficult without expert knowledge."
  },
  {
    "objectID": "blog/TODO-bayesian-ab-tests.html#where-to-learn-more",
    "href": "blog/TODO-bayesian-ab-tests.html#where-to-learn-more",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAccessible introductions to the world of Bayesian inference are common. An example is Will Kurt‚Äôs book ‚ÄúBayesian Statistics the Fun Way‚Äú. See also the papers I cite below. As almost everything else, Google is also a great starting point."
  },
  {
    "objectID": "blog/TODO-bayesian-ab-tests.html#references",
    "href": "blog/TODO-bayesian-ab-tests.html#references",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "References",
    "text": "References\nDeng, A. (2015, May). Objective bayesian two sample hypothesis testing for online controlled experiments. In Proceedings of the 24th International Conference on World Wide Web (pp.¬†923-928).\nKamalbasha, S., & Eugster, M. J. (2021). Bayesian A/B testing for business decisions. In Data Science‚ÄìAnalytics and Applications: Proceedings of the 3rd International Data Science Conference‚ÄìiDSC2020 (pp.¬†50-57). Springer Fachmedien Wiesbaden.\nKurt, Will. Bayesian statistics the fun way: understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. No Starch Press, 2019.\nStevens, N. T., & Hagar, L. (2022). Comparative probability metrics: using posterior probabilities to account for practical equivalence in A/B tests. The American Statistician, 76(3), 224-237."
  },
  {
    "objectID": "blog/TODO-variance-ps-matching.html",
    "href": "blog/TODO-variance-ps-matching.html",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "",
    "text": "Propensity score matching (PSM) is among the most popular methods for estimating causal effects with observational data. It lends its fame to both its power and simplicity. Under a certain set of commonly invoked assumptions, controlling for the propensity score alone (as opposed to the full set of covariates) is enough to remove bias between the treatment and control groups. The underlying idea of matching on the propensity score is incredibly simple and most data scientists can quickly estimate a treatment effect even from scratch.\nWhile estimating treatment effects is common and straightforward, calculating their variance along with the associated confidence intervals and p-values can be more challenging. So, how exactly do you compute the variance of PSM methods? In this article I will describe the most popular ways of doing that. My goal is to cover the intuition and I will spare the technical details. Interested readers can refer to the References section below for more detailed expositions.\nStatistical methods based on the propensity score function come in different flavors (e.g., weighting, matching, subclassification, etc.), but today I will focus on \\(1:1\\) or \\(1:N\\) matching. There are also several potential estimands of interest, such as ATE (average treatment effect), ATT (ATE for the treatment group) but I will talk specifically about the latter."
  },
  {
    "objectID": "blog/TODO-variance-ps-matching.html#background",
    "href": "blog/TODO-variance-ps-matching.html#background",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "",
    "text": "Propensity score matching (PSM) is among the most popular methods for estimating causal effects with observational data. It lends its fame to both its power and simplicity. Under a certain set of commonly invoked assumptions, controlling for the propensity score alone (as opposed to the full set of covariates) is enough to remove bias between the treatment and control groups. The underlying idea of matching on the propensity score is incredibly simple and most data scientists can quickly estimate a treatment effect even from scratch.\nWhile estimating treatment effects is common and straightforward, calculating their variance along with the associated confidence intervals and p-values can be more challenging. So, how exactly do you compute the variance of PSM methods? In this article I will describe the most popular ways of doing that. My goal is to cover the intuition and I will spare the technical details. Interested readers can refer to the References section below for more detailed expositions.\nStatistical methods based on the propensity score function come in different flavors (e.g., weighting, matching, subclassification, etc.), but today I will focus on \\(1:1\\) or \\(1:N\\) matching. There are also several potential estimands of interest, such as ATE (average treatment effect), ATT (ATE for the treatment group) but I will talk specifically about the latter."
  },
  {
    "objectID": "blog/TODO-variance-ps-matching.html#diving-deeper",
    "href": "blog/TODO-variance-ps-matching.html#diving-deeper",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Diving Deeper",
    "text": "Diving Deeper\nWe can classify the methods for estimating the PSM variance in three distinct buckets.\n\nAsymptotic Approximations\nRecall the Law of Total Variance stating that, given a conditioning variable \\(X\\), we can always decompose a random variable \\(Y\\)‚Äôs variance, into two components ‚Äì the expectation of the conditional variance and the variance of the conditional expectation: \\[Var(Y) = Var(E(Y|X)) + E(Var(Y|X)).\\]\nAbadie and Imbens (2006) use this idea to derive the asymptotic variance formula for one-to-one and one-to-many matching estimators. They provide an analytical expression for the variance as \\(n \\to \\infty\\) (and hence, this is an approximation) using the matching covariates as conditioning variables. I will not be going into their work in detail, but a couple of notes are worth discussing.\nThe original formulation assumes the true propensity score function is known. This is rarely true and in practice we rely on an estimate of it. Practitioners then replace this true propensity score with the estimated one. This is OK, but not great ‚Äì it brings me to the second issue.\nOne needs to account for the fact that this propensity score is estimated. As such, it comes with uncertainty, which, if ignored, can potentially understate the variance of your estimator. Hello, type 1 errors!\nIn follow-up work, Abadie and Imbens (2016) propose correction which accounts for this uncertainty. Interestingly, in a work that I will discuss below, Bodory et al.¬†(2020) report that in practice this correction might increase or decrease the ATT variance.\n\n\nApproximation Based on Weights\nWe can express all treatment effect estimators as a difference of weighted means:\n\\[\\hat{\\tau} = \\frac{1}{n} \\sum\\hat{w} D Y - \\frac{1}{n} \\sum\\hat{w} (1-D) Y,\\]\nwhere the weights \\(\\hat{w}\\) may depend on the propensity score.\nLechner (2002) suggested calculating the variance of PSM based on this formula, assuming the weights are non-stochastic. In other words, the approach ignores the fact that the propensity score is estimated in a first step. The PSM variance is then equal to the sum of two terms ‚Äì the one for the treated and the one for the control.\n\n\nBootstrap\nBootstrap is the go-to method for estimating variances in complex settings where analytical expressions are too messy or even do not exist. PSM seems like a natural environment where the bootstrap can help. Not so fast!\nAbadie and Imbens (2006) show that the standard nonparametric bootstrap is inconsistent for non-smooth estimators (such as propensity score matching) with fixed number of matches and continuous covariates. Nevertheless, practitioners routinely ignore this result. While the theory says we should not use the bootstrap here, the extent to which this is of practical relevance is unclear. For instance, if this only makes a difference in the fourth decimal, we might be willing to sacrifice some bias for ease of computation.\nThere are at least two ways one can go around using the bootstrap here. The first one is the standard plain vanilla approach.\n\nRandomly draw B samples of size \\(n\\).\n\n\nAlternatively, you can also sample directly from the matched pairs which works better in certain cases.\n\n\nCompute ATT each time.\nCompute the confidence interval: \\[ \\hat{\\tau}\\pm \\sqrt{\\hat{V}(\\hat{\\tau}) \\times c}, \\]\n\nwhere \\(\\hat{V}(\\tau) = \\frac{1}{B-1}\\sum_{b=1}^B(\\hat{\\tau}^b-\\frac{1}{B}\\sum_b\\hat{\\tau}^b)^2\\) is the bootstrap variance of \\(\\hat{\\tau}}\\) and \\(c\\) is the critical value associated with a confidence level \\(\\alpha\\). Alternatively, in this last step we can directly use the$ $ and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution of \\(\\hat{\\tau}\\).\nThe second approach uses the well-known fact that the bootstrap behaves better when it uses so called asymptotically pivotal statistics ‚Äì i.e., ones which asymptotic distribution does not depend on unknown quantities. In this setting a candidate would be the t-statistic which, as we know, under certain conditions has a standard normal asymptotic distribution. We proceed in three steps:\nCompute the t-stat in the main sample using one of the variance approximations. Draw B random samples of size \\(n\\) and we compute the recentered \\(t\\)-stat with respect to \\(\\hat{\\tau}\\) in the main sample, \\[T^b = \\frac{\\hat{\\tau}^b - \\hat{\\tau}}{\\sqrt{\\hat{V}(\\tau^b)}}.\\]\nThe \\(p\\)-value is the share of (absolute value) bootstrap t-stats larger than the absolute value of the t-stat in the main sample \\[p{\\text{-value}} = 1 - \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|\\leq |T|) = \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|&gt; |T|),\\]\nwhere \\(T\\) is the \\(t\\)-stat from the main sample.\nThere are also newer (wild) bootstrap ideas which are still making their way into the mainstream."
  },
  {
    "objectID": "blog/TODO-variance-ps-matching.html#monte-carlo-simulations",
    "href": "blog/TODO-variance-ps-matching.html#monte-carlo-simulations",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Monte Carlo Simulations",
    "text": "Monte Carlo Simulations\nBodory et al.¬†(2020) compare the finite sample performance of several of these methods (plus others). Their analysis is more thorough in the sense that they include other ATT estimators besides PSM, such as PS weighting or radius matching.\nOverall, their results do not indicate a clear winner which performs best in all scenarios. Interestingly, even some of the bootstrap methods often outperform the asymptotic approximations of Abadie and Imbens (2006) or Lechner (2002)."
  },
  {
    "objectID": "blog/TODO-variance-ps-matching.html#where-to-learn-more",
    "href": "blog/TODO-variance-ps-matching.html#where-to-learn-more",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nI based this post on Bodory et al.¬†(2020)‚Äôs paper, so that is a natural place to start. Many other studies have focused on the performance of propensity score treatment effect estimators (as opposed to their variance) ‚Äì Huber et al.¬†(2013) and Busso et al.¬†(2014) are great starting points. Lastly, Imbens (2015) is a great resource for an overview of matching methods more generally along with some of the problems they solve as well as the pitfalls they entail."
  },
  {
    "objectID": "blog/TODO-variance-ps-matching.html#bottom-line",
    "href": "blog/TODO-variance-ps-matching.html#bottom-line",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThere is no shortage of methods when it comes to estimating the variance of propensity score matching estimators.\nAbadie and Imbens (2006) showed that the standard bootstrap is inconsistent in this setting with continuous covariates and formally derived an asymptotic approximation of the true variance.\nMonte Carlo simulations, however, show that bootstrap methods offer a good tradeoff between simplicity and performance."
  },
  {
    "objectID": "blog/TODO-variance-ps-matching.html#references",
    "href": "blog/TODO-variance-ps-matching.html#references",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "References",
    "text": "References\nAbadie, A., & Imbens, G. W. (2006). Large sample properties of matching estimators for average treatment effects. Econometrica, 74(1), 235-267.\nAbadie, A., & Imbens, G. W. (2008). On the failure of the bootstrap for matching estimators. Econometrica, 76(6), 1537-1557.\nAustin, P. C., & Small, D. S. (2014). The use of bootstrapping when using propensity‚Äêscore matching without replacement: a simulation study. Statistics in medicine, 33(24), 4306-4319.\nBodory, H., Camponovo, L., Huber, M., & Lechner, M. (2020). The finite sample performance of inference methods for propensity score matching and weighting estimators. Journal of Business & Economic Statistics, 38(1), 183-200.\nBusso, M., DiNardo, J., & McCrary, J. (2014). New evidence on the finite sample properties of propensity score reweighting and matching estimators. Review of Economics and Statistics, 96(5), 885-897.\nCaliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of propensity score matching. Journal of economic surveys, 22(1), 31-72.\nHuber, M., Lechner, M., & Wunsch, C. (2013). The performance of estimators based on the propensity score. Journal of Econometrics, 175(1), 1-21.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nLechner, M. (2002). Some practical issues in the evaluation of heterogeneous labour market programmes by matching methods. Journal of the Royal Statistical Society: Series A (Statistics in Society), 165(1), 59-82."
  },
  {
    "objectID": "blog/TODO-overview-ml-methods-ci.html",
    "href": "blog/TODO-overview-ml-methods-ci.html",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "",
    "text": "The most exciting trend in causal inference over the last decade has been the infusion of machine learning (ML) techniques. Supervised machine learning is designed to find complex patterns in data and as such, it is merely occupied with prediction. Causal inference, on the other hand, pays close attention to statistical precision and inference based on asymptotic properties like consistency and normality. The two worlds are, thus, fundamentally different.\nIt should be no surprise then that machine learning and causal inference do not naturally speak to each other, and some modifications are required to marry them. The good news is recent innovations have led to a bunch of ways in which ML models can be used in isolating causal effects especially in settings with many covariates (also called ‚Äúhigh dimensional‚Äù settings).\nIn this article, I will briefly describe the ways in which we can use ML when looking for causal relationships. I will indeed be concise, and I will avoid diving deeper into technical details. This blog post will look more like a laundry list with references to papers and software packages than a tutorial."
  },
  {
    "objectID": "blog/TODO-overview-ml-methods-ci.html#background",
    "href": "blog/TODO-overview-ml-methods-ci.html#background",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "",
    "text": "The most exciting trend in causal inference over the last decade has been the infusion of machine learning (ML) techniques. Supervised machine learning is designed to find complex patterns in data and as such, it is merely occupied with prediction. Causal inference, on the other hand, pays close attention to statistical precision and inference based on asymptotic properties like consistency and normality. The two worlds are, thus, fundamentally different.\nIt should be no surprise then that machine learning and causal inference do not naturally speak to each other, and some modifications are required to marry them. The good news is recent innovations have led to a bunch of ways in which ML models can be used in isolating causal effects especially in settings with many covariates (also called ‚Äúhigh dimensional‚Äù settings).\nIn this article, I will briefly describe the ways in which we can use ML when looking for causal relationships. I will indeed be concise, and I will avoid diving deeper into technical details. This blog post will look more like a laundry list with references to papers and software packages than a tutorial."
  },
  {
    "objectID": "blog/TODO-overview-ml-methods-ci.html#notation",
    "href": "blog/TODO-overview-ml-methods-ci.html#notation",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Notation",
    "text": "Notation\nIt is helpful to quickly summarize some features of the potential outcome framework. Imagine we have a i.i.d. random sample of a binary treatment indicator D, outcome variable Y and a vector of covariates X. Assume the potential outcomes \\(Y(0)\\) and \\(Y(1)\\) are unrelated to the binary treatment status D which is often referred to as the unconfoundedness or ignorability.\nA common estimand of interest is the Average Treatment Effect (ATE)\n\\[ATE = E[Y(1) - Y(0)],\\]\nwhere \\(Y(d)\\) is the potential outcome under treatment regime \\(D=d\\). Another popular estimand is the Conditional ATE (CATE),\n\\[CATE(X) = E[Y(1) - Y(0) | X],\\]\nwhich is the ATE for a particular group of units with a fixed covariates level (e.g., women, men, new users, etc.).\nThe ATE can be expressed in at least three useful ways:\n\\[\\begin{align*} ATE & = \\mathbf{E} \\left[ \\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(outcome model only)} \\\\ & = \\mathbf{E}\\left[ \\frac{YD}{e(X)} - \\frac{Y(1-D)}{1-e(X)} \\right] \\hspace{1cm} \\text{(prop. score model only)} \\\\ & = \\mathbf{E} \\left[ \\frac{[Y-\\mu(1,X)D]}{e(X)} - \\frac{[Y-\\mu(0,X)](1-D)}{1-e(X)} \\right] \\\\ & + \\mathbf{E} \\left[\\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(both models)} \\end{align*}\\]\nwhere\n\\[\\mu(D,X) = \\mathbf{E}[Y|D,X]\\]\nis the outcome model and\n\\[e(x)=\\mathbf{E}[D|X]\\]\nis the propensity score.\nThis formulation is helpful because it naturally splits the types of treatment effect estimation methods into three separate categories ‚Äì (i) those that require only estimation of \\(\\mu(D,X)\\), (ii) those that use only \\(e(X)\\), and (iii) those that need both.\nOne can think of the propensity score (PS) and the outcome models as nuisance functions ‚Äì ones that are not of direct interest but play a part in treatment effect estimation. ML methods are attractive candidates for estimating these nuisance functions flexibly."
  },
  {
    "objectID": "blog/TODO-overview-ml-methods-ci.html#diving-deeper",
    "href": "blog/TODO-overview-ml-methods-ci.html#diving-deeper",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nCovariate Balancing Methods\nUnder the ignorability assumption, all confounding bias comes from differences in the covariates \\(X\\) between the treatment and the control groups. Intuitively, balancing these is enough to guarantee unbiasedness. One line of research develops methods to do exactly that ‚Äì directly equate covariates between the two groups of interest. These approaches circumvent estimation of the two nuisance functions mentioned above.\nThese are inspired by the ML view of data analysis framed as an optimization problem. Examples include Entropy Balancing, Genetic Matching, Stable Weights, and Residual Balancing. The last approach combines balancing with a regression adjustment to reduce extrapolation when estimating the counterfactuals for the treatment group. Some of these methods were designed with a low dimensional setting in mind, but they still carry the spirit of ML type of thinking.\nSoftware Packages: MatchIt, Ebal, BalanceHD.\n\n\nML Methods for the Propensity Score Model\nPropensity score methods rely on correctly specifying the PS model. In low-dimensional settings, it is possible to estimate it nonparametrically. In practice, however, this is unrealistic when data scientists have access to continuous or even bunch of discrete covariates. Can ML methods come to the rescue?\nIn principle, yes. A major challenge in this context, however, is the choice of a loss function. In the ML world loss functions target measures of fit (e.g., Root Mean Squared Error, log likelihood, etc.) but these would be problematic here as they do not aim at balancing covariates important to reduce bias. Thus, these methods do not perform very well unless used with much caution.\nImai and Ratkovic (2014) propose a PS method that directly balances covariates. Another choice is the Boosted CART implementation. As its name suggests, it iteratively forms a bunch of tree models and averages them, but with an appropriately chosen loss function. A series of simulation studies analyze the performance of various ML algorithms used to estimate the PS, but overall, these methods are nowadays dominated by some of the doubly robust approaches described below.\nSoftware Packages: TWANG, CBPS.\n\n\nML Methods for the Outcome Model\nWe can also estimate treatment effects directly by modelling the outcome variable. This also requires correct model specification, and even then, it is prone to extrapolation in finite samples. Examples include Bayesian Additive Regression Trees (BART) and other ensemble methods.\nBelloni et al.¬†(2014) show that the set of features optimal when estimating the outcome model, is not necessarily optimal for estimating treatment effects. The issue is omitting a variable that is correlated with the treatment even if its correlation with the outcome is only modest, can introduce considerable bias. Moreover, typically, the rate of convergence in this context when using ML models will be slower than , meaning that you will need much more data to get a good treatment effect estimate.\nOverall, there is no statistical theory of why ML methods should work well here, but some methods tend to perform well empirically. This brings us to the doubly robust approach.\nSoftware Packages: BART, rBART, BayesTree.\n\n\nML Methods for Both Models & Doubly Robust Methods\nMethods combining models for both the propensity score and the outcome have long been advocated. Intuitively, the propensity score can be seen as a balancing step after which regression adjustment can remove any remaining bias. Imbens (2015), for instance, promotes this type of thinking in matching methods specifically.\nDoubly robust (DR) estimators use both nuisance models and have the amazing property of being consistent even if only one of the two models is correctly specified. You can think of the bias term as a product of the biases in the two nuisance models ‚Äì if one of them is equal to zero, the entire term vanishes. Additionally, if both models are correctly specified, some methods are semiparametrically efficient, (i.e., ‚Äúthe best‚Äù in a large class of flexible models). A simple DR method is the Augmented Inverse Probability Weighting (AIPW) estimator which in linear models comes down to running weighted OLS regression of Y on D and X with the estimated (inverse) propensity score as weights.\nThere is more good news. Amazingly, DR methods can still converge at a rate \\(\\sqrt{N}\\) even if the underlying nuisance models converge at slower rates. The formal requirement is that the nuisance models must belong to something called a Donsker class. In simple words, they should not be too complex and prone to overfit.\nOne line of research has developed the Double ML framework. This work has been so influential that it deserves a blog post of its own. Without going into technical details, the authors of the original paper show that na√Øve application of ML methods when estimating both nuisance functions results in two types of biases ‚Äì regularization and overfitting. DoubleML makes use of something called Neyman orthogonalization (think of the Frisch‚ÄìWaugh‚ÄìLovell theorem) to remove the former, and sample splitting to avoid the latter. In simple settings, this method combines the residuals from regressions of \\(Y\\) on \\(X\\) and \\(Y\\) on \\(D\\), but in general it can take more complicated forms.\nAnother line of research has developed the Double Post Lasso approach. The idea here is simpler ‚Äì use Lasso to select covariates relevant to the outcome regression and then again to select ones relevant to the propensity score. Lastly, use OLS to regress \\(Y\\) on the union of the covariates selected previously. This procedure removes confounding or regularization bias that might be present in methods using ML models to estimate only one of the nuisance models.\nSoftware Packages: DoubleML, hdm, dlsr.\n\n\nHeterogeneous Treatment Effect Estimation\nMining for heterogeneous treatment effects has been a particularly fruitful field for ML methods. A leading example is the causal tree method developed by Athey and Imbens (2016). It resembles the traditional CART algorithm, but it uses a different criterion for splitting the data: instead of focusing on Mean Squared Error (MSE) for the outcome, it uses MSE for treatment effect. The result is a decision tree, in which units in each leaf have similar treatment effects. The method also features ‚Äúhonest‚Äù sample splitting for obtaining variance estimates ‚Äì one half of the data is used to determine the optimal tree, and the other half to estimate the treatment effects.\nBuilding on this idea, Wager and Athey (2018) propose a random forest-based method which generates a bunch of causal trees and averages the results to induce smoothness in the treatment effect‚Äôs function. Magically, the authors show the predictions are asymptotically normal and centered around the true value for each unit! This is exciting as it allows for standard methods for statistical inference.\nOther methods include BART, a Bayesian version of random forests mentioned above, Imai and Ratkovic (2013) who propose adding treatment indicators interacted with covariates in a LASSO regression to determine which variables are important for treatment effect heterogeneity. Similarly, Tian et al.¬†(2014) suggest modifying the covariates in a straightforward way and running a regression of the outcome on the modified variables without an intercept. Other methods include the R-learner of Nie and Wager (2021) which relies on estimating the two nuisance models described above and using a special loss function. K√ºnzel et al.¬†(2019) propose a X-learner metaalogrithm.\nSoftware Packages: FindIt, rlearner, grf, causalToolbox.\n\n\nOthers\nA by-product of estimating treatment effect heterogeneity is that we can determine which units should be treated. Intuitively, if the treatment effect is close to zero (or even negative) for some users, there is not much to be gained from the exposure. Kitagawa and Tetenov (2018) analyze a setting with limited complexity, and Athey and Wager (2021) develop the DoubleML framework discussed above for choosing whom to treat. ML has also been used for variance reduction in randomized experiments via regression adjustments. See, for instance, Wager et al.¬†(2016), Bloniarz et al.¬†(2016), and List et al.¬†(2022)."
  },
  {
    "objectID": "blog/TODO-overview-ml-methods-ci.html#bottom-line",
    "href": "blog/TODO-overview-ml-methods-ci.html#bottom-line",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods are slowly becoming an indispensable part of data scientists‚Äô toolkit for estimating causal relationships. There is an abundance of methods aiding practitioners in both ATE and CATE estimation.\nDoubly robust approaches offer better theoretical guarantees than methods relying on estimating either the outcome or the propensity score models.\nThe leading approaches for estimating ATEs are Double ML and Double Post Lasso.\nThe leading approach for estimating CATEs is the causal forest method."
  },
  {
    "objectID": "blog/TODO-overview-ml-methods-ci.html#where-to-learn-more",
    "href": "blog/TODO-overview-ml-methods-ci.html#where-to-learn-more",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMore technical data scientists will find the following review papers useful:\n\nAthey and Imbens (2019)\nAthey and Imbens (2017)\nVarian (2014)\nKreif and DiazOrdaz (2019)\nMullainathan and Spiess (2017)\nHu (2023)\n\nThere are a few major Python frameworks for using ML in causal inference estimation. More practically-oriented folks might like their documentation:\n\nCausalML\nEconML\nDoubleML"
  },
  {
    "objectID": "blog/TODO-overview-ml-methods-ci.html#references",
    "href": "blog/TODO-overview-ml-methods-ci.html#references",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "References",
    "text": "References\nAthey, S., & Imbens, G. (2016). Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27), 7353-7360.\nAthey, S., & Imbens, G. W. (2017). The state of applied econometrics: Causality and policy evaluation. Journal of Economic perspectives, 31(2), 3-32.\nAthey, S., & Imbens, G. W. (2019). Machine learning methods that economists should know about. Annual Review of Economics, 11, 685-725.\nAthey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 80(4), 597-623.\nAthey, S., & Wager, S. (2021). Policy learning with observational data. Econometrica, 89(1), 133-161.\nAustin, P. C. (2012). Using ensemble-based methods for directly estimating causal effects: an investigation of tree-based G-computation. Multivariate behavioral research, 47(1), 115-135.\nBelloni, A., Chernozhukov, V., & Hansen, C. (2014). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2), 608-650.\nBloniarz, A., Liu, H., Zhang, C. H., Sekhon, J. S., & Yu, B. (2016). Lasso adjustments of treatment effect estimates in randomized experiments. Proceedings of the National Academy of Sciences, 113(27), 7383-7390.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal.\nDiamond, A., & Sekhon, J. S. (2013). Genetic matching for estimating causal effects: A general multivariate matching method for achieving balance in observational studies. Review of Economics and Statistics, 95(3), 932-945.\nHahn, P. R., Murray, J. S., & Carvalho, C. M. (2020). Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion). Bayesian Analysis, 15(3), 965-1056.\nHainmueller, J. (2012). Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20(1), 25-46.\nHill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240.\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. Annals of Applied Statistics\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B: Statistical Methodology, 243-263.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nKitagawa, T., & Tetenov, A. (2018). Who should be treated? empirical welfare maximization methods for treatment choice. Econometrica, 86(2), 591-616.\nKreif, N., & DiazOrdaz, K. (2019). Machine learning in policy evaluation: new tools for causal inference. arXiv preprint arXiv:1903.00402.\nK√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165.\nLee, B. K., Lessler, J., & Stuart, E. A. (2010). Improving propensity score weighting using machine learning. Statistics in medicine, 29(3), 337-346.\nList, J. A., Muir, I., & Sun, G. K. (2022). Using Machine Learning for Efficient Flexible Regression Adjustment in Economic Experiments (No.¬†w30756). National Bureau of Economic Research.\nMcCaffrey, D. F., Ridgeway, G., & Morral, A. R. (2004). Propensity score estimation with boosted regression for evaluating causal effects in observational studies. Psychological methods, 9(4), 403.\nMullainathan, S., & Spiess, J. (2017). Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2), 87-106.\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\nRobins, J. M., Rotnitzky, A., & Zhao, L. P. (1994). Estimation of regression coefficients when some regressors are not always observed. Journal of the American statistical Association, 89(427), 846-866.\nSetoguchi, S., Schneeweiss, S., Brookhart, M. A., Glynn, R. J., & Cook, E. F. (2008). Evaluating uses of data mining techniques in propensity score estimation: a simulation study. Pharmacoepidemiology and drug safety, 17(6), 546-555.\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\nVarian, H. R. (2014). Big data: New tricks for econometrics. Journal of Economic Perspectives, 28(2), 3-28.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nWager, S., Du, W., Taylor, J., & Tibshirani, R. J. (2016). High-dimensional regression adjustments in randomized experiments. Proceedings of the National Academy of Sciences, 113(45), 12673-12678.\nWestreich, D., Lessler, J., & Funk, M. J. (2010). Propensity score estimation: neural networks, support vector machines, decision trees (CART), and meta-classifiers as alternatives to logistic regression. Journal of clinical epidemiology, 63(8), 826-833.\nWyss, R., Ellis, A. R., Brookhart, M. A., Girman, C. J., Jonsson Funk, M., LoCasale, R., & St√ºrmer, T. (2014). The role of prediction modeling in propensity score estimation: an evaluation of logistic regression, bCART, and the covariate-balancing propensity score. American journal of epidemiology, 180(6), 645-655.\nZivich, P. N., & Breskin, A. (2021). Machine learning for causal inference: on the use of cross-fit estimators. Epidemiology (Cambridge, Mass.), 32(3), 393.\nZubizarreta, J. R. (2015). Stable weights that balance covariates for estimation with incomplete outcome data. Journal of the American Statistical Association, 110(511), 910-922."
  },
  {
    "objectID": "blog/DONE-alphabet-het-treatment-effects.html",
    "href": "blog/DONE-alphabet-het-treatment-effects.html",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "",
    "text": "Numerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods ‚Äì the so-called \\(S\\)-, \\(T\\)-, \\(X\\)- and \\(R\\)-learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as the lasso, gradient boosting, or even neural networks. In some clever sense, \\(STXR\\) is the \\(ABC\\) of heterogeneous treatment effect estimation.\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The CausalML Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them."
  },
  {
    "objectID": "blog/DONE-alphabet-het-treatment-effects.html#background",
    "href": "blog/DONE-alphabet-het-treatment-effects.html#background",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "",
    "text": "Numerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods ‚Äì the so-called \\(S\\)-, \\(T\\)-, \\(X\\)- and \\(R\\)-learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as the lasso, gradient boosting, or even neural networks. In some clever sense, \\(STXR\\) is the \\(ABC\\) of heterogeneous treatment effect estimation.\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The CausalML Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them."
  },
  {
    "objectID": "blog/DONE-alphabet-het-treatment-effects.html#notation",
    "href": "blog/DONE-alphabet-het-treatment-effects.html#notation",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Notation",
    "text": "Notation\nAs usual, let‚Äôs begin by setting some mathematical notation. I use D to denote a binary treatment indicator, \\(Y\\) is the observed outcome and \\(X\\) is a covariate of interest. The potential outcomes under each treatment state are \\(Y(0)\\) and \\(Y(1)\\), and \\(p\\) is the (conditional) probability of assignment into the treatment (i.e., the propensity score).\nThe average treatment effect is then the mean difference in potential outcomes across all units:\n\\[ATE = E[Y(1)-E(0)].\\]\nInterest is, instead, in the ATE for units with values \\(X=x\\) which I refer to as the heterogeneous treatment effect, \\(HTE(X)\\):\n\\[HTE(X) = E[Y(1)-E(0)|X].\\]\nIt is also helpful to define the conditional outcome functions under each treatment state:\n\\[\\mu(X,d) = E[Y(d)|X].\\]\nIt then follows that \\(HTE(X)\\) can also be expressed as:\n\\[HTE(X) = \\mu(X,1) - \\mu(X,0).\\]"
  },
  {
    "objectID": "blog/DONE-alphabet-het-treatment-effects.html#a-closer-look",
    "href": "blog/DONE-alphabet-het-treatment-effects.html#a-closer-look",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\nI will now briefly describe the four learners for heterogeneous treatment effects in ascending order of complexity.\n\n\\(S\\)(ingle) Learner\nThe idea behind the \\(S\\)-learner is to estimate a single outcome function \\(\\mu(X,D)\\) and then calculate \\(HTE(X)\\) by taking the difference in the predicted values between the units in the treatment and control groups.\nAlgorithm:\n\nUse the entire sample to estimate \\(\\hat{\\mu}(X,D)\\).\nCompute \\(\\hat{HTE}(X)=\\hat{\\mu}(X,1)-\\hat{\\mu}(X,0)\\).\n\nThis is intuitive and fine. But the problem is that the treatment variable D might be excluded in step 1 when it is not highly correlated with the outcome. (Note that in observational data this does not necessarily imply a null treatment effect.) In this case, we cannot even move to step 2.\n\n\n\\(T\\)(wo) Learner\nThe \\(T\\)-learner solves the above problem by forcing the response models to include \\(D\\). The idea is to first estimate two separate (conditional) outcome functions ‚Äì one for the treatment and one for the control and proceed similarly.\nAlgorithm:\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for \\(\\hat{\\mu}(X,1)\\).\nThen \\(\\hat{HTE}(X)=\\hat{\\mu}(X,1)- \\hat{\\mu}(X,0)\\)\n\nThis is better, but still not great. A potential problem is when there are different number of observations in the treatment and control groups. For instance, in the common case where the treatment group is much smaller, their \\(HTE(X)\\) will be estimated much less precisely than the that of the control group. When combining the two to arrive at a final estimate of \\(HTE(X)\\) the former should get a smaller weight than the latter. This is because the ML algorithms optimize for learning the \\(\\mu(\\cdot)\\) functions, and not the \\(HTE(X)\\) function directly.\n\n\n\\(X\\) Learner\nThe \\(X\\)-learner is designed to overcome the above concern. The procedure starts similarly to the \\(T\\)-learner but then weighs differently the \\(HTE(X)\\)‚Äôs for the treatment and control groups.\nAlgorithm:\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for $ (X,1)$.\nEstimate the unit-level treatment effect for the observations in the control group, \\(\\hat{\\mu}(X,1)-Y\\), and for the treatment group, \\(Y-\\hat{\\mu}(X,0)\\).\nCombine both estimates by weighing them using the predicted propensity score, \\(\\hat{p}\\): \\[HTE(X)=\\hat{p} \\times HTE(X|D=1) + (1-\\hat{p})\\times HTE(X|D=0).\\]\n\nHere \\(\\hat{p}\\) balances the uncertainty associated with the \\(HTE(X)\\)‚Äôs in each group and hence, this approach is particularly effective when there is a significant difference in the number of units between the two groups.\n\n\n\\(R\\)(obinson) Learner\nTo avoid getting too deep into technical details, I will not be describing the entire algorithm. The \\(R\\)-learner models both the outcome, and the propensity score and begins by computing unit-level predicted outcomes and treatment probabilities using cross-fitting and leave-one-out estimation. The key innovation is plugging these into a novel loss function featuring squared deviations of these predictions as well as a regularization term. This ensures ‚Äúoptimality‚Äù in learning the treatment effect heterogeneity."
  },
  {
    "objectID": "blog/DONE-alphabet-het-treatment-effects.html#bottom-line",
    "href": "blog/DONE-alphabet-het-treatment-effects.html#bottom-line",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nML methods offer a promising way of determining which groups of units experience differential response to treatments.\nI summarized four such model-agnostic methods ‚Äì the \\(S\\)-, \\(T\\)-, \\(X\\)-, and \\(R\\)-learners.\nCompared to the simpler \\(S\\)- and \\(T\\)- learners, the \\(X\\)- and \\(R\\)-learners solve some common issues and are more attractive options in most settings."
  },
  {
    "objectID": "blog/DONE-alphabet-het-treatment-effects.html#where-to-learn-more",
    "href": "blog/DONE-alphabet-het-treatment-effects.html#where-to-learn-more",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nI have previously written on how ML can be useful in causal inference, more generally and how we can use the Lasso to estimate heterogeneous treatment effects. Hu (2022) offers a detailed summary of a bunch of ML methods for HTE estimation. A Statistical Odds and Ends blog post describes the \\(S\\)-, \\(T\\)- and \\(X\\)-learners and contains useful advice on when each of them is preferrable. Chapter 21 of Causal Inference for the Brave and True also discusses this material and provides useful examples."
  },
  {
    "objectID": "blog/DONE-alphabet-het-treatment-effects.html#references",
    "href": "blog/DONE-alphabet-het-treatment-effects.html#references",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "References",
    "text": "References\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\nK√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165. Nie,\nXie, N., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319."
  },
  {
    "objectID": "blog/DONE-hypo-testing-linear-ml.html",
    "href": "blog/DONE-hypo-testing-linear-ml.html",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "",
    "text": "Machine learning models are an indispensable part of data science. They are incredibly good at what they are designed for ‚Äì making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, p-values, or anything else related to statistical significance. Why?\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have already selected the most strongly correlated variables.\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The \\(t\\)-stats and \\(p\\)-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models ‚Äì Ridge, Elastic net, SCAD, etc."
  },
  {
    "objectID": "blog/DONE-hypo-testing-linear-ml.html#background",
    "href": "blog/DONE-hypo-testing-linear-ml.html#background",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "",
    "text": "Machine learning models are an indispensable part of data science. They are incredibly good at what they are designed for ‚Äì making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, p-values, or anything else related to statistical significance. Why?\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have already selected the most strongly correlated variables.\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The \\(t\\)-stats and \\(p\\)-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models ‚Äì Ridge, Elastic net, SCAD, etc."
  },
  {
    "objectID": "blog/DONE-hypo-testing-linear-ml.html#notation",
    "href": "blog/DONE-hypo-testing-linear-ml.html#notation",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Notation",
    "text": "Notation\nAs a reminder, \\(\\beta^{lasso}\\) is the solution to:\n\\[\\min_{\\beta} \\frac{1}{2} || Y-x\\beta|| ^2_2 + \\lambda ||\\beta||_1. \\]\nWe are trying to predict a vector \\(Y\\in \\mathbb{R}\\) with a set of features \\(X\\in \\mathbb{R}^{pxn}\\) with \\(p\\leq n\\), and \\(\\lambda\\) is a tuning parameter. When needed, I will use \\(j\\) to index individual columns (i.e., variables) of \\(X\\)."
  },
  {
    "objectID": "blog/DONE-hypo-testing-linear-ml.html#a-closer-look",
    "href": "blog/DONE-hypo-testing-linear-ml.html#a-closer-look",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nTwo Types of Models and Parameters\nWe first need to discuss an important subtlety. There are two distinct ways of thinking about performing hypothesis testing in ML models. The traditional view is that we have a true linear model which includes all variables:\n\\[\\begin{equation} Y=X\\beta_0+\\epsilon. \\end{equation}\\]\nWe are interested in testing whether \\(\\beta_0=0\\) ‚Äì that is, inference on the full model. This is certainly an intuitive target. The interpretation is that this model encapsulates all relevant causal variables for \\(Y\\). Importantly, even if a given variable $X_j $ is not selected, it still belongs to the model and has a meaningful interpretation.\nThere is an alternative way to think about the problem. Imagine we run a variable selection algorithm (e.g., lasso), which selects a subset \\(M=\\{1,\\dots,p\\}\\) of all available predictors (\\(X\\)), \\(M&lt;p\\) leading to the alternative model:\n\\[\\begin{equation} Y=X_M\\beta_M+u. \\end{equation}\\]\nNow we are interested in testing whether \\(\\beta_M=0\\) ‚Äì that is, inference on the selected model. Unlike the scenario above, here \\(\\beta_{Mj}\\) is interpreted as the change in \\(Y\\) for a unit change in \\(X_j\\) when all other variables in \\(X_M\\) (as opposed to all of \\(X\\)) are kept constant.\nWhich of the two targets is more intuitive?\nStatisticians argue vehemently about this. Some claim that the full model interpretation is inherently problematic. It is too na√Øve and perhaps even arrogant to think that (i) mother nature can be explained by a linear equation, (ii) we can measure and include the full set of relevant predictors. On top of this, there are also technical issues with this interpretation beyond the scope of this post.\nTo overcome these challenges, relatively recently, statisticians developed the idea of inference on the selected model. This introduces major technical challenges, however.\n\n\nThe Na√Øve Approach: What Not to Do\nFirst things first ‚Äì here is what we should not do.\n\nRun a Lasso regression.\nRun OLS regression on the subset of selected variables.\nPerform statistical inference with the estimated t-stats, confidence intervals, and p-values.\n\nThis is bad practice. Can you see why?\nIt is simply because we ignored the fact that we already peeked at the data when we ran the Lasso regression. Lasso already chose the variables that are strongly correlated with the outcome. Intuitively, we will need to inflate the \\(p\\)-values to account for the data exploration in the first step.\nIt turns out that, in general, both the finite- and large-sample distributions of these parameters are non-Gaussian and depend on unknown parameters in weird ways. Consequently, the calculated t-stats and p-values are all wrong, and there is little hope that anything simple can be done to save this approach. And no, the standard bootstrap cannot help us either.\nBut are there special cases when this approach might work? A recent paper titled ‚ÄúIn Defense of the Indefensible: A Very Na√Øve Approach to High-Dimensional Inference‚Äù argues that under very strict assumptions on \\(X\\) and \\(\\lambda\\), this method is actually kosher. The reason it works is that in the magical world of those assumptions, the set of variables that Lasso chooses is deterministic, and not random (hence circumventing the issue described above). The resulting estimator is unbiased and asymptotically normal ‚Äì hence hypothesis testing is trivial.\nHere is what we should do instead.\n\n\nThe Classical Approach: Inference on the Full Model\nRoughly speaking, there are at least four ways we can go about doing hypothesis testing for \\(\\beta\\) in equation (1).\n\nData Split\n\nSplit our data into two equal parts.\nRun a Lasso regression on the first part.\nRun OLS on the second part with the selected variables from Step 2.\nPerform inference using the computed \\(t\\)-stats, \\(p\\)-values, and confidence intervals.\n\nThis is simple and intuitive. The problem is that in small samples, the \\(p\\)-values can be quite sensitive to how we split the data in the first step. This is clearly undesirable, as we will be getting different results every time we run this algorithm for no apparent reason.\n\n\nMulti Split\nThis is a modification of the Data Split approach designed to solve the sensitivity issue and increase power.\n\nRepeat \\(B\\) times:\n\n\nReshuffle data.\nRun the Data Split method.\nSave the p-values.\n\n\nAggregate the B \\(p\\)-values into a single final one for each variable.\n\nInstead of splitting the data into two parts only once, we can do it many times, and each time, we get a \\(p\\)-value for every variable. The aggregation goes a long way to solving the instability of the simple data split approach. There is a lot of clever mathematics behind it. For example, there is a complicated expression for aggregating the \\(p\\)-values rather than taking a simple average.\nSoftware Package: hdi.\n\n\nBias Correction\nThis approach tackles the problem from a very different angle. The idea is to directly remove the bias from the na√Øve Lasso procedure without any subsampling or data splitting. Somewhat magically, the resulting estimator is unbiased and asymptotically normally distributed ‚Äì statistical inference is then straightforward.\nThere are multiple versions of this idea, but the general form of these estimators is:\n\\[\\hat{\\beta}^{\\text{bias cor}} = \\hat{\\beta}^{lasso} + \\hat{\\Theta} X'\\epsilon^{lasso},\\]\nWhere \\(\\hat{\\beta}^{lasso}\\) is the lasso estimator and \\(\\epsilon^{lasso}\\) are the residuals. The missing piece is the \\(\\hat{\\Theta}\\) matrix; there are several ways to estimate it depending on the setting. In its simplest form, \\(\\hat{\\Theta}\\) is the inverse of the sample variance-covariance matrix. Other examples include the matrix, which minimizes an error term related to the bias as well as the variance of its Gaussian component. Similar bias-correction methods have been developed for Ridge regression as well.\nSoftware Package: hdi.\n\n\nBootstrap\nAs in many other complicated settings for statistical inference, the bootstrap can come to the rescue. Still, the plain vanilla bootstrap will not do. Instead, here is the general idea of the leading version of the bootstrap estimator for Lasso:\n\nRun a Lasso regression.\nKeep only \\(\\beta^{lasso}\\)‚Äôs larger than some magical threshold.\nCompute the associated residuals and center them around \\(0\\).\nRepeat B times:\n\n\ndraw random samples of these centered residuals,\ncompute new responses \\(\\dot{Y}\\) by adding them to the predictions \\(X'\\beta^{lasso}\\), and\nobtain \\(\\beta^{lasso}\\) coefficients from Lasso regressions on these new responses \\(\\dot{Y}\\).\n\n\nUse the distribution of the obtained coefficients to conduct statistical inference.\n\nThis idea can be generalized to other settings and, for instance, be combined with the bias-corrected estimator.\nThis wraps up our discussion of methods for performing hypothesis testing on equation (1) (i.e., the full model). We now move on to a more challenging topic ‚Äì inference on equation (2) (i.e., the selected model).\n\n\n\nThe Novel Approach: Inference on the Selected Model\n\nPoSI (Post Selection Inference)\nThe goal of the PoSI method is to construct confidence intervals that are valid regardless of the variable selection method and the selected submodel. The benefit is that we would be reaching the correct conclusion even if we did not select the true model. This luxury comes at the expense of often being too conservative (i.e., confidence intervals are ‚Äútoo wide‚Äù). Let me explain how this is done.\nTo take a step back, most confidence intervals in statistics take the form:\n\\[\\hat{\\beta} \\pm m \\times \\hat{SE}(\\hat{\\beta}).\\]\nEvery data scientist has seen a similar formula before. The question is usually one about choosing the constant \\(m\\). When we work with two-sided hypotheses tests and large samples, we often use \\(m = 1.96\\) because this is roughly the \\(97.5\\)th percentile of the \\(t\\)-distribution with many degrees of freedom. This gives a \\(2.5\\%\\) false positive error on both tails of the distribution (\\(5\\%\\) in total) and hence the associated 95% confidence intervals. The larger m, the wider or more conservative the confidence interval.\nThere are a few ways to choose the constant m in the PoSI world. Vaguely speaking, PoSI says we should select this constant to equal the \\(97.5\\)th percentile of a distribution related to the largest \\(t\\)-statistic among all possible models. This is usually approximated with Monte Carlo simulations. Interestingly, we do not need the response variable to approximate the value.\n\\[m = \\max_{\\text{ \\{all models and vars\\} }} |t|.\\]\nAnother and even more conservative choice for \\(m\\) is the Scheffe constant.\n\\[m^{scheffe} = \\sqrt{rank(X) \\times F(rank(X),  n-p)},\\]\nwhere \\(F(\\dot)\\) denotes the \\(95\\)th percentile of the \\(F\\) distribution with the respective degrees of freedom.\nUnfortunately, you guessed correctly that this method does not scale well. In some sense, it is a ‚Äúbrute force‚Äù method that scans through all possible model combinations and all variables within each model and picks the most conservative value. The authors recommend this procedure for datasets with roughly \\(p&lt;20\\). This rules out many practical applications where machine learning is most useful.\nSoftware Package: PoSI\n\n\nEPoSI (Exact PoSI)\nOk, the name of this approach is not super original. The ‚ÄúE‚Äù here stands for ‚Äúexact.‚Äù Unlike its cousin, this approach is valid only in the selected submodel. Because we cover fewer scenarios, the intervals will generally be narrower than the PoSI ones. EPoSI produces valid finite sample (as opposed to asymptotic) confidence intervals and \\(p\\)-values. Like all methods described here, the math behind this is extremely technical. So, I will give you only a high-level description of how this works.\nThe idea is first to get the conditional distribution of \\(\\beta\\) given the selected model. A bit magically, it turns out it is a truncated normal distribution. Really, who would have guessed this? Do you even remember truncated probability distributions (hint: they are just like regular PDFs but bounded from at least one side. This requires further scaling so that the density area sums to 1.)?\nTo dig one layer deeper, the authors show that the selection of Lasso predictors can be recast as a ‚Äúpolyhedral region‚Äù of the form:\n\\[ AY\\leq b. \\]\nIn English, for fixed \\(X\\) and \\(\\lambda\\), the set of alternative outcome values \\(Y^*\\) which yields the same set of selected predictors, can be expressed by the simple inequality above. In it, \\(A\\) and \\(b\\) depend do not depend on \\(Y\\). Under this new result, the distribution of \\(\\hat{\\beta}_M^{\\text{EPoSI}}\\) is now well-understood and tractable, thus enabling valid hypothesis testing.\nThen, we can use the conditional distribution function to construct a whimsical test statistic that is uniformly distributed on the \\([0,1]\\) interval. And we can finally build confidence intervals based on that statistic.\nSelective inference is currently among the hottest topic in all of statistics. There have been a myriad of extensions and improvements on the original paper. Still, this literature is painstakingly technical. What is the polyhedral selection property or a union of polytopes?\nSoftware Package: selectiveInference."
  },
  {
    "objectID": "blog/DONE-hypo-testing-linear-ml.html#an-example",
    "href": "blog/DONE-hypo-testing-linear-ml.html#an-example",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Refer to the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\nUnlike in Monte Carlo simulations, I do not know the ground truth here, so this exercise is not informative about which approaches work well and which do not. Rather, it just serves as an illustrative example.\nHere is a table displaying the number of statistically significant variables with \\(p &lt; .05\\) for various inference methods.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Number of Statistically Significant Predictors (p &lt; .05)\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n# vars p &lt; .05\n7\n5\n2\n3\n2\n2\n\n\n\nAs expected, the naive method results in the smallest \\(p\\)-values and hence the highest number of significant predictors ‚Äì seven. Data Split knocks down two of those seven variables, resulting in five significant ones. The rest are more conservative, leaving only two or three features with \\(p &lt; .05\\).\nBelow is the table with \\(p\\)-values for all variables and each method.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: p-values\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.01\n1.00\n\n\nage\n0.00\n0.04\n1.00\n0.01\n1.00\n1.00\n\n\nsibsp\n0.02\n0.03\n1.00\n0.21\n1.00\n1.00\n\n\nparch\n0.32\n0.64\n1.00\n1.00\n1.00\n1.00\n\n\nfare\n0.20\n0.21\n1.00\n1.00\n1.00\n1.00\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n\n\nembarkedS\n0.00\n0.01\n1.00\n0.06\n-\n1.00\n\n\ncabinA\n0.39\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinB\n0.35\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinD\n0.05\n0.27\n1.00\n0.44\n1.00\n-\n\n\ncabinE\n0.00\n0.64\n1.00\n0.06\n1.00\n1.00\n\n\ncabinF\n0.02\n0.52\n1.00\n0.21\n1.00\n1.00\n\n\nembarkedC\n-\n-\n1.00\n0.11\n1.00\n1.00\n\n\nembarkedQ\n-\n-\n1.00\n1.00\n1.00\n0.00\n\n\ncabinC\n-\n-\n1.00\n1.00\n1.00\n-\n\n\n\nYou can find the code for this exercise in this GitHub repo."
  },
  {
    "objectID": "blog/DONE-hypo-testing-linear-ml.html#bottom-line",
    "href": "blog/DONE-hypo-testing-linear-ml.html#bottom-line",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods mine through datasets to find strong correlations between the response and the features. Quantifying this strength is an open and challenging problem.\nThe naive approach to hypothesis testing is usually invalid.\nThere are two main approaches that work ‚Äì inference on the full model or on the selected model. The latter poses more technical challenges than the former.\nIf we are interested in the full model, the Multi Split approach is general enough to cover a wide range of models and settings.\nIf we believe you have to focus on the selected model, EPoSI is the state-of-the-art.\nSimulation exercises usually show no clear winner, as none of the methods consistently outperforms the rest."
  },
  {
    "objectID": "blog/DONE-hypo-testing-linear-ml.html#where-to-learn-more",
    "href": "blog/DONE-hypo-testing-linear-ml.html#where-to-learn-more",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nTaylor and Tibshirani (2015) give a non-technical introduction to the problem space along with a description of the POSI method ‚Äì a great read but focused on a single approach. Other studies both provide a relatively accessible overview of the various methods for statistical inference on the full model. For technical readers, Zhang et al.¬†(2022) provide an excellent up-to-date review of the literature, which I used extensively."
  },
  {
    "objectID": "blog/DONE-hypo-testing-linear-ml.html#references",
    "href": "blog/DONE-hypo-testing-linear-ml.html#references",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "References",
    "text": "References\nBerk, R., Brown, L., Buja, A., Zhang, K., & Zhao, L. (2013). Valid post-selection inference. The Annals of Statistics, 802-837.\nB√ºhlmann, P. (2013). Statistical significance in high-dimensional linear models. Bernoulli, 19(4), 1212-1242.\nDezeure, R., B√ºhlmann, P., Meier, L., & Meinshausen, N. (2015). High-dimensional inference: confidence intervals, p-values and R-software hdi. Statistical Science, 533-558.\nJavanmard, A., & Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15(1), 2869-2909.\nLee, J. D., Sun, D. L., Sun, Y., & Taylor, J. E. (2016). Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3), 907-927.\nLeeb, H., & P√∂tscher, B. M. (2005). Model selection and inference: Facts and fiction. Econometric Theory, 21(1), 21-59.\nLeeb, H., P√∂tscher, B. M., & Ewald, K. (2015). On various confidence intervals post-model-selection. Statistical Science, 30(2), 216-227.\nMeinshausen, N., Meier, L., & B√ºhlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\nTaylor, J., & Tibshirani, R. J. (2015). Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112(25), 7629-7634.\nVan de Geer, S., B√ºhlmann, P., Ritov, Y. A., & Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. The Annals of Statistics, 42(3), 1166-1202.\nWasserman, L., & Roeder, K. (2009). High dimensional variable selection. Annals of Statistics, 37(5A), 2178.\nZhang, D., Khalili, A., & Asgharian, M. (2022). Post-model-selection inference in linear regression models: An integrated review. Statistics Surveys, 16, 86-136.\nZhang, C. H., & Zhang, S. S. (2014). Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 217-242.\nZhao, S., Witten, D., & Shojaie, A. (2021). In defense of the indefensible: A very naive approach to high-dimensional inference. Statistical Science, 36(4), 562-577."
  },
  {
    "objectID": "blog/TODO-bayesian-ab-tests.html#a-closer-look",
    "href": "blog/TODO-bayesian-ab-tests.html#a-closer-look",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "A Closer Look",
    "text": "A Closer Look\nWe are interested in making inferences about the treatment effect, \\(\\tau\\), of the intervention \\(T\\) on the success rate \\(Y\\). In Bayesian terms, this means we seek the posterior distribution of \\(\\tau\\). Once we obtain this distribution or can draw observations from it, we can calculate various statistics to summarize the impact of the intervention, potentially providing a richer understanding of its effects.\nThe Bayesian methodology of analyzing experiments proceeds in four steps.\n\nStep 1: Specify the Prior Distribution\nWe begin by choosing the prior distributions for Y. The prior distribution represents our beliefs about the parameter before seeing the data. It is usually informed by previous A/B testing experience combined with deep context knowledge. For example, we might believe that the treatment effect is around two percentage points (\\(\\hat{\\tau}=0.02\\)) with some uncertainty around it.\nGiven our binary outcome, the Beta distribution serves as a natural conjugate prior: \\[ Y_i \\sim \\text{Beta} (\\alpha_i, \\beta_i), \\hspace{.5cm} i \\in \\{T,C\\}.  \\]\n\n\nStep 2: Specify the Likelihood Function\nFor binary outcomes, we model the data using a Binomial distribution:\n\\[X_i|Y_i \\sim \\text{Binomial}(N_i, Y_i), \\hspace{.5cm} i\\in\\{T,C\\}.  \\]\nThis choice reflects the inherent structure of our data: counting successes in a fixed number of trials. We are now ready to use the Bayes rule to arrive at the posterior distributions.\n\n\nStep 3: Posterior Distributions Derivation\nThanks to conjugacy between Beta and Binomial distributions, we obtain closed-form posterior distributions:\n\\[ Y_i | X_i \\sim \\text{Beta}(\\alpha_i+X_i, \\beta_i+N_i-X_i), \\hspace{.5cm} i\\in\\{T,C\\}  .\\]\nThis mathematical convenience allows us to avoid more complex numerical methods like MCMC sampling. We can now even plot the two posterior distributions and visually asses their differences.\n\n\nStep 4: Inference and Decision Making\nThe Bayesian framework enables rich analysis beyond traditional frequentist-style hypothesis testing. Here are some examples:\nProbability of Positive Impact\nWe can calculate the probability that the outcomes for the treatment group are higher than those of the control group. In closed form, we have:\n\\[ P(Y_T&gt;Y_C|X_T,X_C) = \\frac{1}{N}\\sum_i\\mathbf{1}(Y_{T,i}&gt;Y_{C,i}),\\]\nwhere \\(\\mathbf{1}(\\cdot)\\) is the indicator function. This is simply share of observations for which \\(Y_T &gt;Y_C\\).\nAverage Treatment Effect\nAnother potential example of an object of interest is the Average Treatment Effect (ATE):\n\\[ ATE = \\frac{1}{N_T}\\sum_{i \\in T}Y_{T,i} -  \\frac{1}{N_C}\\sum_{i \\in C}Y_{C,i} \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\n\\[ ATE \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\nAnd that‚Äôs it. You see how with the cost of additional assumptions we get much more than a single \\(p\\)-value, and that‚Äôs where much of the appeal of this approach lies.\n\n\nAdvantages\nOverall, Bayesian thinking entails some compelling advantages:\n\nIncorporation of Prior Information. It allows you to incorporate prior knowledge or expert opinion into the analysis. This is particularly useful when historical data or domain expertise is available.\nProbabilistic Interpretation: It provides direct probabilistic interpretations of the results, such as the probability that one variant is better than another. This may be more intuitive than frequentist p-values, which are often misinterpreted.\nHandling of Small Sample Sizes: It tends to perform better with small sample sizes because they incorporate prior distributions, which can regularize estimates and prevent overfitting.\nContinuous Learning: As new data comes in, Bayesian methods provide a natural way to update the posterior distribution, leading to continuous learning and adaptation.\n\nThe downsides are mostly related to the choice of prior distribution. In settings where there is a lack of expert knowledge, this Bayesian approach to experimentation might not be very attractive. Computation can also be an issue in more complex settings."
  },
  {
    "objectID": "blog/DONE-bayesian-ab-tests.html",
    "href": "blog/DONE-bayesian-ab-tests.html",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "",
    "text": "Imagine you‚Äôre a data scientist evaluating an A/B test of a new recommendation algorithm. The results show a modest but promising 0.5% lift in conversion rate‚Äîup from \\(8\\%\\) to \\(8.5\\%\\) across \\(20,000\\) users‚Äîalong with a \\(p\\)-value of \\(0.06\\). Given historical data, the improvement seems realistic, the cost to implement is low, and the projected revenue impact is substantial. Yet, the conventional analysis compels you to ‚Äúfail to reject the null hypothesis‚Äù simply because the p-value doesn‚Äôt meet an arbitrary \\(0.05\\) threshold.\nTraditional A/B testing approaches (randomized controlled trials or RCTs) rely primarily on frequentist methods like \\(t\\)-tests or chi-squared tests to detect differences between treatment and control groups. For more precision, analysts often adjust for covariates in linear models. After weeks or months of testing, however, these methods often boil down to a single outcome: the \\(p\\)-value. This binary interpretation‚Äîsignificant or not‚Äîcan obscure valuable insights, as seen in the example above.\nBayesian statistics, by contrast, offers a more nuanced and potentially more informative alternative. In this article, we‚Äôll walk through a practical example, discuss the implementation details, and dive into the Bayesian framework‚Äôs mathematical foundations for experimentation. One of the core advantages of the Bayesian approach is its ability to produce a full posterior distribution, offering a richer view of the experiment‚Äôs outcomes beyond just point estimates.\nThis article assumes a familiarity with Bayesian fundamentals like priors, posteriors, and conjugate distributions. If you‚Äôd like a refresher, see the References section below."
  },
  {
    "objectID": "blog/DONE-bayesian-ab-tests.html#background",
    "href": "blog/DONE-bayesian-ab-tests.html#background",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "",
    "text": "Imagine you‚Äôre a data scientist evaluating an A/B test of a new recommendation algorithm. The results show a modest but promising 0.5% lift in conversion rate‚Äîup from \\(8\\%\\) to \\(8.5\\%\\) across \\(20,000\\) users‚Äîalong with a \\(p\\)-value of \\(0.06\\). Given historical data, the improvement seems realistic, the cost to implement is low, and the projected revenue impact is substantial. Yet, the conventional analysis compels you to ‚Äúfail to reject the null hypothesis‚Äù simply because the p-value doesn‚Äôt meet an arbitrary \\(0.05\\) threshold.\nTraditional A/B testing approaches (randomized controlled trials or RCTs) rely primarily on frequentist methods like \\(t\\)-tests or chi-squared tests to detect differences between treatment and control groups. For more precision, analysts often adjust for covariates in linear models. After weeks or months of testing, however, these methods often boil down to a single outcome: the \\(p\\)-value. This binary interpretation‚Äîsignificant or not‚Äîcan obscure valuable insights, as seen in the example above.\nBayesian statistics, by contrast, offers a more nuanced and potentially more informative alternative. In this article, we‚Äôll walk through a practical example, discuss the implementation details, and dive into the Bayesian framework‚Äôs mathematical foundations for experimentation. One of the core advantages of the Bayesian approach is its ability to produce a full posterior distribution, offering a richer view of the experiment‚Äôs outcomes beyond just point estimates.\nThis article assumes a familiarity with Bayesian fundamentals like priors, posteriors, and conjugate distributions. If you‚Äôd like a refresher, see the References section below."
  },
  {
    "objectID": "blog/DONE-bayesian-ab-tests.html#notation",
    "href": "blog/DONE-bayesian-ab-tests.html#notation",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs establish our notation for a binary intervention randomized experiment:\n\n\\(T \\in {0,1}\\): Treatment indicator\n\\(N_T\\): Number of units in treatment group\n\\(N_C\\): Number of units in control group\n\\(N = N_T + N_C\\): Total sample size\n\\(X_T\\): Number of ‚Äúsuccesses‚Äù in treatment group\n\\(X_C\\): Number of ‚Äúsuccesses‚Äù in control group\n\\(Y\\): Success rate (e.g., conversion rate, employment status)."
  },
  {
    "objectID": "blog/DONE-bayesian-ab-tests.html#a-closer-look",
    "href": "blog/DONE-bayesian-ab-tests.html#a-closer-look",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "A Closer Look",
    "text": "A Closer Look\nWe are interested in making inferences about the treatment effect, \\(\\tau\\), of the intervention \\(T\\) on the success rate \\(Y\\). In Bayesian terms, this means we seek the posterior distribution of \\(\\tau\\). Once we obtain this distribution or can draw observations from it, we can calculate various statistics to summarize the impact of the intervention, potentially providing a richer understanding of its effects.\nThe Bayesian methodology of analyzing experiments proceeds in four steps.\n\nStep 1: Specify the Prior Distribution\nWe begin by choosing the prior distributions for Y. The prior distribution represents our beliefs about the parameter before seeing the data. It is usually informed by previous A/B testing experience combined with deep context knowledge. For example, we might believe that the treatment effect is around two percentage points (\\(\\hat{\\tau}=0.02\\)) with some uncertainty around it.\nGiven our binary outcome, the Beta distribution serves as a natural conjugate prior: \\[ Y_i \\sim \\text{Beta} (\\alpha_i, \\beta_i), \\hspace{.5cm} i \\in \\{T,C\\}.  \\]\n\n\nStep 2: Specify the Likelihood Function\nFor binary outcomes, we model the data using a Binomial distribution:\n\\[X_i|Y_i \\sim \\text{Binomial}(N_i, Y_i), \\hspace{.5cm} i\\in\\{T,C\\}.  \\]\nThis choice reflects the inherent structure of our data: counting successes in a fixed number of trials. We are now ready to use the Bayes rule to arrive at the posterior distributions.\n\n\nStep 3: Posterior Distributions Derivation\nThanks to conjugacy between Beta and Binomial distributions, we obtain closed-form posterior distributions:\n\\[ Y_i | X_i \\sim \\text{Beta}(\\alpha_i+X_i, \\beta_i+N_i-X_i), \\hspace{.5cm} i\\in\\{T,C\\}  .\\]\nThis mathematical convenience allows us to avoid more complex numerical methods like MCMC sampling. We can now even plot the two posterior distributions and visually asses their differences.\n\n\nStep 4: Inference and Decision Making\nThe Bayesian framework enables rich analysis beyond traditional frequentist-style hypothesis testing. Here are some examples:\nProbability of Positive Impact\nWe can calculate the probability that the outcomes for the treatment group are higher than those of the control group. In closed form, we have:\n\\[ P(Y_T&gt;Y_C|X_T,X_C) = \\frac{1}{N}\\sum_i\\mathbf{1}(Y_{T,i}&gt;Y_{C,i}),\\]\nwhere \\(\\mathbf{1}(\\cdot)\\) is the indicator function. This is simply share of observations for which \\(Y_T &gt;Y_C\\).\nAverage Treatment Effect\nAnother potential example of an object of interest is the Average Treatment Effect (ATE):\n\\[ ATE = \\frac{1}{N_T}\\sum_{i \\in T}Y_{T,i} -  \\frac{1}{N_C}\\sum_{i \\in C}Y_{C,i} \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\n\\[ ATE \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\nAnd that‚Äôs it. You see how with the cost of additional assumptions we get much more than a single \\(p\\)-value, and that‚Äôs where much of the appeal of this approach lies.\n\n\nAdvantages\nOverall, Bayesian thinking entails some compelling advantages:\n\nIncorporation of Prior Information. It allows you to incorporate prior knowledge or expert opinion into the analysis. This is particularly useful when historical data or domain expertise is available.\nProbabilistic Interpretation: It provides direct probabilistic interpretations of the results, such as the probability that one variant is better than another. This may be more intuitive than frequentist p-values, which are often misinterpreted.\nHandling of Small Sample Sizes: It tends to perform better with small sample sizes because they incorporate prior distributions, which can regularize estimates and prevent overfitting.\nContinuous Learning: As new data comes in, Bayesian methods provide a natural way to update the posterior distribution, leading to continuous learning and adaptation.\n\nThe downsides are mostly related to the choice of prior distribution. In settings where there is a lack of expert knowledge, this Bayesian approach to experimentation might not be very attractive. Computation can also be an issue in more complex settings."
  },
  {
    "objectID": "blog/DONE-bayesian-ab-tests.html#an-example",
    "href": "blog/DONE-bayesian-ab-tests.html#an-example",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs code an example in R and python. We start with generating some fake data and selecting parameters for the prior distributions.\n\nRPython\n\n\nrm(list=ls())\nset.seed(681)\nlibrary(ggplot2)\n\n# Data: Number of successes and total observations for T and C\nn_T &lt;- 1000\nx_T &lt;- 70\nn_C &lt;- 900\nx_C &lt;- 50\n\n# Prior parameters for the Beta distribution\nalpha_A &lt;- 1\nbeta_A &lt;- 1\nalpha_B &lt;- 1\nbeta_B &lt;- 1\nWe are now ready to get to and sample from the posteriors.\n\n# Posterior parameters\nposterior_alpha_T &lt;- alpha_T + x_T\nposterior_beta_T &lt;- beta_T + n_T - x_T\nposterior_alpha_C &lt;- alpha_C + x_C\nposterior_beta_C &lt;- beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T &lt;- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C &lt;- rbeta(10000, posterior_alpha_C, posterior_beta_C)\nWe are already at the final step, calculating the statistics of interest.\n\n# Sample from the posterior distributions\nposterior_obs_T &lt;- rbeta(10000, posterior_alpha_T, posterior_beta_T)\nposterior_obs_C &lt;- rbeta(10000, posterior_alpha_C, posterior_beta_C)\n\n# Estimate the probability that T is better than C\nprob_T_better &lt;- mean(posterior_obs_T &gt; posterior_obs_C)\nprint(paste(\"Probability that T is better than C:\", round(prob_T_better, digit=3)))\n\n# Estimate the average treatment effect\ntreatment_effect &lt;- mean(posterior_obs_T - posterior_obs_C)\nprint(paste(\"Average change in Y b/w T and C:\", round(treatment_effect, digit=3)))\n\ntreatment_effect_limit &lt;- (alpha_T + x_T)/(alpha_T + beta_T + n_T) - (alpha_C + x_C)/(alpha_C + beta_C + n_C)\nprint(treatment_effect_limit)\n\n# we can even plot both posteriors distributions.\ndf &lt;- data.frame(\n  y = c(posterior_obs_T, posterior_obs_C),\n  group = factor(rep(c(\"T\", \"C\"), each = 10000))\n)\n\nggplot(df, aes(x = y, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Posterior Distributions of Outcomes\",\n       x = \"Y = 1\", y = \"Density\") +\n  theme_minimal()\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seed for reproducibility\nnp.random.seed(681)\n\n# Data: Number of successes and total observations for T and C\nn_T = 1000\nx_T = 70\nn_C = 900\nx_C = 50\n\n# Prior parameters for the Beta distribution\nalpha_T = 1\nbeta_T = 1\nalpha_C = 1\nbeta_C = 1\n\n# Posterior parameters\nposterior_alpha_T = alpha_T + x_T\nposterior_beta_T = beta_T + n_T - x_T\nposterior_alpha_C = alpha_C + x_C\nposterior_beta_C = beta_C + n_C - x_C\n\n# Sample from the posterior distributions\nposterior_obs_T = np.random.beta(posterior_alpha_T, posterior_beta_T, 10000)\nposterior_obs_C = np.random.beta(posterior_alpha_C, posterior_beta_C, 10000)\n\n# Estimate the probability that T is better than C\nprob_T_better = np.mean(posterior_obs_T &gt; posterior_obs_C)\nprint(f\"Probability that T is better than C: {prob_T_better:.3f}\")\n\n# Estimate the average treatment effect\ntreatment_effect = np.mean(posterior_obs_T - posterior_obs_C)\nprint(f\"Average change in Y b/w T and C: {treatment_effect:.3f}\")\n\n# Plot posterior distributions\nplt.figure(figsize=(8, 6))\nsns.kdeplot(posterior_obs_T, fill=True, label=\"T\", alpha=0.5)\nsns.kdeplot(posterior_obs_C, fill=True, label=\"C\", alpha=0.5)\nplt.title(\"Posterior Distributions of Outcomes\")\nplt.xlabel(\"Y = 1\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n\n\n\nHere are the two posterior distributions.\n\nThere is also a specalized bayesAB package in R. It produces some cool charts, so I definitely recommend giving it a try.\nSoftware Package: bayesAB."
  },
  {
    "objectID": "blog/DONE-bayesian-ab-tests.html#bottom-line",
    "href": "blog/DONE-bayesian-ab-tests.html#bottom-line",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nBayesian inference offers a compelling alternative to the traditional methods based on frequentist statistics.\nThe main idea rests on incorporating prior information on the success rates in the treatment and control group as a starting point.\nAdvantages of bayesian methods include incorporating prior information, providing probabilistic interpretations, handling small sample sizes better, and enabling continuous learning.\nThe main challenge is the choice of prior distribution, which can be difficult without expert knowledge."
  },
  {
    "objectID": "blog/DONE-bayesian-ab-tests.html#where-to-learn-more",
    "href": "blog/DONE-bayesian-ab-tests.html#where-to-learn-more",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nAccessible introductions to the world of Bayesian inference are common. An example is Will Kurt‚Äôs book ‚ÄúBayesian Statistics the Fun Way‚Äú. See also the papers I cite below. As almost everything else, Google is also a great starting point."
  },
  {
    "objectID": "blog/DONE-bayesian-ab-tests.html#references",
    "href": "blog/DONE-bayesian-ab-tests.html#references",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "References",
    "text": "References\nDeng, A. (2015, May). Objective bayesian two sample hypothesis testing for online controlled experiments. In Proceedings of the 24th International Conference on World Wide Web (pp.¬†923-928).\nKamalbasha, S., & Eugster, M. J. (2021). Bayesian A/B testing for business decisions. In Data Science‚ÄìAnalytics and Applications: Proceedings of the 3rd International Data Science Conference‚ÄìiDSC2020 (pp.¬†50-57). Springer Fachmedien Wiesbaden.\nKurt, Will. Bayesian statistics the fun way: understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. No Starch Press, 2019.\nStevens, N. T., & Hagar, L. (2022). Comparative probability metrics: using posterior probabilities to account for practical equivalence in A/B tests. The American Statistician, 76(3), 224-237."
  },
  {
    "objectID": "blog/TODO-variance-ps-matching.html#a-closer-look",
    "href": "blog/TODO-variance-ps-matching.html#a-closer-look",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn brief, the idea behind treatment effect estimation with PSM is:\n\nEstimate the propensity score (i.e., the probability of being in the treatment group).\nFor each user in the treatment group find the user(s) in the control group with the most similar predicted propensity score value.\nAnalyze the outcome differences in each pair to arrive at a final ATT (or ATE) estimate.\n\nThroughout the article, I will assume a standard, well-behaved setting with SUTVA, unconfoundedness, and overlap assumptions in place. Similarly, I will ignore many important aspects of PSM methods such as trimming, propensity score and even treatment effect estimation. I will also use standard notation (e.g., \\(Y\\) and \\(D\\) denote outcome and treatment; and \\(\\tau\\) is the ATT) without setting up the entire framework.\nWe can classify the methods for estimating the PSM variance in three distinct buckets.\n\nAsymptotic Approximations\nRecall the Law of Total Variance stating that, given a conditioning variable \\(X\\), we can always decompose a random variable \\(Y\\)‚Äôs variance, into two components ‚Äì the expectation of the conditional variance and the variance of the conditional expectation: \\[Var(Y) = Var(E(Y|X)) + E(Var(Y|X)).\\]\nAbadie and Imbens (2006) use this idea to derive the asymptotic variance formula for one-to-one and one-to-many matching estimators. They provide an analytical expression for the variance as \\(n \\to \\infty\\) (and hence, this is an approximation) using the matching covariates as conditioning variables. I will not be going into their work in detail, but a couple of notes are worth discussing.\nThe original formulation assumes the true propensity score function is known. This is rarely true and in practice we rely on an estimate of it. Practitioners then replace this true propensity score with the estimated one. This is OK, but not great ‚Äì it brings me to the second issue.\nOne needs to account for the fact that this propensity score is estimated. As such, it comes with uncertainty, which, if ignored, can potentially understate the variance of your estimator. Hello, type 1 errors!\nIn follow-up work, Abadie and Imbens (2016) propose correction which accounts for this uncertainty. Interestingly, in a work that I will discuss below, Bodory et al.¬†(2020) report that in practice this correction might increase or decrease the ATT variance.\n\n\nApproximation Based on Weights\nWe can express all treatment effect estimators as a difference of weighted means:\n\\[\\hat{\\tau} = \\frac{1}{n} \\sum\\hat{w} D Y - \\frac{1}{n} \\sum\\hat{w} (1-D) Y,\\]\nwhere the weights \\(\\hat{w}\\) may depend on the propensity score.\nLechner (2002) suggested calculating the variance of PSM based on this formula, assuming the weights are non-stochastic. In other words, the approach ignores the fact that the propensity score is estimated in a first step. The PSM variance is then equal to the sum of two terms ‚Äì the one for the treated and the one for the control.\n\n\nBootstrap\nBootstrap is the go-to method for estimating variances in complex settings where analytical expressions are too messy or even do not exist. PSM seems like a natural environment where the bootstrap can help. Not so fast!\nAbadie and Imbens (2006) show that the standard nonparametric bootstrap is inconsistent for non-smooth estimators (such as propensity score matching) with fixed number of matches and continuous covariates. Nevertheless, practitioners routinely ignore this result. While the theory says we should not use the bootstrap here, the extent to which this is of practical relevance is unclear. For instance, if this only makes a difference in the fourth decimal, we might be willing to sacrifice some bias for ease of computation.\nThere are at least two ways one can go around using the bootstrap here. The first one is the standard plain vanilla approach.\nAlgorithm:\n\nRandomly draw \\(B\\) samples of size \\(n\\).\n\n\nAlternatively, you can also sample directly from the matched pairs which works better in certain cases.\n\n\nCompute ATT each time.\nCompute the confidence interval: \\[ \\hat{\\tau}\\pm \\sqrt{\\hat{V}(\\hat{\\tau}) \\times c}, \\]\n\nwhere \\(\\hat{V}(\\tau) = \\frac{1}{B-1}\\sum_{b=1}^B(\\hat{\\tau}^b-\\frac{1}{B}\\sum_b\\hat{\\tau}^b)^2\\) is the bootstrap variance of \\(\\hat{\\tau}}\\) and \\(c\\) is the critical value associated with a confidence level \\(\\alpha\\).\nAlternatively, in this last step we can directly use the$ $ and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution of \\(\\hat{\\tau}\\).\nThe second approach uses the well-known fact that the bootstrap behaves better when it uses so called asymptotically pivotal statistics ‚Äì i.e., ones which asymptotic distribution does not depend on unknown quantities. In this setting a candidate would be the \\(t\\)-statistic which, as we know, under certain conditions has a standard normal asymptotic distribution. We proceed in three steps:\nCompute the \\(t\\)-stat in the main sample using one of the variance approximations. Draw B random samples of size \\(n\\) and we compute the recentered \\(t\\)-stat with respect to \\(\\hat{\\tau}\\) in the main sample, \\[T^b = \\frac{\\hat{\\tau}^b - \\hat{\\tau}}{\\sqrt{\\hat{V}(\\tau^b)}}.\\]\nThe \\(p\\)-value is the share of (absolute value) bootstrap \\(t\\)-stats larger than the absolute value of the \\(t\\)-stat in the main sample \\[p{\\text{-value}} = 1 - \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|\\leq |T|) = \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|&gt; |T|),\\]\nwhere \\(T\\) is the \\(t\\)-stat from the main sample.\nThere are also newer (wild) bootstrap ideas which are still making their way into the mainstream."
  },
  {
    "objectID": "blog/DONE-variance-ps-matching.html",
    "href": "blog/DONE-variance-ps-matching.html",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "",
    "text": "Propensity score matching (PSM) is among the most popular methods for estimating causal effects with observational data. It lends its fame to both its power and simplicity. Under a certain set of commonly invoked assumptions, controlling for the propensity score alone (as opposed to the full set of covariates) is enough to remove bias between the treatment and control groups. The underlying idea of matching on the propensity score is incredibly simple and most data scientists can quickly estimate a treatment effect even from scratch.\nWhile estimating treatment effects is common and straightforward, calculating their variance along with the associated confidence intervals and p-values can be more challenging. So, how exactly do you compute the variance of PSM methods? In this article I will describe the most popular ways of doing that. My goal is to cover the intuition and I will spare the technical details. Interested readers can refer to the References section below for more detailed expositions.\nStatistical methods based on the propensity score function come in different flavors (e.g., weighting, matching, subclassification, etc.), but today I will focus on \\(1:1\\) or \\(1:N\\) matching. There are also several potential estimands of interest, such as ATE (average treatment effect), ATT (ATE for the treatment group) but I will talk specifically about the latter."
  },
  {
    "objectID": "blog/DONE-variance-ps-matching.html#background",
    "href": "blog/DONE-variance-ps-matching.html#background",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "",
    "text": "Propensity score matching (PSM) is among the most popular methods for estimating causal effects with observational data. It lends its fame to both its power and simplicity. Under a certain set of commonly invoked assumptions, controlling for the propensity score alone (as opposed to the full set of covariates) is enough to remove bias between the treatment and control groups. The underlying idea of matching on the propensity score is incredibly simple and most data scientists can quickly estimate a treatment effect even from scratch.\nWhile estimating treatment effects is common and straightforward, calculating their variance along with the associated confidence intervals and p-values can be more challenging. So, how exactly do you compute the variance of PSM methods? In this article I will describe the most popular ways of doing that. My goal is to cover the intuition and I will spare the technical details. Interested readers can refer to the References section below for more detailed expositions.\nStatistical methods based on the propensity score function come in different flavors (e.g., weighting, matching, subclassification, etc.), but today I will focus on \\(1:1\\) or \\(1:N\\) matching. There are also several potential estimands of interest, such as ATE (average treatment effect), ATT (ATE for the treatment group) but I will talk specifically about the latter."
  },
  {
    "objectID": "blog/DONE-variance-ps-matching.html#a-closer-look",
    "href": "blog/DONE-variance-ps-matching.html#a-closer-look",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn brief, the idea behind treatment effect estimation with PSM is:\n\nEstimate the propensity score (i.e., the probability of being in the treatment group).\nFor each user in the treatment group find the user(s) in the control group with the most similar predicted propensity score value.\nAnalyze the outcome differences in each pair to arrive at a final ATT (or ATE) estimate.\n\nThroughout the article, I will assume a standard, well-behaved setting with SUTVA, unconfoundedness, and overlap assumptions in place. Similarly, I will ignore many important aspects of PSM methods such as trimming, propensity score and even treatment effect estimation. I will also use standard notation (e.g., \\(Y\\) and \\(D\\) denote outcome and treatment; and \\(\\tau\\) is the ATT) without setting up the entire framework.\nWe can classify the methods for estimating the PSM variance in three distinct buckets.\n\nAsymptotic Approximations\nRecall the Law of Total Variance stating that, given a conditioning variable \\(X\\), we can always decompose a random variable \\(Y\\)‚Äôs variance, into two components ‚Äì the expectation of the conditional variance and the variance of the conditional expectation: \\[Var(Y) = Var(E(Y|X)) + E(Var(Y|X)).\\]\nAbadie and Imbens (2006) use this idea to derive the asymptotic variance formula for one-to-one and one-to-many matching estimators. They provide an analytical expression for the variance as \\(n \\to \\infty\\) (and hence, this is an approximation) using the matching covariates as conditioning variables. I will not be going into their work in detail, but a couple of notes are worth discussing.\nThe original formulation assumes the true propensity score function is known. This is rarely true and in practice we rely on an estimate of it. Practitioners then replace this true propensity score with the estimated one. This is OK, but not great ‚Äì it brings me to the second issue.\nOne needs to account for the fact that this propensity score is estimated. As such, it comes with uncertainty, which, if ignored, can potentially understate the variance of your estimator. Hello, type 1 errors!\nIn follow-up work, Abadie and Imbens (2016) propose correction which accounts for this uncertainty. Interestingly, in a work that I will discuss below, Bodory et al.¬†(2020) report that in practice this correction might increase or decrease the ATT variance.\n\n\nApproximation Based on Weights\nWe can express all treatment effect estimators as a difference of weighted means:\n\\[\\hat{\\tau} = \\frac{1}{n} \\sum\\hat{w} D Y - \\frac{1}{n} \\sum\\hat{w} (1-D) Y,\\]\nwhere the weights \\(\\hat{w}\\) may depend on the propensity score.\nLechner (2002) suggested calculating the variance of PSM based on this formula, assuming the weights are non-stochastic. In other words, the approach ignores the fact that the propensity score is estimated in a first step. The PSM variance is then equal to the sum of two terms ‚Äì the one for the treated and the one for the control.\n\n\nBootstrap\nBootstrap is the go-to method for estimating variances in complex settings where analytical expressions are too messy or even do not exist. PSM seems like a natural environment where the bootstrap can help. Not so fast!\nAbadie and Imbens (2006) show that the standard nonparametric bootstrap is inconsistent for non-smooth estimators (such as propensity score matching) with fixed number of matches and continuous covariates. Nevertheless, practitioners routinely ignore this result. While the theory says we should not use the bootstrap here, the extent to which this is of practical relevance is unclear. For instance, if this only makes a difference in the fourth decimal, we might be willing to sacrifice some bias for ease of computation.\nThere are at least two ways one can go around using the bootstrap here. The first one is the standard plain vanilla approach.\nAlgorithm:\n\nRandomly draw \\(B\\) samples of size \\(n\\).\n\n\nAlternatively, you can also sample directly from the matched pairs which works better in certain cases.\n\n\nCompute the ATT each time.\nCompute the confidence interval: \\[ \\hat{\\tau}\\pm \\sqrt{\\hat{V}(\\hat{\\tau}) \\times c}, \\]\n\nwhere \\(\\hat{V}(\\tau) = \\frac{1}{B-1}\\sum_{b=1}^B(\\hat{\\tau}^b-\\frac{1}{B}\\sum_b\\hat{\\tau}^b)^2\\) is the bootstrap variance of \\(\\hat{\\tau}\\) and \\(c\\) is the critical value associated with a confidence level \\(\\alpha\\).\nAlternatively, in this last step we can directly use the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution of \\(\\hat{\\tau}\\).\nThe second approach uses the well-known fact that the bootstrap behaves better when it uses so called asymptotically pivotal statistics ‚Äì i.e., ones which asymptotic distribution does not depend on unknown quantities. In this setting a candidate would be the \\(t\\)-statistic which, as we know, under certain conditions has a standard normal asymptotic distribution. We proceed in three steps:\n\nCompute the \\(t\\)-stat in the main sample using one of the variance approximations.\nDraw \\(B\\) random samples of size \\(n\\) and we compute the recentered \\(t\\)-stat with respect to \\(\\hat{\\tau}\\) in the main sample, \\[T^b = \\frac{\\hat{\\tau}^b - \\hat{\\tau}}{\\sqrt{\\hat{V}(\\tau^b)}}.\\]\nThe \\(p\\)-value is the share of (absolute value) bootstrap \\(t\\)-stats larger than the absolute value of the \\(t\\)-stat in the main sample \\[p{\\text{-value}} = 1 - \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|\\leq |T|) = \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|&gt; |T|),\\]\n\nwhere \\(T\\) is the \\(t\\)-stat from the main sample.\nThere are also newer (wild) bootstrap ideas which are still making their way into the mainstream.\n\n\nMonte Carlo Simulations\nBodory et al.¬†(2020) compare the finite sample performance of several of these methods (plus others). Their analysis is more thorough in the sense that they include other ATT estimators besides PSM, such as PS weighting or radius matching.\nOverall, their results do not indicate a clear winner which performs best in all scenarios. Interestingly, even some of the bootstrap methods often outperform the asymptotic approximations of Abadie and Imbens (2006) or Lechner (2002)."
  },
  {
    "objectID": "blog/DONE-variance-ps-matching.html#where-to-learn-more",
    "href": "blog/DONE-variance-ps-matching.html#where-to-learn-more",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nI based this post on Bodory et al.¬†(2020)‚Äôs paper, so that is a natural place to start. Many other studies have focused on the performance of propensity score treatment effect estimators (as opposed to their variance) ‚Äì Huber et al.¬†(2013) and Busso et al.¬†(2014) are great starting points. Lastly, Imbens (2015) is a great resource for an overview of matching methods more generally along with some of the problems they solve as well as the pitfalls they entail."
  },
  {
    "objectID": "blog/DONE-variance-ps-matching.html#bottom-line",
    "href": "blog/DONE-variance-ps-matching.html#bottom-line",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThere is no shortage of methods when it comes to estimating the variance of propensity score matching estimators.\nAbadie and Imbens (2006) showed that the standard bootstrap is inconsistent in this setting with continuous covariates and formally derived an asymptotic approximation of the true variance.\nMonte Carlo simulations, however, show that bootstrap methods offer a good tradeoff between simplicity and performance."
  },
  {
    "objectID": "blog/DONE-variance-ps-matching.html#references",
    "href": "blog/DONE-variance-ps-matching.html#references",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "References",
    "text": "References\nAbadie, A., & Imbens, G. W. (2006). Large sample properties of matching estimators for average treatment effects. Econometrica, 74(1), 235-267.\nAbadie, A., & Imbens, G. W. (2008). On the failure of the bootstrap for matching estimators. Econometrica, 76(6), 1537-1557.\nAustin, P. C., & Small, D. S. (2014). The use of bootstrapping when using propensity‚Äêscore matching without replacement: a simulation study. Statistics in medicine, 33(24), 4306-4319.\nBodory, H., Camponovo, L., Huber, M., & Lechner, M. (2020). The finite sample performance of inference methods for propensity score matching and weighting estimators. Journal of Business & Economic Statistics, 38(1), 183-200.\nBusso, M., DiNardo, J., & McCrary, J. (2014). New evidence on the finite sample properties of propensity score reweighting and matching estimators. Review of Economics and Statistics, 96(5), 885-897.\nCaliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of propensity score matching. Journal of economic surveys, 22(1), 31-72.\nHuber, M., Lechner, M., & Wunsch, C. (2013). The performance of estimators based on the propensity score. Journal of Econometrics, 175(1), 1-21.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nLechner, M. (2002). Some practical issues in the evaluation of heterogeneous labour market programmes by matching methods. Journal of the Royal Statistical Society: Series A (Statistics in Society), 165(1), 59-82."
  },
  {
    "objectID": "blog/DONE-overview-ml-methods-ci.html",
    "href": "blog/DONE-overview-ml-methods-ci.html",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "",
    "text": "The most exciting trend in causal inference over the last decade has been the infusion of machine learning (ML) techniques. Supervised machine learning is designed to find complex patterns in data and as such, it is merely occupied with prediction. Causal inference, on the other hand, pays close attention to statistical precision and inference based on asymptotic properties like consistency and normality. The two worlds are, thus, fundamentally different.\nIt should be no surprise then that machine learning and causal inference do not naturally speak to each other, and some modifications are required to marry them. The good news is recent innovations have led to a bunch of ways in which ML models can be used in isolating causal effects especially in settings with many covariates (also called ‚Äúhigh dimensional‚Äù settings).\nIn this article, I will briefly describe the ways in which we can use ML when looking for causal relationships. I will indeed be concise, and I will avoid diving deeper into technical details. This blog post will look more like a laundry list with references to papers and software packages than a tutorial."
  },
  {
    "objectID": "blog/DONE-overview-ml-methods-ci.html#background",
    "href": "blog/DONE-overview-ml-methods-ci.html#background",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "",
    "text": "The most exciting trend in causal inference over the last decade has been the infusion of machine learning (ML) techniques. Supervised machine learning is designed to find complex patterns in data and as such, it is merely occupied with prediction. Causal inference, on the other hand, pays close attention to statistical precision and inference based on asymptotic properties like consistency and normality. The two worlds are, thus, fundamentally different.\nIt should be no surprise then that machine learning and causal inference do not naturally speak to each other, and some modifications are required to marry them. The good news is recent innovations have led to a bunch of ways in which ML models can be used in isolating causal effects especially in settings with many covariates (also called ‚Äúhigh dimensional‚Äù settings).\nIn this article, I will briefly describe the ways in which we can use ML when looking for causal relationships. I will indeed be concise, and I will avoid diving deeper into technical details. This blog post will look more like a laundry list with references to papers and software packages than a tutorial."
  },
  {
    "objectID": "blog/DONE-overview-ml-methods-ci.html#notation",
    "href": "blog/DONE-overview-ml-methods-ci.html#notation",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Notation",
    "text": "Notation\nIt is helpful to quickly summarize some features of the potential outcome framework. Imagine we have a i.i.d. random sample of a binary treatment indicator \\(D\\), outcome variable \\(Y\\) and a vector of covariates \\(X\\). Assume the potential outcomes \\(Y(0)\\) and \\(Y(1)\\) are unrelated to the binary treatment status \\(D\\) which is often referred to as the unconfoundedness or ignorability.\nA common estimand of interest is the Average Treatment Effect (ATE)\n\\[ATE = E[Y(1) - Y(0)],\\]\nwhere \\(Y(d)\\) is the potential outcome under treatment regime \\(D=d\\). Another popular estimand is the Conditional ATE (CATE),\n\\[CATE(X) = E[Y(1) - Y(0) | X],\\]\nwhich is the ATE for a particular group of units with a fixed covariates level (e.g., women, men, new users, etc.).\nThe ATE can be expressed in at least three useful ways:\n\\[\\begin{align*} ATE & = \\mathbf{E} \\left[ \\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(outcome model only)} \\\\ & = \\mathbf{E}\\left[ \\frac{YD}{e(X)} - \\frac{Y(1-D)}{1-e(X)} \\right] \\hspace{1cm} \\text{(prop. score model only)} \\\\ & = \\mathbf{E} \\left[ \\frac{[Y-\\mu(1,X)D]}{e(X)} - \\frac{[Y-\\mu(0,X)](1-D)}{1-e(X)} \\right] \\\\ & + \\mathbf{E} \\left[\\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(both models)} \\end{align*}\\]\nwhere\n\\[\\mu(D,X) = \\mathbf{E}[Y|D,X]\\]\nis the outcome model and\n\\[e(x)=\\mathbf{E}[D|X]\\]\nis the propensity score.\nThis formulation is helpful because it naturally splits the types of treatment effect estimation methods into three separate categories ‚Äì (i) those that require only estimation of \\(\\mu(D,X)\\), (ii) those that use only \\(e(X)\\), and (iii) those that need both.\nOne can think of the propensity score (PS) and the outcome models as nuisance functions ‚Äì ones that are not of direct interest but play a part in treatment effect estimation. ML methods are attractive candidates for estimating these nuisance functions flexibly."
  },
  {
    "objectID": "blog/DONE-overview-ml-methods-ci.html#a-closer-look",
    "href": "blog/DONE-overview-ml-methods-ci.html#a-closer-look",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nCovariate Balancing Methods\nUnder the ignorability assumption, all confounding bias comes from differences in the covariates \\(X\\) between the treatment and the control groups. Intuitively, balancing these is enough to guarantee unbiasedness. One line of research develops methods to do exactly that ‚Äì directly equate covariates between the two groups of interest. These approaches circumvent estimation of the two nuisance functions mentioned above.\nThese are inspired by the ML view of data analysis framed as an optimization problem. Examples include Entropy Balancing, Genetic Matching, Stable Weights, and Residual Balancing. The last approach combines balancing with a regression adjustment to reduce extrapolation when estimating the counterfactuals for the treatment group. Some of these methods were designed with a low dimensional setting in mind, but they still carry the spirit of ML type of thinking.\nSoftware Packages: MatchIt, Ebal, BalanceHD.\n\n\nML Methods for the Propensity Score Model\nPropensity score methods rely on correctly specifying the PS model. In low-dimensional settings, it is possible to estimate it nonparametrically. In practice, however, this is unrealistic when data scientists have access to continuous or even bunch of discrete covariates. Can ML methods come to the rescue?\nIn principle, yes. A major challenge in this context, however, is the choice of a loss function. In the ML world loss functions target measures of fit (e.g., Root Mean Squared Error, log likelihood, etc.) but these would be problematic here as they do not aim at balancing covariates important to reduce bias. Thus, these methods do not perform very well unless used with much caution.\nImai and Ratkovic (2014) propose a PS method that directly balances covariates. Another choice is the Boosted CART implementation. As its name suggests, it iteratively forms a bunch of tree models and averages them, but with an appropriately chosen loss function. A series of simulation studies analyze the performance of various ML algorithms used to estimate the PS, but overall, these methods are nowadays dominated by some of the doubly robust approaches described below.\nSoftware Packages: TWANG, CBPS.\n\n\nML Methods for the Outcome Model\nWe can also estimate treatment effects directly by modelling the outcome variable. This also requires correct model specification, and even then, it is prone to extrapolation in finite samples. Examples include Bayesian Additive Regression Trees (BART) and other ensemble methods.\nBelloni et al.¬†(2014) show that the set of features optimal when estimating the outcome model, is not necessarily optimal for estimating treatment effects. The issue is omitting a variable that is correlated with the treatment even if its correlation with the outcome is only modest, can introduce considerable bias. Moreover, typically, the rate of convergence in this context when using ML models will be slower than \\(\\sqrt{n}\\), meaning that you will need much more data to get a good treatment effect estimate.\nOverall, there is no statistical theory of why ML methods should work well here, but some methods tend to perform well empirically. This brings us to the doubly robust approach.\nSoftware Packages: BART, rBART, BayesTree.\n\n\nML Methods for Both Models & Doubly Robust Methods\nMethods combining models for both the propensity score and the outcome have long been advocated. Intuitively, the propensity score can be seen as a balancing step after which regression adjustment can remove any remaining bias. Imbens (2015), for instance, promotes this type of thinking in matching methods specifically.\nDoubly robust (DR) estimators use both nuisance models and have the amazing property of being consistent even if only one of the two models is correctly specified. You can think of the bias term as a product of the biases in the two nuisance models ‚Äì if one of them is equal to zero, the entire term vanishes. Additionally, if both models are correctly specified, some methods are semiparametrically efficient, (i.e., ‚Äúthe best‚Äù in a large class of flexible models). A simple DR method is the Augmented Inverse Probability Weighting (AIPW) estimator which in linear models comes down to running weighted OLS regression of \\(Y\\) on \\(D\\) and \\(X\\) with the estimated (inverse) propensity score as weights.\nThere is more good news. Amazingly, DR methods can still converge at a rate \\(\\sqrt{N}\\) even if the underlying nuisance models converge at slower rates. The formal requirement is that the nuisance models must belong to something called a Donsker class. In simple words, they should not be too complex and prone to overfit.\nOne line of research has developed the Double ML framework. This work has been so influential that it deserves a blog post of its own. Without going into technical details, the authors of the original paper show that na√Øve application of ML methods when estimating both nuisance functions results in two types of biases ‚Äì regularization and overfitting. DoubleML makes use of something called Neyman orthogonalization (think of the Frisch‚ÄìWaugh‚ÄìLovell theorem) to remove the former, and sample splitting to avoid the latter. In simple settings, this method combines the residuals from regressions of \\(Y\\) on \\(X\\) and \\(Y\\) on \\(D\\), but in general it can take more complicated forms.\nAnother line of research has developed the Double Post Lasso approach. The idea here is simpler ‚Äì use Lasso to select covariates relevant to the outcome regression and then again to select ones relevant to the propensity score. Lastly, use OLS to regress \\(Y\\) on the union of the covariates selected previously. This procedure removes confounding or regularization bias that might be present in methods using ML models to estimate only one of the nuisance models.\nSoftware Packages: DoubleML, hdm, dlsr.\n\n\nHeterogeneous Treatment Effect Estimation\nMining for heterogeneous treatment effects has been a particularly fruitful field for ML methods. A leading example is the causal tree method developed by Athey and Imbens (2016). It resembles the traditional CART algorithm, but it uses a different criterion for splitting the data: instead of focusing on Mean Squared Error (MSE) for the outcome, it uses MSE for treatment effect. The result is a decision tree, in which units in each leaf have similar treatment effects. The method also features ‚Äúhonest‚Äù sample splitting for obtaining variance estimates ‚Äì one half of the data is used to determine the optimal tree, and the other half to estimate the treatment effects.\nBuilding on this idea, Wager and Athey (2018) propose a random forest-based method which generates a bunch of causal trees and averages the results to induce smoothness in the treatment effect‚Äôs function. Magically, the authors show the predictions are asymptotically normal and centered around the true value for each unit! This is exciting as it allows for standard methods for statistical inference.\nOther methods include BART, a Bayesian version of random forests mentioned above, Imai and Ratkovic (2013) who propose adding treatment indicators interacted with covariates in a LASSO regression to determine which variables are important for treatment effect heterogeneity. Similarly, Tian et al.¬†(2014) suggest modifying the covariates in a straightforward way and running a regression of the outcome on the modified variables without an intercept. Other methods include the \\(R\\)-learner of Nie and Wager (2021) which relies on estimating the two nuisance models described above and using a special loss function. K√ºnzel et al.¬†(2019) propose a \\(X\\)-learner metaalogrithm.\nSoftware Packages: FindIt, rlearner, grf, causalToolbox.\n\n\nOthers\nA by-product of estimating treatment effect heterogeneity is that we can determine which units should be treated. Intuitively, if the treatment effect is close to zero (or even negative) for some users, there is not much to be gained from the exposure. Kitagawa and Tetenov (2018) analyze a setting with limited complexity, and Athey and Wager (2021) develop the DoubleML framework discussed above for choosing whom to treat. ML has also been used for variance reduction in randomized experiments via regression adjustments. See, for instance, Wager et al.¬†(2016), Bloniarz et al.¬†(2016), and List et al.¬†(2022)."
  },
  {
    "objectID": "blog/DONE-overview-ml-methods-ci.html#bottom-line",
    "href": "blog/DONE-overview-ml-methods-ci.html#bottom-line",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods are slowly becoming an indispensable part of data scientists‚Äô toolkit for estimating causal relationships. There is an abundance of methods aiding practitioners in both ATE and CATE estimation.\nDoubly robust approaches offer better theoretical guarantees than methods relying on estimating either the outcome or the propensity score models.\nThe leading approaches for estimating ATEs are Double ML and Double Post Lasso.\nThe leading approach for estimating CATEs is the causal forest method."
  },
  {
    "objectID": "blog/DONE-overview-ml-methods-ci.html#where-to-learn-more",
    "href": "blog/DONE-overview-ml-methods-ci.html#where-to-learn-more",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMore technical data scientists will find the following review papers useful:\n\nAthey and Imbens (2019)\nAthey and Imbens (2017)\nVarian (2014)\nKreif and DiazOrdaz (2019)\nMullainathan and Spiess (2017)\nHu (2023)\n\nThere are a few major Python frameworks for using ML in causal inference estimation. More practically-oriented folks might like their documentation:\n\nCausalML\nEconML\nDoubleML"
  },
  {
    "objectID": "blog/DONE-overview-ml-methods-ci.html#references",
    "href": "blog/DONE-overview-ml-methods-ci.html#references",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "References",
    "text": "References\nAthey, S., & Imbens, G. (2016). Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27), 7353-7360.\nAthey, S., & Imbens, G. W. (2017). The state of applied econometrics: Causality and policy evaluation. Journal of Economic perspectives, 31(2), 3-32.\nAthey, S., & Imbens, G. W. (2019). Machine learning methods that economists should know about. Annual Review of Economics, 11, 685-725.\nAthey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 80(4), 597-623.\nAthey, S., & Wager, S. (2021). Policy learning with observational data. Econometrica, 89(1), 133-161.\nAustin, P. C. (2012). Using ensemble-based methods for directly estimating causal effects: an investigation of tree-based G-computation. Multivariate behavioral research, 47(1), 115-135.\nBelloni, A., Chernozhukov, V., & Hansen, C. (2014). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2), 608-650.\nBloniarz, A., Liu, H., Zhang, C. H., Sekhon, J. S., & Yu, B. (2016). Lasso adjustments of treatment effect estimates in randomized experiments. Proceedings of the National Academy of Sciences, 113(27), 7383-7390.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal.\nDiamond, A., & Sekhon, J. S. (2013). Genetic matching for estimating causal effects: A general multivariate matching method for achieving balance in observational studies. Review of Economics and Statistics, 95(3), 932-945.\nHahn, P. R., Murray, J. S., & Carvalho, C. M. (2020). Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion). Bayesian Analysis, 15(3), 965-1056.\nHainmueller, J. (2012). Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20(1), 25-46.\nHill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240.\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. Annals of Applied Statistics\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B: Statistical Methodology, 243-263.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nKitagawa, T., & Tetenov, A. (2018). Who should be treated? empirical welfare maximization methods for treatment choice. Econometrica, 86(2), 591-616.\nKreif, N., & DiazOrdaz, K. (2019). Machine learning in policy evaluation: new tools for causal inference. arXiv preprint arXiv:1903.00402.\nK√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165.\nLee, B. K., Lessler, J., & Stuart, E. A. (2010). Improving propensity score weighting using machine learning. Statistics in medicine, 29(3), 337-346.\nList, J. A., Muir, I., & Sun, G. K. (2022). Using Machine Learning for Efficient Flexible Regression Adjustment in Economic Experiments (No.¬†w30756). National Bureau of Economic Research.\nMcCaffrey, D. F., Ridgeway, G., & Morral, A. R. (2004). Propensity score estimation with boosted regression for evaluating causal effects in observational studies. Psychological methods, 9(4), 403.\nMullainathan, S., & Spiess, J. (2017). Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2), 87-106.\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\nRobins, J. M., Rotnitzky, A., & Zhao, L. P. (1994). Estimation of regression coefficients when some regressors are not always observed. Journal of the American statistical Association, 89(427), 846-866.\nSetoguchi, S., Schneeweiss, S., Brookhart, M. A., Glynn, R. J., & Cook, E. F. (2008). Evaluating uses of data mining techniques in propensity score estimation: a simulation study. Pharmacoepidemiology and drug safety, 17(6), 546-555.\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\nVarian, H. R. (2014). Big data: New tricks for econometrics. Journal of Economic Perspectives, 28(2), 3-28.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nWager, S., Du, W., Taylor, J., & Tibshirani, R. J. (2016). High-dimensional regression adjustments in randomized experiments. Proceedings of the National Academy of Sciences, 113(45), 12673-12678.\nWestreich, D., Lessler, J., & Funk, M. J. (2010). Propensity score estimation: neural networks, support vector machines, decision trees (CART), and meta-classifiers as alternatives to logistic regression. Journal of clinical epidemiology, 63(8), 826-833.\nWyss, R., Ellis, A. R., Brookhart, M. A., Girman, C. J., Jonsson Funk, M., LoCasale, R., & St√ºrmer, T. (2014). The role of prediction modeling in propensity score estimation: an evaluation of logistic regression, bCART, and the covariate-balancing propensity score. American journal of epidemiology, 180(6), 645-655.\nZivich, P. N., & Breskin, A. (2021). Machine learning for causal inference: on the use of cross-fit estimators. Epidemiology (Cambridge, Mass.), 32(3), 393.\nZubizarreta, J. R. (2015). Stable weights that balance covariates for estimation with incomplete outcome data. Journal of the American Statistical Association, 110(511), 910-922."
  },
  {
    "objectID": "blog/TODO-gradient-boosting.html#a-closer-look",
    "href": "blog/TODO-gradient-boosting.html#a-closer-look",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Gradient Boosting\nGradient boosting is an ensemble machine learning technique that builds models sequentially, each new model attempting to correct the errors of the previous ones. The general idea is to minimize a loss function by adding weak learners (typically short decision trees) in a stage-wise manner to arrive at a single strong learner. This iterative process allows the model to improve its performance gradually, making it highly effective for complex datasets. Gradient boosting methods are versatile in that they can be used both for regression and classifications problems. In the most common case when the weak learners are decision trees, the algorithm is known as gradient tree boosting.\nMathematically, the model is built as follows:\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m = 1\\) to \\(M\\):\n\n\nCompute the pseudo-residuals: \\(r_{im} = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}\\) for \\(i=1,\\dots,n\\). In regression tasks this is simply \\(y-\\hat{y}\\).\nFit a weak learner such as a tree, \\(h_m(x)\\), on \\(\\{(x_i, r_{im}) \\}_{i=1}^n.\\)\nCompute \\(\\gamma\\) by solving: \\(\\gamma=\\arg\\min\\sum_i L(y_i, f_{m-1}(x_i)+\\gamma h_m(x_i)).\\)\nUpdate the model: \\(f_m(x) = f_{m-1}(x) + \\gamma h_m(x).\\)\n\n\nThe final model is \\(f_M(x).\\)\n\nThis is the most generic recipe for a gradient boosting algorithm. Let‚Äôs now focus on more specific implementations of this idea.\n\n\nAdaBoost\nAdaBoost, short for Adaptive Boosting, is one of the earliest boosting algorithms. It adjusts the weights of incorrectly classified observations so that subsequent learners focus more on difficult ones. In other words, it works by learning from its mistakes ‚Äì after each round of predictions, it identifies which observartions it got wrong and pays special attention to them (i.e., gives them a higher weight) in the next round. Thus, AdaBoost is not strictly speaking a gradient boosting algorithm, in the modern sense of the term.\nLet‚Äôs consider a binary classification problem where the outcome variable takes values in \\(\\{ -1, 1\\}\\). Here is the general AdaBoost algorithm:\n\nInitialize weights \\(w_i = \\frac{1}{n}\\) for \\(i = 1, \\ldots, n.\\)\nFor \\(m = 1 \\dots M\\):\n\n\nTrain a weak learner \\(h_m(x)\\) using weights \\(w_i\\).\nCompute the weighted error: \\(\\epsilon_m = \\frac{\\sum_{i=1}^{n} w_i \\mathbb{I}(y_i \\neq h_m(x_i))}{\\sum_{i=1}^{n} w_i}\\), where \\(\\mathbb{I}(\\cdot)\\) is the indicator function.\nCompute the model weight:$ _m = ()$.\nUpdate and normalize the weights:$ w_i^{m+1}= w_i^{m} (-_m y_i h_m(x_i)))$, and \\(w_i^{m+1} = \\frac{ w_i^{m+1}}{\\sum_j  w_j^{m+1}} for i=1,\\dots,n\\).\n\n\nThe final model is \\(f(x)=sign \\left( \\sum_m \\alpha_m h_m(x) \\right)\\).\n\nAdaBoost is simple to implement while being relatively resistant to overfitting, making it especially effective for problems with clean, well-structured data. Its adaptive nature means it can identify and focus on the most challenging observations. However, AdaBoost has notable weaknesses: it‚Äôs highly sensitive to noisy data and outliers (since it increases weights on misclassified examples), can perform poorly when working with insufficient training data, and tends to be computationally intensive compared to newer, simpler algorithms.\nSoftware Packages. R: adabag, gbm. Python: scikit-learn.\n\n\nXGBoost\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting designed for speed and performance. It revolutionizes the method by using the Newton-Raphson method in function space, setting it apart from traditional gradient boosting‚Äôs simpler gradient descent approach. At its core, XGBoost leverages both the first and second derivatives of the loss function through a second-order Taylor approximation. This sophisticated approach enables faster convergence and more accurate predictions by making better-informed decisions about how to improve the model at each step. When building decision trees, XGBoost considers both the expected improvement and the uncertainty of potential splits, while simultaneously applying regularization to prevent overfitting.\nWith some simplifications, the regression version of the XGBoost is implemented as follows:\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m=1, \\dots M\\):\n\n\nCompute the gradients \\(grad_m(x_i)=\\left( \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right)_{f(x)=f_{m-1}(x)}\\) and hessians \\(hess_m(x_i)=\\left( \\frac{\\partial^2 L(y_i, f(x_i))}{\\partial^2 f(x_i)^2}\\right)_{f(x)=f_{m-1}(x)}\\) of the loss function for \\(i=1,\\dots,n\\).\nFit a weak learner such as a tree, \\(h_m(x)\\) on$ {(x_i, ) }_{i=1}^n$.\nUpdate the model \\(f_m(x)=f_{(m-1)} - \\alpha h_m(x).\\)\n\n\nThe final model is \\(f_M(x)=\\sum_m f_m(x)\\).\n\nXGBoost is ideal for large datasets and competitions where model performance is critical. It is also a good choice when you need a highly optimized and scalable solution as it is highly efficient, supports parallel and distributed computing. Nevertheless, this algorithm can be complex to tune, and may require significant computational resources for very large datasets.\nNext up is CatBoost.\nSoftware Packages: R: xgboost, Python: xgboost.\n\n\nCatBoost\nCatBoost, short for Categorical Boosting, is a gradient boosting library that handles categorical features efficiently. It uses ordered boosting to reduce overfitting and supports GPU training, making it both powerful and versatile. CatBoost modifies the standard gradient boosting algorithm by incorporating two novel techniques ‚Äì ordered boosting and target statistics for categorical features. Let‚Äôs examine each one in turn.\nRather than requiring preprocessing like one-hot encoding, CatBoost encodes categorical values based on the distribution of the target variable without introducing data leakage. This is achieved by encoding each data point as if it were unseen, preventing overfitting. Additionally, ordered boosting is a method that builds each new tree while treating each data point as ‚Äúout-of-fold‚Äù for itself. This helps reduce overfitting, particularly in small datasets, by preventing over-reliance on individual observations during training.\nWe now move on to a popular, faster gradient boosting implementation.\nSoftware Packages: R: catboost, Python: catboost.\n\n\nLightGBM\nLightGBM shares many of XGBoost‚Äôs benefits, such as support for sparse data, parallel training, multiple loss functions, regularization, and early stopping, but it also introduces a bunc of new features and improvements.\nFirst, rather than growing trees level-wise (row by row) as in most implementations, LightGBM grows trees leaf-wise, selecting the leaf that provides the greatest reduction in loss. Second, it does not use the typical sorted-based approach for finding split points, which sorts feature values to locate the best split. Instead, it relies on an efficient histogram-based method that significantly improves both speed and memory efficiency. Third, LightGBM incorporates the so-called Gradient-Based One-Side Sampling, which speeds up training by focusing on the most informative samples. Lastly, the algorithm uses Exclusive Feature Bundling which can group features to reduce dimensionality, enabling faster and more accurate model training.\nSoftware Packages: R: lightgmb, Python: lightgbm.\n\n\nChallenges\nGradinet boosting methods comes with a few common practical challenges. When a predicitve model learns the training data too precisely, it may perform poorly on new, unseen data ‚Äì a problem called overfitting. To combat this, various regularization methods are used to add helpful constraints to the learning process. A main parameter that regulates the model‚Äôs learning precision is the number of boosting rounds (M), which determines how many base models are created. While using more rounds reduces training errors, it also increases overfitting risk. To find the optimal M value, it‚Äôs common to use cross validation. The depth of decision trees is another important control parameter in tree boosting. Deeper trees can capture more complex patterns but are more prone to memorizing the training data rather than learning generalizable patterns.\nThe improved predictive performance of gradient boosting relative to simpler models comes at a cost of reduced model transparency. While a single decision tree‚Äôs reasoning can be easily traced and understood, tracking the decision-making process across numerous trees becomes extremely complex and challenging. Gradient boosting is an excellent example of the inherent tradeoff between model simplicity and performance.\nLet‚Äôs now look at how can we use these methods in practice."
  },
  {
    "objectID": "blog/DONE-gradient-boosting.html",
    "href": "blog/DONE-gradient-boosting.html",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "",
    "text": "Gradient boosting has emerged as one of the most powerful techniques for predictive modeling. In its simplest form, we can think of gradient boosting like having a team of detectives working in sequence, where each new detective specifically focuses on solving the cases where their predecessors stumbled. Each detective contributes their findings to the investigation, with earlier ones catching obvious clues and later ones piecing together the subtle evidence that was initially missed. The final solution emerges from combining all their work.\nWhile the term ‚Äúgradient boosting‚Äù is often used generically, there are notable differences among its implementations‚Äîeach with unique strengths, weaknesses, and practical applications. Understanding these nuances is essential for advanced data scientists aiming to choose the best method for their specific datasets and computational constraints.\nThis article aims to provide a brief overview of the most popular gradient boosting methods, delving into their mathematical foundations and unique characteristics. By the end, you will have a clearer understanding of when and how to use each method to achieve the best results in your predictive modeling tasks. At the end of the article, I‚Äôll provide a step-by-step Python example that implements these algorithms."
  },
  {
    "objectID": "blog/DONE-gradient-boosting.html#background",
    "href": "blog/DONE-gradient-boosting.html#background",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "",
    "text": "Gradient boosting has emerged as one of the most powerful techniques for predictive modeling. In its simplest form, we can think of gradient boosting like having a team of detectives working in sequence, where each new detective specifically focuses on solving the cases where their predecessors stumbled. Each detective contributes their findings to the investigation, with earlier ones catching obvious clues and later ones piecing together the subtle evidence that was initially missed. The final solution emerges from combining all their work.\nWhile the term ‚Äúgradient boosting‚Äù is often used generically, there are notable differences among its implementations‚Äîeach with unique strengths, weaknesses, and practical applications. Understanding these nuances is essential for advanced data scientists aiming to choose the best method for their specific datasets and computational constraints.\nThis article aims to provide a brief overview of the most popular gradient boosting methods, delving into their mathematical foundations and unique characteristics. By the end, you will have a clearer understanding of when and how to use each method to achieve the best results in your predictive modeling tasks. At the end of the article, I‚Äôll provide a step-by-step Python example that implements these algorithms."
  },
  {
    "objectID": "blog/DONE-gradient-boosting.html#notation",
    "href": "blog/DONE-gradient-boosting.html#notation",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Notation",
    "text": "Notation\nI assume a mathematical familiarity with machine learning (ML) basics and some minimal previous exposure to gradient boosting. If you need a refresher, grab any introductory ML textbook.\nBefore diving into the specifics of each method, let‚Äôs establish some common notation that will be used throughout this article:\n\n\\(\\mathbf{X}\\): Covariates/features matrix\n\\(\\mathbf{y}\\): Outcome/target variable\n\\(f(x)\\): Predictive model\n\\(L(y, \\hat{y})\\): Loss function\n\\(\\hat{y}\\): Predicted outcome/target value\n\\(\\gamma\\): Learning rate\n\\(n\\): Number of observations/instances\n\\(M\\): Number of algorithm iterations."
  },
  {
    "objectID": "blog/DONE-gradient-boosting.html#a-closer-look",
    "href": "blog/DONE-gradient-boosting.html#a-closer-look",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Gradient Boosting\nGradient boosting is an ensemble machine learning technique that builds models sequentially, each new model attempting to correct the errors of the previous ones. The general idea is to minimize a loss function by adding weak learners (typically short decision trees) in a stage-wise manner to arrive at a single strong learner. This iterative process allows the model to improve its performance gradually, making it highly effective for complex datasets. Gradient boosting methods are versatile in that they can be used both for regression and classifications problems. In the most common case when the weak learners are decision trees, the algorithm is known as gradient tree boosting.\nMathematically, the model is built as follows:\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m = 1\\) to \\(M\\):\n\n\nCompute the pseudo-residuals: \\(r_{im} = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}\\) for \\(i=1,\\dots,n\\). In regression tasks this is simply \\(y-\\hat{y}\\).\nFit a weak learner such as a tree, \\(h_m(x)\\), on \\(\\{(x_i, r_{im}) \\}_{i=1}^n.\\)\nCompute \\(\\gamma\\) by solving: \\(\\gamma=\\arg\\min\\sum_i L(y_i, f_{m-1}(x_i)+\\gamma h_m(x_i)).\\)\nUpdate the model: \\(f_m(x) = f_{m-1}(x) + \\gamma h_m(x).\\)\n\n\nThe final model is \\(f_M(x).\\)\n\nThis is the most generic recipe for a gradient boosting algorithm. Let‚Äôs now focus on more specific implementations of this idea.\n\n\nAdaBoost\nAdaBoost, short for Adaptive Boosting, is one of the earliest boosting algorithms. It adjusts the weights of incorrectly classified observations so that subsequent learners focus more on difficult ones. In other words, it works by learning from its mistakes ‚Äì after each round of predictions, it identifies which observartions it got wrong and pays special attention to them (i.e., gives them a higher weight) in the next round. Thus, AdaBoost is not strictly speaking a gradient boosting algorithm, in the modern sense of the term.\nLet‚Äôs consider a binary classification problem where the outcome variable takes values in \\(\\{ -1, 1\\}\\). Here is the general AdaBoost algorithm:\n\nInitialize weights \\(w_i = \\frac{1}{n}\\) for \\(i = 1, \\ldots, n.\\)\nFor \\(m = 1 \\dots M\\):\n\n\nTrain a weak learner \\(h_m(x)\\) using weights \\(w_i\\).\nCompute the weighted error: \\(\\epsilon_m = \\frac{\\sum_{i=1}^{n} w_i \\mathbb{I}(y_i \\neq h_m(x_i))}{\\sum_{i=1}^{n} w_i}\\), where \\(\\mathbb{I}(\\cdot)\\) is the indicator function.\nCompute the model weight:$ _m = ()$.\nUpdate and \\(w_i^{m+1}= w_i^{m} \\exp(-\\alpha_m y_i h_m(x_i)))\\), and \\(w_i^{m+1} = \\frac{ w_i^{m+1}}{\\sum_j  w_j^{m+1}} for i=1,\\dots,n\\).\n\n\nThe final model is \\(f(x)=sign \\left( \\sum_m \\alpha_m h_m(x) \\right)\\).\n\nAdaBoost is simple to implement while being relatively resistant to overfitting, making it especially effective for problems with clean, well-structured data. Its adaptive nature means it can identify and focus on the most challenging observations. However, AdaBoost has notable weaknesses: it‚Äôs highly sensitive to noisy data and outliers (since it increases weights on misclassified examples), can perform poorly when working with insufficient training data, and tends to be computationally intensive compared to newer, simpler algorithms.\nSoftware Packages: adabag, gbm, scikit-learn.\n\n\nXGBoost\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting designed for speed and performance. It revolutionizes the method by using the Newton-Raphson method in function space, setting it apart from traditional gradient boosting‚Äôs simpler gradient descent approach. At its core, XGBoost leverages both the first and second derivatives of the loss function through a second-order Taylor approximation. This sophisticated approach enables faster convergence and more accurate predictions by making better-informed decisions about how to improve the model at each step. When building decision trees, XGBoost considers both the expected improvement and the uncertainty of potential splits, while simultaneously applying regularization to prevent overfitting.\nWith some simplifications, the regression version of the XGBoost is implemented as follows:\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m=1, \\dots M\\):\n\n\nCompute the gradients \\(grad_m(x_i)=\\left( \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right)_{f(x)=f_{m-1}(x)}\\) and hessians \\(hess_m(x_i)=\\left( \\frac{\\partial^2 L(y_i, f(x_i))}{\\partial^2 f(x_i)^2}\\right)_{f(x)=f_{m-1}(x)}\\) of the loss function for \\(i=1,\\dots,n\\).\nFit a weak learner such as a tree, \\(h_m(x)\\) on$ {(x_i, ) }_{i=1}^n$.\nUpdate the model \\(f_m(x)=f_{(m-1)} - \\alpha h_m(x).\\)\n\n\nThe final model is \\(f_M(x)=\\sum_m f_m(x)\\).\n\nXGBoost is ideal for large datasets and competitions where model performance is critical. It is also a good choice when you need a highly optimized and scalable solution as it is highly efficient, supports parallel and distributed computing. Nevertheless, this algorithm can be complex to tune, and may require significant computational resources for very large datasets.\nSoftware Packages: xgboost.\n\n\nCatBoost\nCatBoost, short for Categorical Boosting, is a gradient boosting library that handles categorical features efficiently. It uses ordered boosting to reduce overfitting and supports GPU training, making it both powerful and versatile. CatBoost modifies the standard gradient boosting algorithm by incorporating two novel techniques ‚Äì ordered boosting and target statistics for categorical features. Let‚Äôs examine each one in turn.\nRather than requiring preprocessing like one-hot encoding, CatBoost encodes categorical values based on the distribution of the target variable without introducing data leakage. This is achieved by encoding each data point as if it were unseen, preventing overfitting. Additionally, ordered boosting is a method that builds each new tree while treating each data point as ‚Äúout-of-fold‚Äù for itself. This helps reduce overfitting, particularly in small datasets, by preventing over-reliance on individual observations during training.\nSoftware Packages: catboost.\n\n\nLightGBM\nLightGBM shares many of XGBoost‚Äôs benefits, such as support for sparse data, parallel training, multiple loss functions, regularization, and early stopping, but it also introduces a bunc of new features and improvements.\nFirst, rather than growing trees level-wise (row by row) as in most implementations, LightGBM grows trees leaf-wise, selecting the leaf that provides the greatest reduction in loss. Second, it does not use the typical sorted-based approach for finding split points, which sorts feature values to locate the best split. Instead, it relies on an efficient histogram-based method that significantly improves both speed and memory efficiency. Third, LightGBM incorporates the so-called Gradient-Based One-Side Sampling, which speeds up training by focusing on the most informative samples. Lastly, the algorithm uses Exclusive Feature Bundling which can group features to reduce dimensionality, enabling faster and more accurate model training.\nSoftware Packages: lightgbm.\n\n\nChallenges\nGradinet boosting methods comes with a few common practical challenges. When a predicitve model learns the training data too precisely, it may perform poorly on new, unseen data ‚Äì a problem called overfitting. To combat this, various regularization methods are used to add helpful constraints to the learning process. A main parameter that regulates the model‚Äôs learning precision is the number of boosting rounds (\\(M\\)), which determines how many base models are created. While using more rounds reduces training errors, it also increases overfitting risk. To find the optimal \\(M\\) value, it‚Äôs common to use cross validation. The depth of decision trees is another important control parameter in tree boosting. Deeper trees can capture more complex patterns but are more prone to memorizing the training data rather than learning generalizable patterns.\nThe improved predictive performance of gradient boosting relative to simpler models comes at a cost of reduced model transparency. While a single decision tree‚Äôs reasoning can be easily traced and understood, tracking the decision-making process across numerous trees becomes extremely complex and challenging. Gradient boosting is an excellent example of the inherent tradeoff between model simplicity and performance.\nLet‚Äôs now look at how can we use these methods in practice."
  },
  {
    "objectID": "blog/DONE-gradient-boosting.html#an-example",
    "href": "blog/DONE-gradient-boosting.html#an-example",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "An Example",
    "text": "An Example\nHere is some sample python code illustrating the implementation of each algorithm described above on a common dataset. Let‚Äôs look at it in detail.\n# loading the necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\n# we load the data and split it into training and test parts.\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# define and implement the boosting algorithms. \nclassifiers = {\n    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n    \"XGBoost\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42),\n    \"LightGBM\": LGBMClassifier(n_estimators=100, random_state=42),\n    \"CatBoost\": CatBoostClassifier(n_estimators=100, verbose=0, random_state=42)\n}\n\n# save results\nresults = {}\nfor name, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = accuracy\nFinally, we print the accuracy results:\n\nfor name, accuracy in results.items():\n    print(f\"{name}: {accuracy:.4f}\")\n\n# results:\n&gt;AdaBoost: 0.9737\n&gt;XGBoost: 0.9561\n&gt;LightGBM: 0.9649\n&gt;CatBoost: 0.9649\nOverall each method performed reasonably well, with acuracy ranging from \\(95.6\\)% to \\(97.4\\)%. Interestingly, Adaboost outperformed the other more complex algorithms, at least in the in terms of accuracy.\nAnd that‚Äôs it. You are now familiar with the most popular implementations of gradient boosting along with their advantages and weaknesses. You also know how to employ them in practice. Have fun incorporating XGboost and the like into your predictive modeling tasks."
  },
  {
    "objectID": "blog/DONE-gradient-boosting.html#bottom-line",
    "href": "blog/DONE-gradient-boosting.html#bottom-line",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nGradient boosting is a powerful ensemble technique for predictive modeling that comes in a variety of flavors.\nAdaBoost focuses on misclassified instances by adjusting weights.\nXGBoost introduces regularization and optimization for speed and performance.\nCatBoost efficiently handles categorical features and reduces overfitting.\nLightGBM enjoys many of XGBoost‚Äôs strenghts while introducing a few novelties including a different way of building the underlying weak learners.\nCommon practical challenges when implementing gradient boosting include overfitting, decreased interpretability and computational costs."
  },
  {
    "objectID": "blog/DONE-gradient-boosting.html#where-to-learn-more",
    "href": "blog/DONE-gradient-boosting.html#where-to-learn-more",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great startint point, and it‚Äôs a resource I used extensively when preparing this article. ‚ÄúThe Elements of Statistical Learning‚Äù by Hastie, Tibshirani, and Friedman is a comprehensive guide that covers the theoretical foundations of machine learning, including gradient boosting. It is the de facto bible for statistical ML. While this book is phenomenal, it can be challenging for less technical practitioners for which I recommend its lighther versions, ‚ÄúAn Introduction to Statistical Learning‚Äù with R and Python code. All these books are available for free online. Lastly, if you want to dive even deeper into any of the algortihms descibe above, consider studying the papers in the References section below."
  },
  {
    "objectID": "blog/DONE-gradient-boosting.html#references",
    "href": "blog/DONE-gradient-boosting.html#references",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "References",
    "text": "References\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.¬†785-794).\nDorogush, A. V., Ershov, V., & Gulin, A. (2018). CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363.\nFreund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139.\nFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The elements of statistical learning: data mining, inference, and prediction.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An introduction to statistical learning: With applications in python. Springer Nature.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2013). An introduction to statistical learning: With applications in R. Springer Nature.\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ‚Ä¶ & Liu, T. Y. (2017). Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30.\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: unbiased boosting with categorical features. Advances in neural information processing systems, 31."
  },
  {
    "objectID": "blog/DONE-overlapping-conf-intervals.html",
    "href": "blog/DONE-overlapping-conf-intervals.html",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "",
    "text": "This is a mistake I‚Äôve made myself‚Äîmore times than I‚Äôd like to admit. Even seasoned professors and expert data scientists sometimes fall into the same trap.\nIt typically begins with a bar graph showing two sample means side by side, each accompanied by error bars representing 95% confidence intervals. The side-by-side placement suggests a comparison is imminent. Naturally, we check whether the confidence intervals overlap. If they don‚Äôt, we may quickly (and incorrectly) conclude that the difference between the means is statistically significant‚Äîand therefore meaningful.\nThis intuitive but flawed approach to evaluating significance is surprisingly common. Here‚Äôs why it doesn‚Äôt hold up."
  },
  {
    "objectID": "blog/DONE-overlapping-conf-intervals.html#background",
    "href": "blog/DONE-overlapping-conf-intervals.html#background",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "",
    "text": "This is a mistake I‚Äôve made myself‚Äîmore times than I‚Äôd like to admit. Even seasoned professors and expert data scientists sometimes fall into the same trap.\nIt typically begins with a bar graph showing two sample means side by side, each accompanied by error bars representing 95% confidence intervals. The side-by-side placement suggests a comparison is imminent. Naturally, we check whether the confidence intervals overlap. If they don‚Äôt, we may quickly (and incorrectly) conclude that the difference between the means is statistically significant‚Äîand therefore meaningful.\nThis intuitive but flawed approach to evaluating significance is surprisingly common. Here‚Äôs why it doesn‚Äôt hold up."
  },
  {
    "objectID": "blog/DONE-overlapping-conf-intervals.html#diving-deeper",
    "href": "blog/DONE-overlapping-conf-intervals.html#diving-deeper",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Diving Deeper",
    "text": "Diving Deeper\n\nThe Basics of Confidence Intervals\nLet‚Äôs use a simplified example adapted from Schenker and Gentleman (2001). Suppose we are comparing two quantities‚Äî\\(Y_1\\) and \\(Y_2\\)‚Äîsuch as average user engagement on Android vs.¬†iOS or sales in two different regions. We assume ideal conditions: large, random samples; well-behaved distributions; and reliable estimators.\nWe‚Äôre testing the null hypothesis:\n\\[ $H_0: Y_1 = Y_2$. \\]\nWe denote our sample estimates as \\(\\hat{Y}_1\\) and \\(\\hat{Y}_2\\), with corresponding standard errors \\(\\hat{SE}(Y_1)\\) and \\(\\hat{SE}(Y_2)\\). The 95% confidence intervals for these estimates are:\n\\[ \\hat{Y_1} \\pm 1.96 \\times \\hat{SE}(Y_1) \\]\nand \\[ \\hat{Y_2} \\pm 1.96 \\times \\hat{SE}(Y_2). \\]\nCrucially, we can also construct a confidence interval for the difference:\n\\[ (\\hat{Y_1} - \\hat{Y_2}) \\pm 1.96 \\times \\sqrt{ \\hat{SE}(Y_1)^2+ \\hat{SE}(Y_2)^2}. \\]\nThis is the interval we should be analyzing when testing whether \\(Y_1\\) and \\(Y_2\\) differ significantly.\n\n\nTwo Approaches, One Mistake\nThe Na√Øve Approach:\n\nLook at whether the confidence intervals for \\(Y_1\\) and \\(Y_2\\) overlap.\nIf they do not overlap, reject \\(H_0\\); otherwise, do not reject.\n\nThe Correct Approach:\n\nCompute the confidence interval for the difference \\(Y_1 - Y_2\\).\nReject \\(H_0\\) if this interval does not contain 0; otherwise, do not reject.\n\n\n\nWhy the Na√Øve Method Fails\nTo understand the error, consider the following: under the na√Øve method, we‚Äôre implicitly relying on the interval:\n\\[ (\\hat{Y_1} - \\hat{Y_2}) \\pm 1.96 \\times (\\hat{SE}(Y_1) + \\hat{SE}(Y_2)). \\]\nCompare this to the statistically correct confidence interval for the difference:\n\\[ \\frac{\\hat{SE}(Y_1)+ \\hat{SE}(Y_2)}{\\sqrt{\\hat{SE}(Y_1)^2 + \\hat{SE}(Y_2)^2}} \\]\nThe ratio of the widths of these intervals is:\n\\[ \\frac{\\hat{SE}(Y_1)+ \\hat{SE}(Y_2)}{\\sqrt{\\hat{SE}(Y_1)^2 + \\hat{SE}(Y_2)^2}} \\]\nThis ratio is always greater than 1, meaning the na√Øve method uses a wider interval. It is more conservative when the null hypothesis is true (i.e., less likely to reject it), and less conservative when the null is false (i.e., more prone to false positives).\nThe discrepancy is largest when the standard errors are similar, and smallest when one standard error dominates."
  },
  {
    "objectID": "blog/DONE-overlapping-conf-intervals.html#an-example",
    "href": "blog/DONE-overlapping-conf-intervals.html#an-example",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "An Example",
    "text": "An Example\nSchenker and Gentleman (2001) offer a helpful illustration using proportions:\n\n\\(\\hat{Y}_1 = 0.56\\), \\(\\hat{Y}_2 = 0.44\\)\n\\(\\hat{SE}(Y_1) = \\hat{SE}(Y_2) = 0.0351\\)\n\nThe individual \\(95\\%\\) confidence intervals are:\n\nFor \\(Y_1\\): \\([0.49, 0.63]\\)\nFor \\(Y_2\\): \\([0.37, 0.51]\\)\n\nThese intervals do overlap. Under the na√Øve method, we would not reject the null hypothesis.\nHowever, the confidence interval for the difference is:\n\\[[0.02,0.22]\\]\nThis interval does not contain \\(0\\), meaning we would reject the null hypothesis using the correct method. The difference is statistically significant."
  },
  {
    "objectID": "blog/DONE-overlapping-conf-intervals.html#bottom-line",
    "href": "blog/DONE-overlapping-conf-intervals.html#bottom-line",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nVisual overlap of confidence intervals is an intuitive‚Äîbut unreliable‚Äîmethod for assessing statistical significance.\nThis rule of thumb often misleads, particularly when standard errors are similar.\nAlways test for significance by examining the confidence interval for the difference between two estimates."
  },
  {
    "objectID": "blog/DONE-overlapping-conf-intervals.html#where-to-learn-more",
    "href": "blog/DONE-overlapping-conf-intervals.html#where-to-learn-more",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper exploration of this topic, including simulation results and discussion of error rates, see the two papers cited below."
  },
  {
    "objectID": "blog/DONE-overlapping-conf-intervals.html#references",
    "href": "blog/DONE-overlapping-conf-intervals.html#references",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "References",
    "text": "References\nCole, S. R., & Blair, R. C. (1999). Overlapping confidence intervals. Journal of the American Academy of Dermatology, 41(6), 1051-1052.\nSchenker, N., & Gentleman, J. F. (2001). On judging the significance of differences by examining the overlap between confidence intervals. The American Statistician, 55(3), 182-186."
  },
  {
    "objectID": "blog/TODO-lasso-heterogeneous-effects.html#a-closer-look",
    "href": "blog/TODO-lasso-heterogeneous-effects.html#a-closer-look",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLasso with Treatment Variable Interactions\nIn a low-dimensional world where regularization is not necessary and researchers are interested in \\(HTE\\)‚Äôs, they often use a linear model in which the treatment variable is interacted with the covariates. Statistically significant interaction coefficients identify the \\(X\\) variables for which the treatment has differential impact.\nMathematically, when all variables are properly interacted, this is analogous to splitting the sample into subgroups based on \\(X\\) and running OLS on each group separately. This is feasible and convenient when \\(X\\) is binary or categorical, but not when it is continuous. The advantage of this approach, however, is that the linear regression spits out \\(p\\)-values which can be (mis)used to determine statistical significance of these interaction variables.\nThe OLS model is then:\n\\[Y = \\beta_1 T + \\beta_2 X + \\beta_3 X \\times T + \\epsilon,\\]\nwhere \\(\\epsilon\\) is the error term. The attention here falls on the coefficient vector $_3 $ which identifies whether the treatment has had a differential impact on units with a particular characteristic \\(X\\).\nIn high-dimensional settings when we have access to a wide vector \\(X\\), this is again not feasible. Instead, we might want to use an algorithm to pick out the variables in \\(X\\) which are important for the treatment effect heterogeneity.\nImai and Ratkovic (2013) show us how to adapt the Lasso to this setting. It turns out you should not simply throw this model into the Lasso loss function. Can you guess why? Some variables might be predictive of the baseline outcome while others only of the treatment effect heterogeneity. The trick is to have two separate lasso constraints ‚Äì \\(\\lambda_1\\) and \\(\\lambda_2\\).\nSo, the loss function looks something like this:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-f(X,T)\\beta_1 - X\\times T \\beta_2 ||_2^2 + \\lambda_1||\\beta_1||_1 + \\lambda_2||\\beta_2||_1 \\}.\\]\nA simplified version of the algorithm is as follows:\n\nGenerate interaction variables, \\(\\tilde{X}(T)\\).\nRun Lasso of \\(Y\\) on \\(T\\), \\(X\\) and \\(\\tilde{X}(T)\\).\nIdentify statistically significant variables determining treatment effect heterogeneity.\n\nTwo notes. First, this requires some assurance that we are not overfitting, so that some form of sample splitting or cross validation is necessary. On top of this, Athey and Imbens (2017) suggest comparing these results with post-lasso OLS estimates to further guard against overfitting. Second, multiple testing is an issue as is the case with ML algorithms more generally. (You can check my earlier post on multiple hypothesis testing in linear machine learning models.) Options to take care of this include sample splitting and bootstrap, among others.\nSoftware Package: FindIt.\n\n\nLasso with Knockoffs\nAn alternative approach directly regresses the unit-level treatment effects on \\(X\\). To get there, we first have to model the outcome function and impute the missing potential outcome for each unit. See my previous post on using Machine Learning tools for causal inference for more information on how we might do that.\nThis approach was developed by Xie et al.¬†(2018). They recognized the multiple hypothesis issue and suggested using knockoffs to control the False Discovery Rate. This is still not completely kosher as it does not account for the fact that the outcome variable is estimated in an earlier step. Oh well. My guess and hope are that this is more of a theoretical concern, and empirically the inference we get is still ‚Äúcorrect.‚Äù\nHere is a simplified version of their algorithm:\n\nTransform the outcome variable \\(\\tilde{Y}=Y\\times\\frac{(T-p)}{p(1-p)}\\).\nCalculate unit-level treatment effects, \\(\\hat{Y}(1)-\\hat{Y}(0)\\).\nGenerate the difference \\(Y^*=\\tilde{Y}-\\frac{1}{N}\\sum \\hat{Y}(1)-\\hat{Y}(0)\\).\nRun Lasso of \\(Y^*\\) on \\(X\\) and \\(X^*\\) (the knockoff counterparts of \\(X\\)).\nFollow the knockoff method to obtain the set of significant variables.\n\nRemember that \\(\\tilde{Y}\\) has the special property that \\(E[\\tilde{Y} \\mid X=x]=HTE(X)\\) under the unconfoundedness assumption.\nInterestingly, in the special case when \\(p=1/2\\) the algebra reduces further which provides computation scaling advantages. Tian et al.¬†(2014) showed this result first."
  },
  {
    "objectID": "blog/TODO-multiple-testing-overview.html#a-closer-look",
    "href": "blog/TODO-multiple-testing-overview.html#a-closer-look",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhen is MH Control NOT Necessary\nMultiple hypothesis adjustments are not required when:\n\nwe are interested in a purely predictive model, such as in machine learning settings,\nwe have an independent dataset for each correlated hypothesis,\nthe dependent variables we are testing are not correlated. An example is testing the impact of a new feature or intervention on a set of outcomes unrelated to each other ‚Äì e.g., health and education variables. In such cases, it is common to adjust for MH separately within the health- and education-related hypotheses.\n\nBefore we dive into the methods, let‚Äôs establish some basic terminology and notation.\n\n\nFWER vs FDR\nThere are two distinct types of targets that data scientists are after. The first one was introduced in the 1950s by the famous statistician John Tukey and is called Familywise Error Rate (FWER). FWER is the probability of making at least one type 1 error:\n\\[ FWER = \\mathbb{P} (\\text{At Least 1 Type 1 Error}). \\]\nIn the example from the introduction, \\(FWER = 0.3\\).\nControlling the FWER is good, but sometimes it is not great. It does what it is supposed to do ‚Äì limit the number of false positives, but it is often too conservative. In practice, too few variables remain significant after such FWER adjustments, especially when testing many (as opposed to just a few) hypotheses.\nTo correct this, in the 1990s, Yoav Benjamini and Yosef Hochberg, conceptualized controlling the so-called False Discovery Rate (FDR). FDR is the expected proportion of false positives:\n\\[ FDR = \\mathbb{E} \\left[ \\frac{\\text{\\# False Positives}}{\\text{\\# False Positives + \\# True Positives} } \\right]. \\]\nTake a minute to notice the significant difference between these two approaches. FWER controls the probability of having at least one false positive at \\(\\alpha\\). At the same time, FDR is okay with having several false positives as long as they are (on average) less than \\(\\alpha\\) of all hypotheses.\nFWER is more conservative than FDR, resulting in fewer significant variables. In our example, applying an FWER adjustment as opposed to an FDR one will lead to fewer statistically significant differences in the impact of the new feature across various cities.\nLet‚Äôs go over the most popular ways to control the FWER.\n\n\nFWER Control\n\nBonferroni‚Äôs Procedure\nYou might have heard of this one. The procedure is extremely simple ‚Äì it goes like this:\n\nreject \\(H_i\\) if \\(p_i \\times m  \\leq \\alpha\\) for all \\(i\\).\n\nIn words, we multiply all \\(p\\)-values by m and reject the hypotheses with adjusted \\(p\\)-values that are smaller than or equal to \\(\\alpha\\).\nThe Bonferroni adjustment is often considered to control the FWER, but it actually controls an even more conservative target ‚Äì the per-family error rate (PFER). PFER is the sum of probabilities of Type 1 error for all the hypotheses.\nYou correctly guessed that PFER is considered too strict. Although it is common in practice, there are better alternatives. While the Bonferroni correction is simple and elegant, it is always dominated by the Holm-Bonferroni procedure, so there is really no reason ever to use it.\n\n\nHolm & Bonferroni‚Äôs Procedure\nThis adjustment offers greater statistical power than the Bonferroni method regardless of the test size without significantly compromising on simplicity. In algorithmic language:\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m.\\)\nReject the null hypothesis \\(H_1,\\dots H_{k-1}\\), where \\(k\\) is the smallest index for which \\(p_i \\times (m+1-k) \\leq \\alpha\\). Much like the Bonferroni method with a multiplication factor of (\\(m+1-k\\)) instead of \\(m\\).\n\nThese two procedures control the FWER by assuming a kind of a ‚Äòworst-case‚Äô scenario in which the \\(p\\)-values are close to being independent of each other. To see why this is the case, imagine the extreme opposite scenario in which there is perfect positive dependence. Then, there is effectively one test statistic, and FWER adjustment is unnecessary.\nIn practice, however, the \\(p\\)-values are often correlated, and exploiting this dependence can result in even more powerful FWER corrections. This can be achieved with various resampling methods, such as the bootstrap or permutation procedures.\n\n\nWestfall & Young‚Äòs Procedure\nThe Westfall & Young method, developed in the 1990s, was the first resampling procedure controlling the FWER. It uses the bootstrap to account for the dependence structure among the \\(p\\)-values and improves on Holm-Bonferroni‚Äôs method.\nIt goes roughly like this:\n\nOrder the \\(m\\) \\(p\\)-values in ascending order ‚Äì \\(p_1, \\dots, p_M\\).\nBegin with a \\(COUNTER_i = 0\\) for each hypothesis \\(i\\).\nTake a bootstrap sample and compute the \\(m\\) \\(p\\)-values (\\(p^*_1, \\dots, p^*_m\\))\nDefine the successive minima by starting with the largest \\(p\\)-value (\\(\\tilde{p}_M=p^*_M\\)) and for each subsequent hypothesis \\(i\\) taking the minimum between \\(\\tilde{p}_{i+1}\\) and \\(p^*_i\\).\n\n\nThis gives a list of \\(p\\)-values \\(\\tilde{p}_1, \\dots, \\tilde{p}_m\\).\n\n\nIf \\(\\tilde{p}_i \\leq p_i\\) add 1 unit to \\(COUNTER_i\\) for each \\(i\\).\nRepeat steps 3-5 above \\(B\\) times and compute the fraction \\(\\hat{p}_i = COUNT_i/B\\).\n\n\nThis gives a list of \\(p\\)-values \\(\\hat{p}_1, \\dots, \\hat{p}_m\\).\n\n\nEnforce monotonicity by starting with \\(\\hat{p}_1\\) and for each subsequent hypothesis \\(i\\) taking the maximum between \\(\\hat{p}_{i-1}\\) and \\(\\hat{p}_i\\).\n\n\nThis final vector presents the FWER-adjusted \\(p\\)-values.\n\n\nReject all null hypotheses \\(i\\) for which \\(\\hat{p}\\leq \\alpha\\).\n\nYes, this is a complicated procedure. But it does offer substantial improvements in power, especially if the hypotheses are positively correlated.\nImportantly, Westfall and Young‚Äôs method rests on a critical assumption called ‚Äúsubset pivotality.‚Äù Roughly speaking, this condition requires that the joint distribution of any subset of test statistics does not depend on whether the remaining hypotheses are true or not.\nSoftware Package: multtest.\n\n\nRomano & Wolf‚Äòs Procedure(s)\nThis procedure improves on Westfall and Young‚Äôs method by not requiring a pivotality assumption. Here I am describing the method from Romano and Wolf‚Äôs 2005 Econometrica paper, although the authors have developed a few closely related FWER adjustments. This is the rough algorithm for it:\n\nOrder the test statistics in descending order, \\(t_1 \\geq t_2 \\geq, \\dots, t_m\\).\nTake B bootstrap samples, and for each hypothesis i compute the empirical distribution function of the successive maximum test statistic random variable.\n\n\nThat is, the random variable of the maximum test statistic between \\(t_i, t_{i+1}, \\dots, t_m\\).\nCall this distribution \\(c(i)\\).\n\n\nReject \\(H_i\\) if \\(t_i\\) is greater than the 1-quantile of \\(c(1)\\).\nDenote by h the number of rejected hypotheses. If \\(h=0\\) stop. Otherwise, for \\(H_{h+1},\\dots H_s\\):\nReject \\(H_i\\) if \\(t_i &gt; c(h+1)\\)\nIterate until no further hypotheses are rejected.\n\nAnd you thought the Westfall and Young correction was tricky. The good news is that there are software packages for all these adjustments, so you won‚Äôt have to program them yourself. Arguably, this one of the best ways to control FWER; does not rest on pivotality assumptions. However, it can be difficult to explain to non-technical audiences; difficult to explain to anyone, really.\nSoftware Package: hdm.\nThis wraps up the descriptions of the methods controlling the FWER.\n\n\n\nFDR Control\nAs a reminder, FDR procedures have greater power at the cost of increased rates of type 1 errors compared to FWER adjustments. This is the dominant framework in the modern literature on MH adjustment.\n\nBenjamini & Hochberg‚Äòs Procedure\nThe Benjamini-Hochberg (BH) method is simple:\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m}{k} \\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\nBH is valid when the tests are independent and in some, but not all, dependence scenarios. In general, FDR control is tricky with arbitrary dependence.\nThis adjustment has an interesting geometric interpretation:\n\nplot \\(p\\) versus \\(k\\),\ndraw a line through the origin with a slope equal to \\(frac{\\alpha}{m}\\),\nreject all hypotheses associated with the \\(p\\)-values on the left up to the last point below this line.\n\n\n\nBenjamini & Yekutieli‚Äòs Procedure\nBenjamini and Yekutieli (BY) developed a refinement of the BH procedure, which adds a magical constant \\(c(m)\\):\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m.c(m)}{k}\\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\nThe obvious question is in choosing the optimal value for \\(c(m)\\). You can view BH as a special case of BY with \\(c(m)=1\\). The authors show that the following choice works under any arbitrary dependence assumption:\n\\[ c(m) = \\sum_{i=1}^m \\frac{1}{i} \\approx log(m) + 0.57721 + \\frac{1}{2m}. \\]\nFascinating. For most applications, this simple procedure provides a credible way of taming the false positives rate. There are improvements to this method, but it feels like we are entering a world in which we get theoretical gains with only limited practical implications.\nThe main way to improve on BH/BY is to know (or estimate) the share of false hypotheses among the ones we are testing. This, however, goes beyond the scope of the article.\n\n\nKnockoffs\nWhat if we would like to combine variable selection and control the FDR? BH and BY do not really tell us how to do that. Enter knockoffs. The knockoffs approach is the kid on the block, and it does more than just FDR control.\nThe idea behind the knockoffs is novel and unrelated to any of the methods I discussed above. Knockoffs are copies of the variables with specific properties. In particular, for each feature/covariate variable \\(X\\), we create a knockoff copy \\(\\tilde{X}\\) such that the correlation structure between any two knockoff variables is the same as between their original counterparts. These knockoffs are constructed without looking at the outcome variable; hence, they are unrelated to it by design. Consequently, we can gauge whether each variable is statistically significant by comparing its test statistic to that of the one for its knockoff copy.\nKnockoffs are among the most active fields of research in statistics, with new papers coming out daily (ok, maybe weekly). These methods come in two flavors.\nThe fixed-design knockoff filter controls FDR for low-dimensional linear models regardless of the dependency structure of the features. This is great, but it does not work in high dimensions where variable selection is usually most needed.\nThe model-X knockoff filter solves this issue but requires exact knowledge of the joint distribution of the features. This is often infeasible in practice; even estimating that distribution in high dimensions can be really challenging as the curse of dimensionality lays its hammer on the data scientist.\nThe knockoff idea presents a big step forward because it solves two statistical problems at the same time ‚Äì selecting among a wide set of predictors and controlling the FDR. I am really excited to see where the current research will take us next.\nSoftware Package: knockoff.\n\n\nOther Modern Methods\nThere is certainly no shortage of statistical methods in this field. Each week, there is at least one new paper dealing with some aspects of FDR or FWER control. Knockoffs, in particular, have gotten a lot of attention recently, with researchers extending the idea to all kinds of settings and models.\nI will briefly mention some of the newer methods that have caught my eye in recent months. Many of these control the FDR asymptotically, and some are valid under general dependency structures among the test statistics.\nOne recent variation of the knockoffs idea uses Gaussian Mirrors; while others rely on subsampling approaches ‚Äì Data Splitting (based on estimating two independent regression coefficients after splitting the dataset in half) and Data Aggregation. Clever ideas to improve power include using covariate information to optimally weigh hypotheses in the BH framework or incorporating them into FDR regressions.\nTime will show whether any of these newcomers will come to dominate in practice."
  },
  {
    "objectID": "blog/DONE-lasso-heterogeneous-effects.html",
    "href": "blog/DONE-lasso-heterogeneous-effects.html",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "",
    "text": "Lasso is one of my favorite machine learning algorithms. It is so simple, elegant, and powerful. My feelings aside, Lasso indeed has a lot to offer. While, admittedly, it is outperformed by more complex, black-box type methods (e.g., boosting or neural networks), it has several advantages: interpretability, computational efficiency, and flexibility. Even when it comes to accuracy, theory tells us that under appropriate assumptions, Lasso can uncover the true submodel and we can even derive bounds on its prediction loss.\nIn this article I will briefly describe two ways researchers use Lasso to detect heterogeneous treatment effects. The underlying idea is to throw a bunch of covariates in the model and let the \\(L1\\) regularization do the difficult job of identifying which ones are important for treatment effect heterogeneity."
  },
  {
    "objectID": "blog/DONE-lasso-heterogeneous-effects.html#background",
    "href": "blog/DONE-lasso-heterogeneous-effects.html#background",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "",
    "text": "Lasso is one of my favorite machine learning algorithms. It is so simple, elegant, and powerful. My feelings aside, Lasso indeed has a lot to offer. While, admittedly, it is outperformed by more complex, black-box type methods (e.g., boosting or neural networks), it has several advantages: interpretability, computational efficiency, and flexibility. Even when it comes to accuracy, theory tells us that under appropriate assumptions, Lasso can uncover the true submodel and we can even derive bounds on its prediction loss.\nIn this article I will briefly describe two ways researchers use Lasso to detect heterogeneous treatment effects. The underlying idea is to throw a bunch of covariates in the model and let the \\(L1\\) regularization do the difficult job of identifying which ones are important for treatment effect heterogeneity."
  },
  {
    "objectID": "blog/DONE-lasso-heterogeneous-effects.html#notation",
    "href": "blog/DONE-lasso-heterogeneous-effects.html#notation",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Notation",
    "text": "Notation\nAs always, let‚Äôs start with some notation. Let \\(T\\) denote a binary treatment indicator, \\(Y(0), Y(1)\\) be the potential outcomes under each treatment state (\\(Y\\) is the observed one), and \\(X\\) be a covariate vector. Lastly, \\(p\\) is the share of units in the treatment group, \\(p=\\frac{1}{N}\\sum T\\), where \\(N\\) is the sample size.\nThe Lasso coefficient vector is commonly expressed as the solution to the following problem:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-X\\beta||_2^2 + \\lambda||\\beta||_1 \\},\\]\nwhere \\(\\lambda\\) is the regularization parameter governing the variance-bias trade-off.\nWe are interested in the heterogeneous treatment effect given \\(X (HTE(X))\\):\n\\[HTE(X) = E[Y(1)-Y(0)|X=x].\\]\nThat is, \\(HTE(X)\\) is the average treatment effect for units with covariate levels \\(X=x\\).\nMore precisely, our goal is identifying which variables in \\(X\\) divide the population of interest such that there are meaningful treatment effect differences across these groups. For instance, in the case of estimating the impact of school quality on test scores, \\(X\\) might be students‚Äô gender (e.g., girls benefit more than boys), or in the context of online A/B testing, \\(X\\) might denote previous product engagement (e.g., tenured users benefit more from a new feature than inexperienced users).\nBroadly speaking, there are two main approaches to using Lasso to solve this problem ‚Äî (i) a linear model with interactions between \\(T\\) and \\(X\\), and (ii) directly regressing the imputed unit-level treatment effects on \\(X\\)."
  },
  {
    "objectID": "blog/DONE-lasso-heterogeneous-effects.html#a-closer-look",
    "href": "blog/DONE-lasso-heterogeneous-effects.html#a-closer-look",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLasso with Treatment Variable Interactions\nIn a low-dimensional world where regularization is not necessary and researchers are interested in \\(HTE\\)s, they often use a linear model in which the treatment variable is interacted with the covariates. Statistically significant interaction coefficients identify the \\(X\\) variables for which the treatment has a differential impact.\nMathematically, when all variables are properly interacted, this is analogous to splitting the sample into subgroups based on \\(X\\) and running OLS on each group separately. This is feasible and convenient when \\(X\\) is binary or categorical, but not when it is continuous. The advantage of this approach is that linear regression produces \\(p\\)-values which can be (mis)used to determine statistical significance of these interaction variables.\nThe OLS model is then:\n\\[Y = \\beta_1 T + \\beta_2 X + \\beta_3 X \\times T + \\epsilon,\\]\nwhere \\(\\epsilon\\) is the error term. The attention here falls on the coefficient vector \\(\\beta_3\\) which identifies whether the treatment has had a differential impact on units with a particular characteristic \\(X\\).\nIn high-dimensional settings with a wide \\(X\\), this is not feasible. Instead, we can use an algorithm to pick out the variables in \\(X\\) that are important for treatment effect heterogeneity.\nImai and Ratkovic (2013) show how to adapt the Lasso to this setting. It turns out we should not simply throw this model into the Lasso loss function. Can you guess why? Some variables might be predictive of the baseline outcome while others only of the treatment effect heterogeneity. The trick is to have two separate Lasso constraints ‚Äî \\(\\lambda_1\\) and \\(\\lambda_2\\).\nSo, the loss function looks something like this:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-f(X,T)\\beta_1 - X\\times T \\beta_2 ||_2^2 + \\lambda_1||\\beta_1||_1 + \\lambda_2||\\beta_2||_1 \\}.\\]\nA simplified version of the algorithm is as follows:\n\nGenerate interaction variables, \\(\\tilde{X}(T)\\).\nRun Lasso of \\(Y\\) on \\(T\\), \\(X\\) and \\(\\tilde{X}(T)\\).\nIdentify statistically significant variables determining treatment effect heterogeneity.\n\nTwo notes. First, this requires some assurance that we are not overfitting, so that some form of sample splitting or cross validation is necessary. On top of this, Athey and Imbens (2017) suggest comparing these results with post-lasso OLS estimates to further guard against overfitting. Second, multiple testing is an issue as is the case with ML algorithms more generally. (You can check my earlier post on multiple hypothesis testing in linear machine learning models.) Options to take care of this include sample splitting and bootstrap, among others.\nSoftware Package: FindIt.\n\n\nLasso with Knockoffs\nAn alternative approach directly regresses the unit-level treatment effects on \\(X\\). To get there, we first model the outcome function and impute the missing potential outcome for each unit. See my previous post on using Machine Learning tools for causal inference for more information on how we might do that.\nThis approach was developed by Xie et al.¬†(2018). They recognized the multiple hypothesis issue and suggested using knockoffs to control the False Discovery Rate. This is still not completely kosher, as it does not account for the fact that the outcome variable is estimated in an earlier step. Oh well. My guess and hope are that this is more of a theoretical concern, and empirically the inference we get is still ‚Äúcorrect.‚Äù\nHere is a simplified version of their algorithm:\n\nTransform the outcome variable \\(\\tilde{Y}=Y\\times\\frac{(T-p)}{p(1-p)}\\).\nCalculate unit-level treatment effects, \\(\\hat{Y}(1)-\\hat{Y}(0)\\).\nGenerate the difference \\(Y^*=\\tilde{Y}-\\frac{1}{N}\\sum \\hat{Y}(1)-\\hat{Y}(0)\\).\nRun Lasso of \\(Y^*\\) on \\(X\\) and \\(X^*\\) (the knockoff counterparts of \\(X\\)).\nFollow the knockoff method to obtain the set of significant variables.\n\nRemember that \\(\\tilde{Y}\\) has the special property that \\(E[\\tilde{Y} \\mid X=x]=HTE(X)\\) under the unconfoundedness assumption.\nInterestingly, in the special case when \\(p=1/2\\) the algebra reduces further which provides computation scaling advantages. Tian et al.¬†(2014) showed this result first."
  },
  {
    "objectID": "blog/DONE-lasso-heterogeneous-effects.html#an-example",
    "href": "blog/DONE-lasso-heterogeneous-effects.html#an-example",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate the latter method. As such, it is a mere depiction of the approach, and its results should certainly not be taken seriously.\nThe outcome variable was survived, and the treatment variable was male. As is well known, women were much more likely to survive than men (\\(74\\%\\) vs \\(19\\%\\) in this sample). So, I analyzed whether the gender difference in survival was impacted by other factors. I included the following covariates ‚Äì pclass (ticket class), age, sibsp (number of siblings aboard), parch (number of parents aboard), fare, embarked (port of Embarkation), and cabin. Some of these were categorical in which case I converted them to a bunch of binary variables.\nThe knockoff filter identified a single variable as having a significant impact on the treatment effect ‚Äì pclass. For the more affluent passengers (i.e., those in the higher ticket classes), this gender difference in survival was much smaller.\nYou can find the code in this GitHub repository."
  },
  {
    "objectID": "blog/DONE-lasso-heterogeneous-effects.html#bottom-line",
    "href": "blog/DONE-lasso-heterogeneous-effects.html#bottom-line",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe core idea behind using Lasso in HTE estimation is to leverage \\(L_1\\) regularization to select which covariates explain differences in treatment responses.\nThere are two main ways researchers use Lasso to estimate HTEs. Use a linear model with all covariates interacted with the treatment indicator, and apply Lasso with two separate regularization constraints. Directly regress unit-level treatment effects on the covariates.\nI am not aware of simulation studies comparing both approaches.\nWhile Lasso is among the simplest and most popular machine learning algorithms, more suitable methods may exist for estimating HTEs."
  },
  {
    "objectID": "blog/DONE-lasso-heterogeneous-effects.html#where-to-learn-more",
    "href": "blog/DONE-lasso-heterogeneous-effects.html#where-to-learn-more",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nHu (2022) is an excellent summary of the several ways researchers use Machine Learning to uncover heterogeneity in treatment effect estimation."
  },
  {
    "objectID": "blog/DONE-lasso-heterogeneous-effects.html#references",
    "href": "blog/DONE-lasso-heterogeneous-effects.html#references",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics: 2055-2085.\nBarber, R. F., & Cand√®s, E. J. (2019). A knockoff filter for high-dimensional selective inference. The Annals of Statistics, 47(5), 2504-2537.\nChatterjee, A., & Lahiri, S. N. (2011). Bootstrapping lasso estimators. Journal of the American Statistical Association, 106(494), 608-625.\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. The Annals of Applied Statistics (2013): 443-470.\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\nMeinshausen, N., Meier, L., & B√ºhlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nXie, Y., Chen, N., & Shi, X. (2018). False discovery rate controlled heterogeneous treatment effect detection for online controlled experiments. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp.¬†876-885)."
  },
  {
    "objectID": "blog/DONE-multiple-testing-overview.html",
    "href": "blog/DONE-multiple-testing-overview.html",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "",
    "text": "The abundance of data around us is a major factor making the data science field so attractive. It enables all kinds of impactful, interesting, or fun analyses. I admit this is what got me into statistics and causal inference in the first place. However, this luxury has downsides ‚Äì it might require deeper methodological knowledge and attention.\nIf we are interested in measuring statistical significance, the standard frequentist framework relies on testing a single hypothesis on any given dataset. Once we start using the same data multiple times ‚Äì i.e., testing several hypotheses at once ‚Äì as we often do, we might have to make some additional adjustments.\nIn this article, I will walk you through the most popular multiple hypotheses (MH) testing corrections. Common examples of when these come to play include estimating the impact of several variables on a single outcome or estimating the effect of an intervention on several subgroups of users to mine for heterogeneous treatment effects. Both scenarios are ubiquitous in most data analysis projects.\nTo illustrate the problem, imagine we are testing seven hypotheses at the \\(5\\%\\) significance level. Then, the probability that we incorrectly reject at least one of these hypotheses is:\n\\[ 1 - (1-.05)^7 = .30. \\]\nThat is, roughly one in three times, we will reach a wrong conclusion.\nVaguely speaking, MH adjustments can be viewed as inflating the calculated p-values and hence decreasing the probability that we reject any given hypothesis. In other words, we are accounting for using the data multiple times by making the definition of statistical significance more stringent."
  },
  {
    "objectID": "blog/DONE-multiple-testing-overview.html#background",
    "href": "blog/DONE-multiple-testing-overview.html#background",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "",
    "text": "The abundance of data around us is a major factor making the data science field so attractive. It enables all kinds of impactful, interesting, or fun analyses. I admit this is what got me into statistics and causal inference in the first place. However, this luxury has downsides ‚Äì it might require deeper methodological knowledge and attention.\nIf we are interested in measuring statistical significance, the standard frequentist framework relies on testing a single hypothesis on any given dataset. Once we start using the same data multiple times ‚Äì i.e., testing several hypotheses at once ‚Äì as we often do, we might have to make some additional adjustments.\nIn this article, I will walk you through the most popular multiple hypotheses (MH) testing corrections. Common examples of when these come to play include estimating the impact of several variables on a single outcome or estimating the effect of an intervention on several subgroups of users to mine for heterogeneous treatment effects. Both scenarios are ubiquitous in most data analysis projects.\nTo illustrate the problem, imagine we are testing seven hypotheses at the \\(5\\%\\) significance level. Then, the probability that we incorrectly reject at least one of these hypotheses is:\n\\[ 1 - (1-.05)^7 = .30. \\]\nThat is, roughly one in three times, we will reach a wrong conclusion.\nVaguely speaking, MH adjustments can be viewed as inflating the calculated p-values and hence decreasing the probability that we reject any given hypothesis. In other words, we are accounting for using the data multiple times by making the definition of statistical significance more stringent."
  },
  {
    "objectID": "blog/DONE-multiple-testing-overview.html#notation",
    "href": "blog/DONE-multiple-testing-overview.html#notation",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Notation",
    "text": "Notation\nWe are interested in testing a bunch of m null hypotheses about a population parameter \\(\\beta\\).\nAs a running example, you can think of \\(\\beta\\) as the causal impact of a new product feature on user engagement and \\(m\\) as indexing some geographical regions such as cities. We are interested in whether the new feature is more impactful in some cities than others. We will denote these hypotheses with \\(H_1, H_2, \\dots, H_m\\) and refer to their associated p-values with \\(p_1, p_2, \\dots, p_m\\).\nWe use \\(\\alpha\\) to denote the probability of a Type 1 error ‚Äì rejecting a true null (i.e., a false positive). In technical jargon, we refer to \\(\\alpha\\) as test size. We often choose \\(\\alpha=.05\\), meaning that we are allowing a 5% chance that we will make such an error. This corresponds to the 95% confidence intervals that we often see.\nNext, statistical power is the probability of correctly rejecting a false null hypothesis (i.e., a true positive). This is a desirable property ‚Äì the higher it is, the better. While this terminology does not describe the entire hypothesis testing framework, it does cover what is necessary to continue reading the article."
  },
  {
    "objectID": "blog/DONE-multiple-testing-overview.html#a-closer-look",
    "href": "blog/DONE-multiple-testing-overview.html#a-closer-look",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhen is MH Control NOT Necessary\nMultiple hypothesis adjustments are not required when:\n\nwe are interested in a purely predictive model, such as in machine learning settings,\nwe have an independent dataset for each correlated hypothesis,\nthe dependent variables we are testing are not correlated. An example is testing the impact of a new feature or intervention on a set of outcomes unrelated to each other ‚Äì e.g., health and education variables. In such cases, it is common to adjust for MH separately within the health- and education-related hypotheses.\n\nBefore we dive into the methods, let‚Äôs establish some basic terminology and notation.\n\n\nFWER vs FDR\nThere are two distinct types of targets that data scientists are after. The first one was introduced in the 1950s by the famous statistician John Tukey and is called Familywise Error Rate (FWER). FWER is the probability of making at least one type 1 error:\n\\[ FWER = \\mathbb{P} (\\text{At Least 1 Type 1 Error}). \\]\nIn the example from the introduction, \\(FWER = 0.3\\).\nControlling the FWER is good, but sometimes it is not great. It does what it is supposed to do ‚Äì limit the number of false positives, but it is often too conservative. In practice, too few variables remain significant after such FWER adjustments, especially when testing many (as opposed to just a few) hypotheses.\nTo correct this, in the 1990s, Yoav Benjamini and Yosef Hochberg, conceptualized controlling the so-called False Discovery Rate (FDR). FDR is the expected proportion of false positives:\n\\[ FDR = \\mathbb{E} \\left[ \\frac{\\text{\\# False Positives}}{\\text{\\# False Positives + \\# True Positives} } \\right]. \\]\nTake a minute to notice the significant difference between these two approaches. FWER controls the probability of having at least one false positive at \\(\\alpha\\). At the same time, FDR is okay with having several false positives as long as they are (on average) less than \\(\\alpha\\) of all hypotheses.\nFWER is more conservative than FDR, resulting in fewer significant variables. In our example, applying an FWER adjustment as opposed to an FDR one will lead to fewer statistically significant differences in the impact of the new feature across various cities.\nLet‚Äôs go over the most popular ways to control the FWER.\n\n\nFWER Control\n\nBonferroni‚Äôs Procedure\nYou might have heard of this one. The procedure is extremely simple ‚Äì it goes like this:\n\nreject \\(H_i\\) if \\(p_i \\times m  \\leq \\alpha\\) for all \\(i\\).\n\nIn words, we multiply all \\(p\\)-values by m and reject the hypotheses with adjusted \\(p\\)-values that are smaller than or equal to \\(\\alpha\\).\nThe Bonferroni adjustment is often considered to control the FWER, but it actually controls an even more conservative target ‚Äì the per-family error rate (PFER). PFER is the sum of probabilities of Type 1 error for all the hypotheses.\nYou correctly guessed that PFER is considered too strict. Although it is common in practice, there are better alternatives. While the Bonferroni correction is simple and elegant, it is always dominated by the Holm-Bonferroni procedure, so there is really no reason ever to use it.\n\n\nHolm & Bonferroni‚Äôs Procedure\nThis adjustment offers greater statistical power than the Bonferroni method regardless of the test size without significantly compromising on simplicity. In algorithmic language:\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m.\\)\nReject the null hypothesis \\(H_1,\\dots H_{k-1}\\), where \\(k\\) is the smallest index for which \\(p_i \\times (m+1-k) \\leq \\alpha\\). Much like the Bonferroni method with a multiplication factor of (\\(m+1-k\\)) instead of \\(m\\).\n\nThese two procedures control the FWER by assuming a kind of a ‚Äòworst-case‚Äô scenario in which the \\(p\\)-values are close to being independent of each other. To see why this is the case, imagine the extreme opposite scenario in which there is perfect positive dependence. Then, there is effectively one test statistic, and FWER adjustment is unnecessary.\nIn practice, however, the \\(p\\)-values are often correlated, and exploiting this dependence can result in even more powerful FWER corrections. This can be achieved with various resampling methods, such as the bootstrap or permutation procedures.\n\n\nWestfall & Young‚Äòs Procedure\nThe Westfall & Young method, developed in the 1990s, was the first resampling procedure controlling the FWER. It uses the bootstrap to account for the dependence structure among the \\(p\\)-values and improves on Holm-Bonferroni‚Äôs method.\nIt goes roughly like this:\n\nOrder the \\(m\\) \\(p\\)-values in ascending order ‚Äì \\(p_1, \\dots, p_M\\).\nBegin with a \\(COUNTER_i = 0\\) for each hypothesis \\(i\\).\nTake a bootstrap sample and compute the \\(m\\) \\(p\\)-values (\\(p^*_1, \\dots, p^*_m\\))\nDefine the successive minima by starting with the largest \\(p\\)-value (\\(\\tilde{p}_M=p^*_M\\)) and for each subsequent hypothesis \\(i\\) taking the minimum between \\(\\tilde{p}_{i+1}\\) and \\(p^*_i\\).\n\n\nThis gives a list of \\(p\\)-values \\(\\tilde{p}_1, \\dots, \\tilde{p}_m\\).\n\n\nIf \\(\\tilde{p}_i \\leq p_i\\) add 1 unit to \\(COUNTER_i\\) for each \\(i\\).\nRepeat steps 3-5 above \\(B\\) times and compute the fraction \\(\\hat{p}_i = COUNT_i/B\\).\n\n\nThis gives a list of \\(p\\)-values \\(\\hat{p}_1, \\dots, \\hat{p}_m\\).\n\n\nEnforce monotonicity by starting with \\(\\hat{p}_1\\) and for each subsequent hypothesis \\(i\\) taking the maximum between \\(\\hat{p}_{i-1}\\) and \\(\\hat{p}_i\\).\n\n\nThis final vector presents the FWER-adjusted \\(p\\)-values.\n\n\nReject all null hypotheses \\(i\\) for which \\(\\hat{p}\\leq \\alpha\\).\n\nYes, this is a complicated procedure. But it does offer substantial improvements in power, especially if the hypotheses are positively correlated.\nImportantly, Westfall and Young‚Äôs method rests on a critical assumption called ‚Äúsubset pivotality.‚Äù Roughly speaking, this condition requires that the joint distribution of any subset of test statistics does not depend on whether the remaining hypotheses are true or not.\nSoftware Package: multtest.\n\n\nRomano & Wolf‚Äòs Procedure(s)\nThis procedure improves on Westfall and Young‚Äôs method by not requiring a pivotality assumption. Here I am describing the method from Romano and Wolf‚Äôs 2005 Econometrica paper, although the authors have developed a few closely related FWER adjustments. This is the rough algorithm for it:\n\nOrder the test statistics in descending order, \\(t_1 \\geq t_2 \\geq, \\dots, t_m\\).\nTake B bootstrap samples, and for each hypothesis i compute the empirical distribution function of the successive maximum test statistic random variable.\n\n\nThat is, the random variable of the maximum test statistic between \\(t_i, t_{i+1}, \\dots, t_m\\).\nCall this distribution \\(c(i)\\).\n\n\nReject \\(H_i\\) if \\(t_i\\) is greater than the 1-quantile of \\(c(1)\\).\nDenote by h the number of rejected hypotheses. If \\(h=0\\) stop. Otherwise, for \\(H_{h+1},\\dots H_s\\):\nReject \\(H_i\\) if \\(t_i &gt; c(h+1)\\)\nIterate until no further hypotheses are rejected.\n\nAnd you thought the Westfall and Young correction was tricky. The good news is that there are software packages for all these adjustments, so you won‚Äôt have to program them yourself. Arguably, this one of the best ways to control FWER; does not rest on pivotality assumptions. However, it can be difficult to explain to non-technical audiences; difficult to explain to anyone, really.\nSoftware Package: hdm.\nThis wraps up the descriptions of the methods controlling the FWER.\n\n\n\nFDR Control\nAs a reminder, FDR procedures have greater power at the cost of increased rates of type 1 errors compared to FWER adjustments. This is the dominant framework in the modern literature on MH adjustment.\n\nBenjamini & Hochberg‚Äòs Procedure\nThe Benjamini-Hochberg (BH) method is simple:\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m}{k} \\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\nBH is valid when the tests are independent and in some, but not all, dependence scenarios. In general, FDR control is tricky with arbitrary dependence.\nThis adjustment has an interesting geometric interpretation:\n\nplot \\(p\\) versus \\(k\\),\ndraw a line through the origin with a slope equal to \\(frac{\\alpha}{m}\\),\nreject all hypotheses associated with the \\(p\\)-values on the left up to the last point below this line.\n\n\n\nBenjamini & Yekutieli‚Äòs Procedure\nBenjamini and Yekutieli (BY) developed a refinement of the BH procedure, which adds a magical constant \\(c(m)\\):\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m.c(m)}{k}\\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\nThe obvious question is in choosing the optimal value for \\(c(m)\\). You can view BH as a special case of BY with \\(c(m)=1\\). The authors show that the following choice works under any arbitrary dependence assumption:\n\\[ c(m) = \\sum_{i=1}^m \\frac{1}{i} \\approx log(m) + 0.57721 + \\frac{1}{2m}. \\]\nFascinating. For most applications, this simple procedure provides a credible way of taming the false positives rate. There are improvements to this method, but it feels like we are entering a world in which we get theoretical gains with only limited practical implications.\nThe main way to improve on BH/BY is to know (or estimate) the share of false hypotheses among the ones we are testing. This, however, goes beyond the scope of the article.\n\n\nKnockoffs\nWhat if we would like to combine variable selection and control the FDR? BH and BY do not really tell us how to do that. Enter knockoffs. The knockoffs approach is the kid on the block, and it does more than just FDR control.\nThe idea behind the knockoffs is novel and unrelated to any of the methods I discussed above. Knockoffs are copies of the variables with specific properties. In particular, for each feature/covariate variable \\(X\\), we create a knockoff copy \\(\\tilde{X}\\) such that the correlation structure between any two knockoff variables is the same as between their original counterparts. These knockoffs are constructed without looking at the outcome variable; hence, they are unrelated to it by design. Consequently, we can gauge whether each variable is statistically significant by comparing its test statistic to that of the one for its knockoff copy.\nKnockoffs are among the most active fields of research in statistics, with new papers coming out daily (ok, maybe weekly). These methods come in two flavors.\nThe fixed-design knockoff filter controls FDR for low-dimensional linear models regardless of the dependency structure of the features. This is great, but it does not work in high dimensions where variable selection is usually most needed.\nThe model-X knockoff filter solves this issue but requires exact knowledge of the joint distribution of the features. This is often infeasible in practice; even estimating that distribution in high dimensions can be really challenging as the curse of dimensionality lays its hammer on the data scientist.\nThe knockoff idea presents a big step forward because it solves two statistical problems at the same time ‚Äì selecting among a wide set of predictors and controlling the FDR. I am really excited to see where the current research will take us next.\nSoftware Package: knockoff.\n\n\nOther Modern Methods\nThere is certainly no shortage of statistical methods in this field. Each week, there is at least one new paper dealing with some aspects of FDR or FWER control. Knockoffs, in particular, have gotten a lot of attention recently, with researchers extending the idea to all kinds of settings and models.\nI will briefly mention some of the newer methods that have caught my eye in recent months. Many of these control the FDR asymptotically, and some are valid under general dependency structures among the test statistics.\nOne recent variation of the knockoffs idea uses Gaussian Mirrors; while others rely on subsampling approaches ‚Äì Data Splitting (based on estimating two independent regression coefficients after splitting the dataset in half) and Data Aggregation. Clever ideas to improve power include using covariate information to optimally weigh hypotheses in the BH framework or incorporating them into FDR regressions.\nTime will show whether any of these newcomers will come to dominate in practice."
  },
  {
    "objectID": "blog/DONE-multiple-testing-overview.html#an-example",
    "href": "blog/DONE-multiple-testing-overview.html#an-example",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Check the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\n\n\n\nNumber of Statistically Significant Variables\n\n\nHere is a table of the \\(p\\)-values for each feature and various MH adjustments.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaw\nBonferroni\nHolm\nRom.-Wolf\nBenj.-Hoch.\nBenj.-Yek.\n\n\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nage\n0.00\n0.02\n0.01\n0.01\n0.00\n0.01\n\n\nsibsp\n0.02\n0.34\n0.18\n0.16\n0.04\n0.14\n\n\nparch\n0.32\n1.00\n1.00\n0.77\n0.40\n1.00\n\n\nfare\n0.22\n1.00\n1.00\n0.66\n0.30\n0.98\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nembarkedC\n0.01\n0.23\n0.15\n0.14\n0.04\n0.13\n\n\nembarkedQ\n0.05\n0.76\n0.35\n0.29\n0.09\n0.28\n\n\ncabinA\n0.39\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinB\n0.37\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinC\n0.92\n1.00\n1.00\n0.92\n0.92\n1.00\n\n\ncabinD\n0.06\n0.89\n0.36\n0.29\n0.09\n0.30\n\n\ncabinE\n0.01\n0.07\n0.05\n0.05\n0.01\n0.05\n\n\ncabinF\n0.02\n0.31\n0.18\n0.16\n0.04\n0.14\n\n\n\nEight variables were statistically significant without any MH adjustment. As expected, the FWER adjustments lead to fewer significant variables than the FDR ones. Interestingly, there is a noticeable difference between the two FDR methods ‚Äì BH and BY, with the latter being more conservative.\nYou can find the code for this analysis in this GitHub repository."
  },
  {
    "objectID": "blog/DONE-multiple-testing-overview.html#bottom-line",
    "href": "blog/DONE-multiple-testing-overview.html#bottom-line",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMultiple testing corrections are likely necessary when you are using a single dataset multiple times (i.e., testing multiple hypotheses).\nTwo major frameworks exist ‚Äì FWER and FDR control. The former is often considered too conservative, while the latter is the dominant way researchers and practitioners think about correcting for MH testing.\nIn most settings, the Benjamini-Yekuiteli approach offers a great balance between statistical power and technical simplicity.\nKnockoffs are a novel and exciting approach to FDR control."
  },
  {
    "objectID": "blog/DONE-multiple-testing-overview.html#where-to-learn-more",
    "href": "blog/DONE-multiple-testing-overview.html#where-to-learn-more",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great resource on the classic approaches to tackling MH issues (both for FDR and FWER control), but it lacks material on modern methodologies. Emmanuel Cand√®s‚Äô website features an accessible introduction to the world of knockoffs. Clarke et al.¬†(2020) strip down many technicalities and provide accessible descriptions of both Westfall and Young‚Äôs, as well as Romano and Wolf‚Äôs methods. Korthauer et al.¬†(2019) compare some of the more recent approaches to controlling the FDR, which are beyond the scope of this blog post."
  },
  {
    "objectID": "blog/DONE-multiple-testing-overview.html#references",
    "href": "blog/DONE-multiple-testing-overview.html#references",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics, 43(5), 2055-2085.\nBenjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of Statistics, 1165-1188.\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300.\nCandes, E., Fan, Y., Janson, L., & Lv, J. (2018). Panning for gold:‚Äômodel‚ÄêX‚Äôknockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3), 551-577.\nClarke, D., Romano, J. P., & Wolf, M. (2020). The Romano‚ÄìWolf multiple-hypothesis correction in Stata. The Stata Journal, 20(4), 812-843.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2022). False discovery rate control via data splitting. Journal of the American Statistical Association, 1-38.\nKorthauer, K., Kimes, P. K., Duvallet, C., Reyes, A., Subramanian, A., Teng, M., ‚Ä¶ & Hicks, S. C. (2019). A practical guide to methods controlling false discoveries in computational biology. Genome biology, 20(1), 1-21.\nRomano, J. P., & Wolf, M. (2005). Stepwise multiple testing as formalized data snooping. Econometrica, 73(4), 1237-1282.\nRomano, J. P., & Wolf, M. (2005). Exact and approximate stepdown methods for multiple hypothesis testing. Journal of the American Statistical Association, 100(469), 94-108.\nWestfall, P. H., & Young, S. S. (1993). Resampling-based multiple testing: Examples and methods for p-value adjustment (Vol. 279). John Wiley & Sons.\nXing, X., Zhao, Z., & Liu, J. S. (2021). Controlling false discovery rate using gaussian mirrors. Journal of the American Statistical Association, 1-20."
  },
  {
    "objectID": "blog/filling-missing-data-mcmc.html#a-closer-look",
    "href": "blog/filling-missing-data-mcmc.html#a-closer-look",
    "title": "Filling in Missing Data with MCMC",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nMarkov Chain Monte Carlo is a powerful computational technique designed to draw observations from complex probability distributions that are difficult to directly sample from. This might happen because they do not have a nice closed-form analytical expression, or they do, but it‚Äôs too messy. Such distributions often arise in Bayesian statistics, where we aim to estimate the posterior distribution of parameters given observed data.\nThe ‚Äúmagic‚Äù of MCMC lies in its iterative nature. It begins with an initial guess for the parameter \\(\\theta\\). Then, a sophisticated sampling algorithm, such as the Metropolis-Hastings or Gibbs sampler, is employed to generate a sequence of observations. These observations are not independent but are related to each other in a specific way, forming a Markov chain. Crucially, under certain conditions, this Markov chain will eventually converge to the true target distribution.\nIn the context of missing data, MCMC iteratively alternates between the \\(I\\)- and the \\(P\\)-steps. At the \\(t\\)-th iteration with current guess for \\(\\theta\\) denoted \\(\\theta^t\\), these steps are:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nThe \\(I\\)-step (imputation): Draw \\(Y_{\\text{miss}}\\) from \\(P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta^t)\\). That is, from its conditional distribution given the observed data and current parameter estimates.\nThe \\(P\\)-step (posterior): Draw \\(\\theta^{t+1}\\) from \\(P(\\theta \\mid Y_{\\text{obs}}, Y_{\\text{miss}}^{t+1})\\). This is its posterior distribution given the observed data and the newly imputed \\(Y_{\\text{miss}}\\).\n\n\n\nThis back-and-forth dance ensures that the imputed values reflect the uncertainty and structure of the data. And with enough iterations (large \\(t\\)) the chain will converge to our target, \\(P(Y_{\\text{miss}} \\mid Y_{\\text{obs}}, \\theta)\\).\nSoftware Packages: mcmc, MCMCPack, mice.\n\n\nPractical Considerations\nConvergence diagnostics are crucial to ensure the MCMC chains have reached a stable equilibrium, as the initial values can significantly influence the results. In simple words, the chain should run long enough so that the posterior distribution does not change significantly after each additional iteration. It is also common to discard (or ‚Äúburn‚Äù) an initial batch of values since they do not come from the final, stable posterior distribution. Additionally, computational costs can be a significant factor, especially for large datasets or complex models but efficient algorithms and parallel processing can help. Lastly, model specification is critical, as the choice of imputation model directly impacts the quality of the imputed values."
  },
  {
    "objectID": "blog/bootstrap-limitations.html#a-closer-look",
    "href": "blog/bootstrap-limitations.html#a-closer-look",
    "title": "The Bootstrap and its Limitations",
    "section": "A Closer Look",
    "text": "A Closer Look\nConsider the following scenarios:\n\nVery Small Sample Sizes ‚Äì The bootstrap relies on resampling the observed data to approximate the population distribution. With very small samples, there is not enough variability in the data to accurately capture the underlying distribution, leading to unreliable estimates.\nParameter at the Edge of the Parameter Space ‚Äì When the parameter being estimated lies at or near a boundary (e.g., estimating a proportion close to \\(0\\) or \\(1\\)), the bootstrap may fail to reflect the true sampling distribution. The resampling process cannot fully mimic the constraints of the parameter space. This includes situations in which we are interested in learning more about the minimum or maximum value of some statistic.\nPresence of Outliers ‚Äì Outliers can heavily influence bootstrap resamples, leading to biased or overly variable estimates.\nDependence in the Data ‚Äì The bootstrap assumes the data are independent and identically distributed (i.i.d.). For time series or spatial data where observations are dependent, naive application of the bootstrap can yield incorrect inferences unless adapted for the structure (e.g., block bootstrap).\nExtreme Skewness or Rare Events ‚Äì When the data distribution is highly skewed or dominated by rare events, the bootstrap may struggle to approximate the tails of the distribution accurately, affecting confidence interval coverage and tail probability estimates.\nMisspecified Models ‚Äì If the bootstrap is applied to a statistic derived from a poorly specified model, the resulting inferences will inherit the same flaws. The bootstrap cannot correct for model misspecification.\n\nIn some of these cases theoretical approximation methods can provide analytical solutions that bypass the resampling challenges. The parametric bootstrap is like a more structured cousin of the standard bootstrap, generating samples based on a known probability distribution. It‚Äôs particularly helpful when you‚Äôve got a good sense of what your data looks like. Bayesian methods take things a step further, folding in prior knowledge to handle tricky statistical scenarios with flexibility.\nWhile the bootstrap is a versatile and often reliable tool, awareness of these limitations can help you avoid potential pitfalls and ensure more robust statistical analyses."
  },
  {
    "objectID": "blog/alphabet-het-treatment-effects.html#a-closer-look",
    "href": "blog/alphabet-het-treatment-effects.html#a-closer-look",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\nI will now briefly describe the four learners for heterogeneous treatment effects in ascending order of complexity.\n\n\\(S\\)(ingle) Learner\nThe idea behind the \\(S\\)-learner is to estimate a single outcome function \\(\\mu(X,D)\\) and then calculate \\(HTE(X)\\) by taking the difference in the predicted values between the units in the treatment and control groups.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nUse the entire sample to estimate \\(\\hat{\\mu}(X,D)\\).\nCompute \\(\\hat{HTE}(X)=\\hat{\\mu}(X,1)-\\hat{\\mu}(X,0)\\).\n\n\n\nThis is intuitive and fine. But the problem is that the treatment variable D might be excluded in step 1 when it is not highly correlated with the outcome. (Note that in observational data this does not necessarily imply a null treatment effect.) In this case, we cannot even move to step 2.\n\n\n\\(T\\)(wo) Learner\nThe \\(T\\)-learner solves the above problem by forcing the response models to include \\(D\\). The idea is to first estimate two separate (conditional) outcome functions ‚Äì one for the treatment and one for the control and proceed similarly.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for \\(\\hat{\\mu}(X,1)\\).\nThen \\(\\hat{HTE}(X)=\\hat{\\mu}(X,1)- \\hat{\\mu}(X,0)\\)\n\n\n\nThis is better, but still not great. A potential problem is when there are different number of observations in the treatment and control groups. For instance, in the common case where the treatment group is much smaller, their \\(HTE(X)\\) will be estimated much less precisely than the that of the control group. When combining the two to arrive at a final estimate of \\(HTE(X)\\) the former should get a smaller weight than the latter. This is because the ML algorithms optimize for learning the \\(\\mu(\\cdot)\\) functions, and not the \\(HTE(X)\\) function directly.\n\n\n\\(X\\) Learner\nThe \\(X\\)-learner is designed to overcome the above concern. The procedure starts similarly to the \\(T\\)-learner but then weighs differently the \\(HTE(X)\\)‚Äôs for the treatment and control groups.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for \\(\\hat{\\mu}(X,1)\\).\nEstimate the unit-level treatment effect for the observations in the control group, \\(\\hat{\\mu}(X,1)-Y\\), and for the treatment group, \\(Y-\\hat{\\mu}(X,0)\\).\nCombine both estimates by weighing them using the predicted propensity score, \\(\\hat{p}\\): \\[HTE(X)=\\hat{p} \\times HTE(X|D=1) + (1-\\hat{p})\\times HTE(X|D=0).\\]\n\n\n\nHere \\(\\hat{p}\\) balances the uncertainty associated with the \\(HTE(X)\\)‚Äôs in each group and hence, this approach is particularly effective when there is a significant difference in the number of units between the two groups.\n\n\n\\(R\\)(obinson) Learner\nTo avoid getting too deep into technical details, I will not be describing the entire algorithm. The \\(R\\)-learner models both the outcome, and the propensity score and begins by computing unit-level predicted outcomes and treatment probabilities using cross-fitting and leave-one-out estimation. The key innovation is plugging these into a novel loss function featuring squared deviations of these predictions as well as a regularization term. This ensures ‚Äúoptimality‚Äù in learning the treatment effect heterogeneity."
  },
  {
    "objectID": "blog/ml-based-adjustments.html#a-closer-look",
    "href": "blog/ml-based-adjustments.html#a-closer-look",
    "title": "ML-Based Regression Adjustments in Randomized Experiments",
    "section": "A Closer Look",
    "text": "A Closer Look\nBroadly speaking, there are two ML Methods for Variance Reduction.\n\nUsing ML Regression Directly\nThe simplest and most natural way to incorporate covariates is to add them to a linear model (along with the treatment variable and their interactions with the treatment variable). Bloniarz et al.¬†(2015) show we can directly use Lasso regression instead of OLS.\nTo guarantee that the lasso does not omit the treatment variable, we can run two separate regressions, one for each (treatment) group. Then the estimator can be formulated as:\n\\[\\hat{ATE}^{lasso} = (\\bar{Y}^T-\\tilde{X}^T\\beta^{T}_{lasso}) - (\\bar{Y}^C-\\tilde{X}^C\\beta^{C}_{lasso}),\\]\nwhere \\(\\beta^{i}_{lasso}\\) is the coefficient vector from the lasso regressions on observations in group \\(i\\in\\{T,C\\}\\). The authors also give a conservative formula for computing the variance of \\(\\hat{ATE}^{lasso}\\). When the two lasso regressions select different sets of covariates (which is probably common in practice), this is no longer guaranteed to yield equal or lower asymptotic variance compared to the benchmark.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nFor the treatment and control groups separately, run lasso regression of \\(Y\\) on \\(\\tilde{X}\\) go get \\(\\hat{\\beta}^T_{lasso}\\) and \\(\\hat{\\beta}^C_{lasso}\\).\nCalculate the treatment effect estimate \\(\\hat{ATE}^{lasso}\\) using the above formula.\nCalculate the estimate of the variance of \\(\\hat{ATE}^{lasso}\\) using the formula in Blonarz et al.¬†(2015).\n\n\n\nThe authors also propose the lasso+OLS estimator which first uses \\(L1\\) regularization as above to select the covariates and then plugs those in OLS to get the treatment effect estimate.\nA similar idea has also been studied by Wager et al (2016). They show that when additionally, assuming Gaussian data (along with a bunch of regularity assumptions), we can use any ‚Äúrisk consistent‚Äù ML estimator such as ridge, elastic net, etc. ‚ÄúRisk consistent‚Äù here means as we give the algorithm more data, it gets closer to the truth. The lower the risk the higher the variance reduction gains compared to the simple difference-in-means estimator. The authors also propose a simple cross-fitting approach to calculate confidence intervals.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nSplit the data into \\(k\\) equal sized folds.\nFor each fold \\(k\\):\n\n\ncalculate \\(\\bar{Y}^k, \\tilde{X}^k\\).\nget the coefficients \\(\\hat{\\beta}_{lasso}^{-k}\\) based on regressions on all other \\(k-1\\) folds.\ncombine both quantities and calculate \\(\\hat{ATE}^{lasso}\\).\ncalculate its standard error.\n\n\nGet the final estimates \\(\\hat{ATE}^{lasso}\\) and its standard error by taking weighted averages across all \\(k\\) folds.\n\n\n\nThis concludes the discussion of using a ML-type linear regression model to reduce the variance in A/B tests. Let‚Äôs now move on to the second method.\n\n\nUsing ML Regression Indirectly\nAn alternative approach first uses ML to predict \\(Y\\) and then plugs that prediction into an OLS regression of the outcome on the treatment variable. One can then use cross-fitting to do the prediction which ensures the ‚Äúna√Øve‚Äù OLS confidence intervals remain valid. The authors call this procedure MLRATE (machine learning regression-adjusted treatment effect estimator).\nHere is a rough version of the algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nSplit the data in \\(k\\) equal-sized folds.\nFor each fold \\(k\\):\n\n\nPredict \\(Y\\) by applying a ML algorithm to all other \\(k-1\\) folds. Call this prediction \\(\\bar{Y}_k\\).\n\n\nGet a final prediction \\(\\bar{Y}=\\sum_k\\bar{Y}_k\\).\nRun OLS of \\(Y\\) on \\(T\\), \\(\\bar{Y}_k\\) and \\((\\bar{Y}_k-\\bar{Y}) \\times T\\) and use the associated standard errors and \\(p\\)-values."
  },
  {
    "objectID": "blog/lord-paradox.html#a-closer-look",
    "href": "blog/lord-paradox.html#a-closer-look",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nMean Differences Over Time\nTo explore Lord‚Äôs paradox, consider the following scenario: Suppose we have two groups of individuals‚Äî\\(A\\) and \\(B\\)‚Äîwith their body weights measured at two time points: before and after a specific intervention. Let the weight at the initial time point be denoted as \\(W_{\\text{pre}}\\), and the weight at the final time point be \\(W_{\\text{post}}\\). We are interested in whether the intervention caused a change in weight between the two groups.\nOne approach to analyze this is to compute the (unadjusted) mean weight change for each group over time:\n\\[\\Delta = \\Delta^A - \\Delta^B.\\]\nIf this quantity is statistically significant, we might conclude that the intervention had a differential effect.\n\n\nControlling for Baseline Characteristics\nAn alternative approach involves adjusting for baseline weight \\(W_{\\text{pre}}\\) using, for example, a regression model:\n\\[W_{\\text{post}}=\\beta_1 + \\beta_2 G_A + \\beta_3 W_{\\text{pre}} + \\epsilon,\\]\nwhere \\(G\\) is a binary indicator for group \\(A\\) membership and \\(\\epsilon\\) is an error term. Here, \\(\\beta_2\\) captures the group difference in \\(W_{\\text{post}}\\), linearly controlling for baseline body weight.\nSurprisingly, these two approaches can yield conflicting results. For example, the former method might suggest no difference, while the regression adjustment indicates a significant group effect.\n\n\nExplanation\nThis contradiction arises because the two methods implicitly address different causal questions.\n\nMethod 1 asks: ‚ÄúDo Groups \\(A\\) and \\(B\\) gain/lose different amounts of weight?‚Äù\nMethod 2 asks: ‚ÄúGiven the same initial weight, does any of the groups end up at different final weights?‚Äù The regression approach adjusts for baseline differences, assuming \\(W_{\\text{pre}}\\) is a confounder.\n\nThe key insight is that the choice of analysis reflects underlying assumptions about the causal structure of the data. If \\(W_{\\text{pre}}\\) is affected by group membership (e.g., due to selection bias), then adjusting for it may introduce bias rather than remove it. However, in practice we most often should control for baseline characteristics as they are critical in balancing the treatment and control groups.\n\n\nThe Simpson‚Äôs Paradox Once Again\nI recently illustrated the more commonly discussed Simpson‚Äôs paradox. Interestingly, a 2008 paper claims that two phenomena are closely related, with the Lord‚Äôs paradox being a ‚Äúcontinuous version‚Äù of Simpson‚Äôs paradox."
  },
  {
    "objectID": "blog/mutual-information.html#a-closer-look",
    "href": "blog/mutual-information.html#a-closer-look",
    "title": "Mutual Information: What, Why, How, and When",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Entropy\nMutual Information is related to the notion of entropy, a measure of uncertainty or randomness in a random variable. In less formal terms, entropy quantifies how ‚Äúsurprising‚Äù or ‚Äúunpredictable‚Äù the outcomes of \\(X\\) are.\nFormally, for a discrete random variable \\(X\\) with possible outcomes \\(x_1, x_2, \\dots, x_n\\) and associated probabilities \\(p(x_1), p(x_2), \\dots, p(x_n)\\), entropy \\(H(X)\\) is defined as:\n\\[H(X) = -\\sum_{i=1}^n p(x_i) \\log p(x_i).\\]\nFor continuous variables we switch the summation with an integral.\nEntropy equals \\(0\\) when there‚Äôs no uncertainty, such as when \\(X\\) always takes a single outcome. High entropy means greater uncertainty (many possible outcomes, all equally likely), while low entropy indicates less uncertainty, as some outcomes are much more likely than others. In essence, entropy tells us how much ‚Äúinformation‚Äù is gained on average when observing the variable‚Äôs realization.\n\n\nMathematical Definitions of MI\nMI can be defined in mulitple ways. Perhaps the most intiuitive definition of MI between two random variables \\(X\\) and \\(Y\\) is that it measures the difference between the joint distribution and the product of their marginals. If two random variables are independent, their joint distribution is the product of their marginals \\(p(x,y)=p(x)p(y)\\). In some sense, the discrepancy between these two objects (i.e., the two sides of the equality) measures the strength of association between \\(X\\) and \\(Y\\) (i.e., their ‚Äúindependence‚Äù).\nFormally, MI is the Kullback-Leibler divergence between the joint distribution and the product of the two marginals:\n\\[ MI(X,Y) = D_{KL}(p(X,Y) || p(X)p(Y). \\]\nLet‚Äôs now examine MI from a different angle. We can express mutual information as follows:\n\\[ MI(X,Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\left(\\frac{p(x,y)}{p(x)p(y)}\\right)\\]\nAgain, for continuous variables, the sums become integrals.\nAlternatively, MI can also be expressed in terms of entropy. It equals the sum of the entropy of \\(X\\) and \\(Y\\) taking away their joint entropy:\n\\[ \\begin{align*}MI(X,Y) & = H(X) - H(X|Y) \\\\ & = H(Y) - H(Y|X) \\\\ & = H(X) + H(Y) - H(X,Y).\\end{align*} \\]\nYou can compute MI in R with the infotheo package.\nSoftware Package: infotheo.\n\n\nProperties\nMI has several intriguing properties:\n\nNon-negativity: \\(MI(X;Y) \\geq 0\\). Mutual Information is always non-negative, as it measures the amount of information one variable provides about the other. Higher values correspond to stronger association.\nSymmetry: \\(MI(X,Y) = MI(Y,X)\\). This symmetry implies that the information \\(X\\) provides about \\(Y\\) is the same as what Y provides about X.\nIndependence: Similarly to Chatterjee‚Äôs correlation coefficient, \\(MI(X,Y) = 0\\) if and only if \\(X\\) and \\(Y\\) are independent.\nScale-invarance: Mutual information is scale invariant. If you apply a scaling transformation to the variables, their MI will not be affected.\n\n\n\nConditional MI\nConditional Mutual Information (CMI) extends the concept of MI to measure dependency between \\(X\\) and \\(Y\\) given a third variable \\(Z\\). CMI is useful for investigating how much information \\(X\\) and \\(Y\\) share independently of \\(Z\\). It is defined as:\n\\[MI(X,Y|Z) = \\sum_{x,y,z} p(x,y,z) \\log \\left(\\frac{p(x,y|z)}{p(x|z)p(y|z)}\\right). \\]\nThis can be valuable in causal inference, where understanding dependencies conditioned on specific variables aids in interpreting relationships within complex models. CMI is also particularly useful for feature selection when accounting for redundancy among already included features. Let‚Äôs explore this idea in greater detail.\n\n\nFeature Selection with MI\nIn machine learning, MI serves as a useful metric for feature selection (Brown et al.¬†2012, Vergara and Est√©vez 2014). Consider an outcome variable \\(Y\\) and a set of features \\(X\\in\\mathbb{R}^p\\) with n i.i.d. observations. By evaluating the MI between each feature and the target variable, one can retain features with the highest information content, passing a certain threshold. This is somewhat primitive since it assumes independence across features. More sophisticated approaches take feature dependency into acount.\nFor instance, methods like Minimum Redundancy Maximum Relevance (mRMR, Peng et al.¬†2005) aim to maximize the relevance of features to the target while minimizing redundancy among features. Here is a concise version of the mRMR algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nCalculate MI between each feature and the target (relevance).\nCalculate MI between features (redundancy).\nSelect features that maximize relevance while minimizing redundancy: \\[ \\text{mRMR} = \\max_{X_i} \\left[MI(X_i;Y) - \\frac{1}{|S|} \\sum_{X_j \\in S} MI(X_i;X_j)\\right], \\]\n\nwhere \\(S\\) is the set of already selected features.\n\n\n\n\nPros and Cons\nLike any other statistical tool, mutual Information has several advantages and limitations. On the positive side, it captures both linear and nonlinear relationships, is scale-invariant, and works with both continuous and discrete variables, making it a theoretically well-founded measure. However, it requires density estimation for continuous variables, can be computationally intensive for large datasets, and its results can be sensitive to binning choices for continuous variables. Additionally, there is no standard normalization for MI."
  },
  {
    "objectID": "blog/hypo-testing-linear-ml.html#a-closer-look",
    "href": "blog/hypo-testing-linear-ml.html#a-closer-look",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nTwo Types of Models and Parameters\nWe first need to discuss an important subtlety. There are two distinct ways of thinking about performing hypothesis testing in ML models. The traditional view is that we have a true linear model which includes all variables:\n\\[\\begin{equation} Y=X\\beta_0+\\epsilon. \\end{equation}\\]\nWe are interested in testing whether \\(\\beta_0=0\\) ‚Äì that is, inference on the full model. This is certainly an intuitive target. The interpretation is that this model encapsulates all relevant causal variables for \\(Y\\). Importantly, even if a given variable $X_j $ is not selected, it still belongs to the model and has a meaningful interpretation.\nThere is an alternative way to think about the problem. Imagine we run a variable selection algorithm (e.g., lasso), which selects a subset \\(M=\\{1,\\dots,p\\}\\) of all available predictors (\\(X\\)), \\(M&lt;p\\) leading to the alternative model:\n\\[\\begin{equation} Y=X_M\\beta_M+u. \\end{equation}\\]\nNow we are interested in testing whether \\(\\beta_M=0\\) ‚Äì that is, inference on the selected model. Unlike the scenario above, here \\(\\beta_{Mj}\\) is interpreted as the change in \\(Y\\) for a unit change in \\(X_j\\) when all other variables in \\(X_M\\) (as opposed to all of \\(X\\)) are kept constant.\nWhich of the two targets is more intuitive?\nStatisticians argue vehemently about this. Some claim that the full model interpretation is inherently problematic. It is too na√Øve and perhaps even arrogant to think that (i) mother nature can be explained by a linear equation, (ii) we can measure and include the full set of relevant predictors. On top of this, there are also technical issues with this interpretation beyond the scope of this post.\nTo overcome these challenges, relatively recently, statisticians developed the idea of inference on the selected model. This introduces major technical challenges, however.\n\n\nThe Na√Øve Approach: What Not to Do\nFirst things first ‚Äì here is what we should not do.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRun a Lasso regression.\nRun OLS regression on the subset of selected variables.\nPerform statistical inference with the estimated t-stats, confidence intervals, and p-values.\n\n\n\nThis is bad practice. Can you see why?\nIt is simply because we ignored the fact that we already peeked at the data when we ran the Lasso regression. Lasso already chose the variables that are strongly correlated with the outcome. Intuitively, we will need to inflate the \\(p\\)-values to account for the data exploration in the first step.\nIt turns out that, in general, both the finite- and large-sample distributions of these parameters are non-Gaussian and depend on unknown parameters in weird ways. Consequently, the calculated t-stats and p-values are all wrong, and there is little hope that anything simple can be done to save this approach. And no, the standard bootstrap cannot help us either.\nBut are there special cases when this approach might work? A recent paper titled ‚ÄúIn Defense of the Indefensible: A Very Na√Øve Approach to High-Dimensional Inference‚Äù argues that under very strict assumptions on \\(X\\) and \\(\\lambda\\), this method is actually kosher. The reason it works is that in the magical world of those assumptions, the set of variables that Lasso chooses is deterministic, and not random (hence circumventing the issue described above). The resulting estimator is unbiased and asymptotically normal ‚Äì hence hypothesis testing is trivial.\nHere is what we should do instead.\n\n\nThe Classical Approach: Inference on the Full Model\nRoughly speaking, there are at least four ways we can go about doing hypothesis testing for \\(\\beta\\) in equation (1).\n\nData Split\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nSplit our data into two equal parts.\nRun a Lasso regression on the first part.\nRun OLS on the second part with the selected variables from Step 2.\nPerform inference using the computed \\(t\\)-stats, \\(p\\)-values, and confidence intervals.\n\n\n\nThis is simple and intuitive. The problem is that in small samples, the \\(p\\)-values can be quite sensitive to how we split the data in the first step. This is clearly undesirable, as we will be getting different results every time we run this algorithm for no apparent reason.\n\n\nMulti Split\nThis is a modification of the Data Split approach designed to solve the sensitivity issue and increase power.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRepeat \\(B\\) times:\n\n\nReshuffle data.\nRun the Data Split method.\nSave the p-values.\n\n\nAggregate the B \\(p\\)-values into a single final one for each variable.\n\n\n\nInstead of splitting the data into two parts only once, we can do it many times, and each time, we get a \\(p\\)-value for every variable. The aggregation goes a long way to solving the instability of the simple data split approach. There is a lot of clever mathematics behind it. For example, there is a complicated expression for aggregating the \\(p\\)-values rather than taking a simple average.\nSoftware Package: hdi.\n\n\nBias Correction\nThis approach tackles the problem from a very different angle. The idea is to directly remove the bias from the na√Øve Lasso procedure without any subsampling or data splitting. Somewhat magically, the resulting estimator is unbiased and asymptotically normally distributed ‚Äì statistical inference is then straightforward.\nThere are multiple versions of this idea, but the general form of these estimators is:\n\\[\\hat{\\beta}^{\\text{bias cor}} = \\hat{\\beta}^{lasso} + \\hat{\\Theta} X'\\epsilon^{lasso},\\]\nWhere \\(\\hat{\\beta}^{lasso}\\) is the lasso estimator and \\(\\epsilon^{lasso}\\) are the residuals. The missing piece is the \\(\\hat{\\Theta}\\) matrix; there are several ways to estimate it depending on the setting. In its simplest form, \\(\\hat{\\Theta}\\) is the inverse of the sample variance-covariance matrix. Other examples include the matrix, which minimizes an error term related to the bias as well as the variance of its Gaussian component. Similar bias-correction methods have been developed for Ridge regression as well.\nSoftware Package: hdi.\n\n\nBootstrap\nAs in many other complicated settings for statistical inference, the bootstrap can come to the rescue. Still, the plain vanilla bootstrap will not do. Instead, here is the general idea of the leading version of the bootstrap estimator for Lasso:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRun a Lasso regression.\nKeep only \\(\\beta^{lasso}\\)‚Äôs larger than some magical threshold.\nCompute the associated residuals and center them around \\(0\\).\nRepeat B times:\n\n\ndraw random samples of these centered residuals,\ncompute new responses \\(\\dot{Y}\\) by adding them to the predictions \\(X'\\beta^{lasso}\\), and\nobtain \\(\\beta^{lasso}\\) coefficients from Lasso regressions on these new responses \\(\\dot{Y}\\).\n\n\nUse the distribution of the obtained coefficients to conduct statistical inference.\n\n\n\nThis idea can be generalized to other settings and, for instance, be combined with the bias-corrected estimator.\nThis wraps up our discussion of methods for performing hypothesis testing on equation (1) (i.e., the full model). We now move on to a more challenging topic ‚Äì inference on equation (2) (i.e., the selected model).\n\n\n\nThe Novel Approach: Inference on the Selected Model\n\nPoSI (Post Selection Inference)\nThe goal of the PoSI method is to construct confidence intervals that are valid regardless of the variable selection method and the selected submodel. The benefit is that we would be reaching the correct conclusion even if we did not select the true model. This luxury comes at the expense of often being too conservative (i.e., confidence intervals are ‚Äútoo wide‚Äù). Let me explain how this is done.\nTo take a step back, most confidence intervals in statistics take the form:\n\\[\\hat{\\beta} \\pm m \\times \\hat{SE}(\\hat{\\beta}).\\]\nEvery data scientist has seen a similar formula before. The question is usually one about choosing the constant \\(m\\). When we work with two-sided hypotheses tests and large samples, we often use \\(m = 1.96\\) because this is roughly the \\(97.5\\)th percentile of the \\(t\\)-distribution with many degrees of freedom. This gives a \\(2.5\\%\\) false positive error on both tails of the distribution (\\(5\\%\\) in total) and hence the associated 95% confidence intervals. The larger m, the wider or more conservative the confidence interval.\nThere are a few ways to choose the constant m in the PoSI world. Vaguely speaking, PoSI says we should select this constant to equal the \\(97.5\\)th percentile of a distribution related to the largest \\(t\\)-statistic among all possible models. This is usually approximated with Monte Carlo simulations. Interestingly, we do not need the response variable to approximate the value.\n\\[m = \\max_{\\text{ \\{all models and vars\\} }} |t|.\\]\nAnother and even more conservative choice for \\(m\\) is the Scheffe constant.\n\\[m^{scheffe} = \\sqrt{rank(X) \\times F(rank(X),  n-p)},\\]\nwhere \\(F(\\dot)\\) denotes the \\(95\\)th percentile of the \\(F\\) distribution with the respective degrees of freedom.\nUnfortunately, you guessed correctly that this method does not scale well. In some sense, it is a ‚Äúbrute force‚Äù method that scans through all possible model combinations and all variables within each model and picks the most conservative value. The authors recommend this procedure for datasets with roughly \\(p&lt;20\\). This rules out many practical applications where machine learning is most useful.\nSoftware Package: PoSI\n\n\nEPoSI (Exact PoSI)\nOk, the name of this approach is not super original. The ‚ÄúE‚Äù here stands for ‚Äúexact.‚Äù Unlike its cousin, this approach is valid only in the selected submodel. Because we cover fewer scenarios, the intervals will generally be narrower than the PoSI ones. EPoSI produces valid finite sample (as opposed to asymptotic) confidence intervals and \\(p\\)-values. Like all methods described here, the math behind this is extremely technical. So, I will give you only a high-level description of how this works.\nThe idea is first to get the conditional distribution of \\(\\beta\\) given the selected model. A bit magically, it turns out it is a truncated normal distribution. Really, who would have guessed this? Do you even remember truncated probability distributions (hint: they are just like regular PDFs but bounded from at least one side. This requires further scaling so that the density area sums to 1.)?\nTo dig one layer deeper, the authors show that the selection of Lasso predictors can be recast as a ‚Äúpolyhedral region‚Äù of the form:\n\\[ AY\\leq b. \\]\nIn English, for fixed \\(X\\) and \\(\\lambda\\), the set of alternative outcome values \\(Y^*\\) which yields the same set of selected predictors, can be expressed by the simple inequality above. In it, \\(A\\) and \\(b\\) depend do not depend on \\(Y\\). Under this new result, the distribution of \\(\\hat{\\beta}_M^{\\text{EPoSI}}\\) is now well-understood and tractable, thus enabling valid hypothesis testing.\nThen, we can use the conditional distribution function to construct a whimsical test statistic that is uniformly distributed on the \\([0,1]\\) interval. And we can finally build confidence intervals based on that statistic.\nSelective inference is currently among the hottest topic in all of statistics. There have been a myriad of extensions and improvements on the original paper. Still, this literature is painstakingly technical. What is the polyhedral selection property or a union of polytopes?\nSoftware Package: selectiveInference."
  },
  {
    "objectID": "blog/stein-paradox.html#a-closer-look",
    "href": "blog/stein-paradox.html#a-closer-look",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Mean Squared Error (MSE)\nTo understand the paradox, let‚Äôs begin by quantify what we mean by ‚Äúbetter‚Äù estimation. The MSE of an estimator \\(\\hat{\\mu}\\) is defined as its expected squared error (or loss):\n\\[\\text{MSE}(\\hat{\\mu}) = \\mathbb{E}\\left[ ( \\hat{\\mu} - \\mu )^2 \\right],\\]\nwhere \\(\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_p)\\) is the true vector of means. All else equal, the lower MSE the better.\n\n\nMathematical Formulation\nStein‚Äôs paradox arises in the context of estimating multiple parameters simultaneously. Suppose you‚Äôre estimating the mean \\(\\mu_i\\) of several independent normal distributions:\n\\[X_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, p,\\]\nwhere \\(X_i\\) are observed values, \\(\\mu_i\\) are the unknown means, and \\(\\sigma^2\\) is known. We assume non-zero covariance between the variables.\nA natural approach is to estimate each _i using its corresponding sample mean \\(X_i\\), which is the maximum likelihood estimator (MLE). However, in dimensions \\(p \\geq 3\\), this seemingly reasonable approach is dominated by an alternative method.\nThe surprise? Shrinking the individual estimates toward a common value‚Äîsuch as the overall mean‚Äîproduces an estimator with uniformly lower expected squared error.\nThe MSE of the MLE is equal to:\n\\[R(\\hat{\\mu}_\\text{MLE}) = p\\sigma^2.\\]\nNow consider the biased(!) James-Stein (JS) estimator:\n\\[\\hat{\\mu}_\\text{JS} = \\left( 1 - \\frac{(p - 2)\\sigma^2}{\\lVert X \\rVert^2} \\right) X,\\]\nwhere \\(\\lVert X \\rVert^2 = \\sum_i X_i^2\\) is the squared norm of the observed data. The shrinkage factor \\(1 - \\frac{(p - 2)\\sigma^2}{|X|^2}\\) pulls the estimates toward zero (or any other pre-specified point).\nRemarkably, the James-Stein estimator has lower MSE than the MLE for \\(p \\geq 3\\):\n\\[\\text{MSE}(\\hat{\\mu}_{\\text{JS}}) &lt; \\text{MSE}(\\hat{\\mu}_{\\text{MLE}}).\\]\nThis is the mystery at its core.\n\n\nExplanation\nThis result holds because the James-Stein estimator balances variance reduction and bias introduction in a way that minimizes overall MSE. The MLE, in contrast, does not account for the possibility of shared structure among the parameters. The counterintuitive nature of this result stems from the independence of the \\(X_i\\)‚Äôs. Intuition suggests that pooling information across independent observations should not improve estimation, yet the James-Stein estimator demonstrates otherwise."
  },
  {
    "objectID": "blog/binscatter.html#a-closer-look",
    "href": "blog/binscatter.html#a-closer-look",
    "title": "Binscatter: A New Visual Tool for Data Analysis",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFormal Definition\nA binscatter plot is constructed by partitioning the range of the independent variable \\(X\\) into a fixed number of \\(K\\) bins, \\(B_1,\\dots,B_K\\) typically using empirical quantiles. This ensures each bin is of roughly the same size. Within each bin, the average value of the dependent variable \\(Y\\) is calculated. These averages are then plotted against the midpoint of each bin, \\(\\bar{X}\\), resulting in a series of points that represent an estimate of conditional mean of \\(Y\\) given \\(X\\), \\(E[Y\\mid X]\\).\nIn technical jargon binscatter provides a nonparametric estimate of the conditional mean function, offering a visual summary of the relationship between the two variables. The resulting graph allows assessment of linearity, monotonicity, convexity, etc.\n\n\nThe Algorithm\nHere is the step-by-step recipe for construcing a binscatter plot.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nBin construction: Divide the range of \\(X\\) into \\(K\\) equal-width bins, or use quantile-based bins for equal sample sizes within bins. For example, with \\(K=10\\), the observations in \\(B_1\\) would be those between the minimimum value of \\(X\\) and that of its tenth percentile.\nMean calculation: Compute the mean of \\(Y\\) within each bin:\n\n\\[\\bar{Y}_k= \\frac{1}{|B_k|} \\sum_{i \\in B_k} Y_i,\\]\nwhere \\(|B_k|\\) is the number of observations in bin \\(B_k\\)‚Äã.\n\nPlotting: Plot \\(\\bar{Y}_k\\) against the midpoints of each bin, \\(\\bar{X}_k\\).\n\n\n\nSoftware Package: binsreg.\nQuite simple, right? Let‚Äôs explore certain useful extensions of this idea.\n\n\nAdjusting for Covariates: The Wrong Way\nIn many applications, it is essential to control for additional covariates \\(W\\) to isolate the relationship between the primary variables of interest. The object of interest then becomes the conditional mean \\(E[Y\\mid W,X]\\). An example would be focusing on the relationship between income (\\(Y\\)) and education level (\\(X\\)) when controling for parental education (\\(W\\)).\nA common but flawed approach to incorporating covariates in binscatter is residualized binscatter. This method involves first regressing separately both \\(Y\\) and \\(X\\) on the covariates \\(W\\) to obtain residuals \\(\\hat{u}_Y\\)‚Äã and \\(\\hat{u}_X\\)‚Äã, and then applying the binscatter method to these residuals:\n\\[\\bar{\\hat{u}}_{Y,k} = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\hat{u}_{X,i}.\\]\nWhile this approach is motivated by the Frisch-Waugh-Lovell theorem in linear regression, it can lead to incorrect conclusions in more general settings. The residualized binscatter may not accurately reflect the true conditional mean function, especially if the underlying relationship is nonlinear. Therefore, it is generally not recommended for empirical work.\n\n\nAdjusting for Covariates: The Right Way\nInstead, this should be done using a semi-parametric partially linear regression model. This is achieved by modeling the conditional mean function as\n\\[Y = \\mu_0(X) + W \\gamma_0 + \\varepsilon,\\]\nwhere \\(\\mu_0(X)\\) captures the main effect of \\(X\\), and \\(W' \\gamma_0\\) adjusts for the influence of additional covariates. Rather than residualizing, we estimate \\(\\mu_0(X)\\) using the least-squares approach:\n\\[(\\hat{\\beta}, \\hat{\\gamma}) = \\arg\\min_{\\beta, \\gamma} \\sum (Y- b(X)' \\beta - W' \\gamma)^2,\\]\nwhere \\(b(X)\\) represents the binning basis functions. The final binscatter plot displays the estimated conditional mean function\n\\[\\hat{\\mu}(X_k) = b(X_k)' \\hat{\\beta}\\]\nagainst \\(\\bar{X}_k\\), ensuring a correct visualization of the relationship between \\(X\\) and \\(Y\\) after accounting for the covariates \\(W\\).\n\n\nPractical Considerations\nA key decision is the choice of the number of bins \\(K\\). Too few bins can oversmooth the data, masking important features, while too many bins can lead to undersmoothing, resulting in a noisy and less interpretable plot. An optimal choice of \\(K\\) balances bias and variance, often determined using data-driven methods. To address this, Cattaneo et al.¬†(2024) propose an adaptive, Integrated Mean Squared Error (IMSE)-optimal choice of \\(K\\) for which get a plug-in formula.\nThoughtful data scientist always have variance in their mind. If, for instance, we see some linear relationship between \\(Y\\) and \\(X\\), how can we determine whether it is statistically significant? Quantifying the uncertainty around binscatter estimates is crucial. The authors also discuss constructing confidence bands, which can be added to the plot to visually represent estimation uncertainty, enhancing both interpretability and reliability."
  },
  {
    "objectID": "blog/correlation-transitive.html#a-closer-look",
    "href": "blog/correlation-transitive.html#a-closer-look",
    "title": "Correlation is Not (Always) Transitive",
    "section": "A Closer Look",
    "text": "A Closer Look\nLet‚Äôs denote the respective correlations between \\(X\\), \\(Y\\) and \\(Z\\) by \\(cor(X,Y)\\), \\(cor(X,Z)\\), and \\(cor(Y,Z)\\). For simplicity (and without loss of generality), let‚Äôs work with standardized versions of these variables ‚Äì that is, means of \\(0\\) and variances of \\(1\\). This implies, \\(cov(X,Y) = cor(X,Y)\\) for any pair.\nWe can write the linear projections of \\(X\\) and \\(Z\\) on \\(Y\\) as follows:\n\\[ X = cor(X,Y)Y + \\epsilon^{X,Y}, \\]\n\\[ Z = cor(Z,Y)Y + \\epsilon^{Z,Y}. \\]\nThen, we have:\n\\[ cor(X,Z)=cor(X,Y)cor(Z,Y)+cor(\\epsilon^{X,Y},\\epsilon^{Z,Y}).\\]\nWe can use the Cauchy-Schwarz inequality to bound the last term, which gives the final range of possible values for cor(X,Z):\n\\[cor(X,Y)cor(Z,Y) - \\sqrt{(1-cor(X,Y)^2) (1-cor(Z,Y)^2)}\\]\n\\[\\leq cor(X,Z) \\leq  \\]\n\\[cor(X,Y)cor(Z,Y) + \\sqrt{(1-cor(X,Y)^2) (1-cor(Z,Y)^2)}\\]\nFor instance, if we set \\(cor(X,Y)=cor(Z,Y)=0.6\\), then we get:\n\\[-.28 \\leq cor(X,Z) \\leq 1.\\]\nThat is, \\(cor(X,Z)\\) can be negative.\n\nAn (Extremely Simple) Example\nPerhaps the simplest example to illustrate this is:\n\n\\(X\\) and \\(Z\\) are independent random variables,\n\\(Y=X+Z\\). The result follows.\n\nThe following code sets up this example in R and python.\n\nRPython\n\n\nrm(list=ls())\nset.seed(68493)\n\nx &lt;- runif(n=1000)\nz &lt;- runif(n=1000)\ny &lt;- x + z\n\ncor(y, x)\ncor(y, z)\ncor(z, x)\n\ncor.test(y, x, alternative='two.sided', method='pearson')\ncor.test(y, z, alternative='two.sided', method='pearson')\ncor.test(z, x, alternative='two.sided', method='pearson')\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr\n\n# Set seed for reproducibility\nnp.random.seed(68493)\n\n# Generate random variables\nx = np.random.uniform(size=1000)\nz = np.random.uniform(size=1000)\ny = x + z\n\n# Compute correlations\nprint(\"cor(y, x):\", np.corrcoef(y, x)[0, 1])\nprint(\"cor(y, z):\", np.corrcoef(y, z)[0, 1])\nprint(\"cor(z, x):\", np.corrcoef(z, x)[0, 1])\n\n# Perform correlation tests\nprint(\"cor.test(y, x):\", pearsonr(y, x))\nprint(\"cor.test(y, z):\", pearsonr(y, z))\nprint(\"cor.test(z, x):\", pearsonr(z, x))\n\n\n\nBelow is a table with correlation coefficients and \\(p\\)-values associated with the null hypotheses that they are equal to zero.\n\n\n\n\nvars\ncor. coef.\n\\(p\\)-value\n\n\n\n\n\\(cor(X,Y)\\)\n0.68\n0.00\n\n\n\\(cor(Z,Y)\\)\n0.70\n0.00\n\n\n\\(cor(X,Z)\\)\n-0.05\n0.15"
  },
  {
    "objectID": "blog/chatterjee-correlation.html#a-closer-look",
    "href": "blog/chatterjee-correlation.html#a-closer-look",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLinear Relationships\nWhen we simply say ‚Äúcorrelation‚Äù we refer to the so-called Pearson correlation coefficient. Virtually everyone working with data is familiar with it. Given a random sample \\((X_1,Y_1),\\dots,(X_n, Y_n)\\) of two random variables \\(X\\) and \\(Y\\) it is computed by:\n\\[corr^{P}(X,Y) = \\frac{ \\sum_i(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}},\\]\nwhere an upper bar denotes a sample mean.\nThis coefficient lives in the \\([-1,1]\\) interval. Larger values indicate stronger relationship between \\(X\\) and \\(Y\\), be it positive or negative. At the extreme, the Pearson coefficient will equal \\(1\\) when all observations can be perfectly lined up on a upward sloping line. Yes, you guessed it ‚Äì when it equals \\(-1\\) the line is sloping down.\nYou can easily calculate the Pearson correlation in R and python:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'pearson', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr\n\npearson_corr, pearson_pval = pearsonr(x, y)\n\n\n\nThis measure, while widely popular, suffers from a few shortcomings. First, outliers have an outsized impact in skewing its value. Sample means vulnerable to outliers are a key ingredient in the calculation, rendering the measure sensitive to data anomalies. Second, it is designed to detect only linear relationships. Two variables might have a strong but non-linear relationship which this measure will not detect. Lastly, it is not transformation-invariant, meaning that applying a monotone transformation to either of the variables will change the correlation value.\nLet‚Äôs discuss some improvements to the Pearson correlation measure. Enter Spearman correlation.\n\n\nMonotone Relationships\nSpearman correlation is the Pearson correlation among the ranks of \\(X\\) and \\(Y\\):\n\\[corr^{S}(X,Y) = corr^{P}(R(X),R(Y)),\\]\nwhere \\(R(\\cdot)\\) denotes an observation‚Äôs rank (or order) in the sample.\nSpearman correlation is thus a rank correlation measure. As such, it quantifies how well the relationship between \\(X\\) and \\(Y\\) can be described using a monotone (and not necessarily a linear) function. It is therefore a more flexible measure of association. Again, intuitively, Spearman correlation will take on a large positive value when the \\(X\\) and \\(Y\\) observations have similar ranks. This value will be negative when the ranks tend to go in opposite directions.\nSpearman correlation addresses some of the shortcomings associated with Pearson correlation. It is not easily influenced by outliers, and it does not change if we apply a monotone transformation of \\(X\\) and/or \\(Y\\). These benefits come at the expense of a loss in interpretation and potential issues when tied ranks are common, a scenario I ignore here.\nCalculating it is just as simple:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import spearmanr\n\nspearman_corr, spearman_pval = spearmanr(x, y)\n\n\n\nSpearman correlation is not the only rank correlation coefficient out there. –ê popular alternative is the Kendall rank coefficient which is computed slightly differently. Let‚Äôs define a pair of observations \\((X_i, Y_i)\\) and \\((X_j, Y_j)\\) to be agreeing (the technical term is concordant) if the differences \\((X_i - X_j)\\) and \\((Y_i - Y_j)\\) have the same sign (i.e., either both \\(X_i &gt; X_j\\) and $Y_i &gt; Y_j $or both \\(X_i &lt; X_j\\) and \\(Y_i &lt; Y_j\\) ).\nThen, Kendall‚Äôs coefficient is expressed as:\n\\[corr^{K} = \\frac{\\text{number of agreeing pairs} - \\text{number of disagreeing pairs}} {\\text{total number of pairs}}.\\]\nSo, it quantifies the degree of agreement between the ranks of \\(X\\) and \\(Y\\). Like the other coeffients described above, its range is \\([-1, 1]\\) and values away from zero indicate stronger relationship.\nAgain, this coefficient is similarly computed in R and python:\n\nRPython\n\n\ncor(x,y, method = 'kendall')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\nkendall_corr, kendall_pval = kendalltau(x, y)\n\n\n\nKendall‚Äôs measure improves on some of the shortcomings baked in the Spearman‚Äôs coefficients ‚Äì it has a clearer interpretation and it is less sensitive to rank ties.\nHowever, none of these rank correlation coefficients can detect non-monotonic relationships. For instance, \\(X\\) and \\(Y\\) can have a parabola- or wave-like pattern when plotted against each other. We would like a correlation measure flexible enough to capture such non-linear relationships.\nThis is where Chatterjee‚Äôs coefficient comes in.\n\n\nMore General Relationships\nChatterjee recently proposed a new correlation coefficient designed to detect non-monotonic relationships. He discovered a novel estimator of a population quantity first proposed by Dette et al.¬†(2013).\nLet‚Äôs start with the formula. The Chatterjee correlation coefficient is calculated as follows:\n\\[corr^{C}(X,Y) = 1-\\frac{3\\sum_{i=1}^{n-1}|R(Y_{k:R(X_k)=i+1}) - R(Y_{k:R(X_k)=i})|}{n^2-1},\\]\nwhere n is the sample size. This looks complicated, so let‚Äôs try to simplify the numerator. Let‚Äôs sort the data in an ascending order of \\(X\\) so that we have \\((X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})\\), where \\(X_{(1)}&lt;X_{(2)}&lt;\\dots &lt;X_{(n)}\\). Also, denote \\(R(Y_i)\\) be the rank of \\(Y_{(i)}\\). Then:\n\\[corr^{C}(X,Y) = 1 - \\frac{3\\sum_{i=1}^{n-1}|R(Y_{i+1})-R(Y_i)|}{n^2-1}.\\]\nSo, this new coefficient is a scaled version of the sum of the absolute differences in the consecutive ranks of \\(Y\\) when ordered by \\(X\\). It is perhaps best to think about Chatterjee‚Äôs method as a measure of dependence and not strictly a correlation coefficient.\nThere are some major difference comapred to the previous correlation measures. Chatterjee‚Äôs correlation coefficient lies in the \\([0,1]\\) interval. It is equal to zero if and only if \\(X\\) and \\(Y\\) are independent and to one if one of them is a function of the other. Unlike the coefficients descibed above, it is not symmetric in \\(X\\) and \\(Y\\), meaning \\(corr^{C}(X,Y) \\neq corr^{C}(Y,X)\\). This is understandable since we are interested in whether \\(X\\) is a function of \\(Y\\), which does not imply the opposite. The author also develops asymptotic theory for calculating p-values although some researchers have raised concerns about the coefficient‚Äôs power.\nHere is a sample code to calculate its value:\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\nn &lt;- 1000\nx &lt;- runif(n) \ny &lt;- 5 * sin(x) + rnorm(n)\n\ndata &lt;- data.frame(x=x, y=y)\ndata$R &lt;- rank(data$y)\ndata &lt;- data[order(data$x), ]\n\n1 - 3 * sum(abs(diff(data$R))) / (n^2-1)\n&gt;[1] 0.4093024\n\n\nimport numpy as np\n\nnp.random.seed(1988)\nn = 1000\nx = np.random.uniform(size=n)\ny = 5 * np.sin(x) + np.random.normal(size=n)\n\ndata = np.array(sorted(zip(x, y), key=lambda pair: pair[0]))\nranks = np.argsort(np.argsort(data[:, 1]))  # Rank of y\nchatterjee_corr = 1 - 3 * np.sum(np.abs(np.diff(ranks))) / (n**2 - 1)\nprint(f\"Chatterjee's correlation: {chatterjee_corr:.4f}\")\n&gt; Chatterjee's correlation: 0.4050\n\n\n\nSoftware Package: XICOR.\nThere you have it. You are now well-equipped to dive deeper into your datasets and find new exciting relationships."
  },
  {
    "objectID": "blog/gen-vars-predefined-corr.html#a-closer-look",
    "href": "blog/gen-vars-predefined-corr.html#a-closer-look",
    "title": "Generating Variables with Predefined Correlation",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Algorithm\nAssume we want to generate a vector Y with n observations and p variables with a target correlation matrix \\(\\Sigma\\). The algorithm to obtain \\(Y\\) is as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nStart with Independent Variables: Create a matrix \\(X\\) of dimensions \\(n \\times p\\), where each column is independently drawn from N(0,1): \\[ X = \\begin{bmatrix}x_{11} & x_{12} & \\cdots & x_{1p} \\\\x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\x_{n1} & x_{n2} & \\cdots & x_{np}.\\end{bmatrix} \\]\nDecompose the Target Matrix: Perform Cholesky decomposition on the target correlation matrix \\(\\Sigma\\) as: \\[\\Sigma = LL^T,\\] where \\(L\\) is a lower triangular matrix.\nTransform the Independent Variables: Multiply the independent variable matrix \\(X\\) by \\(L\\) to obtain the correlated variables: \\[Y = XL.\\]\n\n\n\nHere \\(Y\\) is an \\(n\\times p\\) matrix where the columns have the desired correlation structure defined by \\(\\Sigma\\). To ensure that \\(\\Sigma\\) is a valid correlation matrix, it must be positive-definite. This condition guarantees the success of Cholesky decomposition and the correctness of the resulting correlated variables.\n\n\nMathematical Explanation\nLet‚Äôs examine how and why this approach works. We know that \\(\\Sigma = LL^T\\) and \\(E(XX^T)=I\\) by definition. We want to show that \\(E(YY^T)=LL^T\\). Here is the simplest way to get there:\n\\[\\begin{align*}\nE(YY^T) &= E((LX)(LX)^T) \\\\\n        &= E(LXX^TL^T) \\\\\n        &= LE(XX^T)L^T \\\\\n        &= LL^T.\n\\end{align*}\\]\nThere you have it ‚Äì the algorithm outlined above is mathematically grounded. The covariance matrix of \\(Y\\) is indeed equal to \\(\\Sigma\\). Let‚Äôs now look at an example."
  },
  {
    "objectID": "blog/conformal-inference.html#a-closer-look",
    "href": "blog/conformal-inference.html#a-closer-look",
    "title": "A Brief Introduction to Conformal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Sample Quantiles\nI will start with reviewing sample quantiles. Given an i.i.d. sample, \\(U_1, \\dots, U_n\\), the (\\(1-\\alpha\\))th quantile is the value \\(\\hat{q}_{1-\\alpha}\\) such that approximately \\((1-\\alpha)\\times100\\%\\) of the data is smaller than it. For instance, the \\(95\\)th quantile (sometimes also called percentile) is the value for which \\(95\\%\\) of the observations are at least as small.\nSo, given a new observation \\(U_{n+1}\\), we know that:\n\\[P(U_{n+1}\\leq \\hat{q}_{1-\\alpha})\\geq 1-\\alpha.\\]\n\n\nThe Na√Øve Approach\nLet‚Äôs turn back to the regression example with Y and X. We are given a new observation \\(X_{n+1}\\) and our focus is on \\(Y_{n+1}\\). Following the fact described above, a na√Øve way to construct a confidence interval for \\(Y_{n+1}\\) is as follows:\n\\[CI^{\\text{na√Øve}}_{1-\\alpha}(X_{n+1}) = \\left[ \\hat{\\mu}(X_{n+1}) \\pm \\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha} \\right].\\]\nHere \\(\\mu(\\cdot)\\) is an estimate of the regression function \\(E[Y \\mid X]\\), and \\(\\hat{q}^{\\mid \\hat{u} \\mid}_{1-\\alpha}\\) is the \\((1-\\alpha)\\)th quantile of empirical distribution function of the fitted residuals \\(\\mid Y-\\hat{\\mu}(X) \\mid\\).\nPut simply, we can look at an interval around our best prediction for \\(Y_{n+1}\\) (i.e., \\(\\hat{\\mu}(X_{n+1})\\)) defined by the residuals estimated on the original data.\nIt turns out this interval is too narrow. In a series of papers Vladimir Vovk and co-authors show that the empirical distribution function of the fitted residuals is often biased downward and hence this interval is invalid. This is where conformal inference comes in.\n\n\nConformal Inference\nConsider the following strategy. For each \\(y\\) we fit a regression $_y $ on the sample \\((Y_1, X_1),\\dots (Y_n, X_n), (y, X_{n+1})\\). We calculate the residuals \\(R^y_i\\) for \\(i=1,\\dots,n\\) and \\(R^y_{n+1}\\) and count the proportion of \\(R^y_i\\)‚Äôs smaller than \\(R^y_{n+1}\\). Let‚Äôs call this number \\(\\sigma(y)\\). That is,\n\\[\\sigma(y) = \\frac{1}{n+1}\\sum_{i=1}^{n+1} I (R^y_i \\leq R^y_{n+1}),\\]\nwhere \\(I(\\cdot)\\) is the indicator function equal to one when the statement in the parenthesis is true and 0 if when it is not.\nThe test statistic \\(\\sigma({Y_{n+1}})\\) is uniformly distributed over the set \\(\\{ \\frac{1}{n+1}, \\frac{2}{n+1},\\dots, 1\\}\\), implying we can use \\(1-\\sigma({Y_{n+1}})\\) as a valid p-value for testing the null that \\(Y_{n+1}=y.\\) Then, using the sample quantiles logic outlined above we arrive at the following confidence interval for \\(Y_{n+1}\\):\n\\[ CI^{\\text{conformal}}_{1-\\alpha}(X_{n+1}) \\approx \\{ y\\in \\mathbb{R} : \\sigma(y)\\leq 1-\\alpha \\}.\\]\nThis is summarized in the following procedure:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nFor each value \\(y\\):\n\n\nFit the regression function \\(\\mu(\\cdot)\\) on \\((X_1, Y_1), \\dots, (X_n, Y_n), (X_{n+1}, y)\\) using your favorite estimator/learner.\nCalculate the \\(n+1\\) residuals.\nCalculate the proportion \\(\\sigma(y)\\).\n\n\nConstruct \\(CI = \\{y: \\sigma(y) \\leq (1-\\alpha)\\}\\).\n\n\n\nSoftware Package: conformalInference\nTwo notes. First, conformal inference guarantees unconditional coverage. This is conceptually different and should not be confused with the conditional statement \\(P(Y_{n+1}\\in CI(x) \\mid X_{n+1}=x)\\geq 1-\\alpha\\). The latter is stronger and more difficult to assert, requiring additional assumptions such as consistency of our estimator of \\(\\mu(\\cdot)\\).\nSecond, this procedure can be computationally expensive. For a given value \\(X_{n+1}\\) we need to fit a regression model and compute residuals for every \\(y\\) which we consider including in the confidence interval. This is where split conformal inference comes in.\n\n\nSplit Conformal Inference\nSplit conformal inference is a modification of the original algorithm that requires significantly less computation power. The idea is to split the fitting and ranking steps, so that the former is done only once. Here is the algorithm.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRandomly split the data in two equal-sized bins.\nGet \\(\\hat{\\mu}\\) on the first bin.\nCalculate the residuals for each observation in the second bin.\nLet \\(d\\) be the \\(s\\)-th smallest residual, where \\(s=(\\frac{n}{2}+1)(1-\\alpha)\\).\nConstruct \\(CI^{\\text{split}}=[\\hat{\\mu}-d,\\hat{\\mu}+d]\\).\n\n\n\nA downside of this splitting approach is the introduction of extra randomness. One way to mitigate this is to perform the split multiple times and construct a final confidence interval by taking the intersection of all intervals. The aggregation decreases the variability from a single data split and, as this paper shows, still remains valid. Similar random split aggregation has also been used in the context of statistical significance in high-dimensional models."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#notation",
    "href": "blog/causality-wo-experiments.html#notation",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs begin by establishing some basic notation. We aim to analyze the impact of a binary, endogenous treatment variable \\(X_1\\) on an outcome variable \\(Y\\), in a setting with exogenous variables \\(X_2\\). We have access to a well-behaved, representative iid sample of size \\(n\\) of \\(Y\\), and \\(X:=[X_1, X_2]\\). These variables are related as follows:\n\\[ Y = \\beta X_1 + \\gamma X_2 + \\epsilon, \\]\nwhere is a mean-zero error term. Our goal is to obtain a consistent estimate of \\(\\beta\\). For simplicity, we‚Äôre using the same notation for both single- and vector-valued quantities, as \\(X_2\\) can be in \\(\\mathbb{R}^p\\) with $ p&gt;1$.\nThe challenge arises because \\(X_1\\) and \\(\\epsilon\\) are correlated, rendering the standard OLS estimator inconsistent. Even in large samples, \\(\\hat{\\beta}_{OLS}\\) will be biased, and getting more data would not help. Specifically:\n\\[\\hat{\\beta}_{OLS} := (X'X)^{-1}X'Y \\nrightarrow \\beta. \\]\nStandard instrument-based methods rely on the existence of an instrumental variable \\(Z\\) which, conditional on \\(X_2\\), correlates with \\(X_1\\) but not with . Estimation then proceeds with 2SLS, LIML, or GMM, potentially yielding good estimates of \\(\\beta\\) given appropriate assumptions. Common issues with instrumental variables include implausibility of the exclusion restriction, weak correlation with \\(X_1\\), and challenges in interpreting \\(\\hat{\\beta}_{IV}\\). Formally:\n\\[\\hat{\\beta}_{IV} := (Z'X)^{-1}Z'Y \\rightarrow \\beta. \\]\nIn this article, we focus on obtaining correct estimates of \\(\\beta\\) in settings where we don‚Äôt have access to such an instrumental variable \\(Z\\)."
  },
  {
    "objectID": "blog/causality-wo-experiments.html#a-closer-look",
    "href": "blog/causality-wo-experiments.html#a-closer-look",
    "title": "Causality without Experiments, Unconfoundedness, or Instruments",
    "section": "A Closer Look",
    "text": "A Closer Look\nLet‚Äôs start with the heteroskedasticity-based approach of Lewbel (2012).\n\nHeteroskedasticity & Higher Moments\nThe main idea here is to construct valid instruments for \\(X_1\\) by using information contained in the heteroskedasticity of \\(\\epsilon\\). Intuitively, if \\(\\epsilon\\) exhibits heteroskedasticity related to $ X_2$, we can use to create instruments‚Äîspecifically by interacting \\(X_2\\) with the residuals of the endogenous regressor‚Äôs reduced form equation. So, this is an IV-based method, but the instrument is ‚Äúinternal‚Äù to the model and does not rely on any external information.\nThe key assumptions are:\n\nThe error term in the structural equation (\\(\\epsilon\\)) is heteroskedastic. This means \\(var(\\epsilon|X_2)\\) is not constant and depends on \\(X_2\\). Moreover, we need \\(cov(X_2,\\epsilon^2) \\neq 0\\). This is an analogue of the first stage assumption in IV methods.\nThe exogenous variable (\\(X_2\\)) are uncorrelated with the product of the endogeneous variable (\\(X_1\\)) and the error term (\\(\\epsilon\\)). That is, \\(cov(X_2, X_1\\epsilon) = 0\\). This is a form of the standard exogeneity assumption in IV estimation.\n\nThe heteroskedasticity-based estimator of Lewbel (2012) proceeds in two steps:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRegress \\(X_1\\) on \\(X_2\\) and save the estimated residuals; call them \\(\\hat{u}\\). Construct an instrument for \\(X_1\\) as \\(\\tilde{Z}=(X_2-\\bar{X}_2)\\hat{u}\\), where \\(\\bar{X}_2\\) is the mean of \\(X_2\\).\nUse \\(\\tilde{Z}\\) as an instrument in a standard 2SLS estimation: \\[\\hat{\\beta}_{LEWBEL} = (X'P_{\\tilde{Z}}X)^{-1}X'P_{\\tilde{Z}}Y,\\]\n\nwhere \\(P_{\\tilde{Z}}\\) ‚Äãis the projection matrix onto the instrument.\n\n\nThis line of thought can also be extended to use higher moments as an alternative or additional way to construct instrumental variables. The original approach uses the variance of the error term, but we can also rely on skewness, kurtosis, etc. The assumptions then must be modified such that these higher moments are correlated with the endogenous variable, etc.\nSoftware Packages: REndo, ivlewbel.\n\n\nLatent IVs\nThe latent IV approach imposes distributional assumptions on the exogenous part of the endogenous variable and employs likelihood-based methods to estimate \\(\\beta\\).\nLet‚Äôs simplify the model above, so that we have:\n\\[ Y=\\beta X + \\epsilon, \\]\nwhere \\(X\\) is endogenous. The key idea is to decompose \\(X\\) into two components:\n\\[ X= \\theta + \\nu, \\]\nwith \\(cov(\\theta, \\epsilon)=0\\), \\(cov(\\theta, \\nu) = 0\\), and \\(cov(\\epsilon, \\nu)\\neq0\\). The first condition states that \\(\\theta\\) is the exogenous part of \\(X\\), and the last one gives rise to the endogeneity problem.\nWe then proceed with adding distributional assumptions. Importantly, \\(\\theta\\) must follow some discrete distribution with a finite number of mass points. A common example imposes:\n\\[ \\theta \\sim \\text{Multinomial}(\\cdot) \\]\nand\n\\[ (\\epsilon, \\nu) \\sim \\text{Gaussian}(\\cdot). \\]\nThese set of assumptions lead to analytical solutions for the conditional and unconditional distributions of \\((Y,X)\\) and all parameters of the model are identified. Maximum likelihood estimation can then give us an estimate of \\(\\beta_{LIV}\\).\nSoftware Packages: REndo.\n\n\nCopulas\nFirst, a word on copulas. A copula is a multivariate cumulative distribution function (CDF) with uniform marginals on \\([0,1]\\). An old theorem states that any multivariate CDF can be expressed with uniform marginals and a copula function that represents the relationship between the variables. Specifically, if \\(A\\) and \\(B\\) are two random variables with marginal CDFs \\(F_A\\) and \\(F_B\\) and joint CDF \\(H\\), then there exists a copula \\(C\\) such that \\(H(a,b)=C(F_A(a), F_B(b))\\).\nHow does this fit into our context and framework? Park and Gupta (2012) introduced two estimation methods for \\(\\beta\\) under the assumption that \\(\\epsilon \\sim Gaussian(\\cdot)\\). The key idea is positing a Gaussian copula to link the marginal distributions of \\(X\\) and \\(\\epsilon\\) and obtain their joint distribution. We can then estimate \\(\\beta\\) in one of two ways: either impose distributional assumptions on these marginals and derive and maximize the joint likelihood function of \\(X\\) and \\(\\epsilon\\), or use a generated regressor approach. We will focus on the latter.\nIn the linear model, endogeneity is tackled by creating a novel variable \\(\\tilde{X}\\) and adding that as a control (i.e., a generated regressor). Using our simplified model where \\(X\\) is single-valued and endogenous, we now have:\n\\[Y=\\beta X + \\mu \\tilde{X} + \\eta, \\]\nwhere \\(\\eta\\) is the error term in this augmented model.\nWe construct \\(\\tilde{X}\\) as follows:\n\\[\\tilde{X}=\\Phi^{-1}(\\hat{F}_X(X)).\\]\nHere \\(\\Phi^{-1}(\\cdot)\\) is the inverse CDF of the standard normal distribution and \\(F_X(\\cdot)\\) is the marginal CDF of \\(X\\). We can estimate the latter using the empirical CDF by sorting the observations in ascending order and calculating the proportion of rows with smaller values for each observation. As you can guess, this introduces further uncertainty into the model, so the standard errors should be estimated using bootstrap.\nSoftware Packages: REndo, copula.\n\n\nComparison\nEach statistical method has its strengths and limitations. While the methods described here circumvent the traditional unconfoundedness and external instruments-based assumptions, they do not provide a magical panacea to the endogeneity problem. Instead, they rely on their own, different assumptions. These methods are not universally superior, but should be considered when traditional approaches do not fit your context.\nThe heteroskedasticity-based approach, as the name suggests, requires a considerable degree of heteroskedasticity to perform well. Latent IVs may offer efficiency advantages but come at the cost of imposing distributional assumptions and requiring a group structure of \\(X_1\\). The copula-based approach, while simple to implement, also requires strong assumptions about the distributions of \\(X\\) and \\(Y\\), as well as their relationship.\nThat‚Äôs it. You are now equipped with a set of new methods designed to identify causal relationships in your data."
  },
  {
    "objectID": "blog/weights-statistics.html#a-closer-look",
    "href": "blog/weights-statistics.html#a-closer-look",
    "title": "Weights in Statistical Analyses",
    "section": "A Closer Look",
    "text": "A Closer Look\nMean estimation does not depend on the type of weights. We have:\n\\[ \\bar{X} = \\frac{\\sum_i wX}{\\sum_i w}. \\]\nVariance estimation depends on the weight type.\nRemember that we estimate the standard error of \\(\\bar{X}\\) as:\n\\[ SE(\\bar{X})=\\frac{s}{\\sqrt{n}}, \\hspace{.3cm} \\text{where} \\hspace{.3cm} s=\\sqrt{\\frac{1}{N-1}\\sum_i(X-\\bar{X})^2} \\]\nis an estimate of the standard deviation of \\(X\\). I will explain below how estimation of \\(SE(\\bar{X})\\) differs for sampling and frequency weights. Imporantly, s_w will denote the weighted version of this standard error.\n\nSampling Weights\nSampling weights measure the inverse probability of an observation entering the sample. They are particularly relevant in surveys when some units in the population are sampled more frequently than others. These weights adjust for sampling discrepancies to ensure that survey estimates are representative of the entire population. Sampling weights are also sometimes called probability weights.\nIntuitively, if we want to use the survey to accurately represent the population, we should assign higher importance to observations that are less likely to be sampled and lower importance to those more likely to appear in our data. This is exactly what sampling weights achieve. Sampling weights are, thus, inversely proportional to the probability of selection. Therefore, a smaller weight indicates a higher probability of being sampled, and vice versa.\nFor example, households in rural areas might be less likely to enter a survey due to the increased resources required to reach remote locations. Conversely, urban households are typically easier to sample and thus more likely to appear in the data.\nThis weighting approach is also common in causal inference analyses using propensity score methods. Similarly, in machine learning, weighting is often employed to adjust for unbalanced samples in the context of rare events (e.g., fraud detection, cancer diagnosis).\nLet‚Äôs now get back to the discussion on variance estimation. With sampling weights, we have:\n\\[ SE^{\\text{sampling}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{S}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\frac{(\\sum_i w)^2}{\\sum_i w^2}}}\\]\nThe numerator is the weighted version of the sum of squared deviations from the mean, emphasizing observations with higher weights. The denominator normalizes the standard error based on the total sum of the weights. When the weights differ greatly‚Äîhigh weights make some observations much more influential, increasing uncertainty about the population mean and so the sampling-weighted standard error is often larger.\nI will move on to describing frequency weights.\nSoftware Package: survey\n\n\nFrequency Weights\nFrequency weights measure the number of times an observation should be counted in the analysis. They are most relevant when there are multiple identical observations or when duplicating observations is possible. Naturally, higher weights correspond to observations that appear more frequently. Frequency weights are common in aggregated datasets. For instance, with market- or city-level data, we might want to weight the rows by market size, thus assigning more importance to larger units.\nWe can gain intuition from a linear algebra perspective. In a regression context, the design matrix X must be of full rank (to ensure invertibility), which implies no two rows can be identical. We thus need to collapse X to keep only distinct rows and record the number of times each row appears in the original data. This record constitutes the frequency weights.\nThe formula for estimating the standard error of X with frequency weights is:\n\\[ SE^{\\text{frequency}}(\\bar{X})=\\frac{s_w}{\\sqrt{n^{\\text{F}}_{\\text{eff}}}}=\\frac{s_w}{\\sqrt{\\sum_i w}}.\\]\nHere \\(n_{\\text{eff}}\\) is the effective sample size (i.e., the sum of all weights) and \\(s_w\\) is the weighted standard deviation of \\(X\\).\nThe frequency-weighted standard error is typically the smaller because it increases the effective sample size without introducing variability in the contribution of different observations. In contrast, the sampling-weighted standard error could be larger if some observations are given much higher weights, increasing the variability and lowering the precision.\nSoftware Package: survey.\n\n\nOne More Thing\nThere is actually one more type of weights which are not so commonly used in practice, precision weights. They represent the precision (\\(1/\\text{variance}\\)) of observations. A weight of 5, for example, reflects 5 times the precision of a weight of 1, originally based on averaging 5 replicate observations. Precision weights often come up in statistical theory in places such as Generalized Least Squares where they promise efficiency gains (i.e., lower variance) relative to traditional Ordinary Least Squares estimation. In practice, precision weights are in fact frequency weights normalized to sum to n.¬†Lastly, Stata‚Äôs user guide refers to them as analytic weights."
  },
  {
    "objectID": "blog/two-types-weights-causality.html#a-closer-look",
    "href": "blog/two-types-weights-causality.html#a-closer-look",
    "title": "The Two Types of Weights in Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nInverse Propensity Score Weights\nInverse propensity score (IPS) weights rely on the estimated propensity score (X). The weights for treated and untreated groups are defined as:\n\\[ \\gamma_{\\text{IPS}}(X) = \\begin{cases} 1 / \\hat{e}(X) & \\text{if } W = 1, \\\\ 1 / (1 - \\hat{e}(X)) & \\text{if } W = 0. \\end{cases} \\]\nIPS weights intuitively correct for the unequal probability of treatment assignment. If an individual has a low probability of treatment \\(e(X)\\approx 0\\) but is treated, their weight is large, amplifying their influence in the analysis. Conversely, individuals with high treatment probabilities are downweighted to prevent overrepresentation.\nA few notes. First, these weights adjust the observed data such that the distribution of covariates in the weighted treated and control groups resembles that of the overall population. This helps mitigate selection bias, ensuring that comparisons between treated and control groups reflect a treatment effect rather than confounded differences in covariates (e.g., the control group is older).\nA second key property is that the weighted average of the outcomes for the treated group is an unbiased estimator for the mean outcome under treatment, \\(\\mu_1\\). This can be expressed mathematically as:\n\\[ \\hat{\\mu}_1(X)=\\frac{\\sum_i W_i Y_i/ \\hat{e}(X)}{ \\sum_i W_i / \\hat{e}(X)}=\\frac{1}{n_T}\\sum_i  \\gamma_{\\text{IPS}}(X_i) W_i Y_i. \\]\nThis equality is particularly useful in the next step, when estimating the treatment effect \\(\\tau\\).\nThird, these IPS weights are generally unknown and have to be estimated from the data. A leading exception is controlled randomized experiments where the researcher determines the probability of treatment. This is, however, uncommon. Traditionally, practitioners rely on methods like logistic regression to first obtain \\(\\hat{e}(X)\\) and then take its reciprocal to construct the weights.\nUsing IPS weights entails two primary challenges. When \\(\\hat{e}(X)\\) is close to \\(0\\) or \\(1\\), weights can become excessively large, leading to high variance in estimates. Practitioners then turn to trim observations with ‚Äútoo large‚Äù or ‚Äútoo small‚Äù \\(\\hat{e}(X)\\) values. Moreover, model misspecification in \\(\\hat{e}(X)\\) can lead to poor covariate balance, introducing bias. More flexible methods such as machin learning models can alleviate this problem.\nSoftware Packages: MatchIt, Matching.\n\n\nCovariate Balancing Weights\nAn alternative approach seeks weights that directly balance the dataset at hand. Typically this is framed as a constrained optimization problem with placing restrictions on the maximum imbalance in X between the treatment and control groups.\nThe weights \\(\\gamma_{\\text{CB}}(X)\\) are obtained by solving:\n\\[ \\min_{\\gamma} \\text{Imbalance}(\\gamma) + \\lambda \\text{Penalty}(\\gamma),\\]\nwhere ‚ÄúImbalance‚Äù measures covariate discrepancies between groups, and ‚ÄúPenalty‚Äù controls for extreme weights. A common formulation balances means:\n\\[ \\frac{1}{n} \\sum_{i=1}^n W_i \\gamma(X_i) f(X_i) \\approx \\frac{1}{n} \\sum_{i=1}^n f(X_i), \\]\nwith \\(f(x_i)=x\\). The same idea can be applied to higher moments of \\(X\\) like variance or skewness. Examples methods relying on this type of weights include include Hainmueller (2012), Chan et al.¬†(2016), and Athey et al.¬†(2018), among many others.\nCovariate balancing weights bypass the need to explicitly estimate \\(e(X)\\). Instead, they solve for weights that ensure the treated and control groups are balanced across predefined covariates or their transformations. This method aligns closely with the goal of causal inference: achieving balance. The main challenge is selecting the right set of covariates to be balanced. Variance estimation can also be more involved, although modern statistical packages take care of it.\nSoftware Packages: MatchIt, Matching.\n\n\nHybrid Approach\nImai and Ratkovic (2013) developed a hybrid method that combines elements of covariate balancing and inverse propensity score methods. Their method, known as Covariate Balancing Propensity Score (CBPS), aims to estimate propensity scores while simultaneously achieving covariate balance. Instead of relying on a tuning parameter, CBPS ensures balance by solving the following moment conditions:\n\\[ \\sum_{i}\\left[ W_i - e(X_i; \\gamma) \\right] X_i = 0. \\]\nAdditionally, CBPS estimates by solving the standard likelihood score equation:\n\\[ \\sum_{i} \\left[ W_i - e(X_i; \\gamma) \\right] \\frac{\\partial e(X_i; \\gamma)}{\\partial \\gamma} = 0. \\]\nThe CBPS is operationalized in the Genralized Method of Moments (GMM) framework from the econometrics literature. This approach ensures that the estimated propensity scores lead to balanced covariates, potentially improving the robustness of causal inference. CBPS can be implemented using available GMM software packages and is particularly useful when traditional propensity score models fail to achieve adequate balance.\nSoftware Packages: CBPS."
  },
  {
    "objectID": "blog/causation-without-correlation.html#a-closer-look",
    "href": "blog/causation-without-correlation.html#a-closer-look",
    "title": "Causation without Correlation",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn the popular book Causal Inference: The Mixtape, Scott Cunningham gives an example of a sailor steering a boat in stormy waters. The wind may be so strong as to offset the boat‚Äôs natural moving direction. For instance, the sailor might steer (treatment, \\(T\\)) the boat north, while a southward wind (confounder, \\(Z\\)) causes the boat to move east (outcome, \\(Y\\)). An onlooker would not observe any direct relationship between \\(T\\) and \\(Y\\), even though \\(T\\) causes \\(Y\\).\nAt first, this sounds counterintuitive. On second thought, such patterns are everywhere. Consider the following.\n\nExample 1: Parenting Styles and Children‚Äôs Behavior\nA parent might adopt a stricter parenting style (\\(T\\)) in response to a child‚Äôs behavioral issues (\\(Y\\)). However, other influences, like peer pressure or school environment (\\(Z\\)), may also shape the child‚Äôs behavior, sometimes overriding the parent‚Äôs efforts. The net observable outcome could show no correlation between stricter parenting and improved behavior, even though the stricter parenting is causally effective in certain contexts.\nThis idea can be taken one step further. An observable relationship might even appear positive when the causal relationship is negative.\n\n\nExample 2: Ice Cream Sales and Shark Attacks\nImagine two beaches with vastly different safety protocols (\\(Z\\)): one has lifeguards trained to prevent shark attacks, while the other does not. On the safer beach, higher ice cream sales (\\(T\\)) correlate positively with shark attacks (\\(Y\\)), because more people visit the beach when safety protocols are in place. This hides the fact that proper safety protocols causally reduce shark attacks. The observed positive correlation between \\(T\\) and \\(Y\\) masks the negative causal relationship.\n\n\nExample 3: Nonlinearity\nA more trivial scenario leading to the lack of correlation in causal relationships is non-linearity. I do not find this scenario too insightful simply because it can be avoided by using more sophisticated measures of correlation. See my earlier post on the Chatterjee correlation coefficient.\nExamples of such relationships abound. Consider a parabolic relationship, where increasing a drug‚Äôs dosage initially improves patient outcomes but becomes harmful at higher doses. Despite a clear causal relationship, the (Pearson) correlation coefficient might be close to zero because the relationship is not linear.\n\n\nExample 4: Threshold Effects and Phase Transitions\nTake the classic example of temperature and water‚Äôs state. Increasing temperature causes water to change state at exactly 100¬∞C. Below and above this point, temperature changes cause minimal effects on the water‚Äôs state. Aggregating these observations leads to a weak correlation, despite the temperature being the direct cause of the phase transition.\nOther fascinating scenarios include Lord‚Äôs Paradox, and Simpson‚Äôs Paradox, where a causal relationship can appear to reverse or disappear when data is aggregated.\n\n\nExample 5: Hospital Mortality Rates\nSuppose two hospitals treat patients with different levels of severity. Hospital \\(A\\) specializes in high-risk patients, while Hospital \\(B\\) treats mostly low-risk cases. When comparing raw mortality rates (\\(Y\\)), Hospital \\(A\\) might appear worse, even though it provides superior care (\\(T\\)). Disaggregating the data by risk level reveals the causal effect of Hospital \\(A\\)‚Äôs superior treatment within each group."
  },
  {
    "objectID": "blog/new-dev-fdr.html#a-closer-look",
    "href": "blog/new-dev-fdr.html#a-closer-look",
    "title": "New Developments in False Discovery Rate",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFDR Control with Mirror Statistics\nThe building block of these methods are the so-called mirror statistics, \\(M_j\\). They have the following two properties:\n\nVariables with larger mirror statistics (\\(M_j\\)‚Äôs) are more likely to be relevant.\nTheir distribution under the null hypothesis is (asymptotically) symmetric around \\(0\\).\n\nThese properties are simple and intuitive. For instance, the commonly used t-statistic for hypothesis testing in the linear model satisfies both. The first property suggests we can order the features and select ones with a mirror statistic exceeding some pre-defined threshold. The second one leads to an approximate upper bound on the number of false positives for any cutoff \\(t\\):\n\\[FDP(t) = \\frac{\\#\\{j: \\text{j is irrelevant, but } M_j &gt; t\\}}{\\# \\{j:M_j &gt; t\\}} \\leq \\frac{\\# \\{j:M_j &lt;- t\\}}{\\# \\{j:M_j &gt; t\\}}. \\]\nNow that we know the mirror statistics‚Äô properties, I will discuss various ways of calculating them.\n\n\nConstructing the Mirror Statistics\nThe mirror statistics \\(M_j\\) take the following general form:\n\\[M_j = sign(\\tilde{\\beta}_j^1, \\tilde{\\beta}_j^2) f(|\\tilde{\\beta}_j^1|, |\\tilde{\\beta}_j^2|),\\]\nwhere the \\(\\tilde{\\beta}\\) denote (standardized) estimates of the true coefficient \\(\\beta\\) and \\(f(\\cdot)\\) is a nonnegative, exchangeable and monotically increasing function. For instance, convenient choices for \\(f(\\cdot)\\) include \\(f(a,b) = 2min(a,b)\\) (Xing et al.¬†2019), \\(f(a,b) = ab\\), and \\(f(a,b) = a+b\\) (Dai et al.¬†2022).\nLet‚Äôs now turn to calculating the \\(\\tilde{\\beta}\\)‚Äôs.\n\n\nConstructing the Regression Coefficients\nThe coefficients \\(\\tilde{\\beta}\\) ought to satisfy the following two conditions:\n\nIndependence ‚Äì The two regression coefficients are (asymptotically) independent.\nSymmetry ‚Äì Under the null hypothesis, the (marginal) distribution of either of the two coefficients is (asymptotically) symmetric around zero.\n\nI will now describe two approaches in constructing them.\n\nMethod #1 ‚Äì Gaussian Mirrors\nSoftware Package: GM.\nThe main idea is to create a set of two perturbed mirror features for each variable \\(X_j\\). Namely,\n\\[X_j^+ = X_j + a_jZ_j, \\text{      and      } X_j^-=X_j -a_jZ_j,\\]\nwhere \\(a_j\\) is a scalar and \\(Z_j \\approx N(0,1)\\). The authors provide some guidance on how to select \\(a_j\\), but I will not get into that here.\nWhile it is possible to generate the mirror features for all columns in \\(X\\) simultaneously, the one-fit-per-feature approach shows better performance in simulations. So, the \\(\\tilde{\\beta}\\) are the estimates of \\(\\beta\\) in the following model:\n\\[ y = \\frac{\\beta_j}{2}X_j^+ +\\frac{\\beta_j}{2}X_j^- + X_{\\text{non-j}}\\beta_{\\text{non-j}} + \\epsilon. \\]\n\n\nMethod #2 ‚Äì Data Splitting\nAn alternative approach for getting two independent coefficient estimates \\(\\tilde{\\beta}\\) is through data splitting. When estimating the linear model, we can get \\(\\tilde{\\beta}^1\\) from one half of the data and $ ^2$ from the other half of the data. While this is simple and intuitive it can result in loss of statistical power. To alleviate this concern, we can do repeated data splitting and aggregate the results in the end. This is reminiscent of the procedure suggested by Meinheusen et al.¬†(2012) in the context of hypothesis testing in for high-dimensional regression. We can then determine the feature importance based on the share of data splits in which it ends up being included. I will omit the technical details here.\nThere is a technical wrinkle I have omitted ‚Äì the regression coefficients have to be standardized so that the \\(M_j\\)‚Äôs have comparable variances across variables. Check the original papers for details on exactly how to do that. Instead, I will now turn to the final algorithm for variable selection with FDR control using the approaches outlined above.\n\n\n\nPutting it All Together\nThis framework sets the stage for the following general algorithm for variable selection with FDR control:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nCalculate the \\(j\\) mirror statistics, \\(M_j\\).\nGiven a FDR level \\(\\alpha\\), set a threshold \\(\\tau(\\alpha)\\) such that \\[\\tau(\\alpha)= min\\{t &gt; 0 : \\hat{FDP}(t) \\leq \\alpha\\}.\\]\nSelect the features \\(\\{ j : M_j &gt;  \\tau(\\alpha) \\}\\).\n\n\n\nIn words, given \\(\\alpha\\) calculate the \\(M_j\\)‚Äôs, find the magical threshold \\(\\tau(\\alpha)\\) and include the variables with \\(M_j &gt; \\tau(\\alpha)\\)."
  },
  {
    "objectID": "blog/simpson-paradox.html#a-closer-look",
    "href": "blog/simpson-paradox.html#a-closer-look",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nAn Example\nImagine two hospitals, \\(A\\) and \\(B\\), treating patients for a particular condition with two treatment options, \\(T_1\\) and \\(T_2\\). Hospital \\(A\\), located in a higher-income neighborhood, primarily receives healthier patients, while Hospital \\(B\\), in a lower-income neighborhood, tends to treat sicker patients. The effectiveness of the treatments is measured as improvement in a continuous health score.\nWe are interested in examining whether one of the treatment options leads to better health outcomes. Consider the following data gathered across both hospitals.\n\n\nHealth improvement by hospital and treatment type. Both treatments \\(T_1\\) and \\(T_2\\) are equally effective within each hospital.\n\n\nHospital\nTreatment\nHealth Improvement\nN\n\n\n\n\nA\n\\(T_1\\)\n20\n90\n\n\nA\n\\(T_2\\)\n20\n10\n\n\nB\n\\(T_1\\)\n10\n10\n\n\nB\n\\(T_2\\)\n10\n90\n\n\n\n\nLet‚Äôs now look at what happens when we combine the data from both hospitals.\n\n\nHealth improvement by treatment type. Treatment \\(T_1\\) is more effective overall.\n\n\nTreatment\nN\nHealth Improvement\n\n\n\n\n\\(T_1\\)\n100\n19 = 20 * .9 + 10 * .1\n\n\n\\(T_2\\)\n100\n11 = 10 * .9 + 20 * .1\n\n\n\n\nWithin each hospital, the data shows that both treatments are equally effective. However, combining the data across both hospitals reveals that treatment \\(T_1\\) appears to be significantly more effective overall. Why does this happen?\nThe confounding variable here is the underlying health status of patients. Hospital \\(A\\) treats mostly healthier patients, while Hospital \\(B\\) handles more severe cases. This difference in patient distribution influences the overall success rates of the treatments, even though both treatments perform identically within each hospital.\n\n\nA Visualization\nTo illustrate, imagine a scatter plot where each dot represents a patient. The color of the dot indicates the treatment they received, and the horizontal lines represent the average health improvement for each group. The vertical axis depicts the outcome variable (health improvement).\n\nRPython\n\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\n# Set random seed for reproducibility\nset.seed(4904)\n\n# Define data dimensions\nn_a &lt;- 200\nn_b &lt;- 20\n\n# Create the dataset\ndata &lt;- data.frame(\n  Hospital = rep(c(\"Hospital A\", \"Hospital B\"), each = n_a + n_b),\n  Treatment = c(\n    rep(\"Treatment 1\", n_a), rep(\"Treatment 2\", n_b),  # Hospital A\n    rep(\"Treatment 1\", n_b), rep(\"Treatment 2\", n_a)   # Hospital B\n  ),\n  Improvement = c(\n    rnorm(n_a, mean = 20, sd = 2), rnorm(n_b, mean = 20, sd = 2),  # Hospital A\n    rnorm(n_b, mean = 10, sd = 2), rnorm(n_a, mean = 10, sd = 2)   # Hospital B\n  ),\n  Aggregated = \"Overall\"\n)\n\n# Calculate mean improvement for each treatment and hospital\nmean_improvement &lt;- data %&gt;%\n  group_by(Hospital, Treatment) %&gt;%\n  summarize(Mean_Improvement = mean(Improvement), .groups = \"drop\")\n\n# Calculate overall mean improvement for each treatment\noverall_mean &lt;- data %&gt;%\n  group_by(Treatment) %&gt;%\n  summarize(Mean_Improvement = mean(Improvement), .groups = \"drop\")\n\n# Create the disaggregated plot\nplot1 &lt;- ggplot(data, aes(x = Hospital, y = Improvement, color = Treatment)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.7) +\n  geom_hline(data = mean_improvement, aes(yintercept = Mean_Improvement, color = Treatment), linetype = \"solid\") +\n  labs(\n    title = \"By Hospital\",\n    x = NULL,\n    y = \"Health Improvement\",\n    color = \"Treatment\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Create the aggregated plot\nplot2 &lt;- ggplot(data, aes(x = Aggregated, y = Improvement, color = Treatment)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.7) +\n  geom_hline(data = overall_mean, aes(yintercept = Mean_Improvement, color = Treatment), linetype = \"solid\") +\n  labs(\n    title = \"Overall\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")\n\n# Combine the plots with a common legend and title\nfinal_plot &lt;- (plot1 + plot2) +\n  plot_annotation(\n    title = \"Simpson's Paradox: Treatment Effectiveness Within Hospitals and Overall\",\n    theme = theme(plot.title = element_text(hjust = 0.5))\n  ) &\n  theme(legend.position = \"bottom\")\n\n# Display the combined plot\nprint(final_plot)\n\n\n# load libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.random.seed(1988)\n\n# Define data dimensions\nn_a = 200\nn_b = 20\n\n# Create the dataset\nhospital = ['Hospital A'] * (n_a + n_b) + ['Hospital B'] * (n_a + n_b)\ntreatment = (\n    ['Treatment 1'] * n_a + ['Treatment 2'] * n_b +  # Hospital A\n    ['Treatment 1'] * n_b + ['Treatment 2'] * n_a   # Hospital B\n)\nimprovement = (\n    list(np.random.normal(20, 2, n_a)) + list(np.random.normal(20, 2, n_b)) +  # Hospital A\n    list(np.random.normal(10, 2, n_b)) + list(np.random.normal(10, 2, n_a))   # Hospital B\n)\naggregated = ['Overall'] * (2 * (n_a + n_b))\n\ndata = pd.DataFrame({\n    'Hospital': hospital,\n    'Treatment': treatment,\n    'Improvement': improvement,\n    'Aggregated': aggregated\n})\n\n# Calculate mean improvement for each treatment and hospital\nmean_improvement = data.groupby(['Hospital', 'Treatment'], as_index=False)['Improvement'].mean()\nmean_improvement.rename(columns={'Improvement': 'Mean_Improvement'}, inplace=True)\n\n# Calculate overall mean improvement for each treatment\noverall_mean = data.groupby(['Treatment'], as_index=False)['Improvement'].mean()\noverall_mean.rename(columns={'Improvement': 'Mean_Improvement'}, inplace=True)\n\n# Create the disaggregated plot\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.stripplot(data=data, x='Hospital', y='Improvement', hue='Treatment', jitter=0.2, alpha=0.7, dodge=True)\nfor _, row in mean_improvement.iterrows():\n    plt.axhline(y=row['Mean_Improvement'], color=sns.color_palette('Set1')[0 if row['Treatment'] == 'Treatment 1' else 1], linestyle='solid')\nplt.title(\"By Hospital\")\nplt.xlabel(None)\nplt.ylabel(\"Health Improvement\")\nplt.legend(title=\"Treatment\", loc='upper right')\nplt.grid(True)\n\n# Create the aggregated plot\nplt.subplot(1, 2, 2)\nsns.stripplot(data=data, x='Aggregated', y='Improvement', hue='Treatment', jitter=0.2, alpha=0.7, dodge=True)\nfor _, row in overall_mean.iterrows():\n    plt.axhline(y=row['Mean_Improvement'], color=sns.color_palette('Set1')[0 if row['Treatment'] == 'Treatment 1' else 1], linestyle='solid')\nplt.title(\"Overall\")\nplt.xlabel(None)\nplt.ylabel(None)\nplt.legend([], [], frameon=False)\nplt.grid(True)\n\n# Add a common title\nplt.suptitle(\"Simpson's Paradox: Treatment Effectiveness Within Hospitals and Overall\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\nIn the aggregated data, \\(T_1\\) shows a higher average improvement, creating the illusion of greater effectiveness. But when disaggregated by hospital, the averages for \\(T_1\\) and \\(T_2\\) are identical."
  },
  {
    "objectID": "blog/multiple-testing-overview.html#a-closer-look",
    "href": "blog/multiple-testing-overview.html#a-closer-look",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhen is MH Control NOT Necessary\nMultiple hypothesis adjustments are not required when:\n\nwe are interested in a purely predictive model, such as in machine learning settings,\nwe have an independent dataset for each correlated hypothesis,\nthe dependent variables we are testing are not correlated. An example is testing the impact of a new feature or intervention on a set of outcomes unrelated to each other ‚Äì e.g., health and education variables. In such cases, it is common to adjust for MH separately within the health- and education-related hypotheses.\n\nBefore we dive into the methods, let‚Äôs establish some basic terminology and notation.\n\n\nFWER vs FDR\nThere are two distinct types of targets that data scientists are after. The first one was introduced in the 1950s by the famous statistician John Tukey and is called Familywise Error Rate (FWER). FWER is the probability of making at least one type 1 error:\n\\[ FWER = \\mathbb{P} (\\text{At Least 1 Type 1 Error}). \\]\nIn the example from the introduction, \\(FWER = 0.3\\).\nControlling the FWER is good, but sometimes it is not great. It does what it is supposed to do ‚Äì limit the number of false positives, but it is often too conservative. In practice, too few variables remain significant after such FWER adjustments, especially when testing many (as opposed to just a few) hypotheses.\nTo correct this, in the 1990s, Yoav Benjamini and Yosef Hochberg, conceptualized controlling the so-called False Discovery Rate (FDR). FDR is the expected proportion of false positives:\n\\[ FDR = \\mathbb{E} \\left[ \\frac{\\text{\\# False Positives}}{\\text{\\# False Positives + \\# True Positives} } \\right]. \\]\nTake a minute to notice the significant difference between these two approaches. FWER controls the probability of having at least one false positive at \\(\\alpha\\). At the same time, FDR is okay with having several false positives as long as they are (on average) less than \\(\\alpha\\) of all hypotheses.\nFWER is more conservative than FDR, resulting in fewer significant variables. In our example, applying an FWER adjustment as opposed to an FDR one will lead to fewer statistically significant differences in the impact of the new feature across various cities.\nLet‚Äôs go over the most popular ways to control the FWER.\n\n\nFWER Control\n\nBonferroni‚Äôs Procedure\nYou might have heard of this one. The procedure is extremely simple ‚Äì it goes like this:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nreject \\(H_i\\) if \\(p_i \\times m  \\leq \\alpha\\) for all \\(i\\).\n\n\n\nIn words, we multiply all \\(p\\)-values by m and reject the hypotheses with adjusted \\(p\\)-values that are smaller than or equal to \\(\\alpha\\).\nThe Bonferroni adjustment is often considered to control the FWER, but it actually controls an even more conservative target ‚Äì the per-family error rate (PFER). PFER is the sum of probabilities of Type 1 error for all the hypotheses.\nYou correctly guessed that PFER is considered too strict. Although it is common in practice, there are better alternatives. While the Bonferroni correction is simple and elegant, it is always dominated by the Holm-Bonferroni procedure, so there is really no reason ever to use it.\n\n\nHolm & Bonferroni‚Äôs Procedure\nThis adjustment offers greater statistical power than the Bonferroni method regardless of the test size without significantly compromising on simplicity. In algorithmic language:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m.\\)\nReject the null hypothesis \\(H_1,\\dots H_{k-1}\\), where \\(k\\) is the smallest index for which \\(p_i \\times (m+1-k) \\leq \\alpha\\).\n\n\n\nMuch like the Bonferroni method with a multiplication factor of (\\(m+1-k\\)) instead of \\(m\\).\nThese two procedures control the FWER by assuming a kind of a ‚Äòworst-case‚Äô scenario in which the \\(p\\)-values are close to being independent of each other. To see why this is the case, imagine the extreme opposite scenario in which there is perfect positive dependence. Then, there is effectively one test statistic, and FWER adjustment is unnecessary.\nIn practice, however, the \\(p\\)-values are often correlated, and exploiting this dependence can result in even more powerful FWER corrections. This can be achieved with various resampling methods, such as the bootstrap or permutation procedures.\n\n\nWestfall & Young‚Äòs Procedure\nThe Westfall & Young method, developed in the 1990s, was the first resampling procedure controlling the FWER. It uses the bootstrap to account for the dependence structure among the \\(p\\)-values and improves on Holm-Bonferroni‚Äôs method.\nIt goes roughly like this:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(m\\) \\(p\\)-values in ascending order ‚Äì \\(p_1, \\dots, p_M\\).\nBegin with a \\(COUNTER_i = 0\\) for each hypothesis \\(i\\).\nTake a bootstrap sample and compute the \\(m\\) \\(p\\)-values (\\(p^*_1, \\dots, p^*_m\\))\nDefine the successive minima by starting with the largest \\(p\\)-value (\\(\\tilde{p}_M=p^*_M\\)) and for each subsequent hypothesis \\(i\\) taking the minimum between \\(\\tilde{p}_{i+1}\\) and \\(p^*_i\\). This gives a list of \\(p\\)-values \\(\\tilde{p}_1, \\dots, \\tilde{p}_m\\).\nIf \\(\\tilde{p}_i \\leq p_i\\) add 1 unit to \\(COUNTER_i\\) for each \\(i\\).\nRepeat steps 3-5 above \\(B\\) times and compute the fraction \\(\\hat{p}_i = COUNT_i/B\\). This gives a list of \\(p\\)-values \\(\\hat{p}_1, \\dots, \\hat{p}_m\\).\nEnforce monotonicity by starting with \\(\\hat{p}_1\\) and for each subsequent hypothesis \\(i\\) taking the maximum between \\(\\hat{p}_{i-1}\\) and \\(\\hat{p}_i\\). This final vector presents the FWER-adjusted \\(p\\)-values.\nReject all null hypotheses \\(i\\) for which \\(\\hat{p}\\leq \\alpha\\).\n\n\n\nYes, this is a complicated procedure. But it does offer substantial improvements in power, especially if the hypotheses are positively correlated.\nImportantly, Westfall and Young‚Äôs method rests on a critical assumption called ‚Äúsubset pivotality.‚Äù Roughly speaking, this condition requires that the joint distribution of any subset of test statistics does not depend on whether the remaining hypotheses are true or not.\nSoftware Package: multtest.\n\n\nRomano & Wolf‚Äòs Procedure(s)\nThis procedure improves on Westfall and Young‚Äôs method by not requiring a pivotality assumption. Here I am describing the method from Romano and Wolf‚Äôs 2005 Econometrica paper, although the authors have developed a few closely related FWER adjustments. This is the rough algorithm for it:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the test statistics in descending order, \\(t_1 \\geq t_2 \\geq, \\dots, t_m\\).\nTake B bootstrap samples, and for each hypothesis i compute the empirical distribution function of the successive maximum test statistic random variable. That is, the random variable of the maximum test statistic between \\(t_i, t_{i+1}, \\dots, t_m\\). Call this distribution \\(c(i)\\).\nReject \\(H_i\\) if \\(t_i\\) is greater than the 1-quantile of \\(c(1)\\).\nDenote by h the number of rejected hypotheses. If \\(h=0\\) stop. Otherwise, for \\(H_{h+1},\\dots H_s\\):\nReject \\(H_i\\) if \\(t_i &gt; c(h+1)\\)\nIterate until no further hypotheses are rejected.\n\n\n\nAnd you thought the Westfall and Young correction was tricky. The good news is that there are software packages for all these adjustments, so you won‚Äôt have to program them yourself. Arguably, this one of the best ways to control FWER; does not rest on pivotality assumptions. However, it can be difficult to explain to non-technical audiences; difficult to explain to anyone, really.\nSoftware Package: hdm.\nThis wraps up the descriptions of the methods controlling the FWER.\n\n\n\nFDR Control\nAs a reminder, FDR procedures have greater power at the cost of increased rates of type 1 errors compared to FWER adjustments. This is the dominant framework in the modern literature on MH adjustment.\n\nBenjamini & Hochberg‚Äòs Procedure\nThe Benjamini-Hochberg (BH) method is simple:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m}{k} \\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\n\n\nBH is valid when the tests are independent and in some, but not all, dependence scenarios. In general, FDR control is tricky with arbitrary dependence.\nThis adjustment has an interesting geometric interpretation:\n\nplot \\(p\\) versus \\(k\\),\ndraw a line through the origin with a slope equal to \\(frac{\\alpha}{m}\\),\nreject all hypotheses associated with the \\(p\\)-values on the left up to the last point below this line.\n\n\n\nBenjamini & Yekutieli‚Äòs Procedure\nBenjamini and Yekutieli (BY) developed a refinement of the BH procedure, which adds a magical constant \\(c(m)\\):\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m.c(m)}{k}\\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\n\n\nThe obvious question is in choosing the optimal value for \\(c(m)\\). You can view BH as a special case of BY with \\(c(m)=1\\). The authors show that the following choice works under any arbitrary dependence assumption:\n\\[ c(m) = \\sum_{i=1}^m \\frac{1}{i} \\approx log(m) + 0.57721 + \\frac{1}{2m}. \\]\nFascinating. For most applications, this simple procedure provides a credible way of taming the false positives rate. There are improvements to this method, but it feels like we are entering a world in which we get theoretical gains with only limited practical implications.\nThe main way to improve on BH/BY is to know (or estimate) the share of false hypotheses among the ones we are testing. This, however, goes beyond the scope of the article.\n\n\nKnockoffs\nWhat if we would like to combine variable selection and control the FDR? BH and BY do not really tell us how to do that. Enter knockoffs. The knockoffs approach is the kid on the block, and it does more than just FDR control.\nThe idea behind the knockoffs is novel and unrelated to any of the methods I discussed above. Knockoffs are copies of the variables with specific properties. In particular, for each feature/covariate variable \\(X\\), we create a knockoff copy \\(\\tilde{X}\\) such that the correlation structure between any two knockoff variables is the same as between their original counterparts. These knockoffs are constructed without looking at the outcome variable; hence, they are unrelated to it by design. Consequently, we can gauge whether each variable is statistically significant by comparing its test statistic to that of the one for its knockoff copy.\nKnockoffs are among the most active fields of research in statistics, with new papers coming out daily (ok, maybe weekly). These methods come in two flavors.\nThe fixed-design knockoff filter controls FDR for low-dimensional linear models regardless of the dependency structure of the features. This is great, but it does not work in high dimensions where variable selection is usually most needed.\nThe model-X knockoff filter solves this issue but requires exact knowledge of the joint distribution of the features. This is often infeasible in practice; even estimating that distribution in high dimensions can be really challenging as the curse of dimensionality lays its hammer on the data scientist.\nThe knockoff idea presents a big step forward because it solves two statistical problems at the same time ‚Äì selecting among a wide set of predictors and controlling the FDR. I am really excited to see where the current research will take us next.\nSoftware Package: knockoff.\n\n\nOther Modern Methods\nThere is certainly no shortage of statistical methods in this field. Each week, there is at least one new paper dealing with some aspects of FDR or FWER control. Knockoffs, in particular, have gotten a lot of attention recently, with researchers extending the idea to all kinds of settings and models.\nI will briefly mention some of the newer methods that have caught my eye in recent months. Many of these control the FDR asymptotically, and some are valid under general dependency structures among the test statistics.\nOne recent variation of the knockoffs idea uses Gaussian Mirrors; while others rely on subsampling approaches ‚Äì Data Splitting (based on estimating two independent regression coefficients after splitting the dataset in half) and Data Aggregation. Clever ideas to improve power include using covariate information to optimally weigh hypotheses in the BH framework or incorporating them into FDR regressions.\nTime will show whether any of these newcomers will come to dominate in practice."
  },
  {
    "objectID": "blog/limits-parametric-models.html#a-closer-look",
    "href": "blog/limits-parametric-models.html#a-closer-look",
    "title": "The Limits of Parametric Models: The Cram√©r-Rao Bound",
    "section": "A Closer Look",
    "text": "A Closer Look\nThe Cram√©r-Rao lower bound provides a theoretical benchmark for how precise an unbiased estimator can be. It sets the minimum variance that any unbiased estimator of a parameter \\(\\theta\\) can achieve, given a specific data-generating process.\n\nThe CRLB Formula\nFor a parameter \\(\\theta\\) in a parametric model with likelihood \\(f(x; \\theta)\\), the CRLB is expressed as:\n\\[\\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{I(\\theta)},\\]\nwhere \\(I(\\theta)\\) is the Fisher information (FI), defined as:\n\\[I(\\theta) = \\mathbb{E}\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta) \\right)^2 \\right].\\]\n\n\nIntuition\nTo understand the CRLB, we must delve into the concept of Fisher information named after one of the modern fathers of statistics R.A. Fisher. Intuitively, FI quantifies how much information the observed data carries about the parameter \\(\\theta\\).\nThink of the likelihood function \\(f(x; \\theta)\\) as describing the probability of observing a given dataset \\(x\\) for a particular value of \\(\\theta\\). If the likelihood changes sharply with \\(\\theta\\) (i.e., \\(\\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta)\\) is large), small changes in \\(\\theta\\) lead to noticeable differences in the likelihood. This variability reflects high information: the data can ‚Äúpinpoint‚Äù \\(\\theta\\) with greater precision. Conversely, if the likelihood changes slowly with \\(\\theta\\), the data offers less information about its true value.\nMathematically, the Fisher information \\(I(\\theta)\\) is the variance of the the partial derivative\n\\[\\frac{\\partial}{\\partial \\theta} logf(x;\\theta),\\]\nwhich we refer to as the score function. This score measures how sensitive the likelihood function is to changes in \\(\\theta\\). Higher variance in the score corresponds to more precise information about \\(\\theta\\).\n\n\nPractical Application\nThe CRLB provides a benchmark for evaluating the performance of estimators. For example, if you propose an unbiased estimator \\(\\hat{\\theta}\\), you can compare its variance to the CRLB. If \\(\\text{Var}(\\hat{\\theta}) = \\frac{1}{I(\\theta)}\\), we say the estimator is efficient. However, if the variance is higher, there may be room to improve the estimation method.\nMoreover, the CRLB also offers insight into the difficulty of estimating a parameter. If \\(I(\\theta)\\) is ‚Äúsmall‚Äù, so that the bound on the variance is high, then no unbiased estimator can achieve high precision with the available data. It is possible to develop a biased estimator for \\(\\theta\\) with lower variance, but it is not clear why you would do that."
  },
  {
    "objectID": "blog/lasso-heterogeneous-effects.html#a-closer-look",
    "href": "blog/lasso-heterogeneous-effects.html#a-closer-look",
    "title": "Lasso for Heterogeneous Treatment Effects Estimation",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLasso with Treatment Variable Interactions\nIn a low-dimensional world where regularization is not necessary and researchers are interested in \\(HTE\\)s, they often use a linear model in which the treatment variable is interacted with the covariates. Statistically significant interaction coefficients identify the \\(X\\) variables for which the treatment has a differential impact.\nMathematically, when all variables are properly interacted, this is analogous to splitting the sample into subgroups based on \\(X\\) and running OLS on each group separately. This is feasible and convenient when \\(X\\) is binary or categorical, but not when it is continuous. The advantage of this approach is that linear regression produces \\(p\\)-values which can be (mis)used to determine statistical significance of these interaction variables.\nThe OLS model is then:\n\\[Y = \\beta_1 T + \\beta_2 X + \\beta_3 X \\times T + \\epsilon,\\]\nwhere \\(\\epsilon\\) is the error term. The attention here falls on the coefficient vector \\(\\beta_3\\) which identifies whether the treatment has had a differential impact on units with a particular characteristic \\(X\\).\nIn high-dimensional settings with a wide \\(X\\), this is not feasible. Instead, we can use an algorithm to pick out the variables in \\(X\\) that are important for treatment effect heterogeneity.\nImai and Ratkovic (2013) show how to adapt the Lasso to this setting. It turns out we should not simply throw this model into the Lasso loss function. Can you guess why? Some variables might be predictive of the baseline outcome while others only of the treatment effect heterogeneity. The trick is to have two separate Lasso constraints ‚Äî \\(\\lambda_1\\) and \\(\\lambda_2\\).\nSo, the loss function looks something like this:\n\\[\\min_\\beta \\{ \\frac{1}{N}||y-f(X,T)\\beta_1 - X\\times T \\beta_2 ||_2^2 + \\lambda_1||\\beta_1||_1 + \\lambda_2||\\beta_2||_1 \\}.\\]\nA simplified version of the algorithm is as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nGenerate interaction variables, \\(\\tilde{X}(T)\\).\nRun Lasso of \\(Y\\) on \\(T\\), \\(X\\) and \\(\\tilde{X}(T)\\).\nIdentify statistically significant variables determining treatment effect heterogeneity.\n\n\n\nTwo notes. First, this requires some assurance that we are not overfitting, so that some form of sample splitting or cross validation is necessary. On top of this, Athey and Imbens (2017) suggest comparing these results with post-lasso OLS estimates to further guard against overfitting. Second, multiple testing is an issue as is the case with ML algorithms more generally. (You can check my earlier post on multiple hypothesis testing in linear machine learning models.) Options to take care of this include sample splitting and bootstrap, among others.\nSoftware Package: FindIt.\n\n\nLasso with Knockoffs\nAn alternative approach directly regresses the unit-level treatment effects on \\(X\\). To get there, we first model the outcome function and impute the missing potential outcome for each unit. See my previous post on using Machine Learning tools for causal inference for more information on how we might do that.\nThis approach was developed by Xie et al.¬†(2018). They recognized the multiple hypothesis issue and suggested using knockoffs to control the False Discovery Rate. This is still not completely kosher, as it does not account for the fact that the outcome variable is estimated in an earlier step. Oh well. My guess and hope are that this is more of a theoretical concern, and empirically the inference we get is still ‚Äúcorrect.‚Äù\nHere is a simplified version of their algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nTransform the outcome variable \\(\\tilde{Y}=Y\\times\\frac{(T-p)}{p(1-p)}\\).\nCalculate unit-level treatment effects, \\(\\hat{Y}(1)-\\hat{Y}(0)\\).\nGenerate the difference \\(Y^*=\\tilde{Y}-\\frac{1}{N}\\sum \\hat{Y}(1)-\\hat{Y}(0)\\).\nRun Lasso of \\(Y^*\\) on \\(X\\) and \\(X^*\\) (the knockoff counterparts of \\(X\\)).\nFollow the knockoff method to obtain the set of significant variables.\n\n\n\nRemember that \\(\\tilde{Y}\\) has the special property that \\(E[\\tilde{Y} \\mid X=x]=HTE(X)\\) under the unconfoundedness assumption.\nInterestingly, in the special case when \\(p=1/2\\) the algebra reduces further which provides computation scaling advantages. Tian et al.¬†(2014) showed this result first."
  },
  {
    "objectID": "blog/overlapping-conf-intervals.html#an-example",
    "href": "blog/overlapping-conf-intervals.html#an-example",
    "title": "Overlapping Confidence Intervals and Statistical (In)Significance",
    "section": "An Example",
    "text": "An Example\nSchenker and Gentleman (2001) offer a helpful illustration using proportions:\n\n\\(\\hat{Y}_1 = 0.56\\), \\(\\hat{Y}_2 = 0.44\\)\n\\(\\hat{SE}(Y_1) = \\hat{SE}(Y_2) = 0.0351\\)\n\nThe individual \\(95\\%\\) confidence intervals are:\n\nFor \\(Y_1\\): \\([0.49, 0.63]\\)\nFor \\(Y_2\\): \\([0.37, 0.51]\\)\n\nThese intervals do overlap. Under the na√Øve method, we would not reject the null hypothesis.\nHowever, the confidence interval for the difference is:\n\\[[0.02,0.22]\\]\nThis interval does not contain \\(0\\), meaning we would reject the null hypothesis using the correct method. The difference is statistically significant."
  },
  {
    "objectID": "blog/limits-nonparametric-models.html#a-closer-look",
    "href": "blog/limits-nonparametric-models.html#a-closer-look",
    "title": "The Limits of Nonparametric Models",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn practice, the choice of kernel is not as consequential. The primary focus of nonparametric statistics is selecting the optimal bandwidth \\(h\\).\n\nThe Bias-Variance Tradeoff\nAt the heart of bandwidth selection is the bias-variance tradeoff. Intuitively:\n\nSmall bandwidth (‚Äúoverfitting‚Äù): Captures fine details but also noise, leading to high variance and low bias.\nLarge bandwidth (‚Äúunderfitting‚Äù): Smoothes over the noise but can miss important structure, leading to high bias and low variance.\nFor nonparmetric curve estimation, the mean integrated squared error (MISE) is a common loss function for evaluating performance:\n\n\\[\\text{MISE}(h) = \\mathbb{E}\\left[ \\int \\left( \\hat{f}_h(x) - f(x) \\right)^2 dx \\right].\\]\nThis decomposes into bias and variance terms:\n\\[\\text{MISE}(h) =  \\int \\text{Bias}^2(\\hat{f}_h(x)) dx + \\int \\text{Var}(\\hat{f}_h(x)) dx.\\]\nMISE is analogous to the mean squared error (MSE) commonly used in machine learning, but integrated, or summed, across the support of the data. Minimizing the MISE yields the optimal bandwidth by balancing the bias-variance tradeoff, a fundamental concept in statistical estimation theory.\n\n\nOptimal \\(h\\) for Kernel Density\nSilverman‚Äôs rule of thumb provides a practical, closed-form approximation for \\(h\\) in KDE, assuming the data is roughly Gaussian. The expression is:\n\\[h^* = \\left( \\frac{4\\sigma^5}{3n} \\right) ^{\\frac{1}{5}}  = 1.06 \\sigma n^{-1/5},\\]\nwhere \\(\\sigma\\) is the standard deviation of the data. This formula emerges from theoretical analysis of the bias-variance tradeoff and serves as an effective initial bandwidth choice. While it performs well for many datasets, particularly those with approximately normal distributions, it may not be optimal for multimodal or heavily skewed distributions.\n\n\nOptimal \\(h\\) for Kernel Regression\nFor Nadaraya-Watson regression, several methods exist for selecting the bandwidth h. Cross-validation provides reliable results but can be computationally demanding. Theoretical analysis based on MISE minimization for common kernel functions suggests that the optimal bandwidth follows the relationship:\n\\[h^* \\propto n^{-1/5}.\\]\nThis expressiom highlights a fundamental characteristic of nonparametric methods: their convergence rate is slower than that of parametric methods. This represents one manifestation of the curse of dimensionality in nonparametric estimation. Furthermore, this relationship only specifies the rate at which the optimal bandwidth should decrease with sample size (proportionality to \\(n^{-1/5}\\)), rather than providing an exact value. Without the proportionality constant, its direct practical application is limited."
  },
  {
    "objectID": "blog/correlation-is-cosine.html#a-closer-look",
    "href": "blog/correlation-is-cosine.html#a-closer-look",
    "title": "Correlation is a Cosine",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Law of Cosines\nThe law of cosines states that in any triangle with sides \\(x\\), \\(y\\), and \\(z\\) and an angle (between \\(x\\) and \\(y\\)) \\(\\theta\\), we have:\n\\[ z^2 = x^2 + y^2 - 2 x y cos(\\theta), \\]\nIn the special case when \\(\\theta=\\frac{\\pi}{2}\\), the term on the right-hand side equals \\(0\\) and the equation reduces to the well-known Pythagorean Theorem.\n\n\nThe Variance of the \\(A+B\\)\nLet‚Äôs imagine two random variables \\(A\\), \\(B\\). The variance of their sum is given by:\n\\[ var(A+B) = var(A)+var(B)+2 cov(A,B), \\]\nwhere \\(cov(\\cdot)\\), denotes covariance. We can substitute the last term with its definition as follows:\n\\[ var(A+B) = var(A)+var(B)+2 corr(A,B) sd(A) sd(B). \\]\nNext, we know that \\(var(\\cdot)=sd^2(\\cdot)\\). Substituting, we get:\n\\[ sd^2(A+B) = sd^2 (A)+ sd^2 (B)+2 corr(A,B) sd(A) sd(B).\\]\n\n\nPutting the Two Together\nSetting \\(x=sd(A)\\), \\(y=sd(B)\\), and \\(z=sd(A+B)\\) in the first equation gives the desired result. With one small caveat ‚Äì the negative sign on the cosine term. To get around this we can simply look at the complementary angle \\(\\delta = \\pi - \\theta\\).\nThat is, we imagine a triangle with sides equal to \\(sd(A)\\), \\(sd(B)\\) and \\(sd(A+B)\\), where \\(\\theta\\) is the angle between \\(sd(A)\\), \\(sd(B)\\). When this angle is small (\\(\\theta &lt; \\frac{\\pi}{2}\\)), the two sides point in the same direction and A and B are positively correlated. The opposite is true for \\(\\theta &gt; \\frac{\\pi}{2}\\). As mentioned above, \\(\\theta = \\frac{\\pi}{2}\\) kills the correlation term, consistent with \\(A\\) and \\(B\\) being independent.\n\n\nCorrelation as a Dot Product\nThere‚Äôs another way to see this connection that makes it even clearer.\nIf you think of \\(A\\) and \\(B\\) as vectors in \\(n\\)-dimensional space (e.g., \\(A = (a_1, a_2, \\ldots, a_n)\\)), the cosine of the angle between them is given by:\n\\[\n\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|},\n\\]\nwhere \\(A \\cdot B\\) is the dot product, and \\(\\|\\cdot\\|\\) denotes the Euclidean norm. When \\(A\\) and \\(B\\) are standardized (i.e., mean zero and unit variance), this cosine becomes the Pearson correlation coefficient:\n\\[\n\\text{corr}(A, B) = \\frac{1}{n} \\sum_{i=1}^n A_i^* B_i^* \\approx \\cos(\\theta),\n\\]\nwhere \\(A^*\\) and \\(B^*\\) are the standardized versions of \\(A\\) and \\(B\\).\n\n\nCosine Similarity vs.¬†Correlation\n\n\n\n\n\n\nCosine similarity vs.¬†correlation\n\n\n\nCosine similarity and Pearson correlation are closely related, but not always the same:\n\nCosine similarity considers only the angle between vectors. It‚Äôs scale-invariant but not shift-invariant.\nCorrelation removes both mean and scale, making it invariant to affine transformations.\n\nSo, while ‚Äúcorrelation is a cosine,‚Äù the statement is strictly true when you‚Äôre working with standardized vectors."
  },
  {
    "objectID": "blog/correlation-is-cosine.html#an-example",
    "href": "blog/correlation-is-cosine.html#an-example",
    "title": "Correlation is a Cosine",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs generate two random vectors, standardize them, compute their correlation and angle, and plot them as vectors. We‚Äôll also see how correlation equals the cosine of the angle between the vectors.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\n# Generate two random vectors\nA &lt;- rnorm(100)\nB &lt;- 0.8 * A + sqrt(1 - 0.8^2) * rnorm(100)  # Correlated with A\n\n# Standardize\nA_std &lt;- scale(A)\nB_std &lt;- scale(B)\n\n# Correlation and cosine\ncorrelation &lt;- cor(A, B)\ncosine &lt;- sum(A_std * B_std) / (sqrt(sum(A_std^2)) * sqrt(sum(B_std^2)))\nangle_deg &lt;- acos(cosine) * 180 / pi\n\n# Print results\ncat(\"Correlation:\", round(correlation, 3), \"\\n\")\n&gt; Correlation: 0.825 \ncat(\"Angle (degrees):\", round(angle_deg, 1), \"\\n\")\n&gt; Angle (degrees): 34.4 \n\n# Plot vectors\nplot(c(0, A_std[1]), c(0, B_std[1]), type = \"n\", xlab = \"A (standardized)\", ylab = \"B (standardized)\",\n     main = \"First Vectors from A and B\")\narrows(0, 0, A_std[1], 0, col = \"blue\", lwd = 2)\narrows(0, 0, A_std[1], B_std[1], col = \"red\", lwd = 2)\nlegend(\"topright\", legend = c(\"A\", \"B\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1988)\n\n# Generate two correlated vectors\nA = np.random.randn(100)\nB = 0.8 * A + np.sqrt(1 - 0.8**2) * np.random.randn(100)\n\n# Standardize\nA_std = (A - np.mean(A)) / np.std(A)\nB_std = (B - np.mean(B)) / np.std(B)\n\n# Correlation and cosine\ncorrelation = np.corrcoef(A, B)[0, 1]\ncosine = np.dot(A_std, B_std) / (np.linalg.norm(A_std) * np.linalg.norm(B_std))\nangle = np.arccos(np.clip(cosine, -1, 1)) * 180 / np.pi\n\nprint(f\"Correlation: {correlation:.3f}\")\nprint(f\"Angle (degrees): {angle:.1f}\")\n\n# Plot first two vectors\nplt.figure(figsize=(5, 5))\nplt.quiver(0, 0, A_std[0], 0, angles='xy', scale_units='xy', scale=1, color='blue', label='A')\nplt.quiver(0, 0, A_std[0], B_std[0], angles='xy', scale_units='xy', scale=1, color='red', label='B')\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nplt.xlabel(\"A (standardized)\")\nplt.ylabel(\"B (standardized)\")\nplt.title(\"First Vectors from A and B\")\nplt.legend()\nplt.grid(True)\nplt.gca().set_aspect('equal')\nplt.show()\n\n\n\nIn this example, \\(cor(A,B)=0.825\\) and the angle between the two vectors is \\(34.4^\\circ\\), and we have \\(cos(34.4^\\circ) = cor(A,B)=0.825\\). You can also visualize these vectors, but I am not showing that graph here."
  },
  {
    "objectID": "blog/correlation-is-cosine.html#references",
    "href": "blog/correlation-is-cosine.html#references",
    "title": "Correlation is a Cosine",
    "section": "References",
    "text": "References\nCosines and correlation, Cook 2010, blog post"
  },
  {
    "objectID": "blog/conformal-inference-var-selection.html#a-closer-look",
    "href": "blog/conformal-inference-var-selection.html#a-closer-look",
    "title": "Using Conformal Inference for Variable Importance in Machine Learning",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Variable Importance\nThe idea of measuring which variables contribute most to a prediction model is not new. The data scientist‚Äôs toolbox contains some useful techniques designed to measure variable importance in ML models. Popular choices include:\n\nGini Importance and Information Gain in tree-based models (e.g., random forest, gradient boosting) measure the decrease in various within-leaf impurity indexes caused by excluding a certain variable. The larger the loss, the more important the variable.\nSHAP Values use a cooperative game-theoretic approach to measure each variable‚Äôs contribution to the final model‚Äôs prediction.\nPermutation Importance assesses a variable‚Äôs significance by randomly shuffling its values and comparing the change in the model‚Äôs performance. The larger the drop, the more important the variable.\nVariable Coefficients in linear ML models (e.g., Lasso, Ridge) can directly signal importance. This requires an appropriate standardization before fitting the model (to make sure all features are on a level playing field).\n\n\n\nVariable Importance with Conformal Inference\nWe can measure the prediction error associated with dropping a feature \\(j\\) when predicting a new observation \\(Y_{n+1}\\) by:\n\\[\\Delta_j^{n+1} = |Y_{n+1} - \\hat{\\mu}_{-j}(X_{n+1})| - |Y_{n+1}-\\hat{\\mu}(X_{n+1})|.\\]\nThe main idea is to use conformal inference ideas to construct a confidence interval for this prediction loss, \\(\\Delta_j^{n+1}\\), as a signal whether that variable is relevant in predicting the outcome.\nSpecifically, let \\(CI(\\cdot)\\) denote the conformal inference interval for \\(Y_{n+1}\\) given \\(X_{n+1}\\). Then, the interval\n\\[S_j(x)=\\{ |y-\\hat{\\mu}_{-j}(x)|-|y-\\hat{\\mu}(x)| : y \\in CI(x) \\}.\\]\nhas a valid finite-sample coverage in the sense that:\n\\[ P(\\Delta_j^{n+1} \\in S_j(X_{n+1})) \\geq 1-\\alpha, \\]\nwhere \\(\\alpha\\) is a pre-specified significance level. This holds for all \\(j\\).\nWe can plot the confidence intervals \\(S_j(X_i)\\) for \\(i=1 \\dots n\\) and roughly interpret them as measuring variable importance. The closer the intervals are to zero, the less important the variable is for predicting new outcomes. The opposite is true as well. The further and more often it is away from zero, the more important the variable.\nAnother, more global, approach to using conformal inference for variable importance focuses on the distribution of \\(\\Delta_j(X_{n+1}, Y_{n+1})\\) and conducts hypothesis testing on its median or mean. Intuitively, failing to reject a hyptohesis that these statistics are non-zero is evidence that variable \\(j\\) does not play a significant role in predicting \\(Y\\)."
  },
  {
    "objectID": "blog/stratified-sampling-cont-var.html#a-closer-look",
    "href": "blog/stratified-sampling-cont-var.html#a-closer-look",
    "title": "Stratified Sampling with Continuous Variables",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Traditional Method: Equal-Sized Binning\nThis approach involves dividing the continuous variable(s) into intervals or bins. For example, churn score, a single continuous variable \\(X\\), can be divided into quantiles (e.g., quartiles or deciles), ensuring each bin contains approximately the same number of observations/users.\nLet‚Äôs focus on the case of building ten equally-sized strata. Mathematically, for a continuous variable \\(X\\), the decile-based binning can be defined as:\n\\[\\text{Bin}_i = \\{x \\in X : Q_{10\\times (i-1)+1} \\leq x &lt; Q_{10\\times i}\\} \\text{ for } i \\in \\{1,\\dots,10\\}, \\]\nwhere \\(Q_k\\) represents the \\(k\\)-th quantile of \\(X\\). This approach splits in the first ten percentiles (i.e., minimum value to the \\(10\\)th percentile) into a single stratum, the next ten percentiles (\\(10\\)th to \\(20\\)th) into another stratum, and so on.\nWhen dealing with multiple variables, this method extends to either marginally stratify each variable or jointly stratify them. However, joint stratification across multiple variables can also fall prey to the curse of dimensionality.\n\n\nThe Modern Method: Unsupervised Clustering\nAn alternative approach uses unsupervised clustering algorithms, such as \\(k\\)-means or hierarchical clustering, to group observations into clusters, treating these clusters as strata. Unlike binning, clustering leverages the distribution of the data to form natural groupings.\nFormally, let \\(X\\) be a matrix of n observations across \\(p\\) continuous variables. One class of clustering algorithms aims to assign each observation \\(i\\) to one of \\(k\\) clusters:\n\\[\\text{minimize} \\quad \\sum_{j=1}^k \\sum_{i \\in \\mathcal{C}_j} \\text{distance}(X_i - \\mu_j), \\]\nwhere \\(\\mu_j=\\frac{1}{|S_j|}\\sum_{i\\in \\mathcal{C}_j} X\\)‚Äã is the centroid of cluster \\(\\mathcal{C}_j\\) of \\(size |S_j|\\).\nCommonly, \\(\\text{distance}(X_i, \\mu_j)=\\|X_i - \\mu_j\\|^2\\) which leads to \\(k\\)-means clustering. Unlike in the binning approach, here we are not restricting each strata to have the same number of observations."
  },
  {
    "objectID": "blog/variance-ps-matching.html#a-closer-look",
    "href": "blog/variance-ps-matching.html#a-closer-look",
    "title": "The Variance of Propensity Score Matching Estimators",
    "section": "A Closer Look",
    "text": "A Closer Look\nIn brief, the idea behind treatment effect estimation with PSM is:\n\nEstimate the propensity score (i.e., the probability of being in the treatment group).\nFor each user in the treatment group find the user(s) in the control group with the most similar predicted propensity score value.\nAnalyze the outcome differences in each pair to arrive at a final ATT (or ATE) estimate.\n\nThroughout the article, I will assume a standard, well-behaved setting with SUTVA, unconfoundedness, and overlap assumptions in place. Similarly, I will ignore many important aspects of PSM methods such as trimming, propensity score and even treatment effect estimation. I will also use standard notation (e.g., \\(Y\\) and \\(D\\) denote outcome and treatment; and \\(\\tau\\) is the ATT) without setting up the entire framework.\nWe can classify the methods for estimating the PSM variance in three distinct buckets.\n\nAsymptotic Approximations\nRecall the Law of Total Variance stating that, given a conditioning variable \\(X\\), we can always decompose a random variable \\(Y\\)‚Äôs variance, into two components ‚Äì the expectation of the conditional variance and the variance of the conditional expectation: \\[Var(Y) = Var(E(Y|X)) + E(Var(Y|X)).\\]\nAbadie and Imbens (2006) use this idea to derive the asymptotic variance formula for one-to-one and one-to-many matching estimators. They provide an analytical expression for the variance as \\(n \\to \\infty\\) (and hence, this is an approximation) using the matching covariates as conditioning variables. I will not be going into their work in detail, but a couple of notes are worth discussing.\nThe original formulation assumes the true propensity score function is known. This is rarely true and in practice we rely on an estimate of it. Practitioners then replace this true propensity score with the estimated one. This is OK, but not great ‚Äì it brings me to the second issue.\nOne needs to account for the fact that this propensity score is estimated. As such, it comes with uncertainty, which, if ignored, can potentially understate the variance of your estimator. Hello, type 1 errors!\nIn follow-up work, Abadie and Imbens (2016) propose correction which accounts for this uncertainty. Interestingly, in a work that I will discuss below, Bodory et al.¬†(2020) report that in practice this correction might increase or decrease the ATT variance.\n\n\nApproximation Based on Weights\nWe can express all treatment effect estimators as a difference of weighted means:\n\\[\\hat{\\tau} = \\frac{1}{n} \\sum\\hat{w} D Y - \\frac{1}{n} \\sum\\hat{w} (1-D) Y,\\]\nwhere the weights \\(\\hat{w}\\) may depend on the propensity score.\nLechner (2002) suggested calculating the variance of PSM based on this formula, assuming the weights are non-stochastic. In other words, the approach ignores the fact that the propensity score is estimated in a first step. The PSM variance is then equal to the sum of two terms ‚Äì the one for the treated and the one for the control.\n\n\nBootstrap\nBootstrap is the go-to method for estimating variances in complex settings where analytical expressions are too messy or even do not exist. PSM seems like a natural environment where the bootstrap can help. Not so fast!\nAbadie and Imbens (2006) show that the standard nonparametric bootstrap is inconsistent for non-smooth estimators (such as propensity score matching) with fixed number of matches and continuous covariates. Nevertheless, practitioners routinely ignore this result. While the theory says we should not use the bootstrap here, the extent to which this is of practical relevance is unclear. For instance, if this only makes a difference in the fourth decimal, we might be willing to sacrifice some bias for ease of computation.\nThere are at least two ways one can go around using the bootstrap here. The first one is the standard plain vanilla approach.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRandomly draw \\(B\\) samples of size \\(n\\). Alternatively, you can also sample directly from the matched pairs which works better in certain cases.\nCompute the ATT each time.\nCompute the confidence interval: \\[ \\hat{\\tau}\\pm \\sqrt{\\hat{V}(\\hat{\\tau}) \\times c}, \\]\n\nwhere \\(\\hat{V}(\\tau) = \\frac{1}{B-1}\\sum_{b=1}^B(\\hat{\\tau}^b-\\frac{1}{B}\\sum_b\\hat{\\tau}^b)^2\\) is the bootstrap variance of \\(\\hat{\\tau}\\) and \\(c\\) is the critical value associated with a confidence level \\(\\alpha\\).\n\n\nAlternatively, in this last step we can directly use the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles of the bootstrap distribution of \\(\\hat{\\tau}\\).\nThe second approach uses the well-known fact that the bootstrap behaves better when it uses so called asymptotically pivotal statistics ‚Äì i.e., ones which asymptotic distribution does not depend on unknown quantities. In this setting a candidate would be the \\(t\\)-statistic which, as we know, under certain conditions has a standard normal asymptotic distribution. We proceed in three steps:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nCompute the \\(t\\)-stat in the main sample using one of the variance approximations.\nDraw \\(B\\) random samples of size \\(n\\) and we compute the recentered \\(t\\)-stat with respect to \\(\\hat{\\tau}\\) in the main sample, \\[T^b = \\frac{\\hat{\\tau}^b - \\hat{\\tau}}{\\sqrt{\\hat{V}(\\tau^b)}}.\\]\nThe \\(p\\)-value is the share of (absolute value) bootstrap \\(t\\)-stats larger than the absolute value of the \\(t\\)-stat in the main sample \\[p{\\text{-value}} = 1 - \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|\\leq |T|) = \\frac{1}{B} \\sum_b \\mathbf{1}(|T^b|&gt; |T|),\\]\n\nwhere \\(T\\) is the \\(t\\)-stat from the main sample.\n\n\nThere are also newer (wild) bootstrap ideas which are still making their way into the mainstream.\n\n\nMonte Carlo Simulations\nBodory et al.¬†(2020) compare the finite sample performance of several of these methods (plus others). Their analysis is more thorough in the sense that they include other ATT estimators besides PSM, such as PS weighting or radius matching.\nOverall, their results do not indicate a clear winner which performs best in all scenarios. Interestingly, even some of the bootstrap methods often outperform the asymptotic approximations of Abadie and Imbens (2006) or Lechner (2002)."
  },
  {
    "objectID": "blog/gradient-boosting.html#a-closer-look",
    "href": "blog/gradient-boosting.html#a-closer-look",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Gradient Boosting\nGradient boosting is an ensemble machine learning technique that builds models sequentially, each new model attempting to correct the errors of the previous ones. The general idea is to minimize a loss function by adding weak learners (typically short decision trees) in a stage-wise manner to arrive at a single strong learner. This iterative process allows the model to improve its performance gradually, making it highly effective for complex datasets. Gradient boosting methods are versatile in that they can be used both for regression and classifications problems. In the most common case when the weak learners are decision trees, the algorithm is known as gradient tree boosting.\nMathematically, the model is built as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m = 1\\) to \\(M\\):\n\nCompute the pseudo-residuals: \\(r_{im} = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}\\) for \\(i=1,\\dots,n\\). In regression tasks this is simply \\(y-\\hat{y}\\).\nFit a weak learner such as a tree, \\(h_m(x)\\), on \\(\\{(x_i, r_{im}) \\}_{i=1}^n.\\)\nCompute \\(\\gamma\\) by solving: \\(\\gamma=\\arg\\min\\sum_i L(y_i, f_{m-1}(x_i)+\\gamma h_m(x_i)).\\)\nUpdate the model: \\(f_m(x) = f_{m-1}(x) + \\gamma h_m(x).\\)\n\nThe final model is \\(f_M(x).\\)\n\n\n\nThis is the most generic recipe for a gradient boosting algorithm. Let‚Äôs now focus on more specific implementations of this idea.\n\n\nAdaBoost\nAdaBoost, short for Adaptive Boosting, is one of the earliest boosting algorithms. It adjusts the weights of incorrectly classified observations so that subsequent learners focus more on difficult ones. In other words, it works by learning from its mistakes ‚Äì after each round of predictions, it identifies which observartions it got wrong and pays special attention to them (i.e., gives them a higher weight) in the next round. Thus, AdaBoost is not strictly speaking a gradient boosting algorithm, in the modern sense of the term.\nLet‚Äôs consider a binary classification problem where the outcome variable takes values in \\(\\{ -1, 1\\}\\). Here is the general AdaBoost algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nInitialize weights \\(w_i = \\frac{1}{n}\\) for \\(i = 1, \\ldots, n.\\)\nFor \\(m = 1 \\dots M\\):\n\nTrain a weak learner \\(h_m(x)\\) using weights \\(w_i\\).\nCompute the weighted error: \\(\\epsilon_m = \\frac{\\sum_{i=1}^{n} w_i \\mathbb{I}(y_i \\neq h_m(x_i))}{\\sum_{i=1}^{n} w_i}\\), where \\(\\mathbb{I}(\\cdot)\\) is the indicator function.\nCompute the model weight:$ _m = ()$.\nUpdate and \\(w_i^{m+1}= w_i^{m} \\exp(-\\alpha_m y_i h_m(x_i))\\), and \\(w_i^{m+1} = \\frac{ w_i^{m+1}}{\\sum_j  w_j^{m+1}}\\) for \\(i=1,\\dots,n\\).\n\nThe final model is \\(f(x)=sign \\left( \\sum_m \\alpha_m h_m(x) \\right)\\).\n\n\n\nAdaBoost is simple to implement while being relatively resistant to overfitting, making it especially effective for problems with clean, well-structured data. Its adaptive nature means it can identify and focus on the most challenging observations. However, AdaBoost has notable weaknesses: it‚Äôs highly sensitive to noisy data and outliers (since it increases weights on misclassified examples), can perform poorly when working with insufficient training data, and tends to be computationally intensive compared to newer, simpler algorithms.\nSoftware Packages: adabag, gbm, scikit-learn.\n\n\nXGBoost\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting designed for speed and performance. It revolutionizes the method by using the Newton-Raphson method in function space, setting it apart from traditional gradient boosting‚Äôs simpler gradient descent approach. At its core, XGBoost leverages both the first and second derivatives of the loss function through a second-order Taylor approximation. This sophisticated approach enables faster convergence and more accurate predictions by making better-informed decisions about how to improve the model at each step. When building decision trees, XGBoost considers both the expected improvement and the uncertainty of potential splits, while simultaneously applying regularization to prevent overfitting.\nWith some simplifications, the regression version of the XGBoost is implemented as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m=1, \\dots M\\):\n\nCompute the gradients \\[grad_m(x_i)=\\left( \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right)_{f(x)=f_{m-1}(x)}\\] and hessians \\[hess_m(x_i)=\\left( \\frac{\\partial^2 L(y_i, f(x_i))}{\\partial^2 f(x_i)^2}\\right)_{f(x)=f_{m-1}(x)}\\] of the loss function for \\(i=1,\\dots,n\\).\nFit a weak learner such as a tree, \\(h_m(x)\\) on$ {(x_i, ) }_{i=1}^n$.\nUpdate the model \\(f_m(x)=f_{(m-1)} - \\alpha h_m(x).\\)\n\nThe final model is \\(f_M(x)=\\sum_m f_m(x)\\).\n\n\n\nXGBoost is ideal for large datasets and competitions where model performance is critical. It is also a good choice when you need a highly optimized and scalable solution as it is highly efficient, supports parallel and distributed computing. Nevertheless, this algorithm can be complex to tune, and may require significant computational resources for very large datasets.\nSoftware Packages: xgboost.\n\n\nCatBoost\nCatBoost, short for Categorical Boosting, is a gradient boosting library that handles categorical features efficiently. It uses ordered boosting to reduce overfitting and supports GPU training, making it both powerful and versatile. CatBoost modifies the standard gradient boosting algorithm by incorporating two novel techniques ‚Äì ordered boosting and target statistics for categorical features. Let‚Äôs examine each one in turn.\nRather than requiring preprocessing like one-hot encoding, CatBoost encodes categorical values based on the distribution of the target variable without introducing data leakage. This is achieved by encoding each data point as if it were unseen, preventing overfitting. Additionally, ordered boosting is a method that builds each new tree while treating each data point as ‚Äúout-of-fold‚Äù for itself. This helps reduce overfitting, particularly in small datasets, by preventing over-reliance on individual observations during training.\nSoftware Packages: catboost.\n\n\nLightGBM\nLightGBM shares many of XGBoost‚Äôs benefits, such as support for sparse data, parallel training, multiple loss functions, regularization, and early stopping, but it also introduces a bunc of new features and improvements.\nFirst, rather than growing trees level-wise (row by row) as in most implementations, LightGBM grows trees leaf-wise, selecting the leaf that provides the greatest reduction in loss. Second, it does not use the typical sorted-based approach for finding split points, which sorts feature values to locate the best split. Instead, it relies on an efficient histogram-based method that significantly improves both speed and memory efficiency. Third, LightGBM incorporates the so-called Gradient-Based One-Side Sampling, which speeds up training by focusing on the most informative samples. Lastly, the algorithm uses Exclusive Feature Bundling which can group features to reduce dimensionality, enabling faster and more accurate model training.\nSoftware Packages: lightgbm.\n\n\nChallenges\nGradinet boosting methods comes with a few common practical challenges. When a predicitve model learns the training data too precisely, it may perform poorly on new, unseen data ‚Äì a problem called overfitting. To combat this, various regularization methods are used to add helpful constraints to the learning process. A main parameter that regulates the model‚Äôs learning precision is the number of boosting rounds (\\(M\\)), which determines how many base models are created. While using more rounds reduces training errors, it also increases overfitting risk. To find the optimal \\(M\\) value, it‚Äôs common to use cross validation. The depth of decision trees is another important control parameter in tree boosting. Deeper trees can capture more complex patterns but are more prone to memorizing the training data rather than learning generalizable patterns.\nThe improved predictive performance of gradient boosting relative to simpler models comes at a cost of reduced model transparency. While a single decision tree‚Äôs reasoning can be easily traced and understood, tracking the decision-making process across numerous trees becomes extremely complex and challenging. Gradient boosting is an excellent example of the inherent tradeoff between model simplicity and performance.\nLet‚Äôs now look at how can we use these methods in practice."
  },
  {
    "objectID": "blog/column-sampling-bootstrap.html#a-closer-look",
    "href": "blog/column-sampling-bootstrap.html#a-closer-look",
    "title": "Column-Sampling Bootstrap?",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nHere‚Äôs the basic algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRandomly sample columns from your dataset with replacement to create fake dataset.\nCompute their correlation coefficient.\nRepeat this many times.\nCompare your observed correlation to the distribution of these synthetic correlations.\nDeclare statistical significance if the observed correlation appears as an ‚Äúoutlier‚Äù in this synthetic distribution.\n\n\n\nThis approach allows you to explore a large number of possible correlations in a computationally efficient way. But does it actually work? Let‚Äôs unpack the key considerations.\nThe column-sampling bootstrap is most valuable when a dataset has many columns but too few rows for traditional bootstrap methods. The abundance of columns provides a rich sampling landscape. The goal is determining whether a correlation is significantly stronger or weaker than what might occur by chance. Let‚Äôs simplify the problem and ignore any issues stemming from being unable to estimate the correlation coefficients well enough.\n\n\nProblems\nHowever, several critical challenges exist. The method assumes columns are independent and identically distributed (i.i.d.), which rarely holds in practice. Columns often represent related variables‚Äîlike gene measurements or interconnected phenomena‚Äîand these dependencies can bias resampled correlations. Moreover, by resampling columns, you ignore row-level relationships, such as connections in time series or grouped data (like patients from the same household).\nInterpreting the null distribution presents another significant challenge. Synthetic correlation coefficients generated through column resampling might not represent a meaningful null hypothesis. If your dataset contains highly correlated features, the null distribution could shift, potentially leading to misleading conclusions. Unlike traditional bootstrapping‚Äîwhere samples reflect a subpopulation‚Äîthis method lacks that fundamental connection.\n\n\nThe Verdict\nWhile the column-sampling bootstrap is an intriguing concept, it will likely prove useful only in very specific, carefully constrained settings."
  },
  {
    "objectID": "blog/overview-ml-methods-ci.html#a-closer-look",
    "href": "blog/overview-ml-methods-ci.html#a-closer-look",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nCovariate Balancing Methods\nUnder the ignorability assumption, all confounding bias comes from differences in the covariates \\(X\\) between the treatment and the control groups. Intuitively, balancing these is enough to guarantee unbiasedness. One line of research develops methods to do exactly that ‚Äì directly equate covariates between the two groups of interest. These approaches circumvent estimation of the two nuisance functions mentioned above.\nThese are inspired by the ML view of data analysis framed as an optimization problem. Examples include Entropy Balancing, Genetic Matching, Stable Weights, and Residual Balancing. The last approach combines balancing with a regression adjustment to reduce extrapolation when estimating the counterfactuals for the treatment group. Some of these methods were designed with a low dimensional setting in mind, but they still carry the spirit of ML type of thinking.\nSoftware Packages: MatchIt, Ebal, BalanceHD.\n\n\nML Methods for the Propensity Score Model\nPropensity score methods rely on correctly specifying the PS model. In low-dimensional settings, it is possible to estimate it nonparametrically. In practice, however, this is unrealistic when data scientists have access to continuous or even bunch of discrete covariates. Can ML methods come to the rescue?\nIn principle, yes. A major challenge in this context, however, is the choice of a loss function. In the ML world loss functions target measures of fit (e.g., Root Mean Squared Error, log likelihood, etc.) but these would be problematic here as they do not aim at balancing covariates important to reduce bias. Thus, these methods do not perform very well unless used with much caution.\nImai and Ratkovic (2014) propose a PS method that directly balances covariates. Another choice is the Boosted CART implementation. As its name suggests, it iteratively forms a bunch of tree models and averages them, but with an appropriately chosen loss function. A series of simulation studies analyze the performance of various ML algorithms used to estimate the PS, but overall, these methods are nowadays dominated by some of the doubly robust approaches described below.\nSoftware Packages: TWANG, CBPS.\n\n\nML Methods for the Outcome Model\nWe can also estimate treatment effects directly by modelling the outcome variable. This also requires correct model specification, and even then, it is prone to extrapolation in finite samples. Examples include Bayesian Additive Regression Trees (BART) and other ensemble methods.\nBelloni et al.¬†(2014) show that the set of features optimal when estimating the outcome model, is not necessarily optimal for estimating treatment effects. The issue is omitting a variable that is correlated with the treatment even if its correlation with the outcome is only modest, can introduce considerable bias. Moreover, typically, the rate of convergence in this context when using ML models will be slower than \\(\\sqrt{n}\\), meaning that you will need much more data to get a good treatment effect estimate.\nOverall, there is no statistical theory of why ML methods should work well here, but some methods tend to perform well empirically. This brings us to the doubly robust approach.\nSoftware Packages: BART, rBART, BayesTree.\n\n\nML Methods for Both Models & Doubly Robust Methods\nMethods combining models for both the propensity score and the outcome have long been advocated. Intuitively, the propensity score can be seen as a balancing step after which regression adjustment can remove any remaining bias. Imbens (2015), for instance, promotes this type of thinking in matching methods specifically.\nDoubly robust (DR) estimators use both nuisance models and have the amazing property of being consistent even if only one of the two models is correctly specified. You can think of the bias term as a product of the biases in the two nuisance models ‚Äì if one of them is equal to zero, the entire term vanishes. Additionally, if both models are correctly specified, some methods are semiparametrically efficient, (i.e., ‚Äúthe best‚Äù in a large class of flexible models). A simple DR method is the Augmented Inverse Probability Weighting (AIPW) estimator which in linear models comes down to running weighted OLS regression of \\(Y\\) on \\(D\\) and \\(X\\) with the estimated (inverse) propensity score as weights.\nThere is more good news. Amazingly, DR methods can still converge at a rate \\(\\sqrt{N}\\) even if the underlying nuisance models converge at slower rates. The formal requirement is that the nuisance models must belong to something called a Donsker class. In simple words, they should not be too complex and prone to overfit.\nOne line of research has developed the Double ML framework. This work has been so influential that it deserves a blog post of its own. Without going into technical details, the authors of the original paper show that na√Øve application of ML methods when estimating both nuisance functions results in two types of biases ‚Äì regularization and overfitting. DoubleML makes use of something called Neyman orthogonalization (think of the Frisch‚ÄìWaugh‚ÄìLovell theorem) to remove the former, and sample splitting to avoid the latter. In simple settings, this method combines the residuals from regressions of \\(Y\\) on \\(X\\) and \\(Y\\) on \\(D\\), but in general it can take more complicated forms.\nAnother line of research has developed the Double Post Lasso approach. The idea here is simpler ‚Äì use Lasso to select covariates relevant to the outcome regression and then again to select ones relevant to the propensity score. Lastly, use OLS to regress \\(Y\\) on the union of the covariates selected previously. This procedure removes confounding or regularization bias that might be present in methods using ML models to estimate only one of the nuisance models.\nSoftware Packages: DoubleML, hdm, dlsr.\n\n\nHeterogeneous Treatment Effect Estimation\nMining for heterogeneous treatment effects has been a particularly fruitful field for ML methods. A leading example is the causal tree method developed by Athey and Imbens (2016). It resembles the traditional CART algorithm, but it uses a different criterion for splitting the data: instead of focusing on Mean Squared Error (MSE) for the outcome, it uses MSE for treatment effect. The result is a decision tree, in which units in each leaf have similar treatment effects. The method also features ‚Äúhonest‚Äù sample splitting for obtaining variance estimates ‚Äì one half of the data is used to determine the optimal tree, and the other half to estimate the treatment effects.\nBuilding on this idea, Wager and Athey (2018) propose a random forest-based method which generates a bunch of causal trees and averages the results to induce smoothness in the treatment effect‚Äôs function. Magically, the authors show the predictions are asymptotically normal and centered around the true value for each unit! This is exciting as it allows for standard methods for statistical inference.\nOther methods include BART, a Bayesian version of random forests mentioned above, Imai and Ratkovic (2013) who propose adding treatment indicators interacted with covariates in a LASSO regression to determine which variables are important for treatment effect heterogeneity. Similarly, Tian et al.¬†(2014) suggest modifying the covariates in a straightforward way and running a regression of the outcome on the modified variables without an intercept. Other methods include the \\(R\\)-learner of Nie and Wager (2021) which relies on estimating the two nuisance models described above and using a special loss function. K√ºnzel et al.¬†(2019) propose a \\(X\\)-learner metaalogrithm.\nSoftware Packages: FindIt, rlearner, grf, causalToolbox.\n\n\nOthers\nA by-product of estimating treatment effect heterogeneity is that we can determine which units should be treated. Intuitively, if the treatment effect is close to zero (or even negative) for some users, there is not much to be gained from the exposure. Kitagawa and Tetenov (2018) analyze a setting with limited complexity, and Athey and Wager (2021) develop the DoubleML framework discussed above for choosing whom to treat. ML has also been used for variance reduction in randomized experiments via regression adjustments. See, for instance, Wager et al.¬†(2016), Bloniarz et al.¬†(2016), and List et al.¬†(2022)."
  },
  {
    "objectID": "blog/bayesian-ab-tests.html#a-closer-look",
    "href": "blog/bayesian-ab-tests.html#a-closer-look",
    "title": "Bayesian Analysis of Randomized Experiments: A Modern Approach",
    "section": "A Closer Look",
    "text": "A Closer Look\nWe are interested in making inferences about the treatment effect, \\(\\tau\\), of the intervention \\(T\\) on the success rate \\(Y\\). In Bayesian terms, this means we seek the posterior distribution of \\(\\tau\\). Once we obtain this distribution or can draw observations from it, we can calculate various statistics to summarize the impact of the intervention, potentially providing a richer understanding of its effects.\nThe Bayesian methodology of analyzing experiments proceeds in four steps.\n\nStep 1: Specify the Prior Distribution\nWe begin by choosing the prior distributions for Y. The prior distribution represents our beliefs about the parameter before seeing the data. It is usually informed by previous A/B testing experience combined with deep context knowledge. For example, we might believe that the treatment effect is around two percentage points (\\(\\hat{\\tau}=0.02\\)) with some uncertainty around it.\nGiven our binary outcome, the Beta distribution serves as a natural conjugate prior: \\[ Y_i \\sim \\text{Beta} (\\alpha_i, \\beta_i), \\hspace{.5cm} i \\in \\{T,C\\}.  \\]\n\n\nStep 2: Specify the Likelihood Function\nFor binary outcomes, we model the data using a Binomial distribution:\n\\[X_i|Y_i \\sim \\text{Binomial}(N_i, Y_i), \\hspace{.5cm} i\\in\\{T,C\\}.  \\]\nThis choice reflects the inherent structure of our data: counting successes in a fixed number of trials. We are now ready to use the Bayes rule to arrive at the posterior distributions.\n\n\nStep 3: Posterior Distributions Derivation\nThanks to conjugacy between Beta and Binomial distributions, we obtain closed-form posterior distributions:\n\\[ Y_i | X_i \\sim \\text{Beta}(\\alpha_i+X_i, \\beta_i+N_i-X_i), \\hspace{.5cm} i\\in\\{T,C\\}  .\\]\nThis mathematical convenience allows us to avoid more complex numerical methods like MCMC sampling. We can now even plot the two posterior distributions and visually asses their differences.\n\n\nStep 4: Inference and Decision Making\nThe Bayesian framework enables rich analysis beyond traditional frequentist-style hypothesis testing. Here are some examples:\nProbability of Positive Impact\nWe can calculate the probability that the outcomes for the treatment group are higher than those of the control group. In closed form, we have:\n\\[ P(Y_T&gt;Y_C|X_T,X_C) = \\frac{1}{N}\\sum_i\\mathbf{1}(Y_{T,i}&gt;Y_{C,i}),\\]\nwhere \\(\\mathbf{1}(\\cdot)\\) is the indicator function. This is simply share of observations for which \\(Y_T &gt;Y_C\\).\nAverage Treatment Effect\nAnother potential example of an object of interest is the Average Treatment Effect (ATE):\n\\[ ATE = \\frac{1}{N_T}\\sum_{i \\in T}Y_{T,i} -  \\frac{1}{N_C}\\sum_{i \\in C}Y_{C,i} \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\n\\[ ATE \\rightarrow \\frac{\\alpha_T+X_T}{\\alpha_T+\\beta_T +N_T} - \\frac{\\alpha_C+X_C}{\\alpha_C+\\beta_C +N_C} \\]\nTo summarize, here is the high-level algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nSpecify the Prior Distribution\nSpecify the Likelihood Function\nPosterior Distributions Derivation\nInference and Decision Making\n\n\n\nAnd that‚Äôs it. You see how with the cost of additional assumptions we get much more than a single \\(p\\)-value, and that‚Äôs where much of the appeal of this approach lies.\n\n\nAdvantages\nOverall, Bayesian thinking entails some compelling advantages:\n\nIncorporation of Prior Information. It allows you to incorporate prior knowledge or expert opinion into the analysis. This is particularly useful when historical data or domain expertise is available.\nProbabilistic Interpretation: It provides direct probabilistic interpretations of the results, such as the probability that one variant is better than another. This may be more intuitive than frequentist p-values, which are often misinterpreted.\nHandling of Small Sample Sizes: It tends to perform better with small sample sizes because they incorporate prior distributions, which can regularize estimates and prevent overfitting.\nContinuous Learning: As new data comes in, Bayesian methods provide a natural way to update the posterior distribution, leading to continuous learning and adaptation.\n\nThe downsides are mostly related to the choice of prior distribution. In settings where there is a lack of expert knowledge, this Bayesian approach to experimentation might not be very attractive. Computation can also be an issue in more complex settings."
  },
  {
    "objectID": "blog/UNPUBLISHED-ols-fixed-random-x.html",
    "href": "blog/UNPUBLISHED-ols-fixed-random-x.html",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "",
    "text": "https://chatgpt.com/c/67b51e50-06d4-800b-abdf-51c2f5e4e7bf"
  },
  {
    "objectID": "blog/UNPUBLISHED-ols-fixed-random-x.html#background",
    "href": "blog/UNPUBLISHED-ols-fixed-random-x.html#background",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "Background",
    "text": "Background\nLinear regression stands as one of the fundamental workhorses of statistical modeling and is ubiquitous in applied work across fields like econometrics, biostatistics, and machine learning. Beneath its seemingly simple exterior lies a nuanced landscape of assumptions and theoretical considerations. One such distinction that often gets overlooked‚Äîeven by seasoned practitioners‚Äîis the difference between fixed and random regressors. This distinction, while subtle, has important implications for how we interpret our models, conduct inference, and understand uncertainty.\nThe goal of this article is to clarify the differences between fixed and random regressors in linear regression. We‚Äôll explore why this distinction matters and how it affects everything from your confidence intervals to your prediction errors. My goal is to help you develop both the technical understanding and the intuition needed to make informed modeling decisions in your day-to-day work."
  },
  {
    "objectID": "blog/UNPUBLISHED-ols-fixed-random-x.html#notation",
    "href": "blog/UNPUBLISHED-ols-fixed-random-x.html#notation",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "Notation",
    "text": "Notation\nTo set the stage, consider the standard linear regression model:\n\\[Y = X\\beta + \\varepsilon,\\]\nwhere:\n\n\\(Y\\) is an $ n $ vector of outcomes,\n\\(X\\) is an $ n p $ matrix of regressors (also called covariates or features),\n\\(\\beta\\) is a \\(p \\times 1\\) vector of coefficients to be estimated,\n\\(\\varepsilon\\) is an $ n $ vector of errors.\n\nThe least squares estimator of \\(\\beta\\) is given by:\n\\[ \\hat{\\beta} = (X'X)^{-1}X'Y. \\]\nThe key question is: What do we assume about \\(X\\)? If $ X $ is considered fixed, we condition on it when deriving properties of \\(\\hat{\\beta}\\). If \\(X\\) is random, we take expectations over its distribution. This affects how we think about the variance of our estimator, the validity of standard errors, and the asymptotic behavior of \\(\\hat{\\beta}\\)."
  },
  {
    "objectID": "blog/UNPUBLISHED-ols-fixed-random-x.html#a-closer-look",
    "href": "blog/UNPUBLISHED-ols-fixed-random-x.html#a-closer-look",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFixed Regressors\nIn the fixed regressor framework, we assume that \\(X\\) is determined beforehand‚Äîperhaps through experimental design or by conditioning on observed values. The randomness in our model comes solely from the error term \\(\\varepsilon\\).\nThe key assumptions typically made under this approach are:\n\nLinearity: The model is correctly specified as \\(Y = X\\beta + \\varepsilon\\).\nExogeneity: The errors satisfy $E[X] = 0 $, ensuring that there is no systematic relationship between \\(X\\) and \\(\\varepsilon\\).\nHomoskedasticity: The variance of errors is constant, i.e., \\(\\text{Var}(\\varepsilon \\mid X) = \\sigma^2 I_n\\).\nNo Perfect Multicollinearity: \\(X'X\\) is full rank, ensuring that the inverse \\((X'X)^{-1}\\) exists.\n\nIn the classical Gauss-Markov framework, the regressors \\(X\\) are treated as fixed. This means that we analyze the behavior of \\(\\hat{\\beta}\\) conditional on the observed \\(X\\).\nUnder these conditions, the ordinary least squares (OLS) estimator is unbiased and has the classical variance formula:\n\\[\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X'X)^{-1}.\\]\nAn important feature here is that all expectations and variances are conditional on \\(X\\). This conditioning makes sense because we‚Äôre treating \\(X\\) as fixed and known.\n\n\nRandom Regressors\nIn most real-world scenarios, our \\(X\\) variables aren‚Äôt actually fixed. We often sample observations from a population, making both \\(Y\\) and \\(X\\) random. The random regressor framework acknowledges this reality. Under this framework, we add assumptions about the distribution of \\(X\\).\nWhen \\(X\\) is treated as random, it is assumed to be drawn from some probability distribution. This changes how we analyze \\(\\hat{\\beta}\\), since expectations are now taken over both \\(\\varepsilon\\) and \\(X\\). The key assumptions in this setting are:\n\nJoint Distribution: \\((X, Y)\\) follows some joint distribution, meaning \\(X\\) is not fixed but rather drawn from a population.\nExogeneity in Expectation: Instead of conditioning on \\(X\\), we assume \\(E[\\varepsilon | X] = 0\\), ensuring that \\(\\varepsilon\\) is mean-independent of \\(X\\).\nLaw of Large Numbers: As \\(n \\to \\infty\\), the sample quantities \\(\\frac{1}{n} X'X\\) converge to their population analogs.\n\nA major consequence of assuming \\(X\\) is random is that the variance of \\(\\hat{\\beta}\\) takes a different form:\n\\[\\text{Var}(\\hat{\\beta}) = E[(X'X)^{-1} X' \\varepsilon \\varepsilon' X (X'X)^{-1}].\\]\nThis expression accounts for uncertainty in both $ X $ and $ $, and under large samples, it converges to the population variance.\n\n\nPractical Implications\nSo what‚Äôs the big deal? Here are some practical implications:\n\nInference: In the fixed regressor case, your inference is about the specific sample at hand. In the random regressor case, you‚Äôre making inference about the population relationship between \\(X\\) and \\(Y\\).\nPrediction Error: With fixed regressors, prediction error only accounts for the randomness in \\(\\varepsilon\\). With random regressors, you must also account for the randomness in future $ $ values.\nRobustness: The random regressor framework tends to provide more realistic assessments of model uncertainty, especially when extrapolating.\n\nLet me put this in more intuitive terms: if you‚Äôre designing an experiment where you precisely control the levels of your predictors, the fixed regressor framework makes sense. If you‚Äôre analyzing observational data where both predictors and responses are sampled from a population, the random regressor framework is more appropriate."
  },
  {
    "objectID": "blog/UNPUBLISHED-ols-fixed-random-x.html#bottom-line",
    "href": "blog/UNPUBLISHED-ols-fixed-random-x.html#bottom-line",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe distinction between fixed and random regressors affects estimation and inference.\nFixed regressors condition on \\(X\\), while random regressors integrate \\(X\\) over its distribution.\nOLS properties such as bias and variance differ under these assumptions.\nIn practice, whether \\(X\\) is fixed or random depends on the study design and intended inference."
  },
  {
    "objectID": "blog/UNPUBLISHED-ci-residualized-reg.html",
    "href": "blog/UNPUBLISHED-ci-residualized-reg.html",
    "title": "Causal Inference with Residualized Regressions",
    "section": "",
    "text": "mention Peng paper fill in XXX‚Äôs mention application for two separate datasets"
  },
  {
    "objectID": "blog/UNPUBLISHED-ci-residualized-reg.html#background",
    "href": "blog/UNPUBLISHED-ci-residualized-reg.html#background",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Background",
    "text": "Background\nWhen estimating causal effects, we often control for confounding variables to isolate the impact of a treatment. A common approach is multiple regression, where we regress the outcome on both the treatment and control variables. However, there‚Äôs a powerful yet sometimes underappreciated result in econometrics that offers an alternative perspective: the Frisch-Waugh-Lovell (FWL) theorem.\nFWL tells us that instead of running a full regression, we can achieve the same coefficient estimate for our treatment variable by first ‚Äúresidualizing‚Äù the outcome and the treatment on the controls and then regressing the residualized outcome on the residualized treatment. This result provides both computational and conceptual insights, particularly in high-dimensional settings or when thinking about causal inference intuitively.\nThis article will explain the theorem and how it applies to causal inference. We will explore why it works, its implications, and walk through an example with R and python code to solidify intuition.\nWhen we‚Äôre trying to estimate causal effects, we often find ourselves in a situation where we have a treatment variable of interest, an outcome we care about, and a whole bunch of control variables we need to account for. The standard approach is to throw everything into one big multiple regression model and look at the coefficient on our treatment variable. But there‚Äôs a more elegant way to think about this problem that provides both computational advantages and deeper intuition ‚Äì enter the Frisch-Waugh-Lovell (FWL) theorem.\nThe FWL theorem is one of those beautiful results that‚Äôs simultaneously profound and practical. At its core, it tells us that we can decompose a multiple regression into a sequence of simpler regressions. This decomposition not only helps us understand what‚Äôs really happening under the hood of our regression models but also provides a powerful framework for causal inference.\nIn this article, I want to walk you through how the FWL theorem works and why it‚Äôs so useful for causal inference. We‚Äôll explore how ‚Äúresidualizing‚Äù our variables ‚Äì essentially removing the parts of them that can be explained by control variables ‚Äì allows us to isolate and estimate treatment effects. Think of it as peeling away layers of an onion until we get to the core relationship we‚Äôre interested in.\nMy goal is to help you develop both a solid technical understanding and an intuitive grasp of this approach. Whether you‚Äôre working on observational studies, quasi-experiments, or just trying to make sense of complex datasets, the FWL theorem offers a powerful lens through which to view causal relationships."
  },
  {
    "objectID": "blog/UNPUBLISHED-ci-residualized-reg.html#notation",
    "href": "blog/UNPUBLISHED-ci-residualized-reg.html#notation",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Notation",
    "text": "Notation\nConsider a standard linear model:\n\\[Y = \\alpha + D\\tau + X\\beta + \\varepsilon, \\]\nwhere:\n\n\\(Y\\) is the outcome variable (e.g., earnings),\n\\(D\\) is the treatment variable (e.g., whether a person attended a job training program),\n\\(X\\) is a vector of control variables (e.g., age, education, experience),\n\\(\\tau\\) is the treatment effect we want to estimate,\n\\(\\beta\\) represents the coefficients on the controls,\n\\(\\varepsilon\\) is the error term."
  },
  {
    "objectID": "blog/UNPUBLISHED-ci-residualized-reg.html#a-closer-look",
    "href": "blog/UNPUBLISHED-ci-residualized-reg.html#a-closer-look",
    "title": "Causal Inference with Residualized Regressions",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe FWL Theorem\nThe OLS estimate of \\(\\tau\\) in the full regression includes both \\(D\\) and \\(X\\). However, FWL tells us that we can obtain the same estimate of \\(\\tau\\) by following these steps:\n\nRegress \\(Y\\) on \\(X\\) and collect the residuals \\(\\tilde{Y}\\).\nRegress \\(D\\) on \\(X\\) and collect the residuals \\(\\tilde{D}\\).\nRegress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) (without an intercept). The coefficient on \\(\\tilde{D}\\) is exactly \\(\\tau\\).\n\n\n\nIntuition\nThe intuition behind FWL is simple yet profound. When we regress \\(Y\\) on \\(X\\), we strip out the variation in \\(Y\\) that is explained by \\(X\\), leaving only the part orthogonal to \\(X\\). Similarly, regressing \\(D\\) on \\(X\\) removes the influence of \\(X\\) on \\(D\\), isolating the component of \\(D\\) that is independent of \\(X\\). Since X has been accounted for in both cases, the regression of \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) retrieves the direct relationship between \\(D\\) and \\(Y\\), net of \\(X\\).\nMathematically, the key result of FWL is:\n\\[\\hat{\\tau} = (D' M_X D)^{-1} D' M_X Y,\\]\nwhere \\(M_X = I - X(X'X)^{-1}X'\\) is the projection matrix that residualizes variables with respect to \\(X\\). This shows that the estimate of remains unchanged whether we use the full regression or the residualized regression.\nSo what‚Äôs really happening here? The residuals \\(XXX\\) represent the part of the treatment variable that cannot be explained by the control variables. Similarly, \\(XXX\\) ‚Äã represents the part of the outcome that cannot be explained by the controls.\nThe FWL procedure essentially isolates the relationship between treatment and outcome after removing the influence of all the control variables. It‚Äôs as if we‚Äôre looking at the relationship in a ‚Äúpurified‚Äù form, with all the confounding effects of the controls stripped away.\nThis provides a powerful framework for causal inference. Under the assumption that we‚Äôve included all relevant confounders in our control set, the coefficient \\(XXX\\) from the residualized regression can be interpreted as the causal effect of the treatment on the outcome.\nThink of it this way: we‚Äôre first ‚Äúadjusting‚Äù both our treatment and outcome variables by removing the predictable parts based on our controls. Then we‚Äôre examining how the ‚Äúadjusted‚Äù treatment relates to the ‚Äúadjusted‚Äù outcome. This residual-on-residual regression gives us our causal estimate.\n\n\nGeometric Interpretation\nGeometrically, we can think of the FWL theorem in terms of projections in vector space. The residuals \\(XXXX\\) and \\(\\tilde{\\mathbf{y}}\\)‚Äã are what remain after projecting \\(D\\) and \\(Y\\) onto the orthogonal complement of the space spanned by \\(Z\\).\nIn other words, we‚Äôre looking at the components of \\(D\\) and \\(Y\\) that are orthogonal to (i.e., cannot be explained by) the control variables. The relationship between these orthogonal components gives us our treatment effect estimate.\n\n\nVariance and Standard Errors\nThe precision of our treatment effect estimate depends on how much variation in the treatment remains after accounting for the controls. If the treatment is highly collinear with the controls (meaning \\(XXXX\\) has little variation), our estimate will be imprecise.\nThis helps us understand the ‚Äúcurse of dimensionality‚Äù in causal inference. As we add more control variables, we might reduce omitted variable bias, but we also risk reducing the effective variation in our treatment, leading to less precise estimates.\n\n\nPractical Implications\n\nConceptual clarity: FWL emphasizes that controlling for \\(X\\) means adjusting both \\(Y\\) and \\(D\\) before examining their relationship.\nComputational benefits: In high-dimensional settings, it is often more efficient to work with residualized variables rather than estimating the full model.\nInstrumental variables and two-stage regression: The residualization step is analogous to first-stage regressions in instrumental variable estimation.\nwith privacy data it might be that two datasets"
  },
  {
    "objectID": "blog/UNPUBLISHED-ci-residualized-reg.html#an-example",
    "href": "blog/UNPUBLISHED-ci-residualized-reg.html#an-example",
    "title": "Causal Inference with Residualized Regressions",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs go through an example in R and python. We will generate synthetic data where the treatment effect is nonzero and show that both the full regression and the residualized approach give the same estimate.\n\nRPython\n\n\nrm(list=ls())\nset.seed(42)\nn &lt;- 1000\nX &lt;- matrix(rnorm(n * 3), ncol = 3)  # Three control variables\nD &lt;- 0.5 * X[,1] + 0.3 * X[,2] + rnorm(n)  # Treatment depends on controls\nY &lt;- 2 * D + 1.5 * X[,1] - 0.5 * X[,2] + 0.3 * X[,3] + rnorm(n)  # Outcome model\n\n# Full Regression\nfull_model &lt;- lm(Y ~ D + X)\nsummary(full_model)coefficients[\"D\",]  \n\n### Residualized Regression \nY_resid &lt;- residuals(lm(Y ~ X)) \nD_resid &lt;- residuals(lm(D ~ X)) \nresid_model &lt;- lm(Y_resid ~ D_resid - 1)  # No intercept \nsummary(resid_model)coefficients[\"D_resid\",]\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data\nn = 1000\nX = np.random.normal(size=(n, 3))  # Three control variables\nD = 0.5 * X[:, 0] + 0.3 * X[:, 1] + np.random.normal(size=n)  # Treatment depends on controls\nY = 2 * D + 1.5 * X[:, 0] - 0.5 * X[:, 1] + 0.3 * X[:, 2] + np.random.normal(size=n)  # Outcome model\n\n# Full Regression\nfull_model = LinearRegression()\nfull_model.fit(np.column_stack((D, X)), Y)\nfull_coef = full_model.coef_[0]  # Coefficient for D\nprint(f\"Full Regression Coefficient for D: {full_coef}\")\n\n# Residualized Regression\n# Step 1: Residualize Y with respect to X\nY_resid_model = LinearRegression()\nY_resid_model.fit(X, Y)\nY_resid = Y - Y_resid_model.predict(X)\n\n# Step 2: Residualize D with respect to X\nD_resid_model = LinearRegression()\nD_resid_model.fit(X, D)\nD_resid = D - D_resid_model.predict(X)\n\n# Step 3: Regress residualized Y on residualized D\nresid_model = LinearRegression(fit_intercept=False)\nresid_model.fit(D_resid.reshape(-1, 1), Y_resid)\nresid_coef = resid_model.coef_[0]\nprint(f\"Residualized Regression Coefficient for D: {resid_coef}\")\n\n\n\nThe coefficient on \\(D\\) in the full model and the coefficient on \\(D_{resid}\\) in the residualized model will be identical. This demonstrates that controlling for \\(X\\) can be done implicitly by working with residuals."
  },
  {
    "objectID": "blog/UNPUBLISHED-ci-residualized-reg.html#bottom-line",
    "href": "blog/UNPUBLISHED-ci-residualized-reg.html#bottom-line",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe FWL theorem shows that controlling for variables in a regression can be done by residualizing first.\nThis approach helps conceptually separate the treatment effect from confounding influences.\nResidualization is particularly useful in high-dimensional settings and instrumental variables estimation.\nWhether you use the full regression or the residualized approach, you get the same treatment effect estimate."
  },
  {
    "objectID": "blog/UNPUBLISHED-ci-residualized-reg.html#references",
    "href": "blog/UNPUBLISHED-ci-residualized-reg.html#references",
    "title": "Causal Inference with Residualized Regressions",
    "section": "References",
    "text": "References\nDing, P. (2021). The Frisch‚ÄìWaugh‚ÄìLovell theorem for standard errors. Statistics & Probability Letters, 168, 108945."
  },
  {
    "objectID": "blog/UNPUBLISHED-ols-fixed-random-x.html#references",
    "href": "blog/UNPUBLISHED-ols-fixed-random-x.html#references",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "References",
    "text": "References\nGreene, W. H. (2012). Econometric Analysis. Pearson. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springe"
  },
  {
    "objectID": "blog/UNPUBLISHED-unconditional-qreg.html",
    "href": "blog/UNPUBLISHED-unconditional-qreg.html",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "mention rank invariance"
  },
  {
    "objectID": "blog/UNPUBLISHED-unconditional-qreg.html#background",
    "href": "blog/UNPUBLISHED-unconditional-qreg.html#background",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Background",
    "text": "Background\nQuantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog/UNPUBLISHED-unconditional-qreg.html#notation",
    "href": "blog/UNPUBLISHED-unconditional-qreg.html#notation",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Notation",
    "text": "Notation\nWe consider an outcome variable \\(Y\\) and a set of covariates \\(X\\). In traditional quantile regression, we estimate the conditional quantile function:\n\\[Q_\\tau(Y | X) = \\inf \\{ q : P(Y \\leq q | X) \\geq \\tau \\}.\\]\nThis tells us how the \\(\\tau\\)-th quantile of Y changes with \\(X\\). However, in many applications, we want to model the unconditional quantiles:\n\\[Q_\\tau(Y) = \\inf \\{ q : P(Y \\leq q) \\geq \\tau \\}.\\]\nUQR allows us to estimate how covariates influence these unconditional quantiles."
  },
  {
    "objectID": "blog/UNPUBLISHED-unconditional-qreg.html#a-closer-look",
    "href": "blog/UNPUBLISHED-unconditional-qreg.html#a-closer-look",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nBasics: Conditional vs.¬†Unconditional Quantiles\nThe fundamental distinction between conditional and unconditional quantile regressions is best understood through an example. Suppose we are studying wage distributions. A conditional quantile regression would estimate the effect of education on the wage quantile within a specific subgroup (e.g., workers with 5 years of experience). But what if we want to know the effect of education on the overall wage distribution? That‚Äôs where unconditional quantiles come in‚Äîthey capture the total effect of education, accounting for all pathways through which education might influence wages.\n\n\nUnconditional Quantile Regression\nFirpo et al.¬†introduced an elegant way to estimate UQR using influence functions. The influence function of a statistic measures how much that statistic changes when an observation is perturbed. The recentered influence function (RIF) for a quantile \\(Q_\\tau\\) is given by:\n\\[RIF(Y; Q_\\tau) = Q_\\tau + \\frac{\\tau - 1\\{Y \\leq Q_\\tau\\}}{f_Y(Q_\\tau)}.\\]\nHere, \\(f_Y(Q_\\tau)\\) is the density of \\(Y\\) at \\(Q_\\tau\\), which can be estimated nonparametrically.\nFirpo et al.¬†showed that regressing \\(RIF(Y; Q_\\tau)\\) on covariates \\(X\\) via OLS provides a valid estimate of how \\(X\\) affects the \\(\\tau\\)-th quantile of \\(Y\\). This method is remarkably simple but powerful‚Äîit transforms a quantile regression problem into a standard linear regression problem.\n\n\nUnconditional Quantile Treatment Effects\nOne limitation of UQR as formulated by Firpo et al.¬†is that it assumes covariates are exogenous. But in many causal inference settings, treatment assignment is endogenous (e.g., workers self-select into training programs). Fr√∂lich and Melly (2013) extended the UQR framework to handle endogeneity using instrumental variables (IV).\nThey showed that under standard IV assumptions, the unconditional quantile treatment effect (UQTE) can be estimated using a two-step approach:\n\nFirst, estimate a propensity score model (or an instrumented version of \\(D\\)) to account for selection bias.\nThen, apply RIF regression to estimate the effect of the treatment on unconditional quantiles.\n\nThis approach provides a way to estimate distributional treatment effects while addressing selection bias‚Äîa crucial tool in policy evaluation and applied econometrics."
  },
  {
    "objectID": "blog/UNPUBLISHED-unconditional-qreg.html#an-example",
    "href": "blog/UNPUBLISHED-unconditional-qreg.html#an-example",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate these ideas with an example in R. We‚Äôll use the iris dataset to estimate the effect of Sepal.Length on different quantiles of Petal.Length using UQR.\n\nRPython\n\n\nrm(list=ls())\nlibrary(quantreg)\n\n# Load dataset\ndata(iris)\n\n# Estimate unconditional quantiles\ntaus &lt;- c(0.25, 0.50, 0.75)\nq_vals &lt;- quantile(iris$Petal.Length, probs = taus)  # Estimate quantiles\nf_hat &lt;- density(iris$Petal.Length)\n\n# Compute RIF values\nrif_values &lt;- lapply(1:3, function(i) {\n  q &lt;- q_vals[i]\n  f &lt;- f_hat$y[which.min(abs(f_hat$x - q))]\n  q + ((taus[i] - (iris$Petal.Length &lt;= q)) / f)\n})\n\n# Run RIF regression\nmodels &lt;- lapply(rif_values, function(rif) lm(rif ~ Sepal.Length, data = iris))\n\n# Print results\nlapply(models, summary)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Estimate unconditional quantiles\ntaus = [0.25, 0.50, 0.75]\nq_vals = np.quantile(iris['Petal.Length'], taus)  # Estimate quantiles\nf_hat = gaussian_kde(iris['Petal.Length'])\n\n# Compute RIF values\nrif_values = []\nfor i, tau in enumerate(taus):\n    q = q_vals[i]\n    f = f_hat(q)  # Density at the quantile\n    rif = q + ((tau - (iris['Petal.Length'] &lt;= q).astype(int)) / f)\n    rif_values.append(rif)\n\n# Run RIF regression\nmodels = []\nfor rif in rif_values:\n    model = LinearRegression(fit_intercept=True)\n    model.fit(iris[['Sepal.Length']], rif)\n    models.append(model)\n\n# Print results\nfor i, model in enumerate(models):\n    print(f\"Model {i + 1}:\")\n    print(f\"Coefficient for Sepal.Length: {model.coef_[0]}\")\n    print(f\"Intercept: {model.intercept_}\")\n\n\n\nThis simple example demonstrates how to estimate the effect of a covariate on unconditional quantiles using the RIF regression approach."
  },
  {
    "objectID": "blog/UNPUBLISHED-unconditional-qreg.html#bottom-line",
    "href": "blog/UNPUBLISHED-unconditional-qreg.html#bottom-line",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nUQR allows us to estimate the effect of covariates on unconditional quantiles, capturing total effects.\nThe RIF regression method transforms a quantile regression problem into a simple linear regression.\nFr√∂lich and Melly (2013) extend UQR to address endogeneity using instrumental variables.\nThese tools are invaluable for policy evaluation and causal inference."
  },
  {
    "objectID": "blog/UNPUBLISHED-unconditional-qreg.html#where-to-learn-more",
    "href": "blog/UNPUBLISHED-unconditional-qreg.html#where-to-learn-more",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive into these methods, the foundational paper by Firpo, Fortin, and Lemieux (2009) provides a detailed introduction to UQR, while Fr√∂lich and Melly (2013) extend the framework to address endogeneity concerns. For a broader perspective on quantile regression, Koenker‚Äôs book Quantile Regression (2005) is a must-read."
  },
  {
    "objectID": "blog/UNPUBLISHED-unconditional-qreg.html#references",
    "href": "blog/UNPUBLISHED-unconditional-qreg.html#references",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "References",
    "text": "References\nAlejo, J., Favata, F., Montes-Rojas, G., & Trombetta, M. (2021). Conditional vs unconditional quantile regression models: A guide to practitioners. Econom√≠a, 44(88), 76-93.\nBorah, B. J., & Basu, A. (2013). Highlighting differences between conditional and unconditional quantile regression approaches through an application to assess medication adherence. Health economics, 22(9), 1052-1070.\nBorgen, N. T. (2016). Fixed effects in unconditional quantile regression. The Stata Journal, 16(2), 403-415.\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional quantile regressions. Econometrica, 77(3), 953-973.\nFr√∂lich, M., & Melly, B. (2013). Unconditional quantile treatment effects under endogeneity. Journal of Business & Economic Statistics, 31(3), 346-357.\nSasaki, Y., Ura, T., & Zhang, Y. (2022). Unconditional quantile regression with high‚Äêdimensional data. Quantitative Economics, 13(3), 955-978."
  },
  {
    "objectID": "blog/brief-intro-e-values.html",
    "href": "blog/brief-intro-e-values.html",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. This means they allow for data-driven alpha selection, making them particularly useful in real-world scientific and policy decisions where fixed significance thresholds are impractical.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog/brief-intro-e-values.html#background",
    "href": "blog/brief-intro-e-values.html#background",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. This means they allow for data-driven alpha selection, making them particularly useful in real-world scientific and policy decisions where fixed significance thresholds are impractical.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog/brief-intro-e-values.html#notation",
    "href": "blog/brief-intro-e-values.html#notation",
    "title": "A Brief Introduction to E-Values",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs set up the basic notation. Suppose we are testing a null hypothesis \\(H_0\\) against an alternative \\(H_1\\) based on observed data \\(Y\\). In classical Neyman‚ÄìPearson testing, we define a test statistic T(Y) and derive a \\(p\\)-value:\n\\[P(Y) = P_{H_0}(T(Y) \\geq T_{obs}),\\]\nwhere \\(P_{H_0}\\) represents probability under the null hypothesis and \\(T_{obs}\\) is the observed test statistic."
  },
  {
    "objectID": "blog/brief-intro-e-values.html#a-closer-look",
    "href": "blog/brief-intro-e-values.html#a-closer-look",
    "title": "A Brief Introduction to E-Values",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nE-values replace this with an alternative statistic, the e-variable, denoted as \\(S(Y)\\), which satisfies:\n\\[E_{H_0}[S(Y)] \\leq 1.\\]\nThis ensures that the e-value does not, on average, exceed 1 under the null, providing a valid way to quantify evidence against \\(H_0\\).\n\n\nWhy E-Values?\nThe fundamental problem with \\(p\\)-values is their lack of a clear decision-theoretic interpretation. They are often misunderstood and misused, leading to issues such as the replication crisis in scientific research. E-values address these issues by offering:\n\nInterpretability: Large e-values provide direct evidence against \\(H_0\\), unlike small \\(p\\)-values, which are difficult to interpret without a fixed alpha threshold.\nOptional Stopping: Because e-values maintain their validity even when a study stops based on interim results, they are useful in sequential testing.\nPost Hoc Decision-Making: Since e-values allow for Type-I risk control after seeing the data, they facilitate more flexible decision-making.\n\n\n\nConstructing the E-Values\nA common way to define an e-value is through a likelihood ratio:\n\\[S(Y) = \\frac{P_{H_1}(Y)}{P_{H_0}(Y)}.\\]\nThis is similar to a Bayes factor but differs in that it does not require a prior distribution over hypotheses. Another approach is to construct empirical e-values based on resampling methods or alternative test statistics that satisfy the expectation constraint.\nA more general definition involves e-processes, which allow for sequential testing. An e-process \\(\\{S_t\\}_{t=1}^{\\infty}\\) is a sequence of nonnegative random variables satisfying:\n\\[E_{H_0}[S_t | S_1, \\dots, S_{t-1}] \\leq S_{t-1}, \\quad \\forall t.\\]\nThis property ensures that the sequence remains a valid e-value throughout a study, making it particularly powerful for adaptive testing procedures.\nAnother important construction is via supermartingales. An e-value can be defined as a nonnegative random variable \\(S(Y)\\) such that \\(\\{S_t\\}\\) forms a nonnegative supermartingale under \\(H_0\\):\n\\[E_{H_0}[S_t | S_{t-1}] \\leq S_{t-1},\\]\nwhich guarantees that the expectation does not increase under the null hypothesis."
  },
  {
    "objectID": "blog/brief-intro-e-values.html#comparison",
    "href": "blog/brief-intro-e-values.html#comparison",
    "title": "A Brief Introduction to E-Values",
    "section": "Comparison",
    "text": "Comparison\nProperty | \\(P\\)-Value | E-Value |\n|‚Äî‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äì| | Interpretability | Indirect (requires fixed alpha) | Direct (large values indicate evidence) | | Optional Stopping | No | Yes | | Sequential Analysis | Problematic | Natural | | Post Hoc Decisions | Not well-defined | Valid risk guarantees |"
  },
  {
    "objectID": "blog/brief-intro-e-values.html#an-example",
    "href": "blog/brief-intro-e-values.html#an-example",
    "title": "A Brief Introduction to E-Values",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs consider a simple hypothesis test using the iris dataset in R. We will test whether the mean Sepal.Length differs between two species using both a \\(p\\)-value and an e-value.\n# Load Data\nrm(list=ls())\nlibrary(dplyr)\nlibrary(tibble)\n\n# Load dataset\ndata(iris)\niris_filtered &lt;- iris %&gt;% filter(Species %in% c(\"setosa\", \"versicolor\"))\n\n### Compute P-Value (Traditional T-Test)\np_value &lt;- t.test(Sepal.Length ~ Species, data = iris_filtered)$p.value\nprint(p_value)\n\n### Compute E-Value\n# Define likelihood under each hypothesis\nlikelihood_ratio &lt;- function(y, mu0, mu1, sigma) {\n  dnorm(y, mean = mu1, sd = sigma) / dnorm(y, mean = mu0, sd = sigma)\n}\n\n# Estimate parameters\nmu0 &lt;- mean(iris_filtered$Sepal.Length[iris_filteredSpecies == \"setosa\"]) \nmu1 &lt;- mean(iris_filtered$Sepal.Length[iris_filteredSpecies == \"versicolor\"])\nsigma &lt;- sd(iris_filtered$Sepal.Length)\n\n# Compute e-value\ne_values &lt;- sapply(iris_filtered$Sepal.Length, likelihood_ratio, mu0, mu1, sigma)\ne_value &lt;- mean(e_values)\nprint(e_value)\nThe e-value directly quantifies the strength of evidence against the null. Unlike the \\(p\\)-value, which requires an arbitrary threshold (e.g., \\(0.05\\)) to make a decision, the e-value provides a more interpretable measure of support for \\(H_1\\)."
  },
  {
    "objectID": "blog/brief-intro-e-values.html#bottom-line",
    "href": "blog/brief-intro-e-values.html#bottom-line",
    "title": "A Brief Introduction to E-Values",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nE-values provide a decision-theoretic alternative to \\(p\\)-values, offering clearer evidence quantification.\nThey naturally handle optional stopping and sequential testing, unlike \\(p\\)-values.\nThey allow for flexible, post hoc decision-making, making them particularly useful in real-world applications.\nLikelihood ratios and supermartingales serve as a natural basis for constructing e-values, making them conceptually simple yet powerful."
  },
  {
    "objectID": "blog/brief-intro-e-values.html#where-to-learn-more",
    "href": "blog/brief-intro-e-values.html#where-to-learn-more",
    "title": "A Brief Introduction to E-Values",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deep dive into e-values, Gr√ºnwald (2024) provides a rigorous mathematical foundation, discussing their application in decision theory and hypothesis testing. Another useful resource is Shafer (2021), which explores connections between e-values and likelihood ratios. For practical applications, Vovk & Wang (2019) discuss the use of e-values in sequential analysis and machine learning contexts."
  },
  {
    "objectID": "blog/brief-intro-e-values.html#references",
    "href": "blog/brief-intro-e-values.html#references",
    "title": "A Brief Introduction to E-Values",
    "section": "References",
    "text": "References\nGr√ºnwald, P. (2024). Beyond Neyman-Pearson: E-values enable hypothesis testing with a data-driven alpha. PNAS.\nShafer, G. (2021). Testing by betting: A strategy for statistical and scientific communication. Cambridge University Press.\nVovk, V., & Wang, R. (2019). E-values: Calibration, combination, and applications. Journal of the Royal Statistical Society."
  },
  {
    "objectID": "blog/UNPUBLISHED-brief-intro-e-values.html",
    "href": "blog/UNPUBLISHED-brief-intro-e-values.html",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. This means they allow for data-driven alpha selection, making them particularly useful in real-world scientific and policy decisions where fixed significance thresholds are impractical.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog/UNPUBLISHED-brief-intro-e-values.html#background",
    "href": "blog/UNPUBLISHED-brief-intro-e-values.html#background",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. This means they allow for data-driven alpha selection, making them particularly useful in real-world scientific and policy decisions where fixed significance thresholds are impractical.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog/UNPUBLISHED-brief-intro-e-values.html#notation",
    "href": "blog/UNPUBLISHED-brief-intro-e-values.html#notation",
    "title": "A Brief Introduction to E-Values",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs set up the basic notation. Suppose we are testing a null hypothesis \\(H_0\\) against an alternative \\(H_1\\) based on observed data \\(Y\\). In classical Neyman‚ÄìPearson testing, we define a test statistic T(Y) and derive a \\(p\\)-value:\n\\[P(Y) = P_{H_0}(T(Y) \\geq T_{obs}),\\]\nwhere \\(P_{H_0}\\) represents probability under the null hypothesis and \\(T_{obs}\\) is the observed test statistic."
  },
  {
    "objectID": "blog/UNPUBLISHED-brief-intro-e-values.html#a-closer-look",
    "href": "blog/UNPUBLISHED-brief-intro-e-values.html#a-closer-look",
    "title": "A Brief Introduction to E-Values",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nE-values replace this with an alternative statistic, the e-variable, denoted as \\(S(Y)\\), which satisfies:\n\\[E_{H_0}[S(Y)] \\leq 1.\\]\nThis ensures that the e-value does not, on average, exceed 1 under the null, providing a valid way to quantify evidence against \\(H_0\\).\n\n\nWhy E-Values?\nThe fundamental problem with \\(p\\)-values is their lack of a clear decision-theoretic interpretation. They are often misunderstood and misused, leading to issues such as the replication crisis in scientific research. E-values address these issues by offering:\n\nInterpretability: Large e-values provide direct evidence against \\(H_0\\), unlike small \\(p\\)-values, which are difficult to interpret without a fixed alpha threshold.\nOptional Stopping: Because e-values maintain their validity even when a study stops based on interim results, they are useful in sequential testing.\nPost Hoc Decision-Making: Since e-values allow for Type-I risk control after seeing the data, they facilitate more flexible decision-making.\n\n\n\nConstructing the E-Values\nA common way to define an e-value is through a likelihood ratio:\n\\[S(Y) = \\frac{P_{H_1}(Y)}{P_{H_0}(Y)}.\\]\nThis is similar to a Bayes factor but differs in that it does not require a prior distribution over hypotheses. Another approach is to construct empirical e-values based on resampling methods or alternative test statistics that satisfy the expectation constraint.\nA more general definition involves e-processes, which allow for sequential testing. An e-process \\(\\{S_t\\}_{t=1}^{\\infty}\\) is a sequence of nonnegative random variables satisfying:\n\\[E_{H_0}[S_t | S_1, \\dots, S_{t-1}] \\leq S_{t-1}, \\quad \\forall t.\\]\nThis property ensures that the sequence remains a valid e-value throughout a study, making it particularly powerful for adaptive testing procedures.\nAnother important construction is via supermartingales. An e-value can be defined as a nonnegative random variable \\(S(Y)\\) such that \\(\\{S_t\\}\\) forms a nonnegative supermartingale under \\(H_0\\):\n\\[E_{H_0}[S_t | S_{t-1}] \\leq S_{t-1},\\]\nwhich guarantees that the expectation does not increase under the null hypothesis."
  },
  {
    "objectID": "blog/UNPUBLISHED-brief-intro-e-values.html#comparison",
    "href": "blog/UNPUBLISHED-brief-intro-e-values.html#comparison",
    "title": "A Brief Introduction to E-Values",
    "section": "Comparison",
    "text": "Comparison\n\n\n\n\n\n\n\n\n\nProperty\n\\(P\\)-Value\nE-Value\n\n\n\n\nInterpretability\nIndirect (requires fixed \\(\\alpha\\))\nDirect (large values indicate evidence)\n\n\nOptional Stopping\nNo\nYes\n\n\nSequential Analysis\nProblematic\nNatural\n\n\nPost Hoc Decisions\nNot well-defined\nValid risk guarantees"
  },
  {
    "objectID": "blog/UNPUBLISHED-brief-intro-e-values.html#an-example",
    "href": "blog/UNPUBLISHED-brief-intro-e-values.html#an-example",
    "title": "A Brief Introduction to E-Values",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs consider a simple hypothesis test using the iris dataset in R. We will test whether the mean Sepal.Length differs between two species using both a \\(p\\)-value and an e-value.\n\nRPython\n\n\n# Load Data\nrm(list=ls())\nlibrary(dplyr)\nlibrary(tibble)\n\n# Load dataset\ndata(iris)\niris_filtered &lt;- iris %&gt;% filter(Species %in% c(\"setosa\", \"versicolor\"))\n\n# Compute P-Value (Traditional T-Test)\np_value &lt;- t.test(Sepal.Length ~ Species, data = iris_filtered)$p.value\nprint(p_value)\n\n# Compute E-Value\n# Define likelihood under each hypothesis\nlikelihood_ratio &lt;- function(y, mu0, mu1, sigma) {\n  dnorm(y, mean = mu1, sd = sigma) / dnorm(y, mean = mu0, sd = sigma)\n}\n\n# Estimate parameters\nmu0 &lt;- mean(iris_filtered$Sepal.Length[iris_filteredSpecies == \"setosa\"]) \nmu1 &lt;- mean(iris_filtered$Sepal.Length[iris_filteredSpecies == \"versicolor\"])\nsigma &lt;- sd(iris_filtered$Sepal.Length)\n\n# Compute e-value\ne_values &lt;- sapply(iris_filtered$Sepal.Length, likelihood_ratio, mu0, mu1, sigma)\ne_value &lt;- mean(e_values)\nprint(e_value)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind, norm\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris['Species'] = iris_data['target']\niris['Species'] = iris['Species'].replace({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Filter dataset for two species\niris_filtered = iris[iris['Species'].isin(['setosa', 'versicolor'])]\n\n# Compute P-Value (Traditional T-Test)\nsetosa = iris_filtered[iris_filtered['Species'] == 'setosa']['Sepal.Length']\nversicolor = iris_filtered[iris_filtered['Species'] == 'versicolor']['Sepal.Length']\np_value = ttest_ind(setosa, versicolor).pvalue\nprint(f\"P-Value: {p_value}\")\n\n# Compute E-Value\n# Define likelihood ratio function\ndef likelihood_ratio(y, mu0, mu1, sigma):\n    return norm.pdf(y, loc=mu1, scale=sigma) / norm.pdf(y, loc=mu0, scale=sigma)\n\n# Estimate parameters\nmu0 = setosa.mean()\nmu1 = versicolor.mean()\nsigma = iris_filtered['Sepal.Length'].std()\n\n# Compute e-value\ne_values = [likelihood_ratio(y, mu0, mu1, sigma) for y in iris_filtered['Sepal.Length']]\ne_value = np.mean(e_values)\nprint(f\"E-Value: {e_value}\")\n\n\n\nThe e-value directly quantifies the strength of evidence against the null. Unlike the \\(p\\)-value, which requires an arbitrary threshold (e.g., \\(0.05\\)) to make a decision, the e-value provides a more interpretable measure of support for \\(H_1\\)."
  },
  {
    "objectID": "blog/UNPUBLISHED-brief-intro-e-values.html#bottom-line",
    "href": "blog/UNPUBLISHED-brief-intro-e-values.html#bottom-line",
    "title": "A Brief Introduction to E-Values",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nE-values provide a decision-theoretic alternative to \\(p\\)-values, offering clearer evidence quantification.\nThey naturally handle optional stopping and sequential testing, unlike \\(p\\)-values.\nThey allow for flexible, post hoc decision-making, making them particularly useful in real-world applications.\nLikelihood ratios and supermartingales serve as a natural basis for constructing e-values, making them conceptually simple yet powerful."
  },
  {
    "objectID": "blog/UNPUBLISHED-brief-intro-e-values.html#where-to-learn-more",
    "href": "blog/UNPUBLISHED-brief-intro-e-values.html#where-to-learn-more",
    "title": "A Brief Introduction to E-Values",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deep dive into e-values, Gr√ºnwald (2024) provides a rigorous mathematical foundation, discussing their application in decision theory and hypothesis testing. Another useful resource is Shafer (2021), which explores connections between e-values and likelihood ratios. For practical applications, Vovk & Wang (2019) discuss the use of e-values in sequential analysis and machine learning contexts."
  },
  {
    "objectID": "blog/UNPUBLISHED-brief-intro-e-values.html#references",
    "href": "blog/UNPUBLISHED-brief-intro-e-values.html#references",
    "title": "A Brief Introduction to E-Values",
    "section": "References",
    "text": "References\nGr√ºnwald, P. (2024). Beyond Neyman-Pearson: E-values enable hypothesis testing with a data-driven alpha. PNAS.\nShafer, G. (2021). Testing by betting: A strategy for statistical and scientific communication. Cambridge University Press.\nVovk, V., & Wang, R. (2019). E-values: Calibration, combination, and applications. Journal of the Royal Statistical Society."
  },
  {
    "objectID": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html",
    "href": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "",
    "text": "use the model by Aronow and Samii but that‚Äôs asymptotically\nZubisaretta show a finite sample formula\nSloczynski focuses on heterogeneous. Show it briefly\nAngrist focuses on saturated models. impossible with continuous X\nOLS regressions do not implicitly assume PO framework"
  },
  {
    "objectID": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#background",
    "href": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#background",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "Background",
    "text": "Background\nOrdinary Least Squares (OLS) regression is a staple in the toolkit of data scientists and statisticians. However, its application in causal inference, especially in observational studies, requires a nuanced understanding of its limitations and the implications of its estimates. In applied research, Ordinary Least Squares (OLS) is often the workhorse method for estimating causal effects from observational data. Its popularity comes from its simplicity and the appealing property that‚Äîwhen treatment effects are homogeneous‚Äîit recovers the average treatment effect (ATE) under standard assumptions. However, in practice, treatment effects are frequently heterogeneous. This heterogeneity means that the impact of a treatment can vary across individuals, and as a result, the standard OLS estimator does not simply estimate the ATE. Instead, it implicitly assigns weights to different subpopulations (e.g., the treated and the untreated), and these weights can be quite counterintuitive.\nTwo recent papers help us understand this phenomenon in detail. Chattopadhyay and Zubizarreta (2022) focuses on deriving closed-form expressions for the implied weights of linear regression estimators. Their analysis shows how, even when treatment effects are homogeneous, the regression adjustment does not ‚Äútreat‚Äù every observation equally‚Äîit rather targets a specific covariate profile that may differ from the sample average. On the other hand, S≈Çoczy≈Ñski (2022) takes a closer look at the scenario with heterogeneous treatment effects. He demonstrates that the OLS treatment coefficient is actually a convex combination of the average treatment effects on the treated (ATT) and the untreated (ATU), and, quite surprisingly, the weights assigned to these groups are inversely related to their sample proportions.\nThis article aims to delve into the interpretation of OLS estimates for causal inference, focusing on the implied weights of linear regression and the impact of treatment effect heterogeneity. The goal of this article is to unpack these findings in detail, explain the underlying mathematical structure, and provide intuition for why OLS behaves the way it does in causal inference settings."
  },
  {
    "objectID": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#notation",
    "href": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#notation",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs start by setting up our notation in the familiar potential outcomes framework. Suppose we have a sample \\(\\{ \\,Y_i, D_i, X_i \\} \\,_{i=1}^n\\), where:\n\n\\(Y_i\\) is the observed outcome for unit \\(i\\)\n\\(Y_i(0), Y_i(1)\\) are the potential outcomes for unit \\(i\\) if in control and treatment, respectively.\n\\(D_i\\) indicates treatment (\\(D_i=1\\) for treated units, and \\(D_i=0\\) for controls)\n\\(X_i\\) is a vector of covariates\n\nThe conventional OLS regression model is written as:\n\\[Y_i=\\alpha + \\tau D_i +X_i \\beta + \\epsilon_i,\\]\nwhere \\(\\epsilon\\) is a mean-zero error term and \\(\\tau\\) is typically interpreted as the causal effect of the treatment.\nThe key causal estimants (i.e., target parameters) are:\n\nAverage Treatment Effect (ATE): \\(E\\left[Y(1)-Y(0)\\right]\\),\nAverage Treatment Effect on the Treated (ATT): \\(E\\left[Y(1)-Y(0)\\mid D=1 \\right]\\),\nAverate Treatment effect on the Control (ATC): \\(E\\left[Y(1)-Y(0) \\mid D=0 \\right]\\).\n\nLastly, it is common to introduce the propensity score \\(p(X)=E\\left[D\\mid X\\right]\\) or its best linear approximation, which plays a central role in understanding the weights that OLS assigns."
  },
  {
    "objectID": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#a-closer-look",
    "href": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#a-closer-look",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nHomogeneous Treatment Effects\nexplore how linear regression adjustments in observational studies emulate key features of randomized experiments. The main focus is on Section 3, where they derive the implied weights of various linear regression estimators.\nThe authors show that the OLS estimator of the ATE can be expressed as a difference of weighted means of the treated and control outcomes. Specifically, they provide closed-form expressions for the implied regression weights. For instance, the URI (uni-regression imputation) estimator of the ATE is given by:\n\\[\\hat{\\tau}_{\\text{OLS}}=\\sum_{i:D=1} w_i ^{\\text{URI}}Y_i - \\sum_{i:D=0} w_i^{\\text{URI}}Y_i\\]\nwhere the weights \\(w_i ^{\\text{URI}}\\) depend on the covariates and treatment indicators but not on the observed outcomes. This weighting representation shows that linear regression can be ‚Äúfit‚Äù without the outcomes, aligning it with the design stage of an observational study.\nThe paper also discusses the properties of these implied weights, such as covariate balance, representativeness, dispersion, and optimality. For example, the URI and MRI (multi-regression imputation) weights exactly balance the means of the covariates included in the model, ensuring that the regression adjustments emulate the covariate balance of a randomized experiment.\n\n\nHeterogeneous Treatment Effects\nS≈Çoczy≈Ñski‚Äôs paper tackles the same question in the world of heterogeneous treatment effects. The key result is that the OLS treatment coefficient is a convex combination of the average treatment effects on the treated (ATT) and untreated (ATU), with weights inversely related to the proportion of observations in each group.\nThe scenario becomes even more intricate when treatment effects vary across individuals. S≈Çoczy≈Ñski (2022) takes on this challenge by examining the causal interpretation of the OLS estimand when treatment effects are heterogeneous. He shows that the OLS treatment coefficient can be decomposed as a convex combination of two group-specific effects:\nMathematically, the OLS estimand can be expressed as:\n\\[\\hat{\\tau}_{\\text{OLS}}=w_1 \\times \\text{ATT} + w_0 \\times \\text{ATC},\\]\nwhere \\(w_1 = \\frac{1}{1}=f(\\rho, p(X))\\) and \\(w_0 = 1 -w_1\\).\nThis result highlights that OLS places more weight on the group with fewer observations, which can lead to substantial biases when interpreting the OLS estimand as the ATE or ATT.\nS≈Çoczy≈Ñski provides diagnostic tools to detect these biases, emphasizing that OLS might often be substantially biased for ATE, ATT, or both. He suggests using alternative estimators or diagnostic methods to avoid potential biases in applied work."
  },
  {
    "objectID": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#bottom-line",
    "href": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#bottom-line",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLinear regression remains among the most popular ways to estimate treatment effects in causal inference\nIt implicitly weights individual observations, which can be represented in closed-form expressions.\nOLS estimates can be biased when treatment effects are heterogeneous, with smaller groups receiving larger weights."
  },
  {
    "objectID": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#references",
    "href": "blog/UNPUBLISHED-interpreting-OLS-causal-inference.html#references",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "References",
    "text": "References\nAngrist, J. D., & Krueger, A. B. (1999). Empirical strategies in labor economics. In Handbook of labor economics (Vol. 3, pp.¬†1277-1366). Elsevier.\nAngrist, J. D., & Pischke, J. S. (2009). Mostly harmless econometrics: An empiricist‚Äôs companion. Princeton university press.\nAronow, P. M., & Samii, C. (2016). Does regression produce representative estimates of causal effects?. American Journal of Political Science, 60(1), 250-267.\nChattopadhyay, A., & Zubizarreta, J. R. (2023). On the implied weights of linear regression for causal inference. Biometrika, 110(3), 615-629.\nHumphreys, M. (2009). Bounds on least squares estimates of causal effects in the presence of heterogeneous assignment probabilities. Manuscript, Columbia University.\nS≈Çoczy≈Ñski, T. (2022). Interpreting OLS estimands when treatment effects are heterogeneous: Smaller groups get larger weights. The Review of Economics and Statistics, 104(3), 501-509."
  },
  {
    "objectID": "blog/DONE-ols-fixed-random-x.html",
    "href": "blog/DONE-ols-fixed-random-x.html",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "",
    "text": "Linear regression stands as one of the fundamental workhorses of statistical modeling and is ubiquitous in applied work across fields like econometrics, biostatistics, and machine learning. Beneath its seemingly simple exterior lies a nuanced landscape of assumptions and theoretical considerations. One such distinction that often gets overlooked‚Äîeven by seasoned practitioners‚Äîis the difference between fixed and random regressors. This distinction, while subtle, has important implications for how we interpret our models, conduct inference, and understand uncertainty.\nThe goal of this article is to clarify the differences between fixed and random regressors in linear regression. We‚Äôll explore why this distinction matters and how it affects everything from your confidence intervals to your prediction errors. My goal is to help you develop both the technical understanding and the intuition needed to make informed modeling decisions in your day-to-day work."
  },
  {
    "objectID": "blog/DONE-ols-fixed-random-x.html#background",
    "href": "blog/DONE-ols-fixed-random-x.html#background",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "",
    "text": "Linear regression stands as one of the fundamental workhorses of statistical modeling and is ubiquitous in applied work across fields like econometrics, biostatistics, and machine learning. Beneath its seemingly simple exterior lies a nuanced landscape of assumptions and theoretical considerations. One such distinction that often gets overlooked‚Äîeven by seasoned practitioners‚Äîis the difference between fixed and random regressors. This distinction, while subtle, has important implications for how we interpret our models, conduct inference, and understand uncertainty.\nThe goal of this article is to clarify the differences between fixed and random regressors in linear regression. We‚Äôll explore why this distinction matters and how it affects everything from your confidence intervals to your prediction errors. My goal is to help you develop both the technical understanding and the intuition needed to make informed modeling decisions in your day-to-day work."
  },
  {
    "objectID": "blog/DONE-ols-fixed-random-x.html#notation",
    "href": "blog/DONE-ols-fixed-random-x.html#notation",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "Notation",
    "text": "Notation\nTo set the stage, consider the standard linear regression model:\n\\[Y = X\\beta + \\varepsilon,\\]\nwhere:\n\n\\(Y\\) is an $ n $ vector of outcomes,\n\\(X\\) is an $ n p $ matrix of regressors (also called covariates or features),\n\\(\\beta\\) is a \\(p \\times 1\\) vector of coefficients to be estimated,\n\\(\\varepsilon\\) is an $ n $ vector of errors.\n\nThe least squares estimator of \\(\\beta\\) is given by:\n\\[ \\hat{\\beta} = (X'X)^{-1}X'Y. \\]\nThe key question is: What do we assume about \\(X\\)? If $ X $ is considered fixed, we condition on it when deriving properties of \\(\\hat{\\beta}\\). If \\(X\\) is random, we take expectations over its distribution. This affects how we think about the variance of our estimator, the validity of standard errors, and the asymptotic behavior of \\(\\hat{\\beta}\\)."
  },
  {
    "objectID": "blog/DONE-ols-fixed-random-x.html#a-closer-look",
    "href": "blog/DONE-ols-fixed-random-x.html#a-closer-look",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFixed Regressors\nIn the fixed regressor framework, we assume that \\(X\\) is determined beforehand‚Äîperhaps through experimental design or by conditioning on observed values. The randomness in our model comes solely from the error term \\(\\varepsilon\\).\nThe key assumptions typically made under this approach are:\n\nLinearity: The model is correctly specified as \\(Y = X\\beta + \\varepsilon\\).\nExogeneity: The errors satisfy $E[X] = 0 $, ensuring that there is no systematic relationship between \\(X\\) and \\(\\varepsilon\\).\nHomoskedasticity: The variance of errors is constant, i.e., \\(\\text{Var}(\\varepsilon \\mid X) = \\sigma^2 I_n\\).\nNo Perfect Multicollinearity: \\(X'X\\) is full rank, ensuring that the inverse \\((X'X)^{-1}\\) exists.\n\nIn the classical Gauss-Markov framework, the regressors \\(X\\) are treated as fixed. This means that we analyze the behavior of \\(\\hat{\\beta}\\) conditional on the observed \\(X\\).\nUnder these conditions, the ordinary least squares (OLS) estimator is unbiased and has the classical variance formula:\n\\[\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X'X)^{-1}.\\]\nAn important feature here is that all expectations and variances are conditional on \\(X\\). This conditioning makes sense because we‚Äôre treating \\(X\\) as fixed and known.\n\n\nRandom Regressors\nIn most real-world scenarios, our \\(X\\) variables aren‚Äôt actually fixed. We often sample observations from a population, making both \\(Y\\) and \\(X\\) random. The random regressor framework acknowledges this reality. Under this framework, we add assumptions about the distribution of \\(X\\).\nWhen \\(X\\) is treated as random, it is assumed to be drawn from some probability distribution. This changes how we analyze \\(\\hat{\\beta}\\), since expectations are now taken over both \\(\\varepsilon\\) and \\(X\\). The key assumptions in this setting are:\n\nJoint Distribution: \\((X, Y)\\) follows some joint distribution, meaning \\(X\\) is not fixed but rather drawn from a population.\nExogeneity in Expectation: In both frameworks, we typically assume \\(E[\\varepsilon \\mid X] = 0\\), which rules out omitted variable bias. However, under the random regressor perspective, we also account for the fact that \\(X\\) itself is drawn from a distribution, and expectations (e.g., for variance) are taken over both \\(X\\) and \\(\\varepsilon\\).\nLaw of Large Numbers: As \\(n \\to \\infty\\), the sample quantities \\(\\frac{1}{n} X'X\\) converge to their population analogs.\n\nA major consequence of assuming \\(X\\) is random is that the variance of \\(\\hat{\\beta}\\) takes a different form:\n\\[\\text{Var}(\\hat{\\beta}) = E[(X'X)^{-1} X' \\varepsilon \\varepsilon' X (X'X)^{-1}].\\]\nThis expression accounts for uncertainty in both $ X $ and $ $, and under large samples, it converges to the population variance.\n\n\nPractical Implications\nSo what‚Äôs the big deal? Here are some practical implications:\n\nInference: In the fixed regressor framework, inference is conditional on the observed values of \\(X\\)‚Äîyou‚Äôre estimating the best linear approximation given this specific sample design. In the random regressor case, inference targets a population-level relationship between \\(X\\) and \\(Y\\).\nPrediction Error: With fixed regressors, prediction error only accounts for the randomness in \\(\\varepsilon\\). With random regressors, you must also account for the randomness in future $ $ values.\nRobustness: The random regressor framework tends to provide more realistic assessments of model uncertainty, especially when extrapolating.\n\nLet me put this in more intuitive terms: if you‚Äôre designing an experiment where you precisely control the levels of your predictors, the fixed regressor framework makes sense. If you‚Äôre analyzing observational data where both predictors and responses are sampled from a population, the random regressor framework is more appropriate."
  },
  {
    "objectID": "blog/DONE-ols-fixed-random-x.html#bottom-line",
    "href": "blog/DONE-ols-fixed-random-x.html#bottom-line",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe distinction between fixed and random regressors affects estimation and inference.\nFixed regressors condition on \\(X\\), while random regressors integrate \\(X\\) over its distribution.\nOLS properties such as bias and variance differ under these assumptions.\nIn practice, whether \\(X\\) is fixed or random depends on the study design and intended inference."
  },
  {
    "objectID": "blog/DONE-ols-fixed-random-x.html#references",
    "href": "blog/DONE-ols-fixed-random-x.html#references",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "References",
    "text": "References\nGreene, W. H. (2012). Econometric Analysis. Pearson. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springe"
  },
  {
    "objectID": "blog/DONE-brief-intro-e-values.html",
    "href": "blog/DONE-brief-intro-e-values.html",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. E-values can support decisions without the need to predefine a fixed significance level, allowing flexibility in interpreting evidence post hoc. Note, however, that this is a different type of control from traditional fixed-\\(\\alpha\\) control.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog/DONE-brief-intro-e-values.html#background",
    "href": "blog/DONE-brief-intro-e-values.html#background",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. E-values can support decisions without the need to predefine a fixed significance level, allowing flexibility in interpreting evidence post hoc. Note, however, that this is a different type of control from traditional fixed-\\(\\alpha\\) control.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog/DONE-brief-intro-e-values.html#notation",
    "href": "blog/DONE-brief-intro-e-values.html#notation",
    "title": "A Brief Introduction to E-Values",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs set up the basic notation. Suppose we are testing a null hypothesis \\(H_0\\) against an alternative \\(H_1\\) based on observed data \\(Y\\). In classical Neyman‚ÄìPearson testing, we define a test statistic T(Y) and derive a \\(p\\)-value:\n\\[P(Y) = P_{H_0}(T(Y) \\geq T_{obs}),\\]\nwhere \\(P_{H_0}\\) represents probability under the null hypothesis and \\(T_{obs}\\) is the observed test statistic."
  },
  {
    "objectID": "blog/DONE-brief-intro-e-values.html#a-closer-look",
    "href": "blog/DONE-brief-intro-e-values.html#a-closer-look",
    "title": "A Brief Introduction to E-Values",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nE-values replace this with an alternative statistic, the e-variable, denoted as \\(S(Y)\\), which satisfies:\n\\[E_{H_0}[S(Y)] \\leq 1.\\]\nThis ensures that the e-value does not, on average, exceed 1 under the null, providing a valid way to quantify evidence against \\(H_0\\).\n\n\nWhy E-Values?\nThe fundamental problem with \\(p\\)-values is their lack of a clear decision-theoretic interpretation. They are often misunderstood and misused, leading to issues such as the replication crisis in scientific research. E-values address these issues by offering:\n\nInterpretability: Large e-values provide direct evidence against \\(H_0\\), unlike small \\(p\\)-values, which are difficult to interpret without a fixed alpha threshold.\nOptional Stopping: Because e-values maintain their validity even when a study stops based on interim results, they are useful in sequential testing.\nPost Hoc Decision-Making: Since e-values allow for Type-I risk control after seeing the data, they facilitate more flexible decision-making.\n\n\n\nConstructing the E-Values\nA common way to define an e-value is through a likelihood ratio:\n\\[S(Y) = \\frac{P_{H_1}(Y)}{P_{H_0}(Y)}.\\]\nThis is similar to a Bayes factor but differs in that it does not require a prior distribution over hypotheses. Another approach is to construct empirical e-values based on resampling methods or alternative test statistics that satisfy the expectation constraint.\nA more general definition involves e-processes, which allow for sequential testing. An e-process \\(\\{S_t\\}_{t=1}^{\\infty}\\) is a sequence of nonnegative random variables satisfying:\n\\[E_{H_0}[S_t | S_1, \\dots, S_{t-1}] \\leq S_{t-1}, \\quad \\forall t.\\]\nThis property ensures that the sequence remains a valid e-value throughout a study, making it particularly powerful for adaptive testing procedures.\nAnother important construction is via supermartingales. An e-value can be defined as a nonnegative random variable \\(S(Y)\\) such that \\(\\{S_t\\}\\) forms a nonnegative supermartingale under \\(H_0\\):\n\\[E_{H_0}[S_t | S_{t-1}] \\leq S_{t-1},\\]\nwhich guarantees that the expectation does not increase under the null hypothesis."
  },
  {
    "objectID": "blog/DONE-brief-intro-e-values.html#comparison",
    "href": "blog/DONE-brief-intro-e-values.html#comparison",
    "title": "A Brief Introduction to E-Values",
    "section": "Comparison",
    "text": "Comparison\n\n\n\n\n\n\n\n\n\nProperty\n\\(P\\)-Value\nE-Value\n\n\n\n\nInterpretability\nIndirect (requires fixed \\(\\alpha\\))\nDirect (large values indicate evidence)\n\n\nOptional Stopping\nNo\nYes\n\n\nSequential Analysis\nProblematic\nNatural\n\n\nPost Hoc Decisions\nNot well-defined\nValid risk guarantees"
  },
  {
    "objectID": "blog/DONE-brief-intro-e-values.html#an-example",
    "href": "blog/DONE-brief-intro-e-values.html#an-example",
    "title": "A Brief Introduction to E-Values",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs consider a simple hypothesis test using the iris dataset in R. We will test whether the mean Sepal.Length differs between two species using both a \\(p\\)-value and an e-value.\n\nRPython\n\n\n# Load Data\nrm(list=ls())\nlibrary(dplyr)\nlibrary(tibble)\n\n# Load dataset\ndata(iris)\niris_filtered &lt;- iris %&gt;% filter(Species %in% c(\"setosa\", \"versicolor\"))\n\n# Compute P-Value (Traditional T-Test)\np_value &lt;- t.test(Sepal.Length ~ Species, data = iris_filtered)$p.value\nprint(p_value)\n\n# Compute E-Value\n# Define likelihood under each hypothesis\nlikelihood_ratio &lt;- function(y, mu0, mu1, sigma) {\n  dnorm(y, mean = mu1, sd = sigma) / dnorm(y, mean = mu0, sd = sigma)\n}\n\n# Estimate parameters\nmu0 &lt;- mean(iris_filtered$Sepal.Length[iris_filtered$Species == \"setosa\"]) \nmu1 &lt;- mean(iris_filtered$Sepal.Length[iris_filtered$Species == \"versicolor\"])\nsigma &lt;- sd(iris_filtered$Sepal.Length)\n\n# Compute e-value\ne_values &lt;- sapply(iris_filtered$Sepal.Length, likelihood_ratio, mu0, mu1, sigma)\ne_value &lt;- mean(e_values)\nprint(e_value)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind, norm\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris['Species'] = iris_data['target']\niris['Species'] = iris['Species'].replace({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Filter dataset for two species\niris_filtered = iris[iris['Species'].isin(['setosa', 'versicolor'])]\n\n# Compute P-Value (Traditional T-Test)\nsetosa = iris_filtered[iris_filtered['Species'] == 'setosa']['Sepal.Length']\nversicolor = iris_filtered[iris_filtered['Species'] == 'versicolor']['Sepal.Length']\np_value = ttest_ind(setosa, versicolor).pvalue\nprint(f\"P-Value: {p_value}\")\n\n# Compute E-Value\n# Define likelihood ratio function\ndef likelihood_ratio(y, mu0, mu1, sigma):\n    return norm.pdf(y, loc=mu1, scale=sigma) / norm.pdf(y, loc=mu0, scale=sigma)\n\n# Estimate parameters\nmu0 = setosa.mean()\nmu1 = versicolor.mean()\nsigma = iris_filtered['Sepal.Length'].std()\n\n# Compute e-value\ne_values = [likelihood_ratio(y, mu0, mu1, sigma) for y in iris_filtered['Sepal.Length']]\ne_value = np.prod(e_values)\nprint(f\"E-Value: {e_value}\")\n\n\n\nThe e-value directly quantifies the strength of evidence against the null. Unlike the \\(p\\)-value, which requires an arbitrary threshold (e.g., \\(0.05\\)) to make a decision, the e-value provides a more interpretable measure of support for \\(H_1\\)."
  },
  {
    "objectID": "blog/DONE-brief-intro-e-values.html#bottom-line",
    "href": "blog/DONE-brief-intro-e-values.html#bottom-line",
    "title": "A Brief Introduction to E-Values",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nE-values provide a decision-theoretic alternative to \\(p\\)-values, offering clearer evidence quantification.\nThey naturally handle optional stopping and sequential testing, unlike \\(p\\)-values.\nThey allow for flexible, post hoc decision-making, making them particularly useful in real-world applications.\nLikelihood ratios and supermartingales serve as a natural basis for constructing e-values, making them conceptually simple yet powerful."
  },
  {
    "objectID": "blog/DONE-brief-intro-e-values.html#where-to-learn-more",
    "href": "blog/DONE-brief-intro-e-values.html#where-to-learn-more",
    "title": "A Brief Introduction to E-Values",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deep dive into e-values, Gr√ºnwald (2024) provides a rigorous mathematical foundation, discussing their application in decision theory and hypothesis testing. Another useful resource is Shafer (2021), which explores connections between e-values and likelihood ratios. For practical applications, Vovk & Wang (2019) discuss the use of e-values in sequential analysis and machine learning contexts."
  },
  {
    "objectID": "blog/DONE-brief-intro-e-values.html#references",
    "href": "blog/DONE-brief-intro-e-values.html#references",
    "title": "A Brief Introduction to E-Values",
    "section": "References",
    "text": "References\nGr√ºnwald, P. (2024). Beyond Neyman-Pearson: E-values enable hypothesis testing with a data-driven alpha. PNAS.\nShafer, G. (2021). Testing by betting: A strategy for statistical and scientific communication. Cambridge University Press.\nVovk, V., & Wang, R. (2019). E-values: Calibration, combination, and applications. Journal of the Royal Statistical Society."
  },
  {
    "objectID": "blog/DONE-ci-residualized-reg.html",
    "href": "blog/DONE-ci-residualized-reg.html",
    "title": "Causal Inference with Residualized Regressions",
    "section": "",
    "text": "The Frisch-Waugh-Lovell (FWL) theorem offers an elegant alternative to standard multiple regression when estimating causal effects. Instead of running a full regression with treatment and control variables, FWL demonstrates that we can obtain identical treatment coefficient estimates by first ‚Äúresidualizing‚Äù both the outcome and treatment variables with respect to controls, then regressing these residuals against each other. This approach provides both computational advantages and conceptual clarity.\nThis theorem effectively decomposes multiple regression into simpler components, helping researchers understand what happens ‚Äúunder the hood‚Äù of regression models. By residualizing variables‚Äîremoving the components explained by control variables‚Äîwe can isolate and estimate treatment effects more intuitively, essentially peeling away confounding layers to reveal the core relationship of interest.\nFWL proves particularly valuable in high-dimensional settings and causal inference applications, offering researchers a powerful framework for understanding causal relationships. Whether working with observational studies, quasi-experiments, or complex datasets, this approach allows analysts to conceptualize how controlling for confounders isolates the treatment-outcome relationship they truly want to measure.\nA related and often underappreciated issue is how standard errors behave under the FWL theorem. A recent paper by Peng Ding (2021) shows that using FWL residualization in regression with random regressors can lead to incorrect standard errors unless the residualization is adjusted appropriately. Ding emphasizes that while the point estimates remain identical, the naive residual-on-residual approach may mischaracterize the variance, especially when the regressors are stochastic. This highlights how intertwined design assumptions and inferential correctness can be."
  },
  {
    "objectID": "blog/DONE-ci-residualized-reg.html#background",
    "href": "blog/DONE-ci-residualized-reg.html#background",
    "title": "Causal Inference with Residualized Regressions",
    "section": "",
    "text": "The Frisch-Waugh-Lovell (FWL) theorem offers an elegant alternative to standard multiple regression when estimating causal effects. Instead of running a full regression with treatment and control variables, FWL demonstrates that we can obtain identical treatment coefficient estimates by first ‚Äúresidualizing‚Äù both the outcome and treatment variables with respect to controls, then regressing these residuals against each other. This approach provides both computational advantages and conceptual clarity.\nThis theorem effectively decomposes multiple regression into simpler components, helping researchers understand what happens ‚Äúunder the hood‚Äù of regression models. By residualizing variables‚Äîremoving the components explained by control variables‚Äîwe can isolate and estimate treatment effects more intuitively, essentially peeling away confounding layers to reveal the core relationship of interest.\nFWL proves particularly valuable in high-dimensional settings and causal inference applications, offering researchers a powerful framework for understanding causal relationships. Whether working with observational studies, quasi-experiments, or complex datasets, this approach allows analysts to conceptualize how controlling for confounders isolates the treatment-outcome relationship they truly want to measure.\nA related and often underappreciated issue is how standard errors behave under the FWL theorem. A recent paper by Peng Ding (2021) shows that using FWL residualization in regression with random regressors can lead to incorrect standard errors unless the residualization is adjusted appropriately. Ding emphasizes that while the point estimates remain identical, the naive residual-on-residual approach may mischaracterize the variance, especially when the regressors are stochastic. This highlights how intertwined design assumptions and inferential correctness can be."
  },
  {
    "objectID": "blog/DONE-ci-residualized-reg.html#notation",
    "href": "blog/DONE-ci-residualized-reg.html#notation",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Notation",
    "text": "Notation\nConsider a standard linear model:\n\\[Y = \\alpha + D\\tau + X\\beta + \\varepsilon, \\]\nwhere:\n\n\\(Y\\) is the outcome variable (e.g., earnings),\n\\(D\\) is the treatment variable (e.g., whether a person attended a job training program),\n\\(X\\) is a vector of control variables (e.g., age, education, experience),\n\\(\\tau\\) is the treatment effect we want to estimate,\n\\(\\beta\\) represents the coefficients on the controls,\n\\(\\varepsilon\\) is the error term."
  },
  {
    "objectID": "blog/DONE-ci-residualized-reg.html#a-closer-look",
    "href": "blog/DONE-ci-residualized-reg.html#a-closer-look",
    "title": "Causal Inference with Residualized Regressions",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe FWL Theorem\nThe OLS estimate of \\(\\tau\\) in the full regression includes both \\(D\\) and \\(X\\). However, FWL tells us that we can obtain the same estimate of \\(\\tau\\) by following these steps:\n\nRegress \\(Y\\) on \\(X\\) and collect the residuals \\(\\tilde{Y}\\).\nRegress \\(D\\) on \\(X\\) and collect the residuals \\(\\tilde{D}\\).\nRegress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) (without an intercept). The coefficient on \\(\\tilde{D}\\) is exactly \\(\\tau\\).\n\n\n\nIntuition\nThe intuition behind FWL is simple yet profound. When we regress \\(Y\\) on \\(X\\), we strip out the variation in \\(Y\\) that is explained by \\(X\\), leaving only the part orthogonal to \\(X\\). Similarly, regressing \\(D\\) on \\(X\\) removes the influence of \\(X\\) on \\(D\\), isolating the component of \\(D\\) that is independent of \\(X\\). Since X has been accounted for in both cases, the regression of \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) retrieves the direct relationship between \\(D\\) and \\(Y\\), net of \\(X\\).\nMathematically, the key result of FWL is:\n\\[\\hat{\\tau} = (D' M_X D)^{-1} D' M_X Y,\\]\nwhere \\(M_X = I - X(X'X)^{-1}X'\\) is the projection matrix that residualizes variables with respect to \\(X\\). This shows that the estimate of remains unchanged whether we use the full regression or the residualized regression.\nSo what‚Äôs really happening here? The residuals \\(\\tilde{D}\\) represent the part of the treatment variable that cannot be explained by the control variables. Similarly, \\(\\tilde{\\mathbf{y}}\\) ‚Äã represents the part of the outcome that cannot be explained by the controls.\nThe FWL procedure essentially isolates the relationship between treatment and outcome after removing the influence of all the control variables. It‚Äôs as if we‚Äôre looking at the relationship in a ‚Äúpurified‚Äù form, with all the confounding effects of the controls stripped away.\nThis provides a powerful framework for causal inference. Under the assumption that we‚Äôve included all relevant confounders in our control set, the coefficient \\(\\hat{\\tau}\\) from the residualized regression can be interpreted as the causal effect of the treatment on the outcome.\nThink of it this way: we‚Äôre first ‚Äúadjusting‚Äù both our treatment and outcome variables by removing the predictable parts based on our controls. Then we‚Äôre examining how the ‚Äúadjusted‚Äù treatment relates to the ‚Äúadjusted‚Äù outcome. This residual-on-residual regression gives us our causal estimate.\n\n\nGeometric Interpretation\nGeometrically, we can think of the FWL theorem in terms of projections in vector space. The residuals \\(\\tilde{D}\\) and \\(\\tilde{\\mathbf{y}}\\)‚Äã are what remain after projecting \\(D\\) and \\(Y\\) onto the orthogonal complement of the space spanned by \\(Z\\).\nIn other words, we‚Äôre looking at the components of \\(D\\) and \\(Y\\) that are orthogonal to (i.e., cannot be explained by) the control variables. The relationship between these orthogonal components gives us our treatment effect estimate.\n\n\nVariance and Standard Errors\nThe precision of our treatment effect estimate depends on how much variation in the treatment remains after accounting for the controls. If the treatment is highly collinear with the controls (meaning \\(\\tilde{D}\\)$ has little variation), our estimate will be imprecise.\nThis helps us understand the ‚Äúcurse of dimensionality‚Äù in causal inference. As we add more control variables, we might reduce omitted variable bias, but we also risk reducing the effective variation in our treatment, leading to less precise estimates.\n\n\nPractical Implications\n\nConceptual clarity: FWL emphasizes that controlling for \\(X\\) means adjusting both \\(Y\\) and \\(D\\) before examining their relationship.\nComputational benefits: In high-dimensional settings, it is often more efficient to work with residualized variables rather than estimating the full model.\nInstrumental variables and two-stage regression: The residualization step is analogous to first-stage regressions in instrumental variable estimation.\nTwo separate data sources: In some cases, possibly due to data privacy concerns, the three variables \\(Y\\), \\(X\\) and \\(D\\) might live in two separate datasets - one with \\(Y\\) and \\(X\\), and the other one with \\(D\\) and \\(X\\). The traditional approach is then not avaialable."
  },
  {
    "objectID": "blog/DONE-ci-residualized-reg.html#an-example",
    "href": "blog/DONE-ci-residualized-reg.html#an-example",
    "title": "Causal Inference with Residualized Regressions",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs go through an example in R and python. We will generate synthetic data where the treatment effect is nonzero and show that both the full regression and the residualized approach give the same estimate.\n\nRPython\n\n\nrm(list=ls())\nset.seed(42)\nn &lt;- 1000\nX &lt;- matrix(rnorm(n * 3), ncol = 3)  # Three control variables\nD &lt;- 0.5 * X[,1] + 0.3 * X[,2] + rnorm(n)  # Treatment depends on controls\nY &lt;- 2 * D + 1.5 * X[,1] - 0.5 * X[,2] + 0.3 * X[,3] + rnorm(n)  # Outcome model\n\n# Full Regression\nfull_model &lt;- lm(Y ~ D + X)\nsummary(full_model)coefficients[\"D\",]  \n\n### Residualized Regression \nY_resid &lt;- residuals(lm(Y ~ X)) \nD_resid &lt;- residuals(lm(D ~ X)) \nresid_model &lt;- lm(Y_resid ~ D_resid - 1)  # No intercept \nsummary(resid_model)coefficients[\"D_resid\",]\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data\nn = 1000\nX = np.random.normal(size=(n, 3))  # Three control variables\nD = 0.5 * X[:, 0] + 0.3 * X[:, 1] + np.random.normal(size=n)  # Treatment depends on controls\nY = 2 * D + 1.5 * X[:, 0] - 0.5 * X[:, 1] + 0.3 * X[:, 2] + np.random.normal(size=n)  # Outcome model\n\n# Full Regression\nfull_model = LinearRegression()\nfull_model.fit(np.column_stack((D, X)), Y)\nfull_coef = full_model.coef_[0]  # Coefficient for D\nprint(f\"Full Regression Coefficient for D: {full_coef}\")\n\n# Residualized Regression\n# Step 1: Residualize Y with respect to X\nY_resid_model = LinearRegression()\nY_resid_model.fit(X, Y)\nY_resid = Y - Y_resid_model.predict(X)\n\n# Step 2: Residualize D with respect to X\nD_resid_model = LinearRegression()\nD_resid_model.fit(X, D)\nD_resid = D - D_resid_model.predict(X)\n\n# Step 3: Regress residualized Y on residualized D\nresid_model = LinearRegression(fit_intercept=False)\nresid_model.fit(D_resid.reshape(-1, 1), Y_resid)\nresid_coef = resid_model.coef_[0]\nprint(f\"Residualized Regression Coefficient for D: {resid_coef}\")\n\n\n\nThe coefficient on \\(D\\) in the full model and the coefficient on \\(D_{resid}\\) in the residualized model will be identical. This demonstrates that controlling for \\(X\\) can be done implicitly by working with residuals."
  },
  {
    "objectID": "blog/DONE-ci-residualized-reg.html#bottom-line",
    "href": "blog/DONE-ci-residualized-reg.html#bottom-line",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe FWL theorem shows that controlling for variables in a regression can be done by residualizing first.\nThis approach helps conceptually separate the treatment effect from confounding influences.\nResidualization is particularly useful in high-dimensional settings and instrumental variables estimation.\nWhether you use the full regression or the residualized approach, you get the same treatment effect estimate."
  },
  {
    "objectID": "blog/DONE-ci-residualized-reg.html#references",
    "href": "blog/DONE-ci-residualized-reg.html#references",
    "title": "Causal Inference with Residualized Regressions",
    "section": "References",
    "text": "References\nDing, P. (2021). The Frisch‚ÄìWaugh‚ÄìLovell theorem for standard errors. Statistics & Probability Letters, 168, 108945."
  },
  {
    "objectID": "blog/DONE-unconditional-qreg.html",
    "href": "blog/DONE-unconditional-qreg.html",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "mention rank invariance"
  },
  {
    "objectID": "blog/DONE-unconditional-qreg.html#background",
    "href": "blog/DONE-unconditional-qreg.html#background",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Background",
    "text": "Background\nQuantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog/DONE-unconditional-qreg.html#notation",
    "href": "blog/DONE-unconditional-qreg.html#notation",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Notation",
    "text": "Notation\nWe consider an outcome variable \\(Y\\) and a set of covariates \\(X\\). In traditional quantile regression, we estimate the conditional quantile function:\n\\[Q_\\tau(Y | X) = \\inf \\{ q : P(Y \\leq q | X) \\geq \\tau \\}.\\]\nThis tells us how the \\(\\tau\\)-th quantile of Y changes with \\(X\\). However, in many applications, we want to model the unconditional quantiles:\n\\[Q_\\tau(Y) = \\inf \\{ q : P(Y \\leq q) \\geq \\tau \\}.\\]\nUQR allows us to estimate how covariates influence these unconditional quantiles."
  },
  {
    "objectID": "blog/DONE-unconditional-qreg.html#a-closer-look",
    "href": "blog/DONE-unconditional-qreg.html#a-closer-look",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nBasics: Conditional vs.¬†Unconditional Quantiles\nThe fundamental distinction between conditional and unconditional quantile regressions is best understood through an example. Suppose we are studying wage distributions. A conditional quantile regression would estimate the effect of education on the wage quantile within a specific subgroup (e.g., workers with 5 years of experience). But what if we want to know the effect of education on the overall wage distribution? That‚Äôs where unconditional quantiles come in‚Äîthey capture the total effect of education, accounting for all pathways through which education might influence wages.\n\n\nUnconditional Quantile Regression\nFirpo et al.¬†introduced an elegant way to estimate UQR using influence functions. The influence function of a statistic measures how much that statistic changes when an observation is perturbed. The recentered influence function (RIF) for a quantile \\(Q_\\tau\\) is given by:\n\\[RIF(Y; Q_\\tau) = Q_\\tau + \\frac{\\tau - 1\\{Y \\leq Q_\\tau\\}}{f_Y(Q_\\tau)}.\\]\nHere, \\(f_Y(Q_\\tau)\\) is the density of \\(Y\\) at \\(Q_\\tau\\), which can be estimated nonparametrically. This nonparametric density estimation is often done via kernel density estimation but may be imprecise in the tails.\nFirpo et al.¬†showed that regressing \\(RIF(Y; Q_\\tau)\\) on covariates \\(X\\) via OLS provides a valid estimate of how \\(X\\) affects the \\(\\tau\\)-th quantile of \\(Y\\). This method is remarkably simple but powerful‚Äîit transforms a quantile regression problem into a standard linear regression problem.\n\n\nUnconditional Quantile Treatment Effects\nOne limitation of UQR as formulated by Firpo et al.¬†is that it assumes covariates are exogenous. But in many causal inference settings, treatment assignment is endogenous (e.g., workers self-select into training programs). Fr√∂lich and Melly (2013) extended the UQR framework to handle endogeneity using instrumental variables (IV).\nThey showed that under standard IV assumptions‚Äîrelevance and exclusion‚Äîthe unconditional quantile treatment effect (UQTE) can be estimated using a two-step approach:\n\nEstimate a propensity score model (or an instrumented version of \\(D\\)) to account for selection bias.\nApply RIF regression to estimate the effect of the treatment on unconditional quantiles.\n\nThis approach provides a way to estimate distributional treatment effects while addressing selection bias‚Äîa crucial tool in policy evaluation and applied econometrics.\n\n\nRank Invariance in QTEs\nA crucial assumption often invoked in the estimation of quantile treatment effects (QTEs) is rank invariance. This assumption states that units maintain their rank in the outcome distribution after receiving the treatment. In other words, if a treated unit was at the 30th percentile of the untreated outcome distribution, it would remain at the 30th percentile of the treated distribution.\nWhile this assumption simplifies identification and interpretation of QTEs, it can be highly restrictive. It rules out the possibility that treatment reshuffles individuals across the distribution‚Äîa scenario that might be not only plausible but central in many applications.\nConsider a school voucher program that offers private school access to low-income students. The effect of such a program may be heterogeneous: for high-performing students, access might enhance performance due to better environments. But for low-performing students, the same access could lead to worse outcomes due to higher academic pressure or poor fit. As a result, the program could re-rank students in the outcome distribution, violating rank invariance.\nIn such settings, assuming rank invariance could lead to misleading conclusions about who benefits and who loses from treatment. Alternative approaches, like those based on quantile treatment effect bounds (e.g., Melly, 2005; Chernozhukov & Hansen, 2005), are more robust to such violations."
  },
  {
    "objectID": "blog/DONE-unconditional-qreg.html#an-example",
    "href": "blog/DONE-unconditional-qreg.html#an-example",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate these ideas with an example in R. We‚Äôll use the iris dataset to estimate the effect of Sepal.Length on different quantiles of Petal.Length using UQR.\n\nRPython\n\n\nrm(list=ls())\nlibrary(quantreg)\n\n# Load dataset\ndata(iris)\n\n# Estimate unconditional quantiles\ntaus &lt;- c(0.25, 0.50, 0.75)\nq_vals &lt;- quantile(iris$Petal.Length, probs = taus)  # Estimate quantiles\nf_hat &lt;- density(iris$Petal.Length)\n\n# Compute RIF values\nrif_values &lt;- lapply(1:3, function(i) {\n  q &lt;- q_vals[i]\n  f &lt;- f_hat$y[which.min(abs(f_hat$x - q))]\n  q + ((taus[i] - (iris$Petal.Length &lt;= q)) / f)\n})\n\n# Run RIF regression\nmodels &lt;- lapply(rif_values, function(rif) lm(rif ~ Sepal.Length, data = iris))\n\n# Print results\nlapply(models, summary)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Estimate unconditional quantiles\ntaus = [0.25, 0.50, 0.75]\nq_vals = np.quantile(iris['Petal.Length'], taus)  # Estimate quantiles\nf_hat = gaussian_kde(iris['Petal.Length'])\n\n# Compute RIF values\nrif_values = []\nfor i, tau in enumerate(taus):\n    q = q_vals[i]\n    f = f_hat(q)  # Density at the quantile\n    rif = q + ((tau - (iris['Petal.Length'] &lt;= q).astype(int)) / f)\n    rif_values.append(rif)\n\n# Run RIF regression\nmodels = []\nfor rif in rif_values:\n    model = LinearRegression(fit_intercept=True)\n    model.fit(iris[['Sepal.Length']], rif)\n    models.append(model)\n\n# Print results\nfor i, model in enumerate(models):\n    print(f\"Model {i + 1}:\")\n    print(f\"Coefficient for Sepal.Length: {model.coef_[0]}\")\n    print(f\"Intercept: {model.intercept_}\")\n\n\n\nThis simple example demonstrates how to estimate the effect of a covariate on unconditional quantiles using the RIF regression approach."
  },
  {
    "objectID": "blog/DONE-unconditional-qreg.html#bottom-line",
    "href": "blog/DONE-unconditional-qreg.html#bottom-line",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nUQR allows us to estimate the effect of covariates on unconditional quantiles, capturing total effects.\nThe RIF regression method transforms a quantile regression problem into a simple linear regression.\nFr√∂lich and Melly (2013) extend UQR to address endogeneity using instrumental variables.\nThese tools are invaluable for policy evaluation and causal inference."
  },
  {
    "objectID": "blog/DONE-unconditional-qreg.html#where-to-learn-more",
    "href": "blog/DONE-unconditional-qreg.html#where-to-learn-more",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive into these methods, the foundational paper by Firpo, Fortin, and Lemieux (2009) provides a detailed introduction to UQR, while Fr√∂lich and Melly (2013) extend the framework to address endogeneity concerns. For a broader perspective on quantile regression, Koenker‚Äôs book Quantile Regression (2005) is a must-read."
  },
  {
    "objectID": "blog/DONE-unconditional-qreg.html#references",
    "href": "blog/DONE-unconditional-qreg.html#references",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "References",
    "text": "References\nAlejo, J., Favata, F., Montes-Rojas, G., & Trombetta, M. (2021). Conditional vs unconditional quantile regression models: A guide to practitioners. Econom√≠a, 44(88), 76-93.\nBorah, B. J., & Basu, A. (2013). Highlighting differences between conditional and unconditional quantile regression approaches through an application to assess medication adherence. Health economics, 22(9), 1052-1070.\nBorgen, N. T. (2016). Fixed effects in unconditional quantile regression. The Stata Journal, 16(2), 403-415.\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional quantile regressions. Econometrica, 77(3), 953-973.\nFr√∂lich, M., & Melly, B. (2013). Unconditional quantile treatment effects under endogeneity. Journal of Business & Economic Statistics, 31(3), 346-357.\nSasaki, Y., Ura, T., & Zhang, Y. (2022). Unconditional quantile regression with high‚Äêdimensional data. Quantitative Economics, 13(3), 955-978."
  },
  {
    "objectID": "blog-unpublished/DONE-ols-fixed-random-x.html",
    "href": "blog-unpublished/DONE-ols-fixed-random-x.html",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "",
    "text": "Linear regression stands as one of the fundamental workhorses of statistical modeling and is ubiquitous in applied work across fields like econometrics, biostatistics, and machine learning. Beneath its seemingly simple exterior lies a nuanced landscape of assumptions and theoretical considerations. One such distinction that often gets overlooked‚Äîeven by seasoned practitioners‚Äîis the difference between fixed and random regressors. This distinction, while subtle, has important implications for how we interpret our models, conduct inference, and understand uncertainty.\nThe goal of this article is to clarify the differences between fixed and random regressors in linear regression. We‚Äôll explore why this distinction matters and how it affects everything from your confidence intervals to your prediction errors. My goal is to help you develop both the technical understanding and the intuition needed to make informed modeling decisions in your day-to-day work."
  },
  {
    "objectID": "blog-unpublished/DONE-ols-fixed-random-x.html#background",
    "href": "blog-unpublished/DONE-ols-fixed-random-x.html#background",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "",
    "text": "Linear regression stands as one of the fundamental workhorses of statistical modeling and is ubiquitous in applied work across fields like econometrics, biostatistics, and machine learning. Beneath its seemingly simple exterior lies a nuanced landscape of assumptions and theoretical considerations. One such distinction that often gets overlooked‚Äîeven by seasoned practitioners‚Äîis the difference between fixed and random regressors. This distinction, while subtle, has important implications for how we interpret our models, conduct inference, and understand uncertainty.\nThe goal of this article is to clarify the differences between fixed and random regressors in linear regression. We‚Äôll explore why this distinction matters and how it affects everything from your confidence intervals to your prediction errors. My goal is to help you develop both the technical understanding and the intuition needed to make informed modeling decisions in your day-to-day work."
  },
  {
    "objectID": "blog-unpublished/DONE-ols-fixed-random-x.html#notation",
    "href": "blog-unpublished/DONE-ols-fixed-random-x.html#notation",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "Notation",
    "text": "Notation\nTo set the stage, consider the standard linear regression model:\n\\[Y = X\\beta + \\varepsilon,\\]\nwhere:\n\n\\(Y\\) is an $ n $ vector of outcomes,\n\\(X\\) is an $ n p $ matrix of regressors (also called covariates or features),\n\\(\\beta\\) is a \\(p \\times 1\\) vector of coefficients to be estimated,\n\\(\\varepsilon\\) is an $ n $ vector of errors.\n\nThe least squares estimator of \\(\\beta\\) is given by:\n\\[ \\hat{\\beta} = (X'X)^{-1}X'Y. \\]\nThe key question is: What do we assume about \\(X\\)? If $ X $ is considered fixed, we condition on it when deriving properties of \\(\\hat{\\beta}\\). If \\(X\\) is random, we take expectations over its distribution. This affects how we think about the variance of our estimator, the validity of standard errors, and the asymptotic behavior of \\(\\hat{\\beta}\\)."
  },
  {
    "objectID": "blog-unpublished/DONE-ols-fixed-random-x.html#a-closer-look",
    "href": "blog-unpublished/DONE-ols-fixed-random-x.html#a-closer-look",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFixed Regressors\nIn the fixed regressor framework, we assume that \\(X\\) is determined beforehand‚Äîperhaps through experimental design or by conditioning on observed values. The randomness in our model comes solely from the error term \\(\\varepsilon\\).\nThe key assumptions typically made under this approach are:\n\nLinearity: The model is correctly specified as \\(Y = X\\beta + \\varepsilon\\).\nExogeneity: The errors satisfy $E[X] = 0 $, ensuring that there is no systematic relationship between \\(X\\) and \\(\\varepsilon\\).\nHomoskedasticity: The variance of errors is constant, i.e., \\(\\text{Var}(\\varepsilon \\mid X) = \\sigma^2 I_n\\).\nNo Perfect Multicollinearity: \\(X'X\\) is full rank, ensuring that the inverse \\((X'X)^{-1}\\) exists.\n\nIn the classical Gauss-Markov framework, the regressors \\(X\\) are treated as fixed. This means that we analyze the behavior of \\(\\hat{\\beta}\\) conditional on the observed \\(X\\).\nUnder these conditions, the ordinary least squares (OLS) estimator is unbiased and has the classical variance formula:\n\\[\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X'X)^{-1}.\\]\nAn important feature here is that all expectations and variances are conditional on \\(X\\). This conditioning makes sense because we‚Äôre treating \\(X\\) as fixed and known.\n\n\nRandom Regressors\nIn most real-world scenarios, our \\(X\\) variables aren‚Äôt actually fixed. We often sample observations from a population, making both \\(Y\\) and \\(X\\) random. The random regressor framework acknowledges this reality. Under this framework, we add assumptions about the distribution of \\(X\\).\nWhen \\(X\\) is treated as random, it is assumed to be drawn from some probability distribution. This changes how we analyze \\(\\hat{\\beta}\\), since expectations are now taken over both \\(\\varepsilon\\) and \\(X\\). The key assumptions in this setting are:\n\nJoint Distribution: \\((X, Y)\\) follows some joint distribution, meaning \\(X\\) is not fixed but rather drawn from a population.\nExogeneity in Expectation: In both frameworks, we typically assume \\(E[\\varepsilon \\mid X] = 0\\), which rules out omitted variable bias. However, under the random regressor perspective, we also account for the fact that \\(X\\) itself is drawn from a distribution, and expectations (e.g., for variance) are taken over both \\(X\\) and \\(\\varepsilon\\).\nLaw of Large Numbers: As \\(n \\to \\infty\\), the sample quantities \\(\\frac{1}{n} X'X\\) converge to their population analogs.\n\nA major consequence of assuming \\(X\\) is random is that the variance of \\(\\hat{\\beta}\\) takes a different form:\n\\[\\text{Var}(\\hat{\\beta}) = E[(X'X)^{-1} X' \\varepsilon \\varepsilon' X (X'X)^{-1}].\\]\nThis expression accounts for uncertainty in both $ X $ and $ $, and under large samples, it converges to the population variance.\n\n\nPractical Implications\nSo what‚Äôs the big deal? Here are some practical implications:\n\nInference: In the fixed regressor framework, inference is conditional on the observed values of \\(X\\)‚Äîyou‚Äôre estimating the best linear approximation given this specific sample design. In the random regressor case, inference targets a population-level relationship between \\(X\\) and \\(Y\\).\nPrediction Error: With fixed regressors, prediction error only accounts for the randomness in \\(\\varepsilon\\). With random regressors, you must also account for the randomness in future $ $ values.\nRobustness: The random regressor framework tends to provide more realistic assessments of model uncertainty, especially when extrapolating.\n\nLet me put this in more intuitive terms: if you‚Äôre designing an experiment where you precisely control the levels of your predictors, the fixed regressor framework makes sense. If you‚Äôre analyzing observational data where both predictors and responses are sampled from a population, the random regressor framework is more appropriate."
  },
  {
    "objectID": "blog-unpublished/DONE-ols-fixed-random-x.html#bottom-line",
    "href": "blog-unpublished/DONE-ols-fixed-random-x.html#bottom-line",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe distinction between fixed and random regressors affects estimation and inference.\nFixed regressors condition on \\(X\\), while random regressors integrate \\(X\\) over its distribution.\nOLS properties such as bias and variance differ under these assumptions.\nIn practice, whether \\(X\\) is fixed or random depends on the study design and intended inference."
  },
  {
    "objectID": "blog-unpublished/DONE-ols-fixed-random-x.html#references",
    "href": "blog-unpublished/DONE-ols-fixed-random-x.html#references",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "References",
    "text": "References\nGreene, W. H. (2012). Econometric Analysis. Pearson. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springe"
  },
  {
    "objectID": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html",
    "href": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "",
    "text": "use the model by Aronow and Samii but that‚Äôs asymptotically\nZubisaretta show a finite sample formula\nSloczynski focuses on heterogeneous. Show it briefly\nAngrist focuses on saturated models. impossible with continuous X\nOLS regressions do not implicitly assume PO framework"
  },
  {
    "objectID": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#background",
    "href": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#background",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "Background",
    "text": "Background\nOrdinary Least Squares (OLS) regression is a staple in the toolkit of data scientists and statisticians. However, its application in causal inference, especially in observational studies, requires a nuanced understanding of its limitations and the implications of its estimates. In applied research, Ordinary Least Squares (OLS) is often the workhorse method for estimating causal effects from observational data. Its popularity comes from its simplicity and the appealing property that‚Äîwhen treatment effects are homogeneous‚Äîit recovers the average treatment effect (ATE) under standard assumptions. However, in practice, treatment effects are frequently heterogeneous. This heterogeneity means that the impact of a treatment can vary across individuals, and as a result, the standard OLS estimator does not simply estimate the ATE. Instead, it implicitly assigns weights to different subpopulations (e.g., the treated and the untreated), and these weights can be quite counterintuitive.\nTwo recent papers help us understand this phenomenon in detail. Chattopadhyay and Zubizarreta (2022) focuses on deriving closed-form expressions for the implied weights of linear regression estimators. Their analysis shows how, even when treatment effects are homogeneous, the regression adjustment does not ‚Äútreat‚Äù every observation equally‚Äîit rather targets a specific covariate profile that may differ from the sample average. On the other hand, S≈Çoczy≈Ñski (2022) takes a closer look at the scenario with heterogeneous treatment effects. He demonstrates that the OLS treatment coefficient is actually a convex combination of the average treatment effects on the treated (ATT) and the untreated (ATU), and, quite surprisingly, the weights assigned to these groups are inversely related to their sample proportions.\nThis article aims to delve into the interpretation of OLS estimates for causal inference, focusing on the implied weights of linear regression and the impact of treatment effect heterogeneity. The goal of this article is to unpack these findings in detail, explain the underlying mathematical structure, and provide intuition for why OLS behaves the way it does in causal inference settings."
  },
  {
    "objectID": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#notation",
    "href": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#notation",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs start by setting up our notation in the familiar potential outcomes framework. Suppose we have a sample \\(\\{ \\,Y_i, D_i, X_i \\} \\,_{i=1}^n\\), where:\n\n\\(Y_i\\) is the observed outcome for unit \\(i\\)\n\\(Y_i(0), Y_i(1)\\) are the potential outcomes for unit \\(i\\) if in control and treatment, respectively.\n\\(D_i\\) indicates treatment (\\(D_i=1\\) for treated units, and \\(D_i=0\\) for controls)\n\\(X_i\\) is a vector of covariates\n\nThe conventional OLS regression model is written as:\n\\[Y_i=\\alpha + \\tau D_i +X_i \\beta + \\epsilon_i,\\]\nwhere \\(\\epsilon\\) is a mean-zero error term and \\(\\tau\\) is typically interpreted as the causal effect of the treatment.\nThe key causal estimants (i.e., target parameters) are:\n\nAverage Treatment Effect (ATE): \\(E\\left[Y(1)-Y(0)\\right]\\),\nAverage Treatment Effect on the Treated (ATT): \\(E\\left[Y(1)-Y(0)\\mid D=1 \\right]\\),\nAverate Treatment effect on the Control (ATC): \\(E\\left[Y(1)-Y(0) \\mid D=0 \\right]\\).\n\nLastly, it is common to introduce the propensity score \\(p(X)=E\\left[D\\mid X\\right]\\) or its best linear approximation, which plays a central role in understanding the weights that OLS assigns."
  },
  {
    "objectID": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#a-closer-look",
    "href": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#a-closer-look",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nHomogeneous Treatment Effects\nexplore how linear regression adjustments in observational studies emulate key features of randomized experiments. The main focus is on Section 3, where they derive the implied weights of various linear regression estimators.\nThe authors show that the OLS estimator of the ATE can be expressed as a difference of weighted means of the treated and control outcomes. Specifically, they provide closed-form expressions for the implied regression weights. For instance, the URI (uni-regression imputation) estimator of the ATE is given by:\n\\[\\hat{\\tau}_{\\text{OLS}}=\\sum_{i:D=1} w_i ^{\\text{URI}}Y_i - \\sum_{i:D=0} w_i^{\\text{URI}}Y_i\\]\nwhere the weights \\(w_i ^{\\text{URI}}\\) depend on the covariates and treatment indicators but not on the observed outcomes. This weighting representation shows that linear regression can be ‚Äúfit‚Äù without the outcomes, aligning it with the design stage of an observational study.\nThe paper also discusses the properties of these implied weights, such as covariate balance, representativeness, dispersion, and optimality. For example, the URI and MRI (multi-regression imputation) weights exactly balance the means of the covariates included in the model, ensuring that the regression adjustments emulate the covariate balance of a randomized experiment.\n\n\nHeterogeneous Treatment Effects\nS≈Çoczy≈Ñski‚Äôs paper tackles the same question in the world of heterogeneous treatment effects. The key result is that the OLS treatment coefficient is a convex combination of the average treatment effects on the treated (ATT) and untreated (ATU), with weights inversely related to the proportion of observations in each group.\nThe scenario becomes even more intricate when treatment effects vary across individuals. S≈Çoczy≈Ñski (2022) takes on this challenge by examining the causal interpretation of the OLS estimand when treatment effects are heterogeneous. He shows that the OLS treatment coefficient can be decomposed as a convex combination of two group-specific effects:\nMathematically, the OLS estimand can be expressed as:\n\\[\\hat{\\tau}_{\\text{OLS}}=w_1 \\times \\text{ATT} + w_0 \\times \\text{ATC},\\]\nwhere \\(w_1 = \\frac{1}{1}=f(\\rho, p(X))\\) and \\(w_0 = 1 -w_1\\).\nThis result highlights that OLS places more weight on the group with fewer observations, which can lead to substantial biases when interpreting the OLS estimand as the ATE or ATT.\nS≈Çoczy≈Ñski provides diagnostic tools to detect these biases, emphasizing that OLS might often be substantially biased for ATE, ATT, or both. He suggests using alternative estimators or diagnostic methods to avoid potential biases in applied work."
  },
  {
    "objectID": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#bottom-line",
    "href": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#bottom-line",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLinear regression remains among the most popular ways to estimate treatment effects in causal inference\nIt implicitly weights individual observations, which can be represented in closed-form expressions.\nOLS estimates can be biased when treatment effects are heterogeneous, with smaller groups receiving larger weights."
  },
  {
    "objectID": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#references",
    "href": "blog-unpublished/TODO-interpreting-OLS-causal-inference.html#references",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "References",
    "text": "References\nAngrist, J. D., & Krueger, A. B. (1999). Empirical strategies in labor economics. In Handbook of labor economics (Vol. 3, pp.¬†1277-1366). Elsevier.\nAngrist, J. D., & Pischke, J. S. (2009). Mostly harmless econometrics: An empiricist‚Äôs companion. Princeton university press.\nAronow, P. M., & Samii, C. (2016). Does regression produce representative estimates of causal effects?. American Journal of Political Science, 60(1), 250-267.\nChattopadhyay, A., & Zubizarreta, J. R. (2023). On the implied weights of linear regression for causal inference. Biometrika, 110(3), 615-629.\nHumphreys, M. (2009). Bounds on least squares estimates of causal effects in the presence of heterogeneous assignment probabilities. Manuscript, Columbia University.\nS≈Çoczy≈Ñski, T. (2022). Interpreting OLS estimands when treatment effects are heterogeneous: Smaller groups get larger weights. The Review of Economics and Statistics, 104(3), 501-509."
  },
  {
    "objectID": "blog-unpublished/DONE-brief-intro-e-values.html",
    "href": "blog-unpublished/DONE-brief-intro-e-values.html",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. E-values can support decisions without the need to predefine a fixed significance level, allowing flexibility in interpreting evidence post hoc. Note, however, that this is a different type of control from traditional fixed-\\(\\alpha\\) control.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog-unpublished/DONE-brief-intro-e-values.html#background",
    "href": "blog-unpublished/DONE-brief-intro-e-values.html#background",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. E-values can support decisions without the need to predefine a fixed significance level, allowing flexibility in interpreting evidence post hoc. Note, however, that this is a different type of control from traditional fixed-\\(\\alpha\\) control.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog-unpublished/DONE-brief-intro-e-values.html#notation",
    "href": "blog-unpublished/DONE-brief-intro-e-values.html#notation",
    "title": "A Brief Introduction to E-Values",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs set up the basic notation. Suppose we are testing a null hypothesis \\(H_0\\) against an alternative \\(H_1\\) based on observed data \\(Y\\). In classical Neyman‚ÄìPearson testing, we define a test statistic T(Y) and derive a \\(p\\)-value:\n\\[P(Y) = P_{H_0}(T(Y) \\geq T_{obs}),\\]\nwhere \\(P_{H_0}\\) represents probability under the null hypothesis and \\(T_{obs}\\) is the observed test statistic."
  },
  {
    "objectID": "blog-unpublished/DONE-brief-intro-e-values.html#a-closer-look",
    "href": "blog-unpublished/DONE-brief-intro-e-values.html#a-closer-look",
    "title": "A Brief Introduction to E-Values",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nE-values replace this with an alternative statistic, the e-variable, denoted as \\(S(Y)\\), which satisfies:\n\\[E_{H_0}[S(Y)] \\leq 1.\\]\nThis ensures that the e-value does not, on average, exceed 1 under the null, providing a valid way to quantify evidence against \\(H_0\\).\n\n\nWhy E-Values?\nThe fundamental problem with \\(p\\)-values is their lack of a clear decision-theoretic interpretation. They are often misunderstood and misused, leading to issues such as the replication crisis in scientific research. E-values address these issues by offering:\n\nInterpretability: Large e-values provide direct evidence against \\(H_0\\), unlike small \\(p\\)-values, which are difficult to interpret without a fixed alpha threshold.\nOptional Stopping: Because e-values maintain their validity even when a study stops based on interim results, they are useful in sequential testing.\nPost Hoc Decision-Making: Since e-values allow for Type-I risk control after seeing the data, they facilitate more flexible decision-making.\n\n\n\nConstructing the E-Values\nA common way to define an e-value is through a likelihood ratio:\n\\[S(Y) = \\frac{P_{H_1}(Y)}{P_{H_0}(Y)}.\\]\nThis is similar to a Bayes factor but differs in that it does not require a prior distribution over hypotheses. Another approach is to construct empirical e-values based on resampling methods or alternative test statistics that satisfy the expectation constraint.\nA more general definition involves e-processes, which allow for sequential testing. An e-process \\(\\{S_t\\}_{t=1}^{\\infty}\\) is a sequence of nonnegative random variables satisfying:\n\\[E_{H_0}[S_t | S_1, \\dots, S_{t-1}] \\leq S_{t-1}, \\quad \\forall t.\\]\nThis property ensures that the sequence remains a valid e-value throughout a study, making it particularly powerful for adaptive testing procedures.\nAnother important construction is via supermartingales. An e-value can be defined as a nonnegative random variable \\(S(Y)\\) such that \\(\\{S_t\\}\\) forms a nonnegative supermartingale under \\(H_0\\):\n\\[E_{H_0}[S_t | S_{t-1}] \\leq S_{t-1},\\]\nwhich guarantees that the expectation does not increase under the null hypothesis."
  },
  {
    "objectID": "blog-unpublished/DONE-brief-intro-e-values.html#comparison",
    "href": "blog-unpublished/DONE-brief-intro-e-values.html#comparison",
    "title": "A Brief Introduction to E-Values",
    "section": "Comparison",
    "text": "Comparison\n\n\n\n\n\n\n\n\n\nProperty\n\\(P\\)-Value\nE-Value\n\n\n\n\nInterpretability\nIndirect (requires fixed \\(\\alpha\\))\nDirect (large values indicate evidence)\n\n\nOptional Stopping\nNo\nYes\n\n\nSequential Analysis\nProblematic\nNatural\n\n\nPost Hoc Decisions\nNot well-defined\nValid risk guarantees"
  },
  {
    "objectID": "blog-unpublished/DONE-brief-intro-e-values.html#an-example",
    "href": "blog-unpublished/DONE-brief-intro-e-values.html#an-example",
    "title": "A Brief Introduction to E-Values",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs consider a simple hypothesis test using the iris dataset in R. We will test whether the mean Sepal.Length differs between two species using both a \\(p\\)-value and an e-value.\n\nRPython\n\n\n# Load Data\nrm(list=ls())\nlibrary(dplyr)\nlibrary(tibble)\n\n# Load dataset\ndata(iris)\niris_filtered &lt;- iris %&gt;% filter(Species %in% c(\"setosa\", \"versicolor\"))\n\n# Compute P-Value (Traditional T-Test)\np_value &lt;- t.test(Sepal.Length ~ Species, data = iris_filtered)$p.value\nprint(p_value)\n\n# Compute E-Value\n# Define likelihood under each hypothesis\nlikelihood_ratio &lt;- function(y, mu0, mu1, sigma) {\n  dnorm(y, mean = mu1, sd = sigma) / dnorm(y, mean = mu0, sd = sigma)\n}\n\n# Estimate parameters\nmu0 &lt;- mean(iris_filtered$Sepal.Length[iris_filtered$Species == \"setosa\"]) \nmu1 &lt;- mean(iris_filtered$Sepal.Length[iris_filtered$Species == \"versicolor\"])\nsigma &lt;- sd(iris_filtered$Sepal.Length)\n\n# Compute e-value\ne_values &lt;- sapply(iris_filtered$Sepal.Length, likelihood_ratio, mu0, mu1, sigma)\ne_value &lt;- mean(e_values)\nprint(e_value)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind, norm\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris['Species'] = iris_data['target']\niris['Species'] = iris['Species'].replace({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Filter dataset for two species\niris_filtered = iris[iris['Species'].isin(['setosa', 'versicolor'])]\n\n# Compute P-Value (Traditional T-Test)\nsetosa = iris_filtered[iris_filtered['Species'] == 'setosa']['Sepal.Length']\nversicolor = iris_filtered[iris_filtered['Species'] == 'versicolor']['Sepal.Length']\np_value = ttest_ind(setosa, versicolor).pvalue\nprint(f\"P-Value: {p_value}\")\n\n# Compute E-Value\n# Define likelihood ratio function\ndef likelihood_ratio(y, mu0, mu1, sigma):\n    return norm.pdf(y, loc=mu1, scale=sigma) / norm.pdf(y, loc=mu0, scale=sigma)\n\n# Estimate parameters\nmu0 = setosa.mean()\nmu1 = versicolor.mean()\nsigma = iris_filtered['Sepal.Length'].std()\n\n# Compute e-value\ne_values = [likelihood_ratio(y, mu0, mu1, sigma) for y in iris_filtered['Sepal.Length']]\ne_value = np.prod(e_values)\nprint(f\"E-Value: {e_value}\")\n\n\n\nThe e-value directly quantifies the strength of evidence against the null. Unlike the \\(p\\)-value, which requires an arbitrary threshold (e.g., \\(0.05\\)) to make a decision, the e-value provides a more interpretable measure of support for \\(H_1\\)."
  },
  {
    "objectID": "blog-unpublished/DONE-brief-intro-e-values.html#bottom-line",
    "href": "blog-unpublished/DONE-brief-intro-e-values.html#bottom-line",
    "title": "A Brief Introduction to E-Values",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nE-values provide a decision-theoretic alternative to \\(p\\)-values, offering clearer evidence quantification.\nThey naturally handle optional stopping and sequential testing, unlike \\(p\\)-values.\nThey allow for flexible, post hoc decision-making, making them particularly useful in real-world applications.\nLikelihood ratios and supermartingales serve as a natural basis for constructing e-values, making them conceptually simple yet powerful."
  },
  {
    "objectID": "blog-unpublished/DONE-brief-intro-e-values.html#where-to-learn-more",
    "href": "blog-unpublished/DONE-brief-intro-e-values.html#where-to-learn-more",
    "title": "A Brief Introduction to E-Values",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deep dive into e-values, Gr√ºnwald (2024) provides a rigorous mathematical foundation, discussing their application in decision theory and hypothesis testing. Another useful resource is Shafer (2021), which explores connections between e-values and likelihood ratios. For practical applications, Vovk & Wang (2019) discuss the use of e-values in sequential analysis and machine learning contexts."
  },
  {
    "objectID": "blog-unpublished/DONE-brief-intro-e-values.html#references",
    "href": "blog-unpublished/DONE-brief-intro-e-values.html#references",
    "title": "A Brief Introduction to E-Values",
    "section": "References",
    "text": "References\nGr√ºnwald, P. (2024). Beyond Neyman-Pearson: E-values enable hypothesis testing with a data-driven alpha. PNAS.\nShafer, G. (2021). Testing by betting: A strategy for statistical and scientific communication. Cambridge University Press.\nVovk, V., & Wang, R. (2019). E-values: Calibration, combination, and applications. Journal of the Royal Statistical Society."
  },
  {
    "objectID": "blog-unpublished/DONE-ci-residualized-reg.html",
    "href": "blog-unpublished/DONE-ci-residualized-reg.html",
    "title": "Causal Inference with Residualized Regressions",
    "section": "",
    "text": "The Frisch-Waugh-Lovell (FWL) theorem offers an elegant alternative to standard multiple regression when estimating causal effects. Instead of running a full regression with treatment and control variables, FWL demonstrates that we can obtain identical treatment coefficient estimates by first ‚Äúresidualizing‚Äù both the outcome and treatment variables with respect to controls, then regressing these residuals against each other. This approach provides both computational advantages and conceptual clarity.\nThis theorem effectively decomposes multiple regression into simpler components, helping researchers understand what happens ‚Äúunder the hood‚Äù of regression models. By residualizing variables‚Äîremoving the components explained by control variables‚Äîwe can isolate and estimate treatment effects more intuitively, essentially peeling away confounding layers to reveal the core relationship of interest.\nFWL proves particularly valuable in high-dimensional settings and causal inference applications, offering researchers a powerful framework for understanding causal relationships. Whether working with observational studies, quasi-experiments, or complex datasets, this approach allows analysts to conceptualize how controlling for confounders isolates the treatment-outcome relationship they truly want to measure.\nA related and often underappreciated issue is how standard errors behave under the FWL theorem. A recent paper by Peng Ding (2021) shows that using FWL residualization in regression with random regressors can lead to incorrect standard errors unless the residualization is adjusted appropriately. Ding emphasizes that while the point estimates remain identical, the naive residual-on-residual approach may mischaracterize the variance, especially when the regressors are stochastic. This highlights how intertwined design assumptions and inferential correctness can be."
  },
  {
    "objectID": "blog-unpublished/DONE-ci-residualized-reg.html#background",
    "href": "blog-unpublished/DONE-ci-residualized-reg.html#background",
    "title": "Causal Inference with Residualized Regressions",
    "section": "",
    "text": "The Frisch-Waugh-Lovell (FWL) theorem offers an elegant alternative to standard multiple regression when estimating causal effects. Instead of running a full regression with treatment and control variables, FWL demonstrates that we can obtain identical treatment coefficient estimates by first ‚Äúresidualizing‚Äù both the outcome and treatment variables with respect to controls, then regressing these residuals against each other. This approach provides both computational advantages and conceptual clarity.\nThis theorem effectively decomposes multiple regression into simpler components, helping researchers understand what happens ‚Äúunder the hood‚Äù of regression models. By residualizing variables‚Äîremoving the components explained by control variables‚Äîwe can isolate and estimate treatment effects more intuitively, essentially peeling away confounding layers to reveal the core relationship of interest.\nFWL proves particularly valuable in high-dimensional settings and causal inference applications, offering researchers a powerful framework for understanding causal relationships. Whether working with observational studies, quasi-experiments, or complex datasets, this approach allows analysts to conceptualize how controlling for confounders isolates the treatment-outcome relationship they truly want to measure.\nA related and often underappreciated issue is how standard errors behave under the FWL theorem. A recent paper by Peng Ding (2021) shows that using FWL residualization in regression with random regressors can lead to incorrect standard errors unless the residualization is adjusted appropriately. Ding emphasizes that while the point estimates remain identical, the naive residual-on-residual approach may mischaracterize the variance, especially when the regressors are stochastic. This highlights how intertwined design assumptions and inferential correctness can be."
  },
  {
    "objectID": "blog-unpublished/DONE-ci-residualized-reg.html#notation",
    "href": "blog-unpublished/DONE-ci-residualized-reg.html#notation",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Notation",
    "text": "Notation\nConsider a standard linear model:\n\\[Y = \\alpha + D\\tau + X\\beta + \\varepsilon, \\]\nwhere:\n\n\\(Y\\) is the outcome variable (e.g., earnings),\n\\(D\\) is the treatment variable (e.g., whether a person attended a job training program),\n\\(X\\) is a vector of control variables (e.g., age, education, experience),\n\\(\\tau\\) is the treatment effect we want to estimate,\n\\(\\beta\\) represents the coefficients on the controls,\n\\(\\varepsilon\\) is the error term."
  },
  {
    "objectID": "blog-unpublished/DONE-ci-residualized-reg.html#a-closer-look",
    "href": "blog-unpublished/DONE-ci-residualized-reg.html#a-closer-look",
    "title": "Causal Inference with Residualized Regressions",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe FWL Theorem\nThe OLS estimate of \\(\\tau\\) in the full regression includes both \\(D\\) and \\(X\\). However, FWL tells us that we can obtain the same estimate of \\(\\tau\\) by following these steps:\n\nRegress \\(Y\\) on \\(X\\) and collect the residuals \\(\\tilde{Y}\\).\nRegress \\(D\\) on \\(X\\) and collect the residuals \\(\\tilde{D}\\).\nRegress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) (without an intercept). The coefficient on \\(\\tilde{D}\\) is exactly \\(\\tau\\).\n\n\n\nIntuition\nThe intuition behind FWL is simple yet profound. When we regress \\(Y\\) on \\(X\\), we strip out the variation in \\(Y\\) that is explained by \\(X\\), leaving only the part orthogonal to \\(X\\). Similarly, regressing \\(D\\) on \\(X\\) removes the influence of \\(X\\) on \\(D\\), isolating the component of \\(D\\) that is independent of \\(X\\). Since X has been accounted for in both cases, the regression of \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) retrieves the direct relationship between \\(D\\) and \\(Y\\), net of \\(X\\).\nMathematically, the key result of FWL is:\n\\[\\hat{\\tau} = (D' M_X D)^{-1} D' M_X Y,\\]\nwhere \\(M_X = I - X(X'X)^{-1}X'\\) is the projection matrix that residualizes variables with respect to \\(X\\). This shows that the estimate of remains unchanged whether we use the full regression or the residualized regression.\nSo what‚Äôs really happening here? The residuals \\(\\tilde{D}\\) represent the part of the treatment variable that cannot be explained by the control variables. Similarly, \\(\\tilde{\\mathbf{y}}\\) ‚Äã represents the part of the outcome that cannot be explained by the controls.\nThe FWL procedure essentially isolates the relationship between treatment and outcome after removing the influence of all the control variables. It‚Äôs as if we‚Äôre looking at the relationship in a ‚Äúpurified‚Äù form, with all the confounding effects of the controls stripped away.\nThis provides a powerful framework for causal inference. Under the assumption that we‚Äôve included all relevant confounders in our control set, the coefficient \\(\\hat{\\tau}\\) from the residualized regression can be interpreted as the causal effect of the treatment on the outcome.\nThink of it this way: we‚Äôre first ‚Äúadjusting‚Äù both our treatment and outcome variables by removing the predictable parts based on our controls. Then we‚Äôre examining how the ‚Äúadjusted‚Äù treatment relates to the ‚Äúadjusted‚Äù outcome. This residual-on-residual regression gives us our causal estimate.\n\n\nGeometric Interpretation\nGeometrically, we can think of the FWL theorem in terms of projections in vector space. The residuals \\(\\tilde{D}\\) and \\(\\tilde{\\mathbf{y}}\\)‚Äã are what remain after projecting \\(D\\) and \\(Y\\) onto the orthogonal complement of the space spanned by \\(Z\\).\nIn other words, we‚Äôre looking at the components of \\(D\\) and \\(Y\\) that are orthogonal to (i.e., cannot be explained by) the control variables. The relationship between these orthogonal components gives us our treatment effect estimate.\n\n\nVariance and Standard Errors\nThe precision of our treatment effect estimate depends on how much variation in the treatment remains after accounting for the controls. If the treatment is highly collinear with the controls (meaning \\(\\tilde{D}\\)$ has little variation), our estimate will be imprecise.\nThis helps us understand the ‚Äúcurse of dimensionality‚Äù in causal inference. As we add more control variables, we might reduce omitted variable bias, but we also risk reducing the effective variation in our treatment, leading to less precise estimates.\n\n\nPractical Implications\n\nConceptual clarity: FWL emphasizes that controlling for \\(X\\) means adjusting both \\(Y\\) and \\(D\\) before examining their relationship.\nComputational benefits: In high-dimensional settings, it is often more efficient to work with residualized variables rather than estimating the full model.\nInstrumental variables and two-stage regression: The residualization step is analogous to first-stage regressions in instrumental variable estimation.\nTwo separate data sources: In some cases, possibly due to data privacy concerns, the three variables \\(Y\\), \\(X\\) and \\(D\\) might live in two separate datasets - one with \\(Y\\) and \\(X\\), and the other one with \\(D\\) and \\(X\\). The traditional approach is then not avaialable."
  },
  {
    "objectID": "blog-unpublished/DONE-ci-residualized-reg.html#an-example",
    "href": "blog-unpublished/DONE-ci-residualized-reg.html#an-example",
    "title": "Causal Inference with Residualized Regressions",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs go through an example in R and python. We will generate synthetic data where the treatment effect is nonzero and show that both the full regression and the residualized approach give the same estimate.\n\nRPython\n\n\nrm(list=ls())\nset.seed(42)\nn &lt;- 1000\nX &lt;- matrix(rnorm(n * 3), ncol = 3)  # Three control variables\nD &lt;- 0.5 * X[,1] + 0.3 * X[,2] + rnorm(n)  # Treatment depends on controls\nY &lt;- 2 * D + 1.5 * X[,1] - 0.5 * X[,2] + 0.3 * X[,3] + rnorm(n)  # Outcome model\n\n# Full Regression\nfull_model &lt;- lm(Y ~ D + X)\nsummary(full_model)coefficients[\"D\",]  \n\n### Residualized Regression \nY_resid &lt;- residuals(lm(Y ~ X)) \nD_resid &lt;- residuals(lm(D ~ X)) \nresid_model &lt;- lm(Y_resid ~ D_resid - 1)  # No intercept \nsummary(resid_model)coefficients[\"D_resid\",]\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data\nn = 1000\nX = np.random.normal(size=(n, 3))  # Three control variables\nD = 0.5 * X[:, 0] + 0.3 * X[:, 1] + np.random.normal(size=n)  # Treatment depends on controls\nY = 2 * D + 1.5 * X[:, 0] - 0.5 * X[:, 1] + 0.3 * X[:, 2] + np.random.normal(size=n)  # Outcome model\n\n# Full Regression\nfull_model = LinearRegression()\nfull_model.fit(np.column_stack((D, X)), Y)\nfull_coef = full_model.coef_[0]  # Coefficient for D\nprint(f\"Full Regression Coefficient for D: {full_coef}\")\n\n# Residualized Regression\n# Step 1: Residualize Y with respect to X\nY_resid_model = LinearRegression()\nY_resid_model.fit(X, Y)\nY_resid = Y - Y_resid_model.predict(X)\n\n# Step 2: Residualize D with respect to X\nD_resid_model = LinearRegression()\nD_resid_model.fit(X, D)\nD_resid = D - D_resid_model.predict(X)\n\n# Step 3: Regress residualized Y on residualized D\nresid_model = LinearRegression(fit_intercept=False)\nresid_model.fit(D_resid.reshape(-1, 1), Y_resid)\nresid_coef = resid_model.coef_[0]\nprint(f\"Residualized Regression Coefficient for D: {resid_coef}\")\n\n\n\nThe coefficient on \\(D\\) in the full model and the coefficient on \\(D_{resid}\\) in the residualized model will be identical. This demonstrates that controlling for \\(X\\) can be done implicitly by working with residuals."
  },
  {
    "objectID": "blog-unpublished/DONE-ci-residualized-reg.html#bottom-line",
    "href": "blog-unpublished/DONE-ci-residualized-reg.html#bottom-line",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe FWL theorem shows that controlling for variables in a regression can be done by residualizing first.\nThis approach helps conceptually separate the treatment effect from confounding influences.\nResidualization is particularly useful in high-dimensional settings and instrumental variables estimation.\nWhether you use the full regression or the residualized approach, you get the same treatment effect estimate."
  },
  {
    "objectID": "blog-unpublished/DONE-ci-residualized-reg.html#references",
    "href": "blog-unpublished/DONE-ci-residualized-reg.html#references",
    "title": "Causal Inference with Residualized Regressions",
    "section": "References",
    "text": "References\nDing, P. (2021). The Frisch‚ÄìWaugh‚ÄìLovell theorem for standard errors. Statistics & Probability Letters, 168, 108945."
  },
  {
    "objectID": "blog-unpublished/DONE-unconditional-qreg.html",
    "href": "blog-unpublished/DONE-unconditional-qreg.html",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "Quantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog-unpublished/DONE-unconditional-qreg.html#background",
    "href": "blog-unpublished/DONE-unconditional-qreg.html#background",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "Quantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog-unpublished/DONE-unconditional-qreg.html#notation",
    "href": "blog-unpublished/DONE-unconditional-qreg.html#notation",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Notation",
    "text": "Notation\nWe consider an outcome variable \\(Y\\) and a set of covariates \\(X\\). In traditional quantile regression, we estimate the conditional quantile function:\n\\[Q_\\tau(Y | X) = \\inf \\{ q : P(Y \\leq q | X) \\geq \\tau \\}.\\]\nThis tells us how the \\(\\tau\\)-th quantile of Y changes with \\(X\\). However, in many applications, we want to model the unconditional quantiles:\n\\[Q_\\tau(Y) = \\inf \\{ q : P(Y \\leq q) \\geq \\tau \\}.\\]\nUQR allows us to estimate how covariates influence these unconditional quantiles."
  },
  {
    "objectID": "blog-unpublished/DONE-unconditional-qreg.html#a-closer-look",
    "href": "blog-unpublished/DONE-unconditional-qreg.html#a-closer-look",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nBasics: Conditional vs.¬†Unconditional Quantiles\nThe fundamental distinction between conditional and unconditional quantile regressions is best understood through an example. Suppose we are studying wage distributions. A conditional quantile regression would estimate the effect of education on the wage quantile within a specific subgroup (e.g., workers with 5 years of experience). But what if we want to know the effect of education on the overall wage distribution? That‚Äôs where unconditional quantiles come in‚Äîthey capture the total effect of education, accounting for all pathways through which education might influence wages.\n\n\nUnconditional Quantile Regression\nFirpo et al.¬†introduced an elegant way to estimate UQR using influence functions. The influence function of a statistic measures how much that statistic changes when an observation is perturbed. The recentered influence function (RIF) for a quantile \\(Q_\\tau\\) is given by:\n\\[RIF(Y; Q_\\tau) = Q_\\tau + \\frac{\\tau - 1\\{Y \\leq Q_\\tau\\}}{f_Y(Q_\\tau)}.\\]\nHere, \\(f_Y(Q_\\tau)\\) is the density of \\(Y\\) at \\(Q_\\tau\\), which can be estimated nonparametrically. This nonparametric density estimation is often done via kernel density estimation but may be imprecise in the tails.\nFirpo et al.¬†showed that regressing \\(RIF(Y; Q_\\tau)\\) on covariates \\(X\\) via OLS provides a valid estimate of how \\(X\\) affects the \\(\\tau\\)-th quantile of \\(Y\\). This method is remarkably simple but powerful‚Äîit transforms a quantile regression problem into a standard linear regression problem.\n\n\nUnconditional Quantile Treatment Effects\nOne limitation of UQR as formulated by Firpo et al.¬†is that it assumes covariates are exogenous. But in many causal inference settings, treatment assignment is endogenous (e.g., workers self-select into training programs). Fr√∂lich and Melly (2013) extended the UQR framework to handle endogeneity using instrumental variables (IV).\nThey showed that under standard IV assumptions‚Äîrelevance and exclusion‚Äîthe unconditional quantile treatment effect (UQTE) can be estimated using a two-step approach:\n\nEstimate a propensity score model (or an instrumented version of \\(D\\)) to account for selection bias.\nApply RIF regression to estimate the effect of the treatment on unconditional quantiles.\n\nThis approach provides a way to estimate distributional treatment effects while addressing selection bias‚Äîa crucial tool in policy evaluation and applied econometrics.\n\n\nRank Invariance in QTEs\nA crucial assumption often invoked in the estimation of quantile treatment effects (QTEs) is rank invariance. This assumption states that units maintain their rank in the outcome distribution after receiving the treatment. In other words, if a treated unit was at the 30th percentile of the untreated outcome distribution, it would remain at the 30th percentile of the treated distribution.\nWhile this assumption simplifies identification and interpretation of QTEs, it can be highly restrictive. It rules out the possibility that treatment reshuffles individuals across the distribution‚Äîa scenario that might be not only plausible but central in many applications.\nConsider a school voucher program that offers private school access to low-income students. The effect of such a program may be heterogeneous: for high-performing students, access might enhance performance due to better environments. But for low-performing students, the same access could lead to worse outcomes due to higher academic pressure or poor fit. As a result, the program could re-rank students in the outcome distribution, violating rank invariance.\nIn such settings, assuming rank invariance could lead to misleading conclusions about who benefits and who loses from treatment. Alternative approaches, like those based on quantile treatment effect bounds (e.g., Melly, 2005; Chernozhukov & Hansen, 2005), are more robust to such violations."
  },
  {
    "objectID": "blog-unpublished/DONE-unconditional-qreg.html#an-example",
    "href": "blog-unpublished/DONE-unconditional-qreg.html#an-example",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate these ideas with an example in R. We‚Äôll use the iris dataset to estimate the effect of Sepal.Length on different quantiles of Petal.Length using UQR.\n\nRPython\n\n\nrm(list=ls())\nlibrary(quantreg)\n\n# Load dataset\ndata(iris)\n\n# Estimate unconditional quantiles\ntaus &lt;- c(0.25, 0.50, 0.75)\nq_vals &lt;- quantile(iris$Petal.Length, probs = taus)  # Estimate quantiles\nf_hat &lt;- density(iris$Petal.Length)\n\n# Compute RIF values\nrif_values &lt;- lapply(1:3, function(i) {\n  q &lt;- q_vals[i]\n  f &lt;- f_hat$y[which.min(abs(f_hat$x - q))]\n  q + ((taus[i] - (iris$Petal.Length &lt;= q)) / f)\n})\n\n# Run RIF regression\nmodels &lt;- lapply(rif_values, function(rif) lm(rif ~ Sepal.Length, data = iris))\n\n# Print results\nlapply(models, summary)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Estimate unconditional quantiles\ntaus = [0.25, 0.50, 0.75]\nq_vals = np.quantile(iris['Petal.Length'], taus)  # Estimate quantiles\nf_hat = gaussian_kde(iris['Petal.Length'])\n\n# Compute RIF values\nrif_values = []\nfor i, tau in enumerate(taus):\n    q = q_vals[i]\n    f = f_hat(q)  # Density at the quantile\n    rif = q + ((tau - (iris['Petal.Length'] &lt;= q).astype(int)) / f)\n    rif_values.append(rif)\n\n# Run RIF regression\nmodels = []\nfor rif in rif_values:\n    model = LinearRegression(fit_intercept=True)\n    model.fit(iris[['Sepal.Length']], rif)\n    models.append(model)\n\n# Print results\nfor i, model in enumerate(models):\n    print(f\"Model {i + 1}:\")\n    print(f\"Coefficient for Sepal.Length: {model.coef_[0]}\")\n    print(f\"Intercept: {model.intercept_}\")\n\n\n\nThis simple example demonstrates how to estimate the effect of a covariate on unconditional quantiles using the RIF regression approach."
  },
  {
    "objectID": "blog-unpublished/DONE-unconditional-qreg.html#bottom-line",
    "href": "blog-unpublished/DONE-unconditional-qreg.html#bottom-line",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nUQR allows us to estimate the effect of covariates on unconditional quantiles, capturing total effects.\nThe RIF regression method transforms a quantile regression problem into a simple linear regression.\nFr√∂lich and Melly (2013) extend UQR to address endogeneity using instrumental variables.\nThese tools are invaluable for policy evaluation and causal inference."
  },
  {
    "objectID": "blog-unpublished/DONE-unconditional-qreg.html#where-to-learn-more",
    "href": "blog-unpublished/DONE-unconditional-qreg.html#where-to-learn-more",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive into these methods, the foundational paper by Firpo, Fortin, and Lemieux (2009) provides a detailed introduction to UQR, while Fr√∂lich and Melly (2013) extend the framework to address endogeneity concerns. For a broader perspective on quantile regression, Koenker‚Äôs book Quantile Regression (2005) is a must-read."
  },
  {
    "objectID": "blog-unpublished/DONE-unconditional-qreg.html#references",
    "href": "blog-unpublished/DONE-unconditional-qreg.html#references",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "References",
    "text": "References\nAlejo, J., Favata, F., Montes-Rojas, G., & Trombetta, M. (2021). Conditional vs unconditional quantile regression models: A guide to practitioners. Econom√≠a, 44(88), 76-93.\nBorah, B. J., & Basu, A. (2013). Highlighting differences between conditional and unconditional quantile regression approaches through an application to assess medication adherence. Health economics, 22(9), 1052-1070.\nBorgen, N. T. (2016). Fixed effects in unconditional quantile regression. The Stata Journal, 16(2), 403-415.\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional quantile regressions. Econometrica, 77(3), 953-973.\nFr√∂lich, M., & Melly, B. (2013). Unconditional quantile treatment effects under endogeneity. Journal of Business & Economic Statistics, 31(3), 346-357.\nSasaki, Y., Ura, T., & Zhang, Y. (2022). Unconditional quantile regression with high‚Äêdimensional data. Quantitative Economics, 13(3), 955-978."
  },
  {
    "objectID": "blog-unpublished/TODO-oracle-property.html",
    "href": "blog-unpublished/TODO-oracle-property.html",
    "title": "The Oracle Property in Machine Learning",
    "section": "",
    "text": "Imagine this: you‚Äôre trying to predict an outcome based on dozens or even hundreds of variables. You suspect only a few of them actually matter, but you don‚Äôt know which ones. Wouldn‚Äôt it be great if you had an oracle‚Äîa magical being who could whisper in your ear and tell you exactly which variables to use?\nIn machine learning and statistics, when we say that an estimator has the oracle property, we mean it behaves as if it had access to that oracle. Specifically, it consistently identifies the correct subset of relevant variables, and then estimates their effects with the same efficiency as if it had known the true model all along. That‚Äôs a big deal. Most estimators don‚Äôt come close.\nIn this post, we‚Äôll unpack the intuition and math behind the oracle property, explore estimators that (claim to) have it‚Äîlike the adaptive lasso‚Äîand clarify what it means for an estimator to not have this magical trait. We‚Äôll also briefly touch on another, broader use of the term ‚Äúoracle‚Äù in machine learning."
  },
  {
    "objectID": "blog-unpublished/TODO-oracle-property.html#background",
    "href": "blog-unpublished/TODO-oracle-property.html#background",
    "title": "The Oracle Property in Machine Learning",
    "section": "",
    "text": "Imagine this: you‚Äôre trying to predict an outcome based on dozens or even hundreds of variables. You suspect only a few of them actually matter, but you don‚Äôt know which ones. Wouldn‚Äôt it be great if you had an oracle‚Äîa magical being who could whisper in your ear and tell you exactly which variables to use?\nIn machine learning and statistics, when we say that an estimator has the oracle property, we mean it behaves as if it had access to that oracle. Specifically, it consistently identifies the correct subset of relevant variables, and then estimates their effects with the same efficiency as if it had known the true model all along. That‚Äôs a big deal. Most estimators don‚Äôt come close.\nIn this post, we‚Äôll unpack the intuition and math behind the oracle property, explore estimators that (claim to) have it‚Äîlike the adaptive lasso‚Äîand clarify what it means for an estimator to not have this magical trait. We‚Äôll also briefly touch on another, broader use of the term ‚Äúoracle‚Äù in machine learning."
  },
  {
    "objectID": "blog-unpublished/TODO-oracle-property.html#notation",
    "href": "blog-unpublished/TODO-oracle-property.html#notation",
    "title": "The Oracle Property in Machine Learning",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs ground ourselves in a simple linear regression model:\n\\[y_i = X_i^\\top \\beta^* + \\varepsilon_i, \\quad i = 1, \\dots, n,\\]\nwhere: - \\(y_i \\in \\mathbb{R}\\) is the outcome, - \\(X_i \\in \\mathbb{R}^p\\) is the vector of predictors (covariates), - \\(\\beta^* \\in \\mathbb{R}^p\\) is the true but unknown coefficient vector, - \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) are independent errors.\nWe assume that the true coefficient vector \\(\\beta^*\\) is sparse‚Äîthat is, many of its entries are exactly zero. Let \\(S = \\{j : \\beta^*_j \\neq 0\\}\\) be the support set of non-zero coefficients, and \\(s = |S|\\) its cardinality.\nThe dream is to recover both the support \\(S\\) and estimate the non-zero coefficients accurately."
  },
  {
    "objectID": "blog-unpublished/TODO-oracle-property.html#a-closer-look",
    "href": "blog-unpublished/TODO-oracle-property.html#a-closer-look",
    "title": "The Oracle Property in Machine Learning",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhat Does the Oracle Property Actually Mean?\nAn estimator \\(\\hat{\\beta}\\) has the oracle property if, as the sample size \\(n \\to \\infty\\):\n\n(Support Recovery) It correctly identifies the set of non-zero coefficients with probability tending to 1:\n\\[\\Pr(\\text{supp}(\\hat{\\beta}) = S) \\to 1.\\]\n(Asymptotic Efficiency) The estimator is asymptotically normal and efficient for the non-zero coefficients, just like the OLS estimator would be if you knew $ S $ in advance: \\[\\sqrt{n}(\\hat{\\beta}_S - \\beta^*_S) \\overset{d}{\\to} \\mathcal{N}(0, \\Sigma_S),\\] where \\(\\Sigma_S\\) is the variance that would result from estimating only on the true subset \\(S\\).\n\n\n\nThe Adaptive Lasso\nThe ordinary lasso doesn‚Äôt quite cut it. While it‚Äôs great for variable selection and shrinkage, it tends to be biased and doesn‚Äôt consistently identify the correct support. But its cousin‚Äîthe adaptive lasso‚Äîfixes this, at least under certain conditions.\nThe adaptive lasso solves:\n\\[\\hat{\\beta}^{\\text{AL}} = \\arg\\min_{\\beta} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^p w_j |\\beta_j| \\right\\},\\]\nwhere $w_j = 1 / |_j|^$, and $ $ is an initial consistent estimator (e.g., OLS or ridge), and \\(\\gamma &gt; 0\\) is a tuning parameter.\nThese weights penalize small coefficients more harshly than large ones, allowing relevant predictors to remain in the model while aggressively zeroing out the rest. Under mild regularity conditions, this approach achieves the oracle property.\n\n\nWhat If an Estimator Lacks the Oracle Property?\nMost estimators don‚Äôt possess the oracle property. They might: - Include irrelevant variables (false positives), - Miss relevant ones (false negatives), - Estimate effects with too much bias or variance.\nEven the basic lasso, which shrinks coefficients toward zero, doesn‚Äôt achieve consistent variable selection unless some strong assumptions hold (e.g., the irrepresentable condition).\nThat‚Äôs why oracle properties are a holy grail: they offer both variable selection and precise estimation.\n\n\nAnother Meaning of ‚ÄúOracle‚Äù in Machine Learning\nOutside variable selection, ‚Äúoracle‚Äù can mean something else entirely. In theoretical machine learning, especially in PAC learning or statistical learning theory, an oracle refers to a black-box access mechanism. You might see references to an ‚Äúoracle access to the true distribution‚Äù or an ‚Äúoracle for labeling.‚Äù Here, it‚Äôs not about variable selection, but about having perfect knowledge or access to something that is typically unknowable.\nSo context matters: the oracle property is a specific feature of an estimator; an oracle in learning theory is more like a tool you‚Äôre given in a thought experiment."
  },
  {
    "objectID": "blog-unpublished/TODO-oracle-property.html#an-example",
    "href": "blog-unpublished/TODO-oracle-property.html#an-example",
    "title": "The Oracle Property in Machine Learning",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs try out the adaptive lasso in action.\n\nRPython\n\n\nlibrary(glmnet)\n\n# Simulated data\nset.seed(123)\nn &lt;- 100; p &lt;- 20\nX &lt;- matrix(rnorm(n * p), n, p)\nbeta_true &lt;- c(rep(2, 5), rep(0, 15))\ny &lt;- X %*% beta_true + rnorm(n)\n\n# Initial OLS estimate for weights\nbeta_ols &lt;- coef(lm(y ~ X - 1))\nweights &lt;- 1 / abs(beta_ols)^1  # gamma = 1\n\n# Adaptive lasso using glmnet with weights\nfit &lt;- glmnet(X, y, alpha = 1, penalty.factor = weights)\ncoef(fit, s = \"lambda.min\")\n\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(123)\nn, p = 100, 20\nX = np.random.randn(n, p)\nbeta_true = np.concatenate([np.repeat(2.0, 5), np.zeros(15)])\ny = X @ beta_true + np.random.randn(n)\n\n# Initial OLS for weights\nols = LinearRegression().fit(X, y)\nweights = 1 / np.abs(ols.coef_)\n\n# Adaptive lasso: reweight features\nX_scaled = StandardScaler().fit_transform(X)\nadaptive_lasso = Lasso(alpha=0.1, max_iter=10000)\nadaptive_lasso.coef_ = adaptive_lasso.fit(X_scaled * weights, y).coef_ / weights\nadaptive_lasso.coef_"
  },
  {
    "objectID": "blog-unpublished/TODO-oracle-property.html#bottom-line",
    "href": "blog-unpublished/TODO-oracle-property.html#bottom-line",
    "title": "The Oracle Property in Machine Learning",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe oracle property means an estimator selects the correct model and estimates coefficients as if it knew the truth.\nThe adaptive lasso is one estimator that can achieve this under certain conditions.\nMost common estimators, including the basic lasso, do not have the oracle property.\nThe term ‚Äúoracle‚Äù can also refer more broadly to hypothetical sources of perfect knowledge in learning theory."
  },
  {
    "objectID": "blog-unpublished/TODO-oracle-property.html#where-to-learn-more",
    "href": "blog-unpublished/TODO-oracle-property.html#where-to-learn-more",
    "title": "The Oracle Property in Machine Learning",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive into oracle properties and related asymptotics, check out the seminal work by Fan and Li (2001) or Zou (2006) on the adaptive lasso. For the broader ‚Äúoracle‚Äù idea in computational learning theory, Michael Kearns‚Äô work on computational learning theory is a great starting point. If you‚Äôre into theory with a practical bent, books like Elements of Statistical Learning also give a more intuitive overview of these ideas."
  },
  {
    "objectID": "blog-unpublished/TODO-oracle-property.html#references",
    "href": "blog-unpublished/TODO-oracle-property.html#references",
    "title": "The Oracle Property in Machine Learning",
    "section": "References",
    "text": "References\nFan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association.\nZou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association."
  },
  {
    "objectID": "blog-unpublished/ols-fixed-random-x.html",
    "href": "blog-unpublished/ols-fixed-random-x.html",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "",
    "text": "Linear regression stands as one of the fundamental workhorses of statistical modeling and is ubiquitous in applied work across fields like econometrics, biostatistics, and machine learning. Beneath its seemingly simple exterior lies a nuanced landscape of assumptions and theoretical considerations. One such distinction that often gets overlooked‚Äîeven by seasoned practitioners‚Äîis the difference between fixed and random regressors. This distinction, while subtle, has important implications for how we interpret our models, conduct inference, and understand uncertainty.\nThe goal of this article is to clarify the differences between fixed and random regressors in linear regression. We‚Äôll explore why this distinction matters and how it affects everything from your confidence intervals to your prediction errors. My goal is to help you develop both the technical understanding and the intuition needed to make informed modeling decisions in your day-to-day work."
  },
  {
    "objectID": "blog-unpublished/ols-fixed-random-x.html#background",
    "href": "blog-unpublished/ols-fixed-random-x.html#background",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "",
    "text": "Linear regression stands as one of the fundamental workhorses of statistical modeling and is ubiquitous in applied work across fields like econometrics, biostatistics, and machine learning. Beneath its seemingly simple exterior lies a nuanced landscape of assumptions and theoretical considerations. One such distinction that often gets overlooked‚Äîeven by seasoned practitioners‚Äîis the difference between fixed and random regressors. This distinction, while subtle, has important implications for how we interpret our models, conduct inference, and understand uncertainty.\nThe goal of this article is to clarify the differences between fixed and random regressors in linear regression. We‚Äôll explore why this distinction matters and how it affects everything from your confidence intervals to your prediction errors. My goal is to help you develop both the technical understanding and the intuition needed to make informed modeling decisions in your day-to-day work."
  },
  {
    "objectID": "blog-unpublished/ols-fixed-random-x.html#notation",
    "href": "blog-unpublished/ols-fixed-random-x.html#notation",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "Notation",
    "text": "Notation\nTo set the stage, consider the standard linear regression model:\n\\[Y = X\\beta + \\varepsilon,\\]\nwhere:\n\n\\(Y\\) is an $ n $ vector of outcomes,\n\\(X\\) is an $ n p $ matrix of regressors (also called covariates or features),\n\\(\\beta\\) is a \\(p \\times 1\\) vector of coefficients to be estimated,\n\\(\\varepsilon\\) is an $ n $ vector of errors.\n\nThe least squares estimator of \\(\\beta\\) is given by:\n\\[ \\hat{\\beta} = (X'X)^{-1}X'Y. \\]\nThe key question is: What do we assume about \\(X\\)? If $ X $ is considered fixed, we condition on it when deriving properties of \\(\\hat{\\beta}\\). If \\(X\\) is random, we take expectations over its distribution. This affects how we think about the variance of our estimator, the validity of standard errors, and the asymptotic behavior of \\(\\hat{\\beta}\\)."
  },
  {
    "objectID": "blog-unpublished/ols-fixed-random-x.html#a-closer-look",
    "href": "blog-unpublished/ols-fixed-random-x.html#a-closer-look",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nFixed Regressors\nIn the fixed regressor framework, we assume that \\(X\\) is determined beforehand‚Äîperhaps through experimental design or by conditioning on observed values. The randomness in our model comes solely from the error term \\(\\varepsilon\\).\nThe key assumptions typically made under this approach are:\n\nLinearity: The model is correctly specified as \\(Y = X\\beta + \\varepsilon\\).\nExogeneity: The errors satisfy $E[X] = 0 $, ensuring that there is no systematic relationship between \\(X\\) and \\(\\varepsilon\\).\nHomoskedasticity: The variance of errors is constant, i.e., \\(\\text{Var}(\\varepsilon \\mid X) = \\sigma^2 I_n\\).\nNo Perfect Multicollinearity: \\(X'X\\) is full rank, ensuring that the inverse \\((X'X)^{-1}\\) exists.\n\nIn the classical Gauss-Markov framework, the regressors \\(X\\) are treated as fixed. This means that we analyze the behavior of \\(\\hat{\\beta}\\) conditional on the observed \\(X\\).\nUnder these conditions, the ordinary least squares (OLS) estimator is unbiased and has the classical variance formula:\n\\[\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X'X)^{-1}.\\]\nAn important feature here is that all expectations and variances are conditional on \\(X\\). This conditioning makes sense because we‚Äôre treating \\(X\\) as fixed and known.\n\n\nRandom Regressors\nIn most real-world scenarios, our \\(X\\) variables aren‚Äôt actually fixed. We often sample observations from a population, making both \\(Y\\) and \\(X\\) random. The random regressor framework acknowledges this reality. Under this framework, we add assumptions about the distribution of \\(X\\).\nWhen \\(X\\) is treated as random, it is assumed to be drawn from some probability distribution. This changes how we analyze \\(\\hat{\\beta}\\), since expectations are now taken over both \\(\\varepsilon\\) and \\(X\\). The key assumptions in this setting are:\n\nJoint Distribution: \\((X, Y)\\) follows some joint distribution, meaning \\(X\\) is not fixed but rather drawn from a population.\nExogeneity in Expectation: In both frameworks, we typically assume \\(E[\\varepsilon \\mid X] = 0\\), which rules out omitted variable bias. However, under the random regressor perspective, we also account for the fact that \\(X\\) itself is drawn from a distribution, and expectations (e.g., for variance) are taken over both \\(X\\) and \\(\\varepsilon\\).\nLaw of Large Numbers: As \\(n \\to \\infty\\), the sample quantities \\(\\frac{1}{n} X'X\\) converge to their population analogs.\n\nA major consequence of assuming \\(X\\) is random is that the variance of \\(\\hat{\\beta}\\) takes a different form:\n\\[\\text{Var}(\\hat{\\beta}) = E[(X'X)^{-1} X' \\varepsilon \\varepsilon' X (X'X)^{-1}].\\]\nThis expression accounts for uncertainty in both $ X $ and $ $, and under large samples, it converges to the population variance.\n\n\nPractical Implications\nSo what‚Äôs the big deal? Here are some practical implications:\n\nInference: In the fixed regressor framework, inference is conditional on the observed values of \\(X\\)‚Äîyou‚Äôre estimating the best linear approximation given this specific sample design. In the random regressor case, inference targets a population-level relationship between \\(X\\) and \\(Y\\).\nPrediction Error: With fixed regressors, prediction error only accounts for the randomness in \\(\\varepsilon\\). With random regressors, you must also account for the randomness in future $ $ values.\nRobustness: The random regressor framework tends to provide more realistic assessments of model uncertainty, especially when extrapolating.\n\nLet me put this in more intuitive terms: if you‚Äôre designing an experiment where you precisely control the levels of your predictors, the fixed regressor framework makes sense. If you‚Äôre analyzing observational data where both predictors and responses are sampled from a population, the random regressor framework is more appropriate."
  },
  {
    "objectID": "blog-unpublished/ols-fixed-random-x.html#bottom-line",
    "href": "blog-unpublished/ols-fixed-random-x.html#bottom-line",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe distinction between fixed and random regressors affects estimation and inference.\nFixed regressors condition on \\(X\\), while random regressors integrate \\(X\\) over its distribution.\nOLS properties such as bias and variance differ under these assumptions.\nIn practice, whether \\(X\\) is fixed or random depends on the study design and intended inference."
  },
  {
    "objectID": "blog-unpublished/ols-fixed-random-x.html#references",
    "href": "blog-unpublished/ols-fixed-random-x.html#references",
    "title": "OLS with Fixed versus Random Regressors",
    "section": "References",
    "text": "References\nGreene, W. H. (2012). Econometric Analysis. Pearson. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springe"
  },
  {
    "objectID": "blog-unpublished/ci-residualized-reg.html",
    "href": "blog-unpublished/ci-residualized-reg.html",
    "title": "Causal Inference with Residualized Regressions",
    "section": "",
    "text": "The Frisch-Waugh-Lovell (FWL) theorem offers an elegant alternative to standard multiple regression when estimating causal effects. Instead of running a full regression with treatment and control variables, FWL demonstrates that we can obtain identical treatment coefficient estimates by first ‚Äúresidualizing‚Äù both the outcome and treatment variables with respect to controls, then regressing these residuals against each other. This approach provides both computational advantages and conceptual clarity.\nThis theorem effectively decomposes multiple regression into simpler components, helping researchers understand what happens ‚Äúunder the hood‚Äù of regression models. By residualizing variables‚Äîremoving the components explained by control variables‚Äîwe can isolate and estimate treatment effects more intuitively, essentially peeling away confounding layers to reveal the core relationship of interest.\nFWL proves particularly valuable in high-dimensional settings and causal inference applications, offering researchers a powerful framework for understanding causal relationships. Whether working with observational studies, quasi-experiments, or complex datasets, this approach allows analysts to conceptualize how controlling for confounders isolates the treatment-outcome relationship they truly want to measure.\nA related and often underappreciated issue is how standard errors behave under the FWL theorem. A recent paper by Peng Ding (2021) shows that using FWL residualization in regression with random regressors can lead to incorrect standard errors unless the residualization is adjusted appropriately. Ding emphasizes that while the point estimates remain identical, the naive residual-on-residual approach may mischaracterize the variance, especially when the regressors are stochastic. This highlights how intertwined design assumptions and inferential correctness can be."
  },
  {
    "objectID": "blog-unpublished/ci-residualized-reg.html#background",
    "href": "blog-unpublished/ci-residualized-reg.html#background",
    "title": "Causal Inference with Residualized Regressions",
    "section": "",
    "text": "The Frisch-Waugh-Lovell (FWL) theorem offers an elegant alternative to standard multiple regression when estimating causal effects. Instead of running a full regression with treatment and control variables, FWL demonstrates that we can obtain identical treatment coefficient estimates by first ‚Äúresidualizing‚Äù both the outcome and treatment variables with respect to controls, then regressing these residuals against each other. This approach provides both computational advantages and conceptual clarity.\nThis theorem effectively decomposes multiple regression into simpler components, helping researchers understand what happens ‚Äúunder the hood‚Äù of regression models. By residualizing variables‚Äîremoving the components explained by control variables‚Äîwe can isolate and estimate treatment effects more intuitively, essentially peeling away confounding layers to reveal the core relationship of interest.\nFWL proves particularly valuable in high-dimensional settings and causal inference applications, offering researchers a powerful framework for understanding causal relationships. Whether working with observational studies, quasi-experiments, or complex datasets, this approach allows analysts to conceptualize how controlling for confounders isolates the treatment-outcome relationship they truly want to measure.\nA related and often underappreciated issue is how standard errors behave under the FWL theorem. A recent paper by Peng Ding (2021) shows that using FWL residualization in regression with random regressors can lead to incorrect standard errors unless the residualization is adjusted appropriately. Ding emphasizes that while the point estimates remain identical, the naive residual-on-residual approach may mischaracterize the variance, especially when the regressors are stochastic. This highlights how intertwined design assumptions and inferential correctness can be."
  },
  {
    "objectID": "blog-unpublished/ci-residualized-reg.html#notation",
    "href": "blog-unpublished/ci-residualized-reg.html#notation",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Notation",
    "text": "Notation\nConsider a standard linear model:\n\\[Y = \\alpha + D\\tau + X\\beta + \\varepsilon, \\]\nwhere:\n\n\\(Y\\) is the outcome variable (e.g., earnings),\n\\(D\\) is the treatment variable (e.g., whether a person attended a job training program),\n\\(X\\) is a vector of control variables (e.g., age, education, experience),\n\\(\\tau\\) is the treatment effect we want to estimate,\n\\(\\beta\\) represents the coefficients on the controls,\n\\(\\varepsilon\\) is the error term."
  },
  {
    "objectID": "blog-unpublished/ci-residualized-reg.html#a-closer-look",
    "href": "blog-unpublished/ci-residualized-reg.html#a-closer-look",
    "title": "Causal Inference with Residualized Regressions",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe FWL Theorem\nThe OLS estimate of \\(\\tau\\) in the full regression includes both \\(D\\) and \\(X\\). However, FWL tells us that we can obtain the same estimate of \\(\\tau\\) by following these steps:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRegress \\(Y\\) on \\(X\\) and collect the residuals \\(\\tilde{Y}\\).\nRegress \\(D\\) on \\(X\\) and collect the residuals \\(\\tilde{D}\\).\nRegress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) (without an intercept). The coefficient on \\(\\tilde{D}\\) is exactly \\(\\tau\\).\n\n\n\n\n\nIntuition\nThe intuition behind FWL is simple yet profound. When we regress \\(Y\\) on \\(X\\), we strip out the variation in \\(Y\\) that is explained by \\(X\\), leaving only the part orthogonal to \\(X\\). Similarly, regressing \\(D\\) on \\(X\\) removes the influence of \\(X\\) on \\(D\\), isolating the component of \\(D\\) that is independent of \\(X\\). Since X has been accounted for in both cases, the regression of \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) retrieves the direct relationship between \\(D\\) and \\(Y\\), net of \\(X\\).\nMathematically, the key result of FWL is:\n\\[\\hat{\\tau} = (D' M_X D)^{-1} D' M_X Y,\\]\nwhere \\(M_X = I - X(X'X)^{-1}X'\\) is the projection matrix that residualizes variables with respect to \\(X\\). This shows that the estimate of remains unchanged whether we use the full regression or the residualized regression.\nSo what‚Äôs really happening here? The residuals \\(\\tilde{D}\\) represent the part of the treatment variable that cannot be explained by the control variables. Similarly, \\(\\tilde{\\mathbf{y}}\\) ‚Äã represents the part of the outcome that cannot be explained by the controls.\nThe FWL procedure essentially isolates the relationship between treatment and outcome after removing the influence of all the control variables. It‚Äôs as if we‚Äôre looking at the relationship in a ‚Äúpurified‚Äù form, with all the confounding effects of the controls stripped away.\nThis provides a powerful framework for causal inference. Under the assumption that we‚Äôve included all relevant confounders in our control set, the coefficient \\(\\hat{\\tau}\\) from the residualized regression can be interpreted as the causal effect of the treatment on the outcome.\nThink of it this way: we‚Äôre first ‚Äúadjusting‚Äù both our treatment and outcome variables by removing the predictable parts based on our controls. Then we‚Äôre examining how the ‚Äúadjusted‚Äù treatment relates to the ‚Äúadjusted‚Äù outcome. This residual-on-residual regression gives us our causal estimate.\n\n\nGeometric Interpretation\nGeometrically, we can think of the FWL theorem in terms of projections in vector space. The residuals \\(\\tilde{D}\\) and \\(\\tilde{\\mathbf{y}}\\)‚Äã are what remain after projecting \\(D\\) and \\(Y\\) onto the orthogonal complement of the space spanned by \\(Z\\).\nIn other words, we‚Äôre looking at the components of \\(D\\) and \\(Y\\) that are orthogonal to (i.e., cannot be explained by) the control variables. The relationship between these orthogonal components gives us our treatment effect estimate.\n\n\nVariance and Standard Errors\nThe precision of our treatment effect estimate depends on how much variation in the treatment remains after accounting for the controls. If the treatment is highly collinear with the controls (meaning \\(\\tilde{D}\\)$ has little variation), our estimate will be imprecise.\nThis helps us understand the ‚Äúcurse of dimensionality‚Äù in causal inference. As we add more control variables, we might reduce omitted variable bias, but we also risk reducing the effective variation in our treatment, leading to less precise estimates.\n\n\nPractical Implications\n\nConceptual clarity: FWL emphasizes that controlling for \\(X\\) means adjusting both \\(Y\\) and \\(D\\) before examining their relationship.\nComputational benefits: In high-dimensional settings, it is often more efficient to work with residualized variables rather than estimating the full model.\nInstrumental variables and two-stage regression: The residualization step is analogous to first-stage regressions in instrumental variable estimation.\nTwo separate data sources: In some cases, possibly due to data privacy concerns, the three variables \\(Y\\), \\(X\\) and \\(D\\) might live in two separate datasets - one with \\(Y\\) and \\(X\\), and the other one with \\(D\\) and \\(X\\). The traditional approach is then not avaialable."
  },
  {
    "objectID": "blog-unpublished/ci-residualized-reg.html#an-example",
    "href": "blog-unpublished/ci-residualized-reg.html#an-example",
    "title": "Causal Inference with Residualized Regressions",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs go through an example in R and python. We will generate synthetic data where the treatment effect is nonzero and show that both the full regression and the residualized approach give the same estimate.\n\nRPython\n\n\nrm(list=ls())\nset.seed(42)\nn &lt;- 1000\nX &lt;- matrix(rnorm(n * 3), ncol = 3)  # Three control variables\nD &lt;- 0.5 * X[,1] + 0.3 * X[,2] + rnorm(n)  # Treatment depends on controls\nY &lt;- 2 * D + 1.5 * X[,1] - 0.5 * X[,2] + 0.3 * X[,3] + rnorm(n)  # Outcome model\n\n# Full Regression\nfull_model &lt;- lm(Y ~ D + X)\nsummary(full_model)coefficients[\"D\",]  \n\n### Residualized Regression \nY_resid &lt;- residuals(lm(Y ~ X)) \nD_resid &lt;- residuals(lm(D ~ X)) \nresid_model &lt;- lm(Y_resid ~ D_resid - 1)  # No intercept \nsummary(resid_model)coefficients[\"D_resid\",]\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data\nn = 1000\nX = np.random.normal(size=(n, 3))  # Three control variables\nD = 0.5 * X[:, 0] + 0.3 * X[:, 1] + np.random.normal(size=n)  # Treatment depends on controls\nY = 2 * D + 1.5 * X[:, 0] - 0.5 * X[:, 1] + 0.3 * X[:, 2] + np.random.normal(size=n)  # Outcome model\n\n# Full Regression\nfull_model = LinearRegression()\nfull_model.fit(np.column_stack((D, X)), Y)\nfull_coef = full_model.coef_[0]  # Coefficient for D\nprint(f\"Full Regression Coefficient for D: {full_coef}\")\n\n# Residualized Regression\n# Step 1: Residualize Y with respect to X\nY_resid_model = LinearRegression()\nY_resid_model.fit(X, Y)\nY_resid = Y - Y_resid_model.predict(X)\n\n# Step 2: Residualize D with respect to X\nD_resid_model = LinearRegression()\nD_resid_model.fit(X, D)\nD_resid = D - D_resid_model.predict(X)\n\n# Step 3: Regress residualized Y on residualized D\nresid_model = LinearRegression(fit_intercept=False)\nresid_model.fit(D_resid.reshape(-1, 1), Y_resid)\nresid_coef = resid_model.coef_[0]\nprint(f\"Residualized Regression Coefficient for D: {resid_coef}\")\n\n\n\nThe coefficient on \\(D\\) in the full model and the coefficient on \\(D_{resid}\\) in the residualized model will be identical. This demonstrates that controlling for \\(X\\) can be done implicitly by working with residuals."
  },
  {
    "objectID": "blog-unpublished/ci-residualized-reg.html#bottom-line",
    "href": "blog-unpublished/ci-residualized-reg.html#bottom-line",
    "title": "Causal Inference with Residualized Regressions",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe FWL theorem shows that controlling for variables in a regression can be done by residualizing first.\nThis approach helps conceptually separate the treatment effect from confounding influences.\nResidualization is particularly useful in high-dimensional settings and instrumental variables estimation.\nWhether you use the full regression or the residualized approach, you get the same treatment effect estimate."
  },
  {
    "objectID": "blog-unpublished/ci-residualized-reg.html#references",
    "href": "blog-unpublished/ci-residualized-reg.html#references",
    "title": "Causal Inference with Residualized Regressions",
    "section": "References",
    "text": "References\nDing, P. (2021). The Frisch‚ÄìWaugh‚ÄìLovell theorem for standard errors. Statistics & Probability Letters, 168, 108945."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg.html",
    "href": "blog-unpublished/unconditional-qreg.html",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "Quantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg.html#background",
    "href": "blog-unpublished/unconditional-qreg.html#background",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "Quantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg.html#notation",
    "href": "blog-unpublished/unconditional-qreg.html#notation",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Notation",
    "text": "Notation\nWe consider an outcome variable \\(Y\\) and a set of covariates \\(X\\). In traditional quantile regression, we estimate the conditional quantile function:\n\\[Q_\\tau(Y | X) = \\inf \\{ q : P(Y \\leq q | X) \\geq \\tau \\}.\\]\nThis tells us how the \\(\\tau\\)-th quantile of Y changes with \\(X\\). However, in many applications, we want to model the unconditional quantiles:\n\\[Q_\\tau(Y) = \\inf \\{ q : P(Y \\leq q) \\geq \\tau \\}.\\]\nUQR allows us to estimate how covariates influence these unconditional quantiles."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg.html#a-closer-look",
    "href": "blog-unpublished/unconditional-qreg.html#a-closer-look",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nBasics: Conditional vs.¬†Unconditional Quantiles\nThe fundamental distinction between conditional and unconditional quantile regressions is best understood through an example. Suppose we are studying wage distributions. A conditional quantile regression would estimate the effect of education on the wage quantile within a specific subgroup (e.g., workers with 5 years of experience). But what if we want to know the effect of education on the overall wage distribution? That‚Äôs where unconditional quantiles come in‚Äîthey capture the total effect of education, accounting for all pathways through which education might influence wages.\n\n\nUnconditional Quantile Regression\nFirpo et al.¬†introduced an elegant way to estimate UQR using influence functions. The influence function of a statistic measures how much that statistic changes when an observation is perturbed. The recentered influence function (RIF) for a quantile \\(Q_\\tau\\) is given by:\n\\[RIF(Y; Q_\\tau) = Q_\\tau + \\frac{\\tau - 1\\{Y \\leq Q_\\tau\\}}{f_Y(Q_\\tau)}.\\]\nHere, \\(f_Y(Q_\\tau)\\) is the density of \\(Y\\) at \\(Q_\\tau\\), which can be estimated nonparametrically. This nonparametric density estimation is often done via kernel density estimation but may be imprecise in the tails.\nFirpo et al.¬†showed that regressing \\(RIF(Y; Q_\\tau)\\) on covariates \\(X\\) via OLS provides a valid estimate of how \\(X\\) affects the \\(\\tau\\)-th quantile of \\(Y\\). This method is remarkably simple but powerful‚Äîit transforms a quantile regression problem into a standard linear regression problem.\n\n\nEstimation\nThe estimation proceeds in three steps:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nEstimate the sample quantile \\(q_{\\tau}\\).\nEstimate the density \\(f_Y(q_{\\tau})\\), typically via kernel density estimation.\nConstruct the RIF for each observation and regress it on the covariates.\n\n\n\nThe basic regression is: \\[\nRIF(Y; q_{\\tau}) = X' \\beta + \\varepsilon,\n\\]\nwhere \\(\\beta\\) now captures the effect of \\(X\\) on the \\(\\tau\\)-th unconditional quantile.\nThe most common implementation is RIF-OLS, though alternatives include RIF-Logit and nonparametric first stages (RIF-NP).\n\n\nInference and Challenges\n\nDensity Estimation: A critical step that affects the quality of inference. Poor density estimation at the quantile point can lead to noisy estimates.\nNonlinearity and Model Misspecification: RIF-OLS assumes a linear relationship between the RIF and covariates. If the true relationship is nonlinear, flexible methods (logit, nonparametric) are preferred.\nStandard Errors: Because of the multi-step estimation (quantile, density, RIF), standard error computation is more complex. Bootstrapping is commonly used.\nTreatment Effects: UQR is especially appealing for estimating treatment effects on the distribution of outcomes. When treatment is exogenous, including treatment indicators in the RIF regression yields estimates of the treatment effect at various unconditional quantiles.\n\n\n\nStrengths and Applications\nUQR shines in settings where the policy question concerns distributional effects, such as: - Wage inequality and labor economics. - Health outcomes across the full distribution. - Policy evaluation where shifting the covariate distribution is plausible.\nIt also generalizes to other distributional statistics (Gini, variance) by using the corresponding influence functions.\n\n\nUnconditional Quantile Treatment Effects\nOne limitation of UQR as formulated by Firpo et al.¬†is that it assumes covariates are exogenous. But in many causal inference settings, treatment assignment is endogenous (e.g., workers self-select into training programs). Fr√∂lich and Melly (2013) extended the UQR framework to handle endogeneity using instrumental variables (IV).\nThey showed that under standard IV assumptions‚Äîrelevance and exclusion‚Äîthe unconditional quantile treatment effect (UQTE) can be estimated using a two-step approach:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nEstimate a propensity score model (or an instrumented version of \\(D\\)) to account for selection bias.\nApply RIF regression to estimate the effect of the treatment on unconditional quantiles.\n\n\n\nThis approach provides a way to estimate distributional treatment effects while addressing selection bias‚Äîa crucial tool in policy evaluation and applied econometrics.\n\n\nRank Invariance in QTEs\nA crucial assumption often invoked in the estimation of quantile treatment effects (QTEs) is rank invariance. This assumption states that units maintain their rank in the outcome distribution after receiving the treatment. In other words, if a treated unit was at the 30th percentile of the untreated outcome distribution, it would remain at the 30th percentile of the treated distribution.\nWhile this assumption simplifies identification and interpretation of QTEs, it can be highly restrictive. It rules out the possibility that treatment reshuffles individuals across the distribution‚Äîa scenario that might be not only plausible but central in many applications.\nConsider a school voucher program that offers private school access to low-income students. The effect of such a program may be heterogeneous: for high-performing students, access might enhance performance due to better environments. But for low-performing students, the same access could lead to worse outcomes due to higher academic pressure or poor fit. As a result, the program could re-rank students in the outcome distribution, violating rank invariance.\nIn such settings, assuming rank invariance could lead to misleading conclusions about who benefits and who loses from treatment. Alternative approaches, like those based on quantile treatment effect bounds (e.g., Melly, 2005; Chernozhukov & Hansen, 2005), are more robust to such violations."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg.html#an-example",
    "href": "blog-unpublished/unconditional-qreg.html#an-example",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate these ideas with an example in R. We‚Äôll use the iris dataset to estimate the effect of Sepal.Length on different quantiles of Petal.Length using UQR.\n\nRPython\n\n\nrm(list=ls())\nlibrary(quantreg)\n\n# Load dataset\ndata(iris)\n\n# Estimate unconditional quantiles\ntaus &lt;- c(0.25, 0.50, 0.75)\nq_vals &lt;- quantile(iris$Petal.Length, probs = taus)  # Estimate quantiles\nf_hat &lt;- density(iris$Petal.Length)\n\n# Compute RIF values\nrif_values &lt;- lapply(1:3, function(i) {\n  q &lt;- q_vals[i]\n  f &lt;- f_hat$y[which.min(abs(f_hat$x - q))]\n  q + ((taus[i] - (iris$Petal.Length &lt;= q)) / f)\n})\n\n# Run RIF regression\nmodels &lt;- lapply(rif_values, function(rif) lm(rif ~ Sepal.Length, data = iris))\n\n# Print results\nlapply(models, summary)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Estimate unconditional quantiles\ntaus = [0.25, 0.50, 0.75]\nq_vals = np.quantile(iris['Petal.Length'], taus)  # Estimate quantiles\nf_hat = gaussian_kde(iris['Petal.Length'])\n\n# Compute RIF values\nrif_values = []\nfor i, tau in enumerate(taus):\n    q = q_vals[i]\n    f = f_hat(q)  # Density at the quantile\n    rif = q + ((tau - (iris['Petal.Length'] &lt;= q).astype(int)) / f)\n    rif_values.append(rif)\n\n# Run RIF regression\nmodels = []\nfor rif in rif_values:\n    model = LinearRegression(fit_intercept=True)\n    model.fit(iris[['Sepal.Length']], rif)\n    models.append(model)\n\n# Print results\nfor i, model in enumerate(models):\n    print(f\"Model {i + 1}:\")\n    print(f\"Coefficient for Sepal.Length: {model.coef_[0]}\")\n    print(f\"Intercept: {model.intercept_}\")\n\n\n\nThis simple example demonstrates how to estimate the effect of a covariate on unconditional quantiles using the RIF regression approach."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg.html#bottom-line",
    "href": "blog-unpublished/unconditional-qreg.html#bottom-line",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nUQR allows us to estimate the effect of covariates on unconditional quantiles, capturing total effects.\nThe RIF regression method transforms a quantile regression problem into a simple linear regression.\nFr√∂lich and Melly (2013) extend UQR to address endogeneity using instrumental variables.\nThese tools are invaluable for policy evaluation and causal inference."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg.html#where-to-learn-more",
    "href": "blog-unpublished/unconditional-qreg.html#where-to-learn-more",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive into these methods, the foundational paper by Firpo, Fortin, and Lemieux (2009) provides a detailed introduction to UQR, while Fr√∂lich and Melly (2013) extend the framework to address endogeneity concerns. For a broader perspective on quantile regression, Koenker‚Äôs book Quantile Regression (2005) is a must-read."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg.html#references",
    "href": "blog-unpublished/unconditional-qreg.html#references",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "References",
    "text": "References\nAlejo, J., Favata, F., Montes-Rojas, G., & Trombetta, M. (2021). Conditional vs unconditional quantile regression models: A guide to practitioners. Econom√≠a, 44(88), 76-93.\nBitler, M. P., Gelbach, J. B., & Hoynes, H. W. (2006). What mean impacts miss: Distributional effects of welfare reform experiments. American Economic Review, 96(4), 988-1012.\nBorah, B. J., & Basu, A. (2013). Highlighting differences between conditional and unconditional quantile regression approaches through an application to assess medication adherence. Health economics, 22(9), 1052-1070.\nBorgen, N. T. (2016). Fixed effects in unconditional quantile regression. The Stata Journal, 16(2), 403-415.\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional quantile regressions. Econometrica, 77(3), 953-973.\nFr√∂lich, M., & Melly, B. (2013). Unconditional quantile treatment effects under endogeneity. Journal of Business & Economic Statistics, 31(3), 346-357.\nSasaki, Y., Ura, T., & Zhang, Y. (2022). Unconditional quantile regression with high‚Äêdimensional data. Quantitative Economics, 13(3), 955-978."
  },
  {
    "objectID": "blog-unpublished/brief-intro-e-values.html",
    "href": "blog-unpublished/brief-intro-e-values.html",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. E-values can support decisions without the need to predefine a fixed significance level, allowing flexibility in interpreting evidence post hoc. Note, however, that this is a different type of control from traditional fixed-\\(\\alpha\\) control.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog-unpublished/brief-intro-e-values.html#background",
    "href": "blog-unpublished/brief-intro-e-values.html#background",
    "title": "A Brief Introduction to E-Values",
    "section": "",
    "text": "Statistical hypothesis testing has long been dominated by the use of \\(p\\)-values. However, \\(p\\)-values have several conceptual and practical limitations, particularly in their frequentist interpretation. They require a prespecified significance level and do not naturally accommodate sequential testing or decision-making in a flexible way. This has led to the search for alternative measures of statistical evidence, and one promising alternative is the e-value.\nE-values, as introduced in recent work by Gr√ºnwald (2024) and others, offer a decision-theoretic foundation for hypothesis testing that extends beyond the classical Neyman‚ÄìPearson framework. Unlike \\(p\\)-values, e-values provide Type-I risk control even when decision tasks are formulated post hoc. E-values can support decisions without the need to predefine a fixed significance level, allowing flexibility in interpreting evidence post hoc. Note, however, that this is a different type of control from traditional fixed-\\(\\alpha\\) control.\nIn this article, we‚Äôll explore the motivation behind e-values, define their mathematical properties, and illustrate their application using a simple example in R and python."
  },
  {
    "objectID": "blog-unpublished/brief-intro-e-values.html#notation",
    "href": "blog-unpublished/brief-intro-e-values.html#notation",
    "title": "A Brief Introduction to E-Values",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs set up the basic notation. Suppose we are testing a null hypothesis \\(H_0\\) against an alternative \\(H_1\\) based on observed data \\(Y\\). In classical Neyman‚ÄìPearson testing, we define a test statistic T(Y) and derive a \\(p\\)-value:\n\\[P(Y) = P_{H_0}(T(Y) \\geq T_{obs}),\\]\nwhere \\(P_{H_0}\\) represents probability under the null hypothesis and \\(T_{obs}\\) is the observed test statistic."
  },
  {
    "objectID": "blog-unpublished/brief-intro-e-values.html#a-closer-look",
    "href": "blog-unpublished/brief-intro-e-values.html#a-closer-look",
    "title": "A Brief Introduction to E-Values",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nDefinition\nE-values replace this with an alternative statistic, the e-variable, denoted as \\(S(Y)\\), which satisfies:\n\\[E_{H_0}[S(Y)] \\leq 1.\\]\nThis ensures that the e-value does not, on average, exceed 1 under the null, providing a valid way to quantify evidence against \\(H_0\\).\n\n\nWhy E-Values?\nThe fundamental problem with \\(p\\)-values is their lack of a clear decision-theoretic interpretation. They are often misunderstood and misused, leading to issues such as the replication crisis in scientific research. E-values address these issues by offering:\n\nInterpretability: Large e-values provide direct evidence against \\(H_0\\), unlike small \\(p\\)-values, which are difficult to interpret without a fixed alpha threshold.\nOptional Stopping: Because e-values maintain their validity even when a study stops based on interim results, they are useful in sequential testing.\nPost Hoc Decision-Making: Since e-values allow for Type-I risk control after seeing the data, they facilitate more flexible decision-making.\n\n\n\nConstructing the E-Values\nA common way to define an e-value is through a likelihood ratio:\n\\[S(Y) = \\frac{P_{H_1}(Y)}{P_{H_0}(Y)}.\\]\nThis is similar to a Bayes factor but differs in that it does not require a prior distribution over hypotheses. Another approach is to construct empirical e-values based on resampling methods or alternative test statistics that satisfy the expectation constraint.\nA more general definition involves e-processes, which allow for sequential testing. An e-process \\(\\{S_t\\}_{t=1}^{\\infty}\\) is a sequence of nonnegative random variables satisfying:\n\\[E_{H_0}[S_t | S_1, \\dots, S_{t-1}] \\leq S_{t-1}, \\quad \\forall t.\\]\nThis property ensures that the sequence remains a valid e-value throughout a study, making it particularly powerful for adaptive testing procedures.\nAnother important construction is via supermartingales. An e-value can be defined as a nonnegative random variable \\(S(Y)\\) such that \\(\\{S_t\\}\\) forms a nonnegative supermartingale under \\(H_0\\):\n\\[E_{H_0}[S_t | S_{t-1}] \\leq S_{t-1},\\]\nwhich guarantees that the expectation does not increase under the null hypothesis."
  },
  {
    "objectID": "blog-unpublished/brief-intro-e-values.html#comparison",
    "href": "blog-unpublished/brief-intro-e-values.html#comparison",
    "title": "A Brief Introduction to E-Values",
    "section": "Comparison",
    "text": "Comparison\n\n\n\n\n\n\n\n\n\nProperty\n\\(P\\)-Value\nE-Value\n\n\n\n\nInterpretability\nIndirect (requires fixed \\(\\alpha\\))\nDirect (large values indicate evidence)\n\n\nOptional Stopping\nNo\nYes\n\n\nSequential Analysis\nProblematic\nNatural\n\n\nPost Hoc Decisions\nNot well-defined\nValid risk guarantees"
  },
  {
    "objectID": "blog-unpublished/brief-intro-e-values.html#an-example",
    "href": "blog-unpublished/brief-intro-e-values.html#an-example",
    "title": "A Brief Introduction to E-Values",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs consider a simple hypothesis test using the iris dataset in R. We will test whether the mean Sepal.Length differs between two species using both a \\(p\\)-value and an e-value.\n\nRPython\n\n\n# Load Data\nrm(list=ls())\nlibrary(dplyr)\nlibrary(tibble)\n\n# Load dataset\ndata(iris)\niris_filtered &lt;- iris %&gt;% filter(Species %in% c(\"setosa\", \"versicolor\"))\n\n# Compute P-Value (Traditional T-Test)\np_value &lt;- t.test(Sepal.Length ~ Species, data = iris_filtered)$p.value\nprint(p_value)\n\n# Compute E-Value\n# Define likelihood under each hypothesis\nlikelihood_ratio &lt;- function(y, mu0, mu1, sigma) {\n  dnorm(y, mean = mu1, sd = sigma) / dnorm(y, mean = mu0, sd = sigma)\n}\n\n# Estimate parameters\nmu0 &lt;- mean(iris_filtered$Sepal.Length[iris_filtered$Species == \"setosa\"]) \nmu1 &lt;- mean(iris_filtered$Sepal.Length[iris_filtered$Species == \"versicolor\"])\nsigma &lt;- sd(iris_filtered$Sepal.Length)\n\n# Compute e-value\ne_values &lt;- sapply(iris_filtered$Sepal.Length, likelihood_ratio, mu0, mu1, sigma)\ne_value &lt;- mean(e_values)\nprint(e_value)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind, norm\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris['Species'] = iris_data['target']\niris['Species'] = iris['Species'].replace({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Filter dataset for two species\niris_filtered = iris[iris['Species'].isin(['setosa', 'versicolor'])]\n\n# Compute P-Value (Traditional T-Test)\nsetosa = iris_filtered[iris_filtered['Species'] == 'setosa']['Sepal.Length']\nversicolor = iris_filtered[iris_filtered['Species'] == 'versicolor']['Sepal.Length']\np_value = ttest_ind(setosa, versicolor).pvalue\nprint(f\"P-Value: {p_value}\")\n\n# Compute E-Value\n# Define likelihood ratio function\ndef likelihood_ratio(y, mu0, mu1, sigma):\n    return norm.pdf(y, loc=mu1, scale=sigma) / norm.pdf(y, loc=mu0, scale=sigma)\n\n# Estimate parameters\nmu0 = setosa.mean()\nmu1 = versicolor.mean()\nsigma = iris_filtered['Sepal.Length'].std()\n\n# Compute e-value\ne_values = [likelihood_ratio(y, mu0, mu1, sigma) for y in iris_filtered['Sepal.Length']]\ne_value = np.prod(e_values)\nprint(f\"E-Value: {e_value}\")\n\n\n\nThe e-value directly quantifies the strength of evidence against the null. Unlike the \\(p\\)-value, which requires an arbitrary threshold (e.g., \\(0.05\\)) to make a decision, the e-value provides a more interpretable measure of support for \\(H_1\\)."
  },
  {
    "objectID": "blog-unpublished/brief-intro-e-values.html#bottom-line",
    "href": "blog-unpublished/brief-intro-e-values.html#bottom-line",
    "title": "A Brief Introduction to E-Values",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nE-values provide a decision-theoretic alternative to \\(p\\)-values, offering clearer evidence quantification.\nThey naturally handle optional stopping and sequential testing, unlike \\(p\\)-values.\nThey allow for flexible, post hoc decision-making, making them particularly useful in real-world applications.\nLikelihood ratios and supermartingales serve as a natural basis for constructing e-values, making them conceptually simple yet powerful."
  },
  {
    "objectID": "blog-unpublished/brief-intro-e-values.html#where-to-learn-more",
    "href": "blog-unpublished/brief-intro-e-values.html#where-to-learn-more",
    "title": "A Brief Introduction to E-Values",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deep dive into e-values, Gr√ºnwald (2024) provides a rigorous mathematical foundation, discussing their application in decision theory and hypothesis testing. Another useful resource is Shafer (2021), which explores connections between e-values and likelihood ratios. For practical applications, Vovk & Wang (2019) discuss the use of e-values in sequential analysis and machine learning contexts."
  },
  {
    "objectID": "blog-unpublished/brief-intro-e-values.html#references",
    "href": "blog-unpublished/brief-intro-e-values.html#references",
    "title": "A Brief Introduction to E-Values",
    "section": "References",
    "text": "References\nGr√ºnwald, P. (2024). Beyond Neyman-Pearson: E-values enable hypothesis testing with a data-driven alpha. PNAS.\nShafer, G. (2021). Testing by betting: A strategy for statistical and scientific communication. Cambridge University Press.\nVovk, V., & Wang, R. (2019). E-values: Calibration, combination, and applications. Journal of the Royal Statistical Society."
  },
  {
    "objectID": "blog-unpublished/TODO-propensity-score-cont-var.html",
    "href": "blog-unpublished/TODO-propensity-score-cont-var.html",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "",
    "text": "Propensity scores are one of the most celebrated ideas in modern causal inference. They elegantly reduce a high-dimensional covariate adjustment problem into a one-dimensional balancing act. But here‚Äôs the catch: the canonical setup assumes a binary treatment. Treated or not. Drug or placebo. Exposed or unexposed. That‚Äôs great, but real-world interventions are rarely so black-and-white.\nWhat if your treatment is a dosage? Or a level of exposure? Or some index that varies continuously, like air pollution levels, advertising intensity, or participation hours in a program? That‚Äôs when we enter the more nuanced world of continuous treatment causal inference, and our trusty binary propensity score needs a serious upgrade.\nIn this article, we‚Äôll journey from the classic binary case through discrete levels of treatment, and ultimately arrive at the continuous treatment setting. Along the way, we‚Äôll unpack the intuition and math behind generalized propensity scores, density estimation, and what it really means to estimate a dose-response function. We‚Äôll arm you with the tools to think clearly and rigorously about treatment effects when there are more than just two treatment arms‚Äîor infinitely many."
  },
  {
    "objectID": "blog-unpublished/TODO-propensity-score-cont-var.html#background",
    "href": "blog-unpublished/TODO-propensity-score-cont-var.html#background",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "",
    "text": "Propensity scores are one of the most celebrated ideas in modern causal inference. They elegantly reduce a high-dimensional covariate adjustment problem into a one-dimensional balancing act. But here‚Äôs the catch: the canonical setup assumes a binary treatment. Treated or not. Drug or placebo. Exposed or unexposed. That‚Äôs great, but real-world interventions are rarely so black-and-white.\nWhat if your treatment is a dosage? Or a level of exposure? Or some index that varies continuously, like air pollution levels, advertising intensity, or participation hours in a program? That‚Äôs when we enter the more nuanced world of continuous treatment causal inference, and our trusty binary propensity score needs a serious upgrade.\nIn this article, we‚Äôll journey from the classic binary case through discrete levels of treatment, and ultimately arrive at the continuous treatment setting. Along the way, we‚Äôll unpack the intuition and math behind generalized propensity scores, density estimation, and what it really means to estimate a dose-response function. We‚Äôll arm you with the tools to think clearly and rigorously about treatment effects when there are more than just two treatment arms‚Äîor infinitely many."
  },
  {
    "objectID": "blog-unpublished/TODO-propensity-score-cont-var.html#notation",
    "href": "blog-unpublished/TODO-propensity-score-cont-var.html#notation",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs set up some notation that will help us keep our thoughts clean as we move through different levels of treatment granularity.\nLet: - \\(Y \\in \\mathbb{R}\\): observed outcome. - \\(T \\in \\mathbb{R}\\): treatment variable, which can be binary, discrete, or continuous. - \\(X \\in \\mathbb{R}^p\\): vector of observed pre-treatment covariates. - \\(Y(t)\\): potential outcome if the individual were assigned treatment level \\(t\\).\nOur goal is to estimate a treatment effect function, like: - For binary: \\(\\mathbb{E}[Y(1) - Y(0)]\\), - For continuous: \\(\\mathbb{E}[Y(t)]\\) for all \\(t \\in \\mathbb{R}\\), also known as the dose-response function.\nThe key object of interest generalizes accordingly: - In the binary case: the propensity score is \\(e(X) = \\Pr(T=1 \\mid X)\\). - In the continuous case: the generalized propensity score (GPS) is the conditional density \\(r(t, X) = f_{T|X}(t \\mid X)\\)."
  },
  {
    "objectID": "blog-unpublished/TODO-propensity-score-cont-var.html#a-closer-look",
    "href": "blog-unpublished/TODO-propensity-score-cont-var.html#a-closer-look",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nBinary Treatment: The Classic Setup\nIn the binary world, Rosenbaum and Rubin (1983) showed that adjusting for the propensity score \\(e(X)\\) is sufficient to remove confounding, under the assumption of strong ignorability: \\[\nY(1), Y(0) \\perp T \\mid X.\n\\]\nThey also proved that \\(Y(1), Y(0) \\perp T \\mid e(X)\\), which means we can reduce the dimensionality of covariate adjustment from \\(p\\) to 1. Score!\nPropensity scores can be estimated via logistic regression or any predictive ML method, and treatment effects can be estimated via matching, inverse probability weighting (IPW), or regression adjustment using \\(e(X)\\).\n\n\nDiscrete Treatment: More Than Two Levels\nWhat if treatment takes on three or more levels? Say, a low, medium, and high dose of a medication?\nIn that case, we generalize the propensity score into a vector: one probability for each treatment level, conditional on covariates: \\[\ne_j(X) = \\Pr(T = j \\mid X), \\quad j = 1, \\dots, K.\n\\]\nThe adjustment strategy is similar: match or weight across strata of these multinomial probabilities to balance covariates. Some methods treat this as a multi-class classification problem. But what if treatment isn‚Äôt just levels 1, 2, or 3, but a smooth continuum?\n\n\nContinuous Treatment: The Generalized Propensity Score\nNow we hit the interesting case. Suppose \\(T \\in \\mathbb{R}\\) and can take on many (even infinite) values. Instead of estimating a probability, we estimate a density. The generalized propensity score is defined as:\n\\[\nr(t, X) = f_{T|X}(t \\mid X),\n\\]\nthe conditional density of the treatment given covariates. This is a continuous analogue of the classic propensity score.\nJust like before, we assume weak unconfoundedness (Hirano and Imbens, 2004):\n\\[\nY(t) \\perp T \\mid X \\quad \\text{for all } t.\n\\]\nAnd just like in the binary case, conditioning on the GPS balances covariates, but now at each level of \\(t\\). Hirano and Imbens showed that adjusting for \\(r(T, X)\\) is sufficient for identification of the dose-response function \\(\\mu(t) = \\mathbb{E}[Y(t)]\\).\n\n\nEstimating the Dose-Response Function\nThe general workflow goes like this:\n\nEstimate the GPS: Fit a model for the conditional density \\(f_{T|X}(t \\mid X)\\). This could be a normal model, or a more flexible density estimator.\nModel the outcome given treatment and GPS: Fit a model for \\(\\mathbb{E}[Y \\mid T=t, R=r]\\), where \\(R = r(t, X)\\) is the estimated GPS.\nAverage over the population: For a fixed value \\(t\\), compute: \\[\n\\hat{\\mu}(t) = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}(t, \\hat{r}(t, X_i)),\n\\] where \\(\\hat{m}\\) is the estimated conditional expectation of \\(Y\\) given \\(T\\) and \\(R\\).\n\nThis approach estimates the full dose-response curve \\(t \\mapsto \\mu(t)\\), giving you a complete picture of how outcomes evolve with different levels of treatment.\n\n\nA Note on Density Estimation\nDensity estimation is the crux of this whole approach. You‚Äôre estimating \\(f_{T|X}(t \\mid X)\\), which can be tricky. If you assume normality:\n\\[\nT \\mid X \\sim \\mathcal{N}(\\mu(X), \\sigma^2(X)),\n\\]\nthen you can fit a regression model for \\(T\\) and use the residuals to compute the density. For more flexibility, kernel density estimation or machine learning methods like normalizing flows can be used to approximate the conditional distribution of \\(T \\mid X\\).\nKeep in mind: poor density estimation leads to poor GPS, which leads to biased treatment effect estimates. Garbage in, garbage out."
  },
  {
    "objectID": "blog-unpublished/TODO-propensity-score-cont-var.html#an-example",
    "href": "blog-unpublished/TODO-propensity-score-cont-var.html#an-example",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs try estimating a dose-response function using simulated data.\n\nRPython\n\n\nlibrary(MASS)\nlibrary(np)\n\n# Simulate data\nset.seed(42)\nn &lt;- 500\nX &lt;- matrix(rnorm(n * 3), n, 3)\nT &lt;- 0.5 * X[,1] - 0.3 * X[,2] + rnorm(n)\nY &lt;- 2 + 3 * T - T^2 + 0.5 * X[,1] + rnorm(n)\n\n# Estimate GPS via kernel regression\ngps_model &lt;- npcdensbw(xdat = data.frame(X), ydat = T)\ngps &lt;- npcdens(bws = gps_model, xdat = data.frame(X), ydat = T)$condens\n\n# Estimate outcome model\ndf &lt;- data.frame(Y = Y, T = T, GPS = gps)\nfit &lt;- lm(Y ~ poly(T, 2) + GPS + T*GPS, data = df)\n\n# Estimate dose-response\nt_vals &lt;- seq(min(T), max(T), length.out = 100)\ngps_vals &lt;- predict(np::npudens(~ T + X1 + X2 + X3, data = data.frame(T = t_vals, X1 = X[,1], X2 = X[,2], X3 = X[,3])))\npreds &lt;- predict(fit, newdata = data.frame(T = t_vals, GPS = gps_vals))\nplot(t_vals, preds, type = 'l', lwd = 2)\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KernelDensity\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Simulate data\nnp.random.seed(42)\nn = 500\nX = np.random.randn(n, 3)\nT = 0.5 * X[:, 0] - 0.3 * X[:, 1] + np.random.randn(n)\nY = 2 + 3 * T - T**2 + 0.5 * X[:, 0] + np.random.randn(n)\n\n# Estimate GPS: assume normality\nfrom sklearn.linear_model import LinearRegression\ngps_model = LinearRegression().fit(X, T)\nmu = gps_model.predict(X)\nresid = T - mu\nsigma = np.std(resid)\ngps = norm.pdf(T, loc=mu, scale=sigma)\n\n# Fit outcome model\ndf = pd.DataFrame({'Y': Y, 'T': T, 'GPS': gps})\ndf['T2'] = T**2\ndf['T_GPS'] = T * gps\nX_outcome = df[['T', 'T2', 'GPS', 'T_GPS']]\nfit = LinearRegression().fit(X_outcome, df['Y'])\n\n# Estimate dose-response\nt_vals = np.linspace(T.min(), T.max(), 100)\nmu_vals = gps_model.predict(X)\ndose_response = []\nfor t in t_vals:\n    gps_t = norm.pdf(t, loc=mu_vals, scale=sigma)\n    X_pred = np.column_stack((np.repeat(t, n), np.repeat(t**2, n), gps_t, t * gps_t))\n    preds = fit.predict(X_pred)\n    dose_response.append(np.mean(preds))\n\nplt.plot(t_vals, dose_response)\nplt.title(\"Estimated Dose-Response Function\")\nplt.xlabel(\"Treatment (T)\")\nplt.ylabel(\"Expected Outcome\")\nplt.show()"
  },
  {
    "objectID": "blog-unpublished/TODO-propensity-score-cont-var.html#bottom-line",
    "href": "blog-unpublished/TODO-propensity-score-cont-var.html#bottom-line",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nPropensity score methods can be extended beyond binary treatments to continuous treatments using generalized propensity scores.\nThe GPS is the conditional density of treatment given covariates: \\(f_{T‚à£X}(t\\mid X)\\).\nYou estimate the dose-response function by modeling outcomes as a function of both treatment and GPS, then averaging.\nEstimating the GPS well is crucial‚Äîgarbage density estimates lead to poor causal conclusions."
  },
  {
    "objectID": "blog-unpublished/TODO-propensity-score-cont-var.html#where-to-learn-more",
    "href": "blog-unpublished/TODO-propensity-score-cont-var.html#where-to-learn-more",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nIf you‚Äôre interested in mastering this topic, I recommend starting with Hirano and Imbens‚Äô 2004 Econometrica paper, which formally introduces the generalized propensity score framework. From there, take a look at more recent work in causal machine learning and semiparametric methods (like those in the DoubleML or EconML libraries), which handle continuous treatments and flexible outcome models. Also, keep an eye on newer approaches using generative models and normalizing flows for high-quality density estimation‚Äîan exciting frontier for continuous causal inference."
  },
  {
    "objectID": "blog-unpublished/TODO-propensity-score-cont-var.html#references",
    "href": "blog-unpublished/TODO-propensity-score-cont-var.html#references",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "References",
    "text": "References\nHirano, K., & Imbens, G. W. (2004). The propensity score with continuous treatments. Econometrica, 73(2), 731‚Äì748.\nImai, K., & van Dyk, D. A. (2004). Causal inference with general treatment regimes: Generalizing the propensity score. Journal of the American Statistical Association.\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika."
  },
  {
    "objectID": "blog-unpublished/propensity-score-cont-var.html",
    "href": "blog-unpublished/propensity-score-cont-var.html",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "",
    "text": "Propensity scores are one of the most celebrated ideas in modern causal inference. They elegantly reduce a high-dimensional covariate adjustment problem into a one-dimensional balancing act. But here‚Äôs the catch: the canonical setup assumes a binary treatment. Treated or not. Drug or placebo. Exposed or unexposed. That‚Äôs great, but real-world interventions are rarely so black-and-white.\nWhat if your treatment is a dosage? Or a level of exposure? Or some index that varies continuously, like air pollution levels, advertising intensity, or participation hours in a program? That‚Äôs when we enter the more nuanced world of continuous treatment causal inference, and our trusty binary propensity score needs a serious upgrade.\nIn this article, we‚Äôll journey from the classic binary case through discrete levels of treatment, and ultimately arrive at the continuous treatment setting. Along the way, we‚Äôll unpack the intuition and math behind generalized propensity scores, density estimation, and what it really means to estimate a dose-response function. We‚Äôll arm you with the tools to think clearly and rigorously about treatment effects when there are more than just two treatment arms‚Äîor infinitely many."
  },
  {
    "objectID": "blog-unpublished/propensity-score-cont-var.html#background",
    "href": "blog-unpublished/propensity-score-cont-var.html#background",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "",
    "text": "Propensity scores are one of the most celebrated ideas in modern causal inference. They elegantly reduce a high-dimensional covariate adjustment problem into a one-dimensional balancing act. But here‚Äôs the catch: the canonical setup assumes a binary treatment. Treated or not. Drug or placebo. Exposed or unexposed. That‚Äôs great, but real-world interventions are rarely so black-and-white.\nWhat if your treatment is a dosage? Or a level of exposure? Or some index that varies continuously, like air pollution levels, advertising intensity, or participation hours in a program? That‚Äôs when we enter the more nuanced world of continuous treatment causal inference, and our trusty binary propensity score needs a serious upgrade.\nIn this article, we‚Äôll journey from the classic binary case through discrete levels of treatment, and ultimately arrive at the continuous treatment setting. Along the way, we‚Äôll unpack the intuition and math behind generalized propensity scores, density estimation, and what it really means to estimate a dose-response function. We‚Äôll arm you with the tools to think clearly and rigorously about treatment effects when there are more than just two treatment arms‚Äîor infinitely many."
  },
  {
    "objectID": "blog-unpublished/propensity-score-cont-var.html#notation",
    "href": "blog-unpublished/propensity-score-cont-var.html#notation",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs set up some notation that will help us keep our thoughts clean as we move through different levels of treatment granularity.\nLet: - \\(Y \\in \\mathbb{R}\\): observed outcome. - \\(T \\in \\mathbb{R}\\): treatment variable, which can be binary, discrete, or continuous. - \\(X \\in \\mathbb{R}^p\\): vector of observed pre-treatment covariates. - \\(Y(t)\\): potential outcome if the individual were assigned treatment level \\(t\\).\nOur goal is to estimate a treatment effect function, like: - For binary: \\(\\mathbb{E}[Y(1) - Y(0)]\\), - For continuous: \\(\\mathbb{E}[Y(t)]\\) for all \\(t \\in \\mathbb{R}\\), also known as the dose-response function.\nThe key object of interest generalizes accordingly: - In the binary case: the propensity score is \\(e(X) = \\Pr(T=1 \\mid X)\\). - In the continuous case: the generalized propensity score (GPS) is the conditional density \\(r(t, X) = f_{T|X}(t \\mid X)\\)."
  },
  {
    "objectID": "blog-unpublished/propensity-score-cont-var.html#a-closer-look",
    "href": "blog-unpublished/propensity-score-cont-var.html#a-closer-look",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nBinary Treatment: The Classic Setup\nIn the binary world, Rosenbaum and Rubin (1983) showed that adjusting for the propensity score \\(e(X)\\) is sufficient to remove confounding, under the assumption of strong ignorability: \\[\nY(1), Y(0) \\perp T \\mid X.\n\\]\nThey also proved that \\(Y(1), Y(0) \\perp T \\mid e(X)\\), which means we can reduce the dimensionality of covariate adjustment from \\(p\\) to 1. Score!\nPropensity scores can be estimated via logistic regression or any predictive ML method, and treatment effects can be estimated via matching, inverse probability weighting (IPW), or regression adjustment using \\(e(X)\\).\n\n\nDiscrete Treatment: More Than Two Levels\nWhat if treatment takes on three or more levels? Say, a low, medium, and high dose of a medication?\nIn that case, we generalize the propensity score into a vector: one probability for each treatment level, conditional on covariates: \\[\ne_j(X) = \\Pr(T = j \\mid X), \\quad j = 1, \\dots, K.\n\\]\nThe adjustment strategy is similar: match or weight across strata of these multinomial probabilities to balance covariates. Some methods treat this as a multi-class classification problem. But what if treatment isn‚Äôt just levels 1, 2, or 3, but a smooth continuum?\n\n\nContinuous Treatment: The Generalized Propensity Score\nNow we hit the interesting case. Suppose \\(T \\in \\mathbb{R}\\) and can take on many (even infinite) values. Instead of estimating a probability, we estimate a density. The generalized propensity score is defined as:\n\\[\nr(t, X) = f_{T|X}(t \\mid X),\n\\]\nthe conditional density of the treatment given covariates. This is a continuous analogue of the classic propensity score.\nJust like before, we assume weak unconfoundedness (Hirano and Imbens, 2004):\n\\[\nY(t) \\perp T \\mid X \\quad \\text{for all } t.\n\\]\nAnd just like in the binary case, conditioning on the GPS balances covariates, but now at each level of \\(t\\). Hirano and Imbens showed that adjusting for \\(r(T, X)\\) is sufficient for identification of the dose-response function \\(\\mu(t) = \\mathbb{E}[Y(t)]\\).\n\n\nEstimating the Dose-Response Function\nThe general workflow goes like this:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nEstimate the GPS: Fit a model for the conditional density \\(f_{T|X}(t \\mid X)\\). This could be a normal model, or a more flexible density estimator.\nModel the outcome given treatment and GPS: Fit a model for \\(\\mathbb{E}[Y \\mid T=t, R=r]\\), where \\(R = r(t, X)\\) is the estimated GPS.\nAverage over the population: For a fixed value \\(t\\), compute: \\[\\hat{\\mu}(t) = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}(t, \\hat{r}(t, X_i)),\\] where \\(\\hat{m}\\) is the estimated conditional expectation of \\(Y\\) given \\(T\\) and \\(R\\).\n\n\n\nThis approach estimates the full dose-response curve \\(t \\mapsto \\mu(t)\\), giving you a complete picture of how outcomes evolve with different levels of treatment.\n\n\nA Note on Density Estimation\nDensity estimation is the crux of this whole approach. You‚Äôre estimating \\(f_{T|X}(t \\mid X)\\), which can be tricky. If you assume normality:\n\\[T \\mid X \\sim \\mathcal{N}(\\mu(X), \\sigma^2(X)),\\]\nthen you can fit a regression model for \\(T\\) and use the residuals to compute the density. For more flexibility, kernel density estimation or machine learning methods like normalizing flows can be used to approximate the conditional distribution of \\(T \\mid X\\).\nKeep in mind: poor density estimation leads to poor GPS, which leads to biased treatment effect estimates. Garbage in, garbage out.\nWhile density estimation is the cornerstone of the GPS approach, it becomes increasingly challenging as the number of covariates grows‚Äîa phenomenon often called the ‚Äúcurse of dimensionality.‚Äù In high-dimensional spaces issues include data sparistiy making local density estimation unreliable, parametric assumptions also become increasingly restircive. To address these challenges, consider dimensionality reduction techniques before density estimation, semi-parametric approaches that make assumptions on the functional form but allow flexibility in other aspects, or Bayesian nonparametric methods that adapt to the complexity of the data."
  },
  {
    "objectID": "blog-unpublished/propensity-score-cont-var.html#an-example",
    "href": "blog-unpublished/propensity-score-cont-var.html#an-example",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs try estimating a dose-response function using simulated data.\n\nRPython\n\n\nlibrary(MASS)\nlibrary(np)\n\n# Simulate data\nset.seed(1988)\nn &lt;- 500\nX &lt;- matrix(rnorm(n * 3), n, 3)\nT &lt;- 0.5 * X[,1] - 0.3 * X[,2] + rnorm(n)\nY &lt;- 2 + 3 * T - T^2 + 0.5 * X[,1] + rnorm(n)\n\n# Estimate GPS via kernel regression\ngps_model &lt;- npcdensbw(xdat = data.frame(X), ydat = T)\ngps &lt;- npcdens(bws = gps_model, xdat = data.frame(X), ydat = T)$condens\n\n# Estimate outcome model\ndf &lt;- data.frame(Y = Y, T = T, GPS = gps)\nfit &lt;- lm(Y ~ poly(T, 2) + GPS + T*GPS, data = df)\n\n# Estimate dose-response\nt_vals &lt;- seq(min(T), max(T), length.out = 100)\ngps_vals &lt;- predict(np::npudens(~ T + X1 + X2 + X3, data = data.frame(T = t_vals, X1 = X[,1], X2 = X[,2], X3 = X[,3])))\npreds &lt;- predict(fit, newdata = data.frame(T = t_vals, GPS = gps_vals))\nplot(t_vals, preds, type = 'l', lwd = 2)\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KernelDensity\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Simulate data\nnp.random.seed(1988)\nn = 500\nX = np.random.randn(n, 3)\nT = 0.5 * X[:, 0] - 0.3 * X[:, 1] + np.random.randn(n)\nY = 2 + 3 * T - T**2 + 0.5 * X[:, 0] + np.random.randn(n)\n\n# Estimate GPS: assume normality\nfrom sklearn.linear_model import LinearRegression\ngps_model = LinearRegression().fit(X, T)\nmu = gps_model.predict(X)\nresid = T - mu\nsigma = np.std(resid)\ngps = norm.pdf(T, loc=mu, scale=sigma)\n\n# Fit outcome model\ndf = pd.DataFrame({'Y': Y, 'T': T, 'GPS': gps})\ndf['T2'] = T**2\ndf['T_GPS'] = T * gps\nX_outcome = df[['T', 'T2', 'GPS', 'T_GPS']]\nfit = LinearRegression().fit(X_outcome, df['Y'])\n\n# Estimate dose-response\nt_vals = np.linspace(T.min(), T.max(), 100)\nmu_vals = gps_model.predict(X)\ndose_response = []\nfor t in t_vals:\n    gps_t = norm.pdf(t, loc=mu_vals, scale=sigma)\n    X_pred = np.column_stack((np.repeat(t, n), np.repeat(t**2, n), gps_t, t * gps_t))\n    preds = fit.predict(X_pred)\n    dose_response.append(np.mean(preds))\n\nplt.plot(t_vals, dose_response)\nplt.title(\"Estimated Dose-Response Function\")\nplt.xlabel(\"Treatment (T)\")\nplt.ylabel(\"Expected Outcome\")\nplt.show()"
  },
  {
    "objectID": "blog-unpublished/propensity-score-cont-var.html#bottom-line",
    "href": "blog-unpublished/propensity-score-cont-var.html#bottom-line",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nPropensity score methods can be extended beyond binary treatments to continuous treatments using generalized propensity scores.\nThe GPS is the conditional density of treatment given covariates: \\(f_{T‚à£X}(t\\mid X)\\).\nYou estimate the dose-response function by modeling outcomes as a function of both treatment and GPS, then averaging.\nEstimating the GPS well is crucial‚Äîgarbage density estimates lead to poor causal conclusions."
  },
  {
    "objectID": "blog-unpublished/propensity-score-cont-var.html#where-to-learn-more",
    "href": "blog-unpublished/propensity-score-cont-var.html#where-to-learn-more",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nIf you‚Äôre interested in mastering this topic, I recommend starting with Hirano and Imbens‚Äô 2004 Econometrica paper, which formally introduces the generalized propensity score framework. From there, take a look at more recent work in causal machine learning and semiparametric methods (like those in the DoubleML or EconML libraries), which handle continuous treatments and flexible outcome models. Also, keep an eye on newer approaches using generative models and normalizing flows for high-quality density estimation‚Äîan exciting frontier for continuous causal inference."
  },
  {
    "objectID": "blog-unpublished/propensity-score-cont-var.html#references",
    "href": "blog-unpublished/propensity-score-cont-var.html#references",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "References",
    "text": "References\nBrown, D. W., Greene, T. J., Swartz, M. D., Wilkinson, A. V., & DeSantis, S. M. (2021). Propensity score stratification methods for continuous treatments. Statistics in medicine, 40(5), 1189-1203.\nHirano, K., & Imbens, G. W. (2004). The propensity score with continuous treatments. Econometrica, 73(2), 731‚Äì748.\nImai, K., & van Dyk, D. A. (2004). Causal inference with general treatment regimes: Generalizing the propensity score. Journal of the American Statistical Association.\nFong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score for a continuous treatment: Application to the efficacy of political advertisements. The Annals of Applied Statistics, 12(1), 156-177.\nKluve, J., Schneider, H., Uhlendorff, A., & Zhao, Z. (2012). Evaluating continuous training programmes by using the generalized propensity score. Journal of the Royal Statistical Society Series A: Statistics in Society, 175(2), 587-617.\nMcCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). A tutorial on propensity score estimation for multiple treatments using generalized boosted models. Statistics in medicine, 32(19), 3388-3414.\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika."
  },
  {
    "objectID": "blog-unpublished/oracle-property.html",
    "href": "blog-unpublished/oracle-property.html",
    "title": "The Oracle Property in Machine Learning",
    "section": "",
    "text": "Imagine this: you‚Äôre trying to predict an outcome based on dozens or even hundreds of variables. You suspect only a few of them actually matter, but you don‚Äôt know which ones. Wouldn‚Äôt it be great if you had an oracle‚Äîa magical being who could whisper in your ear and tell you exactly which variables to use?\nIn machine learning and statistics, when we say that an estimator has the oracle property, we mean it behaves as if it had access to that oracle. Specifically, it consistently identifies the correct subset of relevant variables, and then estimates their effects with the same efficiency as if it had known the true model all along. That‚Äôs a big deal. Most estimators don‚Äôt come close.\nIn this post, we‚Äôll unpack the intuition and math behind the oracle property, explore estimators that (claim to) have it‚Äîlike the adaptive lasso‚Äîand clarify what it means for an estimator to not have this magical trait. We‚Äôll also briefly touch on another, broader use of the term ‚Äúoracle‚Äù in machine learning."
  },
  {
    "objectID": "blog-unpublished/oracle-property.html#background",
    "href": "blog-unpublished/oracle-property.html#background",
    "title": "The Oracle Property in Machine Learning",
    "section": "",
    "text": "Imagine this: you‚Äôre trying to predict an outcome based on dozens or even hundreds of variables. You suspect only a few of them actually matter, but you don‚Äôt know which ones. Wouldn‚Äôt it be great if you had an oracle‚Äîa magical being who could whisper in your ear and tell you exactly which variables to use?\nIn machine learning and statistics, when we say that an estimator has the oracle property, we mean it behaves as if it had access to that oracle. Specifically, it consistently identifies the correct subset of relevant variables, and then estimates their effects with the same efficiency as if it had known the true model all along. That‚Äôs a big deal. Most estimators don‚Äôt come close.\nIn this post, we‚Äôll unpack the intuition and math behind the oracle property, explore estimators that (claim to) have it‚Äîlike the adaptive lasso‚Äîand clarify what it means for an estimator to not have this magical trait. We‚Äôll also briefly touch on another, broader use of the term ‚Äúoracle‚Äù in machine learning."
  },
  {
    "objectID": "blog-unpublished/oracle-property.html#notation",
    "href": "blog-unpublished/oracle-property.html#notation",
    "title": "The Oracle Property in Machine Learning",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs ground ourselves in a simple linear regression model:\n\\[y_i = X_i^\\top \\beta^* + \\varepsilon_i, \\quad i = 1, \\dots, n,\\]\nwhere: - \\(y_i \\in \\mathbb{R}\\) is the outcome, - \\(X_i \\in \\mathbb{R}^p\\) is the vector of predictors (covariates), - \\(\\beta^* \\in \\mathbb{R}^p\\) is the true but unknown coefficient vector, - \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) are independent errors.\nWe assume that the true coefficient vector \\(\\beta^*\\) is sparse‚Äîthat is, many of its entries are exactly zero. Let \\(S = \\{j : \\beta^*_j \\neq 0\\}\\) be the support set of non-zero coefficients, and \\(s = |S|\\) its cardinality.\nThe dream is to recover both the support \\(S\\) and estimate the non-zero coefficients accurately."
  },
  {
    "objectID": "blog-unpublished/oracle-property.html#a-closer-look",
    "href": "blog-unpublished/oracle-property.html#a-closer-look",
    "title": "The Oracle Property in Machine Learning",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhat Does the Oracle Property Actually Mean?\nAn estimator \\(\\hat{\\beta}\\) has the oracle property if, as the sample size \\(n \\to \\infty\\):\n\n(Support Recovery) It correctly identifies the set of non-zero coefficients with probability tending to 1:\n\\[\\Pr(\\text{supp}(\\hat{\\beta}) = S) \\to 1.\\]\n(Asymptotic Efficiency) The estimator is asymptotically normal and efficient for the non-zero coefficients, just like the OLS estimator would be if you knew $ S $ in advance: \\[\\sqrt{n}(\\hat{\\beta}_S - \\beta^*_S) \\overset{d}{\\to} \\mathcal{N}(0, \\Sigma_S),\\] where \\(\\Sigma_S\\) is the variance that would result from estimating only on the true subset \\(S\\).\n\n\n\nThe Adaptive Lasso & SCAD\nThe ordinary lasso doesn‚Äôt quite cut it. While it‚Äôs great for variable selection and shrinkage, it tends to be biased and doesn‚Äôt consistently identify the correct support. But its cousin‚Äîthe adaptive lasso‚Äîfixes this, at least under certain conditions.\nThe adaptive lasso solves:\n\\[\\hat{\\beta}^{\\text{AL}} = \\arg\\min_{\\beta} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^p w_j |\\beta_j| \\right\\},\\]\nwhere \\(w_j = 1 / |\\tilde{\\beta}_j|^\\gamma\\), and \\(\\tilde{\\beta}\\) is an initial consistent estimator (e.g., OLS or ridge), and \\(\\gamma &gt; 0\\) is a tuning parameter.\nThese weights penalize small coefficients more harshly than large ones, allowing relevant predictors to remain in the model while aggressively zeroing out the rest. Under mild regularity conditions, this approach achieves the oracle property.\nThe Smoothly Clipped Absolute Deviation (SCAD) penalty, proposed by Fan and Li (2001), was one of the pioneering approaches that achieves the oracle property. Unlike the lasso, SCAD uses a non-concave penalty function that applies the same rate of penalization to small coefficients but continuously relaxes the penalty for larger coefficients, effectively reducing the estimation bias. Beyond SCAD and the adaptive lasso, several other estimators have been developed with oracle properties, including the Minimax Concave Penalty (MCP) introduced by Zhang (2010), which provides a smoother transition between penalized and unpenalized coefficients than SCAD. The elastic net with adaptive weights (adaptive elastic net) also possesses the oracle property while handling correlated predictors better than pure L1 methods. More recently, folded concave penalties like the transformed L1 (TL1) and the Log penalty have gained attention for their theoretical guarantees regarding the oracle property while offering computational advantages.\n\n\nWhat If an Estimator Lacks the Oracle Property?\nMost estimators don‚Äôt possess the oracle property. They might:\n\nInclude irrelevant variables (false positives),\nMiss relevant ones (false negatives),\nEstimate effects with too much bias or variance.\n\nEven the basic lasso, which shrinks coefficients toward zero, doesn‚Äôt achieve consistent variable selection unless some strong assumptions hold (e.g., the irrepresentable condition).\nThat‚Äôs why oracle properties are a holy grail: they offer both variable selection and precise estimation."
  },
  {
    "objectID": "blog-unpublished/oracle-property.html#an-example",
    "href": "blog-unpublished/oracle-property.html#an-example",
    "title": "The Oracle Property in Machine Learning",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs try out the adaptive lasso in action.\n\nRPython\n\n\nlibrary(glmnet)\n\n# Simulated data\nset.seed(1988)\nn &lt;- 100; p &lt;- 20\nX &lt;- matrix(rnorm(n * p), n, p)\nbeta_true &lt;- c(rep(2, 5), rep(0, 15))\ny &lt;- X %*% beta_true + rnorm(n)\n\n# Initial OLS estimate for weights\nbeta_ols &lt;- coef(lm(y ~ X - 1))\nweights &lt;- 1 / abs(beta_ols)^1  # gamma = 1\n\n# Adaptive lasso using glmnet with weights\nfit &lt;- glmnet(X, y, alpha = 1, penalty.factor = weights)\ncoef(fit, s = \"lambda.min\")\n\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(1988)\nn, p = 100, 20\nX = np.random.randn(n, p)\nbeta_true = np.concatenate([np.repeat(2.0, 5), np.zeros(15)])\ny = X @ beta_true + np.random.randn(n)\n\n# Initial OLS for weights\nols = LinearRegression().fit(X, y)\nweights = 1 / np.abs(ols.coef_)\n\n# Adaptive lasso: reweight features\nX_scaled = StandardScaler().fit_transform(X)\nadaptive_lasso = Lasso(alpha=0.1, max_iter=10000)\nadaptive_lasso.coef_ = adaptive_lasso.fit(X_scaled * weights, y).coef_ / weights\nadaptive_lasso.coef_"
  },
  {
    "objectID": "blog-unpublished/oracle-property.html#bottom-line",
    "href": "blog-unpublished/oracle-property.html#bottom-line",
    "title": "The Oracle Property in Machine Learning",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe oracle property means an estimator selects the correct model and estimates coefficients as if it knew the truth.\nThe adaptive lasso is one estimator that can achieve this under certain conditions.\nMost common estimators, including the basic lasso, do not have the oracle property.\nThe term ‚Äúoracle‚Äù can also refer more broadly to hypothetical sources of perfect knowledge in learning theory."
  },
  {
    "objectID": "blog-unpublished/oracle-property.html#where-to-learn-more",
    "href": "blog-unpublished/oracle-property.html#where-to-learn-more",
    "title": "The Oracle Property in Machine Learning",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive into oracle properties and related asymptotics, check out the seminal work by Fan and Li (2001) or Zou (2006) on the adaptive lasso. For the broader ‚Äúoracle‚Äù idea in computational learning theory, Michael Kearns‚Äô work on computational learning theory is a great starting point. If you‚Äôre into theory with a practical bent, books like Elements of Statistical Learning also give a more intuitive overview of these ideas."
  },
  {
    "objectID": "blog-unpublished/oracle-property.html#references",
    "href": "blog-unpublished/oracle-property.html#references",
    "title": "The Oracle Property in Machine Learning",
    "section": "References",
    "text": "References\nFan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association.\nZou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association."
  },
  {
    "objectID": "blog-unpublished/propensity-score-cont-var.html#covariate-balancing-generalized-propensity-scores-cbgps",
    "href": "blog-unpublished/propensity-score-cont-var.html#covariate-balancing-generalized-propensity-scores-cbgps",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Covariate Balancing Generalized Propensity Scores (CBGPS)",
    "text": "Covariate Balancing Generalized Propensity Scores (CBGPS)\nCBGPS, introduced by Fong, Hazlett, and Imai (2018), directly optimizes covariate balance rather than focusing on accurately modeling the treatment mechanism. It estimates weights \\(w_i\\) by solving a set of moment conditions that ensure covariates are uncorrelated with treatment after weighting: \\[ \\sum_{i=1}^n w_i(T_i - \\bar{T})X_i = 0 \\]\nThis approach avoids sequential modeling and estimation, potentially reducing bias from model misspecification. It‚Äôs particularly valuable when the treatment mechanism is complex or difficult to model accurately.\n\nDiagnostics\nAssessing GPS quality requires thorough diagnostics focused on three key areas: covariate balance checks, model diagnostics, and sensitivity analysis. For covariate balance, practitioners should evaluate whether correlations between covariates and treatment approach zero after GPS adjustment, check balance within treatment strata, and create visual plots of standardized differences. GPS model quality can be verified through residual analysis against covariates and treatments, Q-Q plots to assess distributional assumptions, and cross-validation to evaluate predictive performance. Sensitivity testing should include trimming extreme GPS values, comparing results across different model specifications, and conducting placebo tests on outcomes that should be unaffected by treatment. While perfect balance may be unattainable, these diagnostics build confidence in causal estimates by revealing substantial improvements over unadjusted comparisons and identifying potential estimation issues."
  },
  {
    "objectID": "blog-unpublished/difference-stat-sig-not-stat-sig.html",
    "href": "blog-unpublished/difference-stat-sig-not-stat-sig.html",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "",
    "text": "We‚Äôve all seen it‚Äîtwo estimates side by side, one with a \\(p\\)-value of 0.04 and the other with a \\(p\\)-value of 0.06. Cue the celebratory confetti for the ‚Äúsignificant‚Äù effect, and a sad trombone for the ‚Äúnot significant‚Äù one. But wait! Are those results really different from each other? If one \\(p\\)-value dips below the magical 0.05 and the other doesn‚Äôt, can we say that the two estimates differ in a meaningful way?\nThe short answer is: no. And that‚Äôs the central insight of Gelman and Stern (2006), a paper that gently but firmly calls out a persistent mistake in the way statistical results are interpreted. In this article, we‚Äôre going to unpack the logic, show the math, and make sure you never fall into this trap again‚Äîespecially if you‚Äôre doing research at scale or comparing multiple models or subgroups."
  },
  {
    "objectID": "blog-unpublished/difference-stat-sig-not-stat-sig.html#background",
    "href": "blog-unpublished/difference-stat-sig-not-stat-sig.html#background",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "",
    "text": "We‚Äôve all seen it‚Äîtwo estimates side by side, one with a \\(p\\)-value of 0.04 and the other with a \\(p\\)-value of 0.06. Cue the celebratory confetti for the ‚Äúsignificant‚Äù effect, and a sad trombone for the ‚Äúnot significant‚Äù one. But wait! Are those results really different from each other? If one \\(p\\)-value dips below the magical 0.05 and the other doesn‚Äôt, can we say that the two estimates differ in a meaningful way?\nThe short answer is: no. And that‚Äôs the central insight of Gelman and Stern (2006), a paper that gently but firmly calls out a persistent mistake in the way statistical results are interpreted. In this article, we‚Äôre going to unpack the logic, show the math, and make sure you never fall into this trap again‚Äîespecially if you‚Äôre doing research at scale or comparing multiple models or subgroups."
  },
  {
    "objectID": "blog-unpublished/difference-stat-sig-not-stat-sig.html#notation",
    "href": "blog-unpublished/difference-stat-sig-not-stat-sig.html#notation",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs consider two estimated effects, say from two different subgroups or models:\n\n\\(\\hat{\\theta}_1\\): Estimate from group 1 with standard error \\(SE_1\\)\n\\(\\hat{\\theta}_2\\): Estimate from group 2 with standard error \\(SE_2\\)\n\nEach of these is associated with a \\(p\\)-value from a test of the null hypothesis \\(H_0: \\theta_i = 0\\).\nYou might be tempted to look at the individual \\(p\\)-values:\n\n\\(p_1 &lt; 0.05\\) ‚Üí statistically significant\n\\(p_2 &gt; 0.05\\) ‚Üí not statistically significant\n\nThen jump to the conclusion: ‚ÄúThe effects are different!‚Äù\nBut to formally test whether the effects differ, what you actually need is a test of:\n\\[\nH_0: \\theta_1 = \\theta_2\n\\]\nThis is not the same as testing \\(\\theta_1 = 0\\) and \\(\\theta_2 = 0\\) separately."
  },
  {
    "objectID": "blog-unpublished/difference-stat-sig-not-stat-sig.html#a-closer-look",
    "href": "blog-unpublished/difference-stat-sig-not-stat-sig.html#a-closer-look",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Fallacy: ‚ÄúSignificant vs.¬†Not Significant‚Äù\nThis is the statistical equivalent of assuming that because one basketball team won a game by 1 point and another lost by 1 point, the winning team must be ‚Äúbetter‚Äù than the losing one. That doesn‚Äôt necessarily follow‚Äîit could just be noise.\nWhat we need to emphasize is this: The difference between significant and not significant is not itself statistically significant.\nIf \\(\\hat{\\theta}_1\\) is significantly different from zero, and \\(\\hat{\\theta}_2\\) is not, that tells us very little about whether \\(\\theta_1 \\neq \\theta_2\\). To evaluate that, we need to compare the two estimates directly.\n\n\nFormal Testing of Differences\nSo how do we actually test whether the estimates differ?\nThe key is to compute the standard error of the difference:\n\\[\nSE_{\\text{diff}} = \\sqrt{SE_1^2 + SE_2^2}\n\\]\nThen, compute the test statistic:\n\\[\nZ = \\frac{\\hat{\\theta}_1 - \\hat{\\theta}_2}{SE_{\\text{diff}}}\n\\]\nThis is just a standard Wald test for the difference between two independent estimates. Under the null hypothesis \\(H_0: \\theta_1 = \\theta_2\\), the \\(Z\\)-statistic is approximately standard normal, so you can compute a \\(p\\)-value from it directly.\nThe big insight? It‚Äôs entirely possible that:\n\n\\(\\hat{\\theta}_1\\) is statistically significant (\\(p_1 &lt; 0.05\\))\n\\(\\hat{\\theta}_2\\) is not (\\(p_2 &gt; 0.05\\))\nBut \\(|Z|\\) is small ‚Üí there‚Äôs no significant difference between the two!\n\nThis happens all the time in subgroup analyses, A/B tests with interactions, and model comparisons.\n\n\nIntuition and Visualization\nHere‚Äôs a helpful mental model. Imagine two normal distributions:\n\nOne centered slightly above zero (say, \\(\\hat{\\theta}_1 = 1.96\\))\nOne centered slightly below zero (say, \\(\\hat{\\theta}_2 = 1.65\\))\n\nWith standard errors of 1, the first just clears the 5% significance threshold, the second doesn‚Äôt. But the difference between the two estimates is a mere 0.31. That‚Äôs tiny relative to their standard errors, and certainly not enough to declare them ‚Äúdifferent.‚Äù\nIf you plotted the two distributions with their confidence intervals, you‚Äôd see huge overlap‚Äîso why treat them as meaningfully different?"
  },
  {
    "objectID": "blog-unpublished/difference-stat-sig-not-stat-sig.html#bottom-line",
    "href": "blog-unpublished/difference-stat-sig-not-stat-sig.html#bottom-line",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nA statistically significant result and a non-significant result do not imply a significant difference between effects.\nTo compare effects, test the difference directly using a formal hypothesis test.\nMisinterpreting significance this way leads to false conclusions in subgroup analyses and model comparisons.\nAlways report and interpret confidence intervals for comparisons‚Äînot just \\(p\\)-values."
  },
  {
    "objectID": "blog-unpublished/difference-stat-sig-not-stat-sig.html#where-to-learn-more",
    "href": "blog-unpublished/difference-stat-sig-not-stat-sig.html#where-to-learn-more",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive, the original Gelman and Stern (2006) paper is a classic. Andrew Gelman has also written extensively on this topic in his blog, often using real-life examples from scientific publications and the media. If you‚Äôre looking for broader reading on the pitfalls of significance testing, check out ‚ÄúThe Cult of Statistical Significance‚Äù by Ziliak and McCloskey, or ‚ÄúStatistical Rethinking‚Äù by Richard McElreath for a more Bayesian angle. Lastly, brushing up on basic hypothesis testing logic in texts like Casella and Berger or Wasserman‚Äôs All of Statistics is always a good idea."
  },
  {
    "objectID": "blog-unpublished/difference-stat-sig-not-stat-sig.html#references",
    "href": "blog-unpublished/difference-stat-sig-not-stat-sig.html#references",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "References",
    "text": "References\nGelman, A., & Stern, H. (2006). The difference between ‚Äúsignificant‚Äù and ‚Äúnot significant‚Äù is not itself statistically significant. The American Statistician, 60(4), 328‚Äì331.\nZiliak, S. T., & McCloskey, D. N. (2008). The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives. University of Michigan Press."
  },
  {
    "objectID": "blog-unpublished/diff-causal-predictive-models.html",
    "href": "blog-unpublished/diff-causal-predictive-models.html",
    "title": "Causal vs.¬†Predictive Modeling: Why the Difference Matters More Than You Think",
    "section": "",
    "text": "It‚Äôs one of the most common mix-ups I see among data scientists‚Äîespecially those coming from a machine learning background: confusing causal modeling with predictive modeling. On the surface, they look similar. You build a model, you include some variables, you fit it, and then you do‚Ä¶ something with the results. But under the hood, these two approaches serve fundamentally different goals and require very different mindsets.\nPredictive modeling is about building a machine that can forecast outcomes. Causal modeling is about understanding how the world works. And mixing them up can lead to some really bad decisions‚Äîlike launching a product based on a spurious correlation or controlling for the wrong variables and wiping out your treatment effect.\nThis post is for all the data scientists who‚Äôve ever wondered, ‚ÄúWhy can‚Äôt I just throw everything into my causal model like I do with my random forest?‚Äù Let‚Äôs unpack it."
  },
  {
    "objectID": "blog-unpublished/diff-causal-predictive-models.html#background",
    "href": "blog-unpublished/diff-causal-predictive-models.html#background",
    "title": "Causal vs.¬†Predictive Modeling: Why the Difference Matters More Than You Think",
    "section": "",
    "text": "It‚Äôs one of the most common mix-ups I see among data scientists‚Äîespecially those coming from a machine learning background: confusing causal modeling with predictive modeling. On the surface, they look similar. You build a model, you include some variables, you fit it, and then you do‚Ä¶ something with the results. But under the hood, these two approaches serve fundamentally different goals and require very different mindsets.\nPredictive modeling is about building a machine that can forecast outcomes. Causal modeling is about understanding how the world works. And mixing them up can lead to some really bad decisions‚Äîlike launching a product based on a spurious correlation or controlling for the wrong variables and wiping out your treatment effect.\nThis post is for all the data scientists who‚Äôve ever wondered, ‚ÄúWhy can‚Äôt I just throw everything into my causal model like I do with my random forest?‚Äù Let‚Äôs unpack it."
  },
  {
    "objectID": "blog-unpublished/diff-causal-predictive-models.html#notation",
    "href": "blog-unpublished/diff-causal-predictive-models.html#notation",
    "title": "Causal vs.¬†Predictive Modeling: Why the Difference Matters More Than You Think",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs fix some notation for clarity. Suppose we have:\n\n\\(Y\\): the outcome\n\\(T\\): a treatment or exposure variable\n\\(X\\): a set of covariates (features)\n\nIn predictive modeling, the goal is to estimate:\n\\[\n\\mathbb{E}[Y \\mid X]\n\\]\nIn causal modeling, we care about quantities like:\n\\[\n\\mathbb{E}[Y(1) - Y(0)]\n\\]\nwhere \\(Y(1)\\) is the potential outcome under treatment, and \\(Y(0)\\) is the outcome under control. This is the average treatment effect (ATE). The tricky part? We never observe both \\(Y(1)\\) and \\(Y(0)\\) for the same unit.\nSo causal inference becomes a game of counterfactuals‚Äîand that‚Äôs where all the complexity comes in."
  },
  {
    "objectID": "blog-unpublished/diff-causal-predictive-models.html#a-closer-look",
    "href": "blog-unpublished/diff-causal-predictive-models.html#a-closer-look",
    "title": "Causal vs.¬†Predictive Modeling: Why the Difference Matters More Than You Think",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nPredictive Modeling: Just Give Me Accuracy\nLet‚Äôs start with what most machine learning folks are familiar with: predictive models.\nIn predictive modeling, you‚Äôre judged by how well you can forecast \\(Y\\). That‚Äôs it. You can (and often do) throw in everything and the kitchen sink‚Äîlagged outcomes, future values of other variables (careful though!), variables that are correlated with the outcome but not necessarily meaningful in a causal sense.\nIt‚Äôs all good as long as it helps you reduce RMSE, increase AUC, or minimize cross-entropy loss. Data leakage is your main enemy, but otherwise, the bar for ‚Äúwhat goes in the model‚Äù is pretty low.\nNo one cares why your model works, only that it does.\n\n\nCausal Modeling: Think Harder, Control Smarter\nNow, enter the world of causal inference. The rules are completely different.\nIn causal modeling, the goal is not prediction, but isolation of the effect of \\(T\\) on \\(Y\\). And to do that, you need to control for confounders‚Äîvariables that affect both the treatment and the outcome. But here‚Äôs the catch: not all variables should be controlled for.\nThis is where the concept of bad controls comes in‚Äîvariables that are affected by the treatment (post-treatment variables), or colliders that open up backdoor paths and induce spurious associations.\nIn other words, in causal inference:\n\nIncluding the wrong variable can make things worse.\nYou must think hard about the causal structure of your data.\nDomain knowledge is critical.\n\nThrowing in ‚Äúeverything‚Äù like in a predictive model? That can completely destroy your estimate.\n\n\nPropensity Scores: Where Predictive and Causal Worlds Collide\nOne place where this confusion often plays out is in propensity score modeling.\nTo recap, the propensity score \\(e(X) = P(T = 1 \\mid X)\\) is the probability of receiving treatment given covariates. It‚Äôs often estimated via a logistic regression or ML model. Then, you use this score to adjust for differences between treated and control groups (e.g., via weighting or matching).\nAnd here‚Äôs the key point: your goal is not to get the best prediction of treatment. Your goal is to use the propensity score to balance covariates between groups. That‚Äôs it.\nSo even if a fancy XGBoost model gives you higher prediction accuracy, it may overfit or fail to achieve covariate balance‚Äîwhich defeats the purpose. In fact, some of the best-performing PS models (for causal purposes) may have terrible predictive accuracy but excel at achieving balance.\nThere‚Äôs a trade-off here:\n\nPredictive ML models focus on minimizing error.\nPropensity score models should optimize covariate balance.\n\nAnd that trade-off is why a more accurate model is not necessarily better for causal inference."
  },
  {
    "objectID": "blog-unpublished/diff-causal-predictive-models.html#an-example-if-applicable",
    "href": "blog-unpublished/diff-causal-predictive-models.html#an-example-if-applicable",
    "title": "Causal vs.¬†Predictive Modeling: Why the Difference Matters More Than You Think",
    "section": "An Example (if applicable)",
    "text": "An Example (if applicable)\nNot including one here, but a great exercise is to fit a logistic regression and a random forest for a propensity score and compare covariate balance (e.g., standardized mean differences) after weighting. You‚Äôll often be surprised."
  },
  {
    "objectID": "blog-unpublished/diff-causal-predictive-models.html#bottom-line",
    "href": "blog-unpublished/diff-causal-predictive-models.html#bottom-line",
    "title": "Causal vs.¬†Predictive Modeling: Why the Difference Matters More Than You Think",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nPredictive models are about forecasting outcomes; causal models are about estimating effects.\nIn causal inference, you must think carefully about what to include in the model‚Äî‚Äúbad controls‚Äù can bias results.\nPropensity scores should be judged by how well they balance covariates, not by how well they predict treatment.\nMore context and domain knowledge is usually required for causal models than for predictive ones."
  },
  {
    "objectID": "blog-unpublished/diff-causal-predictive-models.html#where-to-learn-more",
    "href": "blog-unpublished/diff-causal-predictive-models.html#where-to-learn-more",
    "title": "Causal vs.¬†Predictive Modeling: Why the Difference Matters More Than You Think",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor causal inference, the gold standard is Causal Inference: The Mixtape by Scott Cunningham or Mostly Harmless Econometrics by Angrist and Pischke. For a more technical treatment, check out Hern√°n and Robins‚Äô Causal Inference. Judea Pearl‚Äôs Book of Why adds more philosophical background. For those working with propensity scores specifically, papers by Peter Austin and Elizabeth Stuart are a great starting point. If you‚Äôre trying to navigate the blurry line between prediction and causation, Andrew Gelman‚Äôs blog has a wealth of insights too."
  },
  {
    "objectID": "blog-unpublished/diff-causal-predictive-models.html#references-if-applicable",
    "href": "blog-unpublished/diff-causal-predictive-models.html#references-if-applicable",
    "title": "Causal vs.¬†Predictive Modeling: Why the Difference Matters More Than You Think",
    "section": "References (if applicable)",
    "text": "References (if applicable)\nHern√°n, M. A., & Robins, J. M. (2020). Causal Inference: What If.\nCunningham, S. (2021). Causal Inference: The Mixtape.\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly Harmless Econometrics.\nPearl, J., & Mackenzie, D. (2018). The Book of Why: The New Science of Cause and Effect."
  },
  {
    "objectID": "blog-unpublished/non-symmetric-conf-intervals.html",
    "href": "blog-unpublished/non-symmetric-conf-intervals.html",
    "title": "Why Are Some Confidence Intervals Not Symmetric?",
    "section": "",
    "text": "If you‚Äôve ever looked at a confidence interval and thought, ‚ÄúHuh, that‚Äôs weird‚Äîit‚Äôs not centered around the estimate,‚Äù you‚Äôre not alone. Many data scientists are used to the idea that a \\(95\\%\\) confidence interval looks like estimate \\(\\pm\\) margin of error. And that‚Äôs often true‚Äîespecially for things like means from large samples, where the normal approximation kicks in. But it‚Äôs not always the case.\nIn this post, we‚Äôll look at why some confidence intervals are asymmetric, what causes the skew, and when to expect this behavior. We‚Äôll also see how different methods (and different data!) can lead to intervals that don‚Äôt look like what we might expect from a simple \\(t\\)-test."
  },
  {
    "objectID": "blog-unpublished/non-symmetric-conf-intervals.html#background",
    "href": "blog-unpublished/non-symmetric-conf-intervals.html#background",
    "title": "Why Are Some Confidence Intervals Not Symmetric?",
    "section": "",
    "text": "If you‚Äôve ever looked at a confidence interval and thought, ‚ÄúHuh, that‚Äôs weird‚Äîit‚Äôs not centered around the estimate,‚Äù you‚Äôre not alone. Many data scientists are used to the idea that a \\(95\\%\\) confidence interval looks like estimate \\(\\pm\\) margin of error. And that‚Äôs often true‚Äîespecially for things like means from large samples, where the normal approximation kicks in. But it‚Äôs not always the case.\nIn this post, we‚Äôll look at why some confidence intervals are asymmetric, what causes the skew, and when to expect this behavior. We‚Äôll also see how different methods (and different data!) can lead to intervals that don‚Äôt look like what we might expect from a simple \\(t\\)-test."
  },
  {
    "objectID": "blog-unpublished/non-symmetric-conf-intervals.html#notation",
    "href": "blog-unpublished/non-symmetric-conf-intervals.html#notation",
    "title": "Why Are Some Confidence Intervals Not Symmetric?",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs say we have a parameter \\(\\theta\\) that we want to estimate, and we compute an estimator \\(\\hat{\\theta}\\) from data. A \\((1‚àí\\alpha)\\) confidence interval is an interval \\([L,U]\\) such that:\n\\[P(L\\leq \\theta \\leq U) \\geq 1-\\alpha.\\]\nIf \\(\\hat{\\theta}\\) is symmetric and normally distributed, this confidence interval will typically be of the form:\n\\[ \\hat{\\theta} \\pm z_{\\frac{\\alpha}{2}} \\times SE(\\hat{\\theta}),\\]\nwhere \\(z_{\\frac{\\alpha}{2}}\\) is called ‚Äúcritical value‚Äù (often equal to \\(1.96\\)). But things get more interesting‚Äîand more asymmetric‚Äîwhen the distribution of is skewed, bounded, or derived from a nonlinear transformation."
  },
  {
    "objectID": "blog-unpublished/non-symmetric-conf-intervals.html#a-closer-look",
    "href": "blog-unpublished/non-symmetric-conf-intervals.html#a-closer-look",
    "title": "Why Are Some Confidence Intervals Not Symmetric?",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nSimple Example: Proportion Near 0 or 1\nSuppose you‚Äôre estimating a proportion \\(p\\), like the rate of success in a small sample. If \\(p\\) is close to \\(0\\), the distribution of \\(\\hat{p}\\) is skewed, and the Wald confidence interval (the usual \\(\\pm\\)$ formula) can produce nonsense‚Äîlike a lower bound less than \\(0\\).\nInstead, intervals based on the logit or Wilson score can be asymmetric. This is because the underlying transformation (like log-odds) isn‚Äôt symmetric in \\(p\\).\n\n\nWhy Intervals Get Skewed\nHere are some reasons why confidence intervals might be asymmetric:\n\nSkewed sampling distribution: Common when estimating quantities like variance or proportions near the boundaries.\nNonlinear transformations: If your estimator is transformed (like \\(log(\\hat{\\theta})\\) or \\(\\frac{1}{\\hat{\\theta}}\\)), the resulting CI will not be symmetric in \\(\\hat{\\theta}\\).\nBoundary constraints: If the parameter lies on \\([0,1]\\) or must be positive, then symmetric intervals may include impossible values.\nBootstrap methods: Percentile bootstrap intervals often yield asymmetric CIs because they use the empirical quantiles of a skewed sampling distribution.\nMaximum likelihood estimation: Asymptotic normality applies, but in small samples or near boundaries, the intervals can be skewed.\n\n\n\nBootstrap Percentile Example\nLet‚Äôs illustrate this with a small example using a skewed distribution.\n\nRPython\n\n\nset.seed(1982)\nx &lt;- rexp(50, rate = 1)  # Exponential distribution\nboot_means &lt;- replicate(1000, mean(sample(x, replace = TRUE)))\nquantile(boot_means, c(0.025, 0.975))  # Asymmetric CI\n\n\nimport numpy as np\nnp.random.seed(42)\nx = np.random.exponential(scale=1.0, size=50)\nboot_means = [np.mean(np.random.choice(x, size=50, replace=True)) for _ in range(1000)]\nnp.percentile(boot_means, [2.5, 97.5])  # Asymmetric CI\n\n\n\nIn both cases, you‚Äôll likely see that the CI is skewed‚Äîbecause the sampling distribution of the mean is skewed, especially with small samples from an exponential distribution.\nLet‚Äôs expand on the exponential distribution example to demonstrate exactly how much asymmetry can appear in confidence intervals. The exponential distribution is right-skewed, making it perfect for illustrating asymmetric intervals.\n\nRPython\n\n\n# Set seed for reproducibility\nset.seed(1982)\n\n# Generate 50 observations from an exponential distribution\nx &lt;- rexp(50, rate = 1)\n\n# Calculate the sample mean\nsample_mean &lt;- mean(x)\n\n# Generate 10,000 bootstrap samples and calculate means\nboot_means &lt;- replicate(10000, mean(sample(x, replace = TRUE)))\n\n# Calculate the percentile-based 95% confidence interval\nci_percentile &lt;- quantile(boot_means, c(0.025, 0.975))\n\n# Calculate how far each bound is from the point estimate\nlower_distance &lt;- sample_mean - ci_percentile[1]\nupper_distance &lt;- ci_percentile[2] - sample_mean\n\n# for comparison - Symmetric 95% CI using normal approximation\nse &lt;- sd(x) / sqrt(length(x))\nci_symmetric &lt;- c(sample_mean - 1.96*se, sample_mean + 1.96*se)\n\n\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(1982)\n\n# Generate 50 observations from an exponential distribution\nx = np.random.exponential(scale=1.0, size=50)\n\n# Calculate the sample mean\nsample_mean = np.mean(x)\n\n# Generate 10,000 bootstrap samples and calculate means\nboot_means = [np.mean(np.random.choice(x, size=50, replace=True)) for _ in range(10000)]\n\n# Calculate the percentile-based 95% confidence interval\nci_percentile = np.percentile(boot_means, [2.5, 97.5])\n\n# Calculate how far each bound is from the point estimate\nlower_distance = sample_mean - ci_percentile[0]\nupper_distance = ci_percentile[1] - sample_mean\n\n# For comparison - Symmetric 95% CI using normal approximation\nse = np.std(x, ddof=1) / np.sqrt(len(x))\nci_symmetric = [sample_mean - 1.96 * se, sample_mean + 1.96 * se]\n\n# Print results\nprint(\"Sample Mean:\", sample_mean)\nprint(\"Percentile-based 95% CI:\", ci_percentile)\nprint(\"Lower Distance:\", lower_distance)\nprint(\"Upper Distance:\", upper_distance)\nprint(\"Symmetric 95% CI:\", ci_symmetric)\n\n\n\nAs we can see, the confidence interval extends 0.23 units below the mean but 0.27 units above it - the upper bound is about 17% further from the mean than the lower bound. This asymmetry directly reflects the right-skewed nature of the exponential distribution‚Äôs sampling distribution.\nThis numerical example demonstrates that with skewed data, the distance from the point estimate to the lower bound can differ substantially from the distance to the upper bound. When reporting results, acknowledging this asymmetry provides a more accurate representation of the uncertainty in your estimate than simply reporting ‚Äúestimate ¬± margin of error.‚Äù The degree of asymmetry often depends on both the sample size and the underlying distribution - with smaller samples from more skewed distributions showing greater asymmetry in their confidence intervals."
  },
  {
    "objectID": "blog-unpublished/non-symmetric-conf-intervals.html#bottom-line",
    "href": "blog-unpublished/non-symmetric-conf-intervals.html#bottom-line",
    "title": "Why Are Some Confidence Intervals Not Symmetric?",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nSymmetric confidence intervals come from symmetric distributions‚Äîdon‚Äôt expect them when that‚Äôs not the case.\nAsymmetric intervals are common with proportions, skewed data, nonlinear functions, and bootstrap methods.\nAlways check if your CI method makes assumptions about symmetry or normality.\nDon‚Äôt blindly use \\(\\pm\\) formulas‚Äîthere are better (and more honest) ways to quantify uncertainty."
  },
  {
    "objectID": "blog-unpublished/non-symmetric-conf-intervals.html#references",
    "href": "blog-unpublished/non-symmetric-conf-intervals.html#references",
    "title": "Why Are Some Confidence Intervals Not Symmetric?",
    "section": "References",
    "text": "References\nEfron, B., & Tibshirani, R. J. (1993). An Introduction to the Bootstrap.\nCasella, G., & Berger, R. L. (2002). Statistical Inference.\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis."
  },
  {
    "objectID": "blog-unpublished/wmw-test-fails-medians.html",
    "href": "blog-unpublished/wmw-test-fails-medians.html",
    "title": "The Wilcoxon-Mann-Whitney Test is Not a Test of Medians",
    "section": "",
    "text": "Nonparametric tests like the Wilcoxon-Mann-Whitney (WMW) procedure are statistical workhorses in data science and biomedical research, especially when assumptions like normality break down. Often described as a ‚Äútest of medians,‚Äù WMW is used when comparing two independent groups without making strong assumptions about the underlying distributions.\nBut here‚Äôs the rub: despite what you might have heard in Stats 101 or read in an online forum, the WMW test is not a test of medians‚Äîat least not in general. Divine et al.¬†(2018) dive deep into this misconception and show convincingly (with both mathematical rigor and illustrative examples) how the WMW test can lead you astray if you‚Äôre specifically interested in comparing medians.\nThis article explains why that happens, provides some intuition and math, and shows you how to think more clearly about what the WMW test actually does."
  },
  {
    "objectID": "blog-unpublished/wmw-test-fails-medians.html#background",
    "href": "blog-unpublished/wmw-test-fails-medians.html#background",
    "title": "The Wilcoxon-Mann-Whitney Test is Not a Test of Medians",
    "section": "",
    "text": "Nonparametric tests like the Wilcoxon-Mann-Whitney (WMW) procedure are statistical workhorses in data science and biomedical research, especially when assumptions like normality break down. Often described as a ‚Äútest of medians,‚Äù WMW is used when comparing two independent groups without making strong assumptions about the underlying distributions.\nBut here‚Äôs the rub: despite what you might have heard in Stats 101 or read in an online forum, the WMW test is not a test of medians‚Äîat least not in general. Divine et al.¬†(2018) dive deep into this misconception and show convincingly (with both mathematical rigor and illustrative examples) how the WMW test can lead you astray if you‚Äôre specifically interested in comparing medians.\nThis article explains why that happens, provides some intuition and math, and shows you how to think more clearly about what the WMW test actually does."
  },
  {
    "objectID": "blog-unpublished/wmw-test-fails-medians.html#notation",
    "href": "blog-unpublished/wmw-test-fails-medians.html#notation",
    "title": "The Wilcoxon-Mann-Whitney Test is Not a Test of Medians",
    "section": "Notation",
    "text": "Notation\nLet \\(X_1, \\ldots, X_m \\sim F\\) and \\(Y_1, \\ldots, Y_n \\sim G\\) be two independent random samples from distributions \\(F\\) and \\(G\\), respectively. The Wilcoxon-Mann-Whitney statistic is based on the probability:\n\\[P(X &lt; Y) + \\frac{1}{2}P(X = Y)\\]\nThis quantity is sometimes referred to as the probability of superiority.\nLet \\(\\theta_F\\) and \\(\\theta_G\\) denote the medians of \\(F\\) and \\(G\\). We often want to test:\n\\[H_0: \\theta_F = \\theta_G\\]\nBut WMW does not directly test this hypothesis unless very specific conditions are met."
  },
  {
    "objectID": "blog-unpublished/wmw-test-fails-medians.html#a-closer-look",
    "href": "blog-unpublished/wmw-test-fails-medians.html#a-closer-look",
    "title": "The Wilcoxon-Mann-Whitney Test is Not a Test of Medians",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhat Does WMW Actually Test?\nThe WMW test assesses whether one distribution tends to produce larger values than the other. More formally, it tests:\n\\[H_0: P(X &lt; Y) + \\frac{1}{2}P(X = Y) = 0.5\\]\nThis is equivalent to testing whether the distributions are stochastically equal, not whether the medians are equal.\nThe WMW test can be performed via rank sums. After combining both samples, we rank all observations from smallest to largest. The test statistic W is the sum of ranks assigned to the first sample:\n\\[W = \\sum_{i=1}^m R(X_i)\\]\nwhere \\(R(X_i)\\) is the rank of \\(X_i\\) in the combined sample.\nThis rank-based formulation is mathematically equivalent to counting how many pairs \\((X_i, Y_j)\\) have \\(X_i &lt; Y_j\\), which relates to the probability interpretation above. Under the null hypothesis, we expect W to be approximately \\(m(m+n+1)/2\\).\n\n\nUnderstanding Stochastic Dominance\nWhen we say the WMW test examines ‚Äústochastic dominance,‚Äù we mean it tests whether values from one distribution tend to exceed values from the other. Specifically, distribution G stochastically dominates distribution \\(F\\) if:\n\\[G(x) \\leq F(x) \\text{ for all } x\\]\nwith strict inequality for at least some values of \\(x\\). Intuitively, this means a randomly selected value from \\(G\\) is more likely to be larger than a randomly selected value from \\(F\\).\nThis is quite different from comparing medians. Two distributions can have identical medians but exhibit stochastic dominance, or they can have different medians but neither stochastically dominates the other.\n\n\nWhen Does It Coincide with a Median Test?\nThe WMW test only functions as a test of medians under symmetric distributions with equal shape and spread. If the shapes differ‚Äîsay, one is skewed left and the other right‚Äîthen even if the medians are the same, WMW can reject the null. Worse, it might fail to reject when the medians are different but the distributions have similar overall ranks.\n\n\nAlternative Tests\nIf your research question specifically concerns differences in medians, more appropriate tests include:\n\nMood‚Äôs median test: A true test of median equality that uses contingency tables based on counts above and below the combined median.\nQuantile regression: For more complex designs, quantile regression directly models the median (or other quantiles) and tests differences between groups.\nBootstrap confidence intervals: Calculating confidence intervals for the difference in medians via bootstrapping provides both a test and measure of uncertainty.\n\nThese approaches directly address median differences rather than the stochastic ordering tested by WMW."
  },
  {
    "objectID": "blog-unpublished/wmw-test-fails-medians.html#bottom-line",
    "href": "blog-unpublished/wmw-test-fails-medians.html#bottom-line",
    "title": "The Wilcoxon-Mann-Whitney Test is Not a Test of Medians",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe Wilcoxon-Mann-Whitney test is not a general test of medians.\nIt tests for stochastic dominance or shift in distribution, not specifically median difference.\nIt behaves like a median test only under certain conditions (e.g., identical shape).\nBe cautious interpreting WMW results as saying something about medians unless distributional assumptions are met."
  },
  {
    "objectID": "blog-unpublished/wmw-test-fails-medians.html#where-to-learn-more",
    "href": "blog-unpublished/wmw-test-fails-medians.html#where-to-learn-more",
    "title": "The Wilcoxon-Mann-Whitney Test is Not a Test of Medians",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive, read the original Divine et al.¬†(2018) paper. You might also want to look at literature on robust location tests or permutation-based alternatives that better target the median. Textbooks on nonparametric statistics such as Hollander, Wolfe, and Chicken (2013) also provide strong foundational understanding."
  },
  {
    "objectID": "blog-unpublished/wmw-test-fails-medians.html#references-if-applicable",
    "href": "blog-unpublished/wmw-test-fails-medians.html#references-if-applicable",
    "title": "Why the Wilcoxon-Mann-Whitney Test Fails as a Test of Medians",
    "section": "References (if applicable)",
    "text": "References (if applicable)\nDivine, G. W., Norton, H. J., Bar√≥n, A. E., & Juarez-Colunga, E. (2018). The Wilcoxon‚ÄìMann‚ÄìWhitney procedure fails as a test of medians. The American Statistician, 72(3), 278‚Äì286.\nHollander, M., Wolfe, D. A., & Chicken, E. (2013). Nonparametric Statistical Methods (3rd ed.). Wiley."
  },
  {
    "objectID": "blog-unpublished/wmw-test-fails-medians.html#references",
    "href": "blog-unpublished/wmw-test-fails-medians.html#references",
    "title": "The Wilcoxon-Mann-Whitney Test is Not a Test of Medians",
    "section": "References",
    "text": "References\nDivine, G. W., Norton, H. J., Bar√≥n, A. E., & Juarez-Colunga, E. (2018). The Wilcoxon‚ÄìMann‚ÄìWhitney procedure fails as a test of medians. The American Statistician, 72(3), 278‚Äì286.\nHollander, M., Wolfe, D. A., & Chicken, E. (2013). Nonparametric Statistical Methods (3rd ed.). Wiley."
  },
  {
    "objectID": "blog-unpublished/ks-test-one-sample.html",
    "href": "blog-unpublished/ks-test-one-sample.html",
    "title": "The Kolmogorov‚ÄìSmirnov Test as a Goodness-of-fit",
    "section": "",
    "text": "The Kolmogorov‚ÄìSmirnov (KS) test is a staple in the statistical toolbox for checking how well data fit a hypothesized distribution. It‚Äôs nonparametric, straightforward to compute, and widely implemented in R, Python, and just about every statistical software you can think of. But‚Äîand this is a big but‚Äîusing the KS test naively can lead to some serious misinterpretations, especially when parameters are estimated from the data.\nThis article is based on the 2024 paper by Zeimbekakis, Schifano, and Yan, which takes a hard look at the common misuses of the one-sample KS test. We‚Äôll walk through what the KS test is supposed to do, when it goes wrong, and how to think more clearly about assessing goodness-of-fit."
  },
  {
    "objectID": "blog-unpublished/ks-test-one-sample.html#background",
    "href": "blog-unpublished/ks-test-one-sample.html#background",
    "title": "The Kolmogorov‚ÄìSmirnov Test as a Goodness-of-fit",
    "section": "",
    "text": "The Kolmogorov‚ÄìSmirnov (KS) test is a staple in the statistical toolbox for checking how well data fit a hypothesized distribution. It‚Äôs nonparametric, straightforward to compute, and widely implemented in R, Python, and just about every statistical software you can think of. But‚Äîand this is a big but‚Äîusing the KS test naively can lead to some serious misinterpretations, especially when parameters are estimated from the data.\nThis article is based on the 2024 paper by Zeimbekakis, Schifano, and Yan, which takes a hard look at the common misuses of the one-sample KS test. We‚Äôll walk through what the KS test is supposed to do, when it goes wrong, and how to think more clearly about assessing goodness-of-fit."
  },
  {
    "objectID": "blog-unpublished/ks-test-one-sample.html#notation",
    "href": "blog-unpublished/ks-test-one-sample.html#notation",
    "title": "The Kolmogorov‚ÄìSmirnov Test as a Goodness-of-fit",
    "section": "Notation",
    "text": "Notation\nLet \\(X_1, \\dots, X_n\\) be i.i.d. random variables with unknown distribution function \\(F\\). We want to test whether \\(F = F_0\\), for some known distribution function \\(F_0\\).\nThe empirical distribution function (EDF) is: \\[F_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i \\leq x)\\]\nThe KS statistic is: \\[D_n = \\sup_{x \\in \\mathbb{R}} |F_n(x) - F_0(x)|\\]\nThe null distribution of \\(D_n\\) is known under the assumption that \\(F_0\\) is fully specified, i.e., no parameters have been estimated from the data."
  },
  {
    "objectID": "blog-unpublished/ks-test-one-sample.html#a-closer-look",
    "href": "blog-unpublished/ks-test-one-sample.html#a-closer-look",
    "title": "The Kolmogorov‚ÄìSmirnov Test as a Goodness-of-fit",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhat the KS Test Measures\nThe KS test is sensitive to discrepancies in the cumulative distribution function. Intuitively, it‚Äôs measuring the largest vertical distance between the EDF and the hypothesized CDF \\(F_0\\). This gives you a global measure of discrepancy, not a local one‚Äîso it‚Äôs less powerful for detecting issues like tail misspecification or multimodality.\nThe KS test gives equal weight to all observations when computing the maximum deviation, making it less effective at detecting differences in distribution tails compared to other tests like Anderson-Darling. This is important because in many applications, tail behavior is critically important, such as in risk modeling or extreme value analysis.\nWith small samples, the test has limited power to detect distributional differences, while with very large samples, it may detect statistically significant but practically trivial deviations from the hypothesized distribution.\n\n\nWhen Things Go Wrong: Parameter Estimation\nHere‚Äôs the catch: the null distribution of the KS statistic assumes \\(F_0\\) is fully known. But in practice, people often use the test to evaluate model fit after estimating parameters‚Äîe.g., fitting a normal distribution by MLE and then checking fit with KS.\nThat invalidates the test.\nWhy? Because the theoretical distribution of \\(D_n\\) changes when parameters are estimated. The true distribution of the test statistic becomes conditional on the data, and the critical values are no longer accurate. This leads to an inflated Type I error rate: you‚Äôre more likely to incorrectly reject the null.\n\n\nBetter Alternatives\nWhen parameters are estimated, we need modified procedures:\n\nLilliefors test: An adaptation of the KS test that adjusts the null distribution when testing for normality with estimated parameters.\nParametric bootstrap: Simulate the null distribution of the test statistic by repeatedly fitting the model and computing \\(D_n\\) on simulated data.\nOther GOF tests: Anderson-Darling and Cram√©r-von Mises tests have versions that handle estimated parameters more gracefully."
  },
  {
    "objectID": "blog-unpublished/ks-test-one-sample.html#an-example",
    "href": "blog-unpublished/ks-test-one-sample.html#an-example",
    "title": "The Kolmogorov‚ÄìSmirnov Test as a Goodness-of-fit",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nset.seed(42)\nx &lt;- rnorm(100, mean = 5, sd = 2)\nks.test(x, \"pnorm\", mean = mean(x), sd = sd(x)) # misuse!\n\n# Better: use Lilliefors test\nlibrary(nortest)\nlillie.test(x)\n\n\nimport numpy as np\nfrom scipy.stats import kstest, norm\nfrom statsmodels.stats.diagnostic import lilliefors\n\nnp.random.seed(42)\nx = np.random.normal(loc=5, scale=2, size=100)\n\n# Misuse: parameters estimated from x\nkstest(x, 'norm', args=(np.mean(x), np.std(x, ddof=1)))\n\n# Better: use Lilliefors test\nstat, pval = lilliefors(x)\nprint(f\"Lilliefors test: statistic={stat}, p-value={pval}\")"
  },
  {
    "objectID": "blog-unpublished/ks-test-one-sample.html#bottom-line",
    "href": "blog-unpublished/ks-test-one-sample.html#bottom-line",
    "title": "The Kolmogorov‚ÄìSmirnov Test as a Goodness-of-fit",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe KS test assumes no parameters are estimated‚Äîviolating this leads to invalid inference.\nEstimating parameters from the same data used in the test inflates Type I error.\nUse alternatives like the Lilliefors test or bootstrap methods when parameters are estimated.\nThe KS test is less sensitive in the tails‚Äîbe cautious with it as a global fit measure."
  },
  {
    "objectID": "blog-unpublished/ks-test-one-sample.html#where-to-learn-more",
    "href": "blog-unpublished/ks-test-one-sample.html#where-to-learn-more",
    "title": "The Kolmogorov‚ÄìSmirnov Test as a Goodness-of-fit",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nIf you‚Äôre regularly checking distributional assumptions, it‚Äôs worth diving into goodness-of-fit tests in more depth. The American Statistician paper by Zeimbekakis et al.¬†(2024) is a great read. Also consider books like Goodness-of-Fit Techniques by D‚ÄôAgostino and Stephens, or All of Statistics by Larry Wasserman for more intuitive overviews."
  },
  {
    "objectID": "blog-unpublished/ks-test-one-sample.html#references",
    "href": "blog-unpublished/ks-test-one-sample.html#references",
    "title": "The Kolmogorov‚ÄìSmirnov Test as a Goodness-of-fit",
    "section": "References",
    "text": "References\nZeimbekakis, A., Schifano, E. D., & Yan, J. (2024). On Misuses of the Kolmogorov‚ÄìSmirnov Test for One-Sample Goodness-of-Fit. The American Statistician, 78(4), 481-487."
  },
  {
    "objectID": "blog-unpublished/correlation-myths-views.html",
    "href": "blog-unpublished/correlation-myths-views.html",
    "title": "Correlation: Myths and Alternative Views",
    "section": "",
    "text": "Correlation is one of the most used (and misused) statistical concepts. It seems simple enough: a number between \\(-1\\) and \\(1\\) that tells you how strongly two variables are related. But lurking beneath that tidy number are a host of assumptions, limitations, and interpretations that often go unrecognized by even experienced analysts. In this article, we revisit two fascinating papers that try to untangle the myths and layers of meaning wrapped around correlation: van den Heuvel and Zhan (2022), and Rodgers and Nicewander (1988). Our goal is to sharpen our intuition and clear up misconceptions around three of the most popular correlation measures‚ÄîPearson‚Äôs \\(r\\), Spearman‚Äôs \\(\\rho\\), and Kendall‚Äôs \\(\\tau\\)‚Äîwhile also exploring thirteen perspectives on what correlation actually tells us."
  },
  {
    "objectID": "blog-unpublished/correlation-myths-views.html#background",
    "href": "blog-unpublished/correlation-myths-views.html#background",
    "title": "Correlation: Myths and Alternative Views",
    "section": "",
    "text": "Correlation is one of the most used (and misused) statistical concepts. It seems simple enough: a number between \\(-1\\) and \\(1\\) that tells you how strongly two variables are related. But lurking beneath that tidy number are a host of assumptions, limitations, and interpretations that often go unrecognized by even experienced analysts. In this article, we revisit two fascinating papers that try to untangle the myths and layers of meaning wrapped around correlation: van den Heuvel and Zhan (2022), and Rodgers and Nicewander (1988). Our goal is to sharpen our intuition and clear up misconceptions around three of the most popular correlation measures‚ÄîPearson‚Äôs \\(r\\), Spearman‚Äôs \\(\\rho\\), and Kendall‚Äôs \\(\\tau\\)‚Äîwhile also exploring thirteen perspectives on what correlation actually tells us."
  },
  {
    "objectID": "blog-unpublished/correlation-myths-views.html#notation",
    "href": "blog-unpublished/correlation-myths-views.html#notation",
    "title": "Correlation: Myths and Alternative Views",
    "section": "Notation",
    "text": "Notation\nLet \\(X\\) and \\(Y\\) be two random variables with realizations \\((x_i, y_i)\\) for \\(i = 1, \\ldots, n\\). We assume all variables are centered unless stated otherwise.\n\nPearson‚Äôs \\(r\\) is defined as: \\[r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2} \\sqrt{\\sum (y_i - \\bar{y})^2}}\\]\nSpearman‚Äôs \\(\\rho\\) is Pearson‚Äôs \\(r\\) computed on the ranks of the data.\nKendall‚Äôs \\(\\tau\\) is based on the number of concordant and discordant pairs: \\[\\tau = \\frac{\\#\\text{concordant} - \\#\\text{discordant}}{\\binom{n}{2}}\\]\n\nConcordant pairs of observations refer to pairs where the ranks of both variables move in the same direction. For example, if one observation is higher than another in both variables, they are concordant. Conversely, discordant pairs occur when the ranks of the variables move in opposite directions; one observation is higher in one variable but lower in the other."
  },
  {
    "objectID": "blog-unpublished/correlation-myths-views.html#a-closer-look",
    "href": "blog-unpublished/correlation-myths-views.html#a-closer-look",
    "title": "Correlation: Myths and Alternative Views",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nMyths About Correlation Coefficients\nvan den Heuvel and Zhan (2022) outline several persistent myths about correlation, and it‚Äôs worth tackling them head-on. One of the biggest is that Pearson‚Äôs \\(r\\) measures a linear relationship, while Spearman‚Äôs \\(\\rho\\) and Kendall‚Äôs \\(\\tau\\) measure monotonic relationships. That sounds tidy but is too simplistic. In fact, none of the three are pure tests of ‚Äúlinearity‚Äù or ‚Äúmonotonicity.‚Äù Their sensitivity to nonlinearity depends on the distributional form, heteroscedasticity, and outliers.\nAnother common misconception is that rank-based correlations are more ‚Äúrobust.‚Äù While it‚Äôs true that rank correlations are less sensitive to outliers in the marginal distributions, they can still behave poorly under certain forms of non-monotonic or heteroscedastic relationships. For instance, a \\(U\\)-shaped relationship will likely be missed by all three coefficients.\nWhat‚Äôs most illuminating is the realization that none of these coefficients are silver bullets. They summarize different aspects of association but rarely tell the whole story. It‚Äôs a good idea to pair them with visualization and formal tests of fit or non-linearity.\n\n\nThirteen Ways to Look at the Correlation Coefficient\nRodgers and Nicewander (1988) offer a brilliant framing of correlation by listing thirteen distinct ways to interpret Pearson‚Äôs \\(r\\). Here‚Äôs a quick tour, each providing a slightly different angle:\n\nAs a measure of standardized covariance, it tells you how two variables co-vary after accounting for their units.\nAs a regression slope between standardized variables, it equals the slope of the line predicting \\(z\\)-scored \\(Y\\) from \\(z\\)-scored \\(X\\).\nAs a symmetric regression slope, since r is the same whether you regress \\(Y\\) on \\(X\\) or \\(X\\) on \\(Y\\), once both are standardized.\nAs the cosine of the angle between two vectors, showing their geometric alignment.\nAs a function of sums of squares, where it relates directly to the decomposition of total variance.\nAs a measure of shared variance, where \\(r^2\\) tells you the proportion of variance explained.\nAs a special case of canonical correlation, when only one variable is in each set.\nAs a maximum likelihood estimator, under a bivariate normal model.\nAs a test statistic, r can be tested for significance under certain nulls.\nAs an estimator sensitive to range restriction, showing attenuation if \\(X\\) or \\(Y\\) is truncated.\nAs an indicator of predictability, but only under linear assumptions.\nAs a metric dependent on scale, since nonlinear transformations can dramatically change it.\nAs a guide, not a truth, because it tells part of the story but rarely all of it.\n\nEach interpretation highlights a different trade-off or caveat. For example, the geometric view gives a great intuition, but the regression slope interpretation connects more directly to causal inference. And perhaps most importantly, several of these views are not invariant to nonlinear transformations, which matters a lot in real data."
  },
  {
    "objectID": "blog-unpublished/correlation-myths-views.html#bottom-line",
    "href": "blog-unpublished/correlation-myths-views.html#bottom-line",
    "title": "Correlation: Myths and Alternative Views",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nPearson‚Äôs \\(r\\), Spearman‚Äôs \\(\\rho\\), and Kendall‚Äôs \\(\\tau\\) measure different aspects of association‚Äînone is a catch-all indicator.\nThe ‚Äúmonotonic vs.¬†linear‚Äù framing is a helpful heuristic, but it breaks down in many real-world scenarios.\nRodgers and Nicewander‚Äôs thirteen perspectives on correlation reveal its multifaceted nature and limitations.\nAlways visualize your data‚Äîcorrelation coefficients should not replace your eyes or your understanding of the domain."
  },
  {
    "objectID": "blog-unpublished/correlation-myths-views.html#where-to-learn-more",
    "href": "blog-unpublished/correlation-myths-views.html#where-to-learn-more",
    "title": "Correlation: Myths and Alternative Views",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nTo dig deeper into the nuances of correlation, the papers discussed here are essential. You might also enjoy Bollen and Pearl‚Äôs work on correlation vs.¬†causation, or more modern texts on exploratory data analysis and robust statistics. Visualization tools like scatterplot matrices, partial residual plots, and nonparametric smoothers (e.g., loess) can complement numeric summaries."
  },
  {
    "objectID": "blog-unpublished/correlation-myths-views.html#references-if-applicable",
    "href": "blog-unpublished/correlation-myths-views.html#references-if-applicable",
    "title": "Correlation: Myths and Alternative Views",
    "section": "References (if applicable)",
    "text": "References (if applicable)\n\nvan den Heuvel, E., & Zhan, Z. (2022). Myths about linear and monotonic associations: Pearson‚Äôs \\(r\\), Spearman‚Äôs \\(\\rho\\), and Kendall‚Äôs \\(\\tau\\). The American Statistician, 76(1), 44‚Äì52.\nLee Rodgers, J., & Nicewander, W. A. (1988). Thirteen ways to look at the correlation coefficient. The American Statistician, 42(1), 59‚Äì66."
  },
  {
    "objectID": "blog-unpublished/wmw-test-fails-medians.html#an-example",
    "href": "blog-unpublished/wmw-test-fails-medians.html#an-example",
    "title": "The Wilcoxon-Mann-Whitney Test is Not a Test of Medians",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs see this in action with a small simulation.\n\nRPython\n\n\nset.seed(123)\nx &lt;- rexp(100, rate = 1)         # Right-skewed\ny &lt;- rexp(100, rate = 1.5)       # Also right-skewed, different rate\n\nmedian(x)  # Median of x\nmedian(y)  # Median of y\n\nwilcox.test(x, y)\n\n\nimport numpy as np\nfrom scipy.stats import mannwhitneyu\n\nnp.random.seed(123)\nx = np.random.exponential(scale=1.0, size=100)\ny = np.random.exponential(scale=2/3, size=100)  # Higher scale = lower rate\n\nprint(\"Median x:\", np.median(x))\nprint(\"Median y:\", np.median(y))\n\nres = mannwhitneyu(x, y, alternative='two-sided')\nprint(res)\n\n\n\nThis example demonstrates our point perfectly: The medians are clearly different (0.6334 vs.¬†0.4865), and the WMW test correctly rejects the null hypothesis (p = 0.004). However, this rejection occurs because the exponential distributions with different rates create a consistent stochastic ordering, not because it‚Äôs specifically testing the medians.\nDespite different medians, the WMW test might not reject the null. Or it might reject it because of shape differences, not the medians."
  },
  {
    "objectID": "blog-unpublished/correlated-random-effects.html",
    "href": "blog-unpublished/correlated-random-effects.html",
    "title": "Understanding Correlated Random Effects Models",
    "section": "",
    "text": "In the world of panel data analysis, we often find ourselves choosing between fixed effects (FE) and random effects (RE) models. Each has its strengths and limitations. The FE model controls for all time-invariant unobserved heterogeneity but does not allow us to estimate the effects of those time-invariant covariates. On the other hand, the RE model allows for the inclusion of time-invariant variables but makes a strong assumption: that the unobserved individual-specific effects are uncorrelated with the regressors. What happens when that assumption doesn‚Äôt hold? That‚Äôs where the correlated random effects (CRE) model‚Äîor the hybrid model‚Äîsteps in.\nThis article explores the motivation behind the CRE model, its mechanics, and its advantages over traditional FE and RE models. We also provide a hands-on example using R and python to illustrate how it works in practice."
  },
  {
    "objectID": "blog-unpublished/correlated-random-effects.html#background",
    "href": "blog-unpublished/correlated-random-effects.html#background",
    "title": "Understanding Correlated Random Effects Models",
    "section": "",
    "text": "In the world of panel data analysis, we often find ourselves choosing between fixed effects (FE) and random effects (RE) models. Each has its strengths and limitations. The FE model controls for all time-invariant unobserved heterogeneity but does not allow us to estimate the effects of those time-invariant covariates. On the other hand, the RE model allows for the inclusion of time-invariant variables but makes a strong assumption: that the unobserved individual-specific effects are uncorrelated with the regressors. What happens when that assumption doesn‚Äôt hold? That‚Äôs where the correlated random effects (CRE) model‚Äîor the hybrid model‚Äîsteps in.\nThis article explores the motivation behind the CRE model, its mechanics, and its advantages over traditional FE and RE models. We also provide a hands-on example using R and python to illustrate how it works in practice."
  },
  {
    "objectID": "blog-unpublished/correlated-random-effects.html#notation",
    "href": "blog-unpublished/correlated-random-effects.html#notation",
    "title": "Understanding Correlated Random Effects Models",
    "section": "Notation",
    "text": "Notation\nLet us consider a standard panel data setup where we observe units \\(i=1,\\dots,N\\) over time periods \\(t = 1, \\dots, T\\). The outcome is \\(y_{it}\\), and \\(x_{it}\\) is a vector of time-varying covariates. The generic random effects model is:\n\\[\ny_{it} = x_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\n\\]\nwhere \\(\\alpha_i\\) is the individual-specific effect and \\(\\varepsilon_{it}\\) is the idiosyncratic error term. The crucial RE assumption is:\n\\[\n\\text{Cov}(x_{it}, \\alpha_i) = 0\n\\]\nThe CRE model relaxes this assumption by explicitly modeling the correlation between \\(x_{it}\\) and \\(\\alpha_i\\)."
  },
  {
    "objectID": "blog-unpublished/correlated-random-effects.html#a-closer-look",
    "href": "blog-unpublished/correlated-random-effects.html#a-closer-look",
    "title": "Understanding Correlated Random Effects Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nA Refresher on Fixed and Random Effects\n\n\nA Refresher on Fixed and Random Effects\nIn panel data models, the goal is often to account for unobserved heterogeneity across units (e.g., individuals, firms, regions). Two popular approaches to handle this are fixed effects (FE) and random effects (RE) models. Understanding these two approaches is critical before we dive into correlated random effects.\n\nFixed Effects (FE) Model\nThe fixed effects model controls for all time-invariant characteristics of the units by allowing each unit to have its own intercept. The model is specified as:\n\\[\ny_{it} = x_{it}'\\beta + \\alpha_i + \\varepsilon_{it},\n\\]\nwhere: - \\(y_{it}\\) is the outcome for unit \\(i\\) at time \\(t\\), - \\(x_{it}\\) are the observed covariates, - \\(\\alpha_i\\) is the unit-specific intercept (the fixed effect), - \\(\\varepsilon_{it}\\) is the error term.\nThe key feature of FE models is that \\(\\alpha_i\\) is treated as a set of unknown parameters to be estimated (or differenced out). This approach eliminates all time-invariant confounders, whether observed or unobserved.\nFixed effects estimation often proceeds by demeaning the data within each unit (also known as the ‚Äúwithin transformation‚Äù), removing \\(\\alpha_i\\):\n\\[\ny_{it} - \\bar{y}_i = (x_{it} - \\bar{x}_i)'\\beta + (\\varepsilon_{it} - \\bar{\\varepsilon}_i),\n\\]\nwhere \\(\\bar{y}_i\\) and \\(\\bar{x}_i\\) are the within-unit means.\nFixed effects are especially popular in causal inference because they remove bias from any time-invariant omitted variables. They can be seen as a generalization of the familiar difference-in-differences (DiD) approach, which is just a special case of FE with two time periods and a treatment indicator.\n\n\nRandom Effects (RE) Model\nThe random effects model uses the same setup as the FE model:\n\\[\ny_{it} = x_{it}'\\beta + \\alpha_i + \\varepsilon_{it}.\n\\]\nBut here, \\(\\alpha_i\\) is treated as a random variable drawn from a distribution (usually assumed to be normal):\n\\[\n\\alpha_i \\sim N(0, \\sigma_\\alpha^2).\n\\]\nThe crucial assumption in RE models is:\n\\[\n\\text{Cov}(x_{it}, \\alpha_i) = 0.\n\\]\nIn other words, the unit effects \\(\\alpha_i\\) must be uncorrelated with the covariates \\(x_{it}\\). This allows for more efficient estimation through Generalized Least Squares (GLS), but if the assumption fails, the RE estimates will be biased and inconsistent.\n\n\nSummary\n\n\n\n\n\n\n\n\nFeature\nFixed Effects (FE)\nRandom Effects (RE)\n\n\n\n\nTreatment of \\(\\alpha_i\\)\nFixed parameters (unknown intercepts per unit)\nRandom variables (assumed uncorrelated with \\(x_{it}\\))\n\n\nCan estimate time-invariant covariates?\nNo\nYes\n\n\nSuitable for causal inference?\nYes, widely used\nOnly if exogeneity assumption holds\n\n\nEfficiency\nLower (uses within variation only)\nHigher (uses both within and between variation)\n\n\n\nIn short, the FE model is robust but discards between-unit variation, while the RE model is more efficient but relies on a strong independence assumption between covariates and unobserved heterogeneity.\nThe correlated random effects (CRE) model differs from standard fixed and random effects by explicitly modeling the correlation between the unit-specific effects \\(\\alpha_i\\) and the covariates \\(x_{it}\\). Instead of assuming independence (as in RE) or differencing out the effects entirely (as in FE), CRE includes the unit-level means of the covariates as additional regressors, allowing for consistent estimation while still retaining the ability to estimate time-invariant variables.\n\n\n\nWhy Use Correlated Random Effects?\nThe correlated random effects (CRE) model offers a middle ground between FE and RE approaches. Traditional RE models assume that unobserved heterogeneity is uncorrelated with covariates. FE models remove all unit-level heterogeneity but cannot estimate time-invariant covariates. CRE models address these limitations by including group means of time-varying covariates, decomposing variation into within and between components.\n\n\nModel Estimation and Inference\nIn the linear case:\n\\[\ny_{it} = \\beta_0 + \\beta_1 x_{it} + \\gamma \\bar{x}_i + u_i + \\varepsilon_{it},\n\\]\nwhere \\(\\bar{x}_i\\) is the individual mean of \\(x_{it}\\).\nEstimation uses RE methods but includes \\(\\bar{x}_i\\) to account for potential correlation with \\(u_i\\). This enables hypothesis testing comparing within and between effects.\n\n\nAdvantages and Challenges\nThe CRE model offers a few attractive advantages:\n\nEstimation of time-invariant variables.\nDecomposition of effects into within and between components.\nImproved efficiency under relaxed assumptions.\nDiagnostic insight into the plausibility of RE assumptions.\n\nHere is a brief list of its downsides:\n\nThe random intercept assumption remains.\nNo remedy for level-2 confounding.\nCare needed with interaction terms.\nPotential bias with a small number of clusters.\n\n\n\nWhere the CRE Model Shines\nCRE models are ideal for repeated measures data where: - Both time-varying and time-invariant predictors matter. - There‚Äôs potential endogeneity between covariates and individual effects.\nApplications include policy evaluation, health outcomes research, and educational studies."
  },
  {
    "objectID": "blog-unpublished/correlated-random-effects.html#an-example",
    "href": "blog-unpublished/correlated-random-effects.html#an-example",
    "title": "Understanding Correlated Random Effects Models",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nlibrary(plm)\nlibrary(dplyr)\n\nset.seed(42)\nn &lt;- 100\nt &lt;- 5\ndata &lt;- data.frame(\n  id = rep(1:n, each = t),\n  time = rep(1:t, n)\n)\ndata &lt;- data %&gt;%\n  group_by(id) %&gt;%\n  mutate(\n    x = rnorm(n(), mean = id/10),\n    alpha = rnorm(1),\n    eps = rnorm(n(), sd = 1),\n    y = 1 + 0.5 * x + alpha + eps\n  )\n\npdata &lt;- pdata.frame(data, index = c(\"id\", \"time\"))\n\nfe_model &lt;- plm(y ~ x, data = pdata, model = \"within\")\nre_model &lt;- plm(y ~ x, data = pdata, model = \"random\")\npdata$mean_x &lt;- ave(pdata$x, pdata$id, FUN = mean)\ncre_model &lt;- plm(y ~ x + mean_x, data = pdata, model = \"random\")\n\nsummary(fe_model)\nsummary(re_model)\nsummary(cre_model)\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(42)\nn, t = 100, 5\ndf = pd.DataFrame({\n    'id': np.repeat(np.arange(1, n+1), t),\n    'time': np.tile(np.arange(1, t+1), n)\n})\ndf['x'] = df['id'] / 10 + np.random.randn(n*t)\ndf['alpha'] = np.repeat(np.random.randn(n), t)\ndf['eps'] = np.random.randn(n*t)\ndf['y'] = 1 + 0.5 * df['x'] + df['alpha'] + df['eps']\ndf['mean_x'] = df.groupby('id')['x'].transform('mean')\n\n# Short model (RE approximation)\nX_short = sm.add_constant(df[['x']])\nmodel_short = sm.OLS(df['y'], X_short).fit()\n\n# CRE model\nX_cre = sm.add_constant(df[['x', 'mean_x']])\nmodel_cre = sm.OLS(df['y'], X_cre).fit()\n\nprint(model_short.summary())\nprint(model_cre.summary())"
  },
  {
    "objectID": "blog-unpublished/correlated-random-effects.html#bottom-line",
    "href": "blog-unpublished/correlated-random-effects.html#bottom-line",
    "title": "Understanding Correlated Random Effects Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nCRE models relax the strict RE assumptions by modeling the correlation between unit effects and covariates.\nThey provide within and between estimates while allowing time-invariant variables.\nAppropriate for longitudinal, multilevel, and policy evaluation studies."
  },
  {
    "objectID": "blog-unpublished/correlated-random-effects.html#where-to-learn-more",
    "href": "blog-unpublished/correlated-random-effects.html#where-to-learn-more",
    "title": "Understanding Correlated Random Effects Models",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nSchunck (2013) provides a comprehensive overview of CRE models. Mundlak‚Äôs foundational work is essential for understanding the theoretical basis. Tools like R‚Äôs plm and Python‚Äôs statsmodels can implement these models with the correct transformations."
  },
  {
    "objectID": "blog-unpublished/correlated-random-effects.html#references",
    "href": "blog-unpublished/correlated-random-effects.html#references",
    "title": "Understanding Correlated Random Effects Models",
    "section": "References",
    "text": "References\n\nSchunck, R. (2013). Within and between estimates in random-effects models: Advantages and drawbacks of correlated random effects and hybrid models. The Stata Journal, 13(1), 65-76.\nMundlak, Y. (1978). On the pooling of time series and cross section data. Econometrica, 46(1), 69‚Äì85."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg.html#examples",
    "href": "blog-unpublished/unconditional-qreg.html#examples",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Examples",
    "text": "Examples\n\nBitler et al.¬†(2006)\nWhen evaluating the effects of welfare reform, traditional analyses often focus on mean impacts, which can obscure critical insights into the distributional effects of policy changes. ‚Äã Quantile Treatment Effects (QTE) provide a powerful tool for understanding how reforms impact different segments of the population, revealing heterogeneity that mean impacts fail to capture. ‚Äã For example, the study ‚ÄúWhat Mean Impacts Miss: Distributional Effects of Welfare Reform Experiments‚Äù by Bitler, Gelbach, and Hoynes uses QTE to analyze Connecticut‚Äôs Jobs First program, a welfare reform initiative. The authors find that while mean impacts suggest modest income gains, QTE reveal substantial variation: earnings effects are zero at the bottom, positive in the middle, and negative at the top of the distribution before time limits take effect. ‚Äã After time limits, income effects are mixed, with gains concentrated in higher quantiles and losses at the lower end. ‚Äã This nuanced approach highlights the importance of QTE in uncovering the true breadth of policy impacts, enabling data scientists to better inform decision-making and address equity concerns in policy design.\n\n\n\nLet‚Äôs illustrate these ideas with an example in R and python. We‚Äôll use the iris dataset to estimate the effect of Sepal.Length on different quantiles of Petal.Length using UQR.\n\nRPython\n\n\nrm(list=ls())\nlibrary(quantreg)\n\n# Load dataset\ndata(iris)\n\n# Estimate unconditional quantiles\ntaus &lt;- c(0.25, 0.50, 0.75)\nq_vals &lt;- quantile(iris$Petal.Length, probs = taus)  # Estimate quantiles\nf_hat &lt;- density(iris$Petal.Length)\n\n# Compute RIF values\nrif_values &lt;- lapply(1:3, function(i) {\n  q &lt;- q_vals[i]\n  f &lt;- f_hat$y[which.min(abs(f_hat$x - q))]\n  q + ((taus[i] - (iris$Petal.Length &lt;= q)) / f)\n})\n\n# Run RIF regression\nmodels &lt;- lapply(rif_values, function(rif) lm(rif ~ Sepal.Length, data = iris))\n\n# Print results\nlapply(models, summary)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Estimate unconditional quantiles\ntaus = [0.25, 0.50, 0.75]\nq_vals = np.quantile(iris['Petal.Length'], taus)  # Estimate quantiles\nf_hat = gaussian_kde(iris['Petal.Length'])\n\n# Compute RIF values\nrif_values = []\nfor i, tau in enumerate(taus):\n    q = q_vals[i]\n    f = f_hat(q)  # Density at the quantile\n    rif = q + ((tau - (iris['Petal.Length'] &lt;= q).astype(int)) / f)\n    rif_values.append(rif)\n\n# Run RIF regression\nmodels = []\nfor rif in rif_values:\n    model = LinearRegression(fit_intercept=True)\n    model.fit(iris[['Sepal.Length']], rif)\n    models.append(model)\n\n# Print results\nfor i, model in enumerate(models):\n    print(f\"Model {i + 1}:\")\n    print(f\"Coefficient for Sepal.Length: {model.coef_[0]}\")\n    print(f\"Intercept: {model.intercept_}\")\n\n\n\nThis simple example demonstrates how to estimate the effect of a covariate on unconditional quantiles using the RIF regression approach."
  },
  {
    "objectID": "blog-unpublished/how-much-controls-matter.html",
    "href": "blog-unpublished/how-much-controls-matter.html",
    "title": "How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster‚Äôs Œ¥ Method",
    "section": "",
    "text": "In regression analysis, especially in social science and economics, a key question often arises: how sensitive are our estimates to the inclusion (or exclusion) of covariates? Are we picking up a genuine causal relationship, or is our coefficient of interest just soaking up omitted variable bias? Two influential papers‚ÄîGelbach (2016) and Oster (2019)‚Äîtackle this head-on but from different angles. Gelbach focuses on decomposing changes in coefficient estimates, while Oster proposes a method for assessing robustness to unobserved confounding. In this article, we explore both approaches and highlight how they can inform our understanding of variable importance and robustness."
  },
  {
    "objectID": "blog-unpublished/how-much-controls-matter.html#background",
    "href": "blog-unpublished/how-much-controls-matter.html#background",
    "title": "How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster‚Äôs Œ¥ Method",
    "section": "",
    "text": "In regression analysis, especially in social science and economics, a key question often arises: how sensitive are our estimates to the inclusion (or exclusion) of covariates? Are we picking up a genuine causal relationship, or is our coefficient of interest just soaking up omitted variable bias? Two influential papers‚ÄîGelbach (2016) and Oster (2019)‚Äîtackle this head-on but from different angles. Gelbach focuses on decomposing changes in coefficient estimates, while Oster proposes a method for assessing robustness to unobserved confounding. In this article, we explore both approaches and highlight how they can inform our understanding of variable importance and robustness."
  },
  {
    "objectID": "blog-unpublished/how-much-controls-matter.html#notation",
    "href": "blog-unpublished/how-much-controls-matter.html#notation",
    "title": "How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster‚Äôs Œ¥ Method",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs consider a standard linear regression model:\n\\[\ny = X\\beta + Z\\gamma + \\varepsilon\n\\]\nHere: - \\(y\\) is the outcome, - \\(X\\) is a variable of interest (say, a treatment), - \\(Z\\) is a set of controls, - \\(\\varepsilon\\) is the error term.\nSuppose we estimate a ‚Äúshort‚Äù regression without \\(Z\\), and then a ‚Äúlong‚Äù regression with \\(Z\\). How does the inclusion of \\(Z\\) affect the estimate of \\(\\beta\\)?"
  },
  {
    "objectID": "blog-unpublished/how-much-controls-matter.html#a-closer-look",
    "href": "blog-unpublished/how-much-controls-matter.html#a-closer-look",
    "title": "How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster‚Äôs Œ¥ Method",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nGelbach Decomposition\nGelbach (2016) proposes a formal method to break down the change in your coefficient of interest when you add additional covariates. Instead of just eyeballing the before-and-after change, Gelbach tells you exactly how much of the change is due to each new variable (or group of variables).\nThe key result comes from the omitted variable bias formula. For a regression of the form:\n\\[\ny = X_1 \\beta_1 + X_2 \\beta_2 + \\varepsilon,\n\\]\nwhere \\(X_1\\) is the variable of interest (e.g., treatment) and \\(X_2\\) is the set of additional controls, the difference between the coefficient on \\(X_1\\) in the ‚Äúshort‚Äù model (without \\(X_2\\)) and the ‚Äúlong‚Äù model (with \\(X_2\\)) can be written as:\n\\[\n\\hat{\\beta}^{short}_1 - \\hat{\\beta}^{long}_1 = (X_1' X_1)^{-1}X_1'X_2 \\hat{\\beta_2}.\n\\]\nThis formula lets you attribute the coefficient change to each control variable in \\(X_2\\). Crucially, this decomposition is order-invariant‚Äîunlike the naive practice of sequentially adding controls (which can produce wildly different results depending on the order of inclusion, as Gelbach demonstrates with wage gap studies).\nIntuition: Think of the decomposition as answering the question: Which control variables are responsible for the observed shift in my estimate? And by how much?\n\n\nGelbach Decomposition 2\nGelbach (2016) offers a way to formally decompose the change in \\(\\hat{\\beta}\\) when controls are added. The key result is:\n\\[\n\\hat{\\beta}_{\\text{long}} - \\hat{\\beta}_{\\text{short}} = \\hat{\\delta}'(\\hat{\\gamma}_{\\text{aux}})\n\\]\nWhere: - \\(\\hat{\\delta}\\) comes from regressing \\(Z\\) on \\(X\\) (auxiliary regression), - \\(\\hat{\\gamma}_{\\text{aux}}\\) are the coefficients from regressing \\(y\\) on \\(Z\\) while controlling for \\(X\\).\nIntuitively, this tells us exactly how much of the change in \\(\\hat{\\beta}\\) is due to each added covariate in \\(Z\\).\n\n\nOster‚Äôs \\(\\delta\\) Method\nWhile Gelbach helps you understand what‚Äôs happening with your observed controls, Oster (2019) tackles the next big question: What about the stuff I can‚Äôt observe?\nOster extends ideas from Altonji, Elder, and Taber (2005) and formalizes the relationship between coefficient stability and omitted variable bias. But here‚Äôs the critical insight: coefficient stability alone is not enough. You also need to look at changes in \\(R^2\\).\nIf adding controls barely budges your coefficient and dramatically increases your \\(R^2\\), that suggests the included controls are genuinely explaining a lot of variation‚Äîand unobserved factors may not be a huge threat. On the other hand, if the \\(R^2\\) hardly changes, even a stable coefficient might not mean much.\nThe Oster bounding formula allows you to compute what your estimate would be if you could observe everything. The adjusted coefficient is given by:\n\\[\n\\tilde{\\beta} = \\hat{\\beta}_{\\text{long}} - \\delta(\\hat{\\beta}_{\\text{short}} - \\hat{\\beta}_{\\text{long}}) \\cdot \\left(\\frac{R^2_{\\text{max}} - R^2_{\\text{long}}}{R^2_{\\text{long}} - R^2_{\\text{short}}}\\right),\n\\]\nwhere:\n\n\\(R^2_{\\text{short}}\\) is the fit of the model without controls,\n\\(R^2_{\\text{long}}\\) is the fit with controls,\n\\(R^2_{\\text{max}}\\) is the hypothetical fit if all confounders were observed,\n\\(\\delta\\) is the key assumption: the relative importance of selection on unobservables versus observables.\n\nOster (2019) suggests that assuming \\(\\delta=1\\) (equal selection) is a reasonable upper bound, but this can be adjusted based on context.\n\n\nOster‚Äôs \\(\\delta\\) Method 2\nOster (2019) tackles a different but related question: what if we‚Äôre worried about selection on unobservables? Her approach uses coefficient stability and changes in \\(R^2\\) to bound the effect of unobserved confounding.\nThe core idea is to calculate a value \\(\\delta\\) such that:\n\\[\n\\frac{\\Delta\\beta}{\\Delta R^2} = \\delta\n\\]\nIf this ratio is stable as you add more controls, and assuming selection on unobservables is not much worse than selection on observables, you can project how much \\(\\hat{\\beta}\\) would change if you could observe everything. She formalizes this with the adjusted coefficient formula:\n\\[\n\\tilde{\\beta} = \\hat{\\beta}_{\\text{long}} - \\delta(\\hat{\\beta}_{\\text{short}} - \\hat{\\beta}_{\\text{long}}) \\cdot \\left(\\frac{R^2_{\\text{max}} - R^2_{\\text{long}}}{R^2_{\\text{long}} - R^2_{\\text{short}}}\\right)\n\\]\nThis gives a way to estimate bounds on \\(\\beta\\) under assumptions about how much more could be explained if we had access to the unobservables.\n\n\nIntuition and Contrast\nGelbach‚Äôs method is like a post-mortem dissection: it tells you exactly how much each covariate changed your result. Oster‚Äôs method is more of a risk assessment: given the changes you‚Äôve seen, how scared should you be of the things you can‚Äôt see?\nBoth are powerful tools, but they address different kinds of uncertainty‚ÄîGelbach focuses on observable confounding, while Oster extends this to unobservable confounding.\n\n\nWhy Coefficient Stability Alone Can Mislead\nBoth papers caution against over-interpreting coefficient stability on its own:\n\nGelbach shows that apparent robustness may depend on which controls you added and in what order.\nOster shows that stability without \\(R^2\\) movement is meaningless‚Äîif your controls aren‚Äôt explaining much of the outcome, their failure to shift the coefficient tells you little.\n\nOster‚Äôs illustrative example of wage returns to education highlights this issue clearly: if you add a weak control (say, a poor proxy for ability), the coefficient on education may appear stable, but that doesn‚Äôt mean there‚Äôs no bias‚Äîit just means your control wasn‚Äôt very good."
  },
  {
    "objectID": "blog-unpublished/how-much-controls-matter.html#an-example",
    "href": "blog-unpublished/how-much-controls-matter.html#an-example",
    "title": "How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster‚Äôs Œ¥ Method",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nlibrary(hdm)\nlibrary(oaxaca)\n\n# Simulated data\ndf &lt;- data.frame(\n  y = rnorm(1000),\n  x = rnorm(1000),\n  z1 = rnorm(1000),\n  z2 = rnorm(1000)\n)\n\n# Short model\nshort &lt;- lm(y ~ x, data = df)\n\n# Long model\nlong &lt;- lm(y ~ x + z1 + z2, data = df)\n\n# Gelbach decomposition using Oaxaca-Blinder (as approximation)\nlibrary(oaxaca)\noaxaca(y ~ x | z1 + z2, data = df, R = 30)\n\n# Oster bounds (simplified)\nlibrary(psacalc)\npsa(y = df$y, d = df$x, X = df[, c(\"z1\", \"z2\")], R2max = 0.9)\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Simulated data\ndf = pd.DataFrame({\n    'y': np.random.randn(1000),\n    'x': np.random.randn(1000),\n    'z1': np.random.randn(1000),\n    'z2': np.random.randn(1000)\n})\n\n# Short model\nX_short = sm.add_constant(df['x'])\nmodel_short = sm.OLS(df['y'], X_short).fit()\n\n# Long model\nX_long = sm.add_constant(df[['x', 'z1', 'z2']])\nmodel_long = sm.OLS(df['y'], X_long).fit()\n\nprint(model_short.params)\nprint(model_long.params)\n\n# No direct analog to Gelbach in Python, but differences in coefficients can be manually computed."
  },
  {
    "objectID": "blog-unpublished/how-much-controls-matter.html#bottom-line",
    "href": "blog-unpublished/how-much-controls-matter.html#bottom-line",
    "title": "How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster‚Äôs Œ¥ Method",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nGelbach decomposition explains why coefficients change when you add covariates.\nOster‚Äôs method helps you assess how robust your findings are to unobserved variables.\nBoth approaches require strong assumptions‚Äîbut when applied carefully, they offer insight into what‚Äôs driving your regression results.\nDon‚Äôt blindly include controls‚Äîuse tools like these to make sense of what they‚Äôre doing to your estimates."
  },
  {
    "objectID": "blog-unpublished/how-much-controls-matter.html#where-to-learn-more",
    "href": "blog-unpublished/how-much-controls-matter.html#where-to-learn-more",
    "title": "How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster‚Äôs Œ¥ Method",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nIf you‚Äôre into causal inference and want to go beyond just ‚Äúthrowing in controls,‚Äù both these papers are must-reads. For applied guidance, check out Emily Oster‚Äôs book ‚ÄúUncontrolled‚Äù and Guido Imbens‚Äô lecture notes on sensitivity analysis. You might also explore sensitivity tools like sensemakr in R, which is built around similar ideas to Oster‚Äôs method."
  },
  {
    "objectID": "blog-unpublished/how-much-controls-matter.html#references",
    "href": "blog-unpublished/how-much-controls-matter.html#references",
    "title": "How Much Do Controls Matter? Unpacking Gelbach Decomposition and Oster‚Äôs Œ¥ Method",
    "section": "References",
    "text": "References\n\nGelbach, J. B. (2016). When do covariates matter? And which ones, and how much? Journal of Labor Economics, 34(2), 509‚Äì543.\nOster, E. (2019). Unobservable selection and coefficient stability: Theory and evidence. Journal of Business & Economic Statistics, 37(2), 187‚Äì204."
  },
  {
    "objectID": "blog-unpublished/interpreting-OLS-causal-inference.html",
    "href": "blog-unpublished/interpreting-OLS-causal-inference.html",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "",
    "text": "use the model by Aronow and Samii but that‚Äôs asymptotically\nZubisaretta show a finite sample formula\nSloczynski focuses on heterogeneous. Show it briefly\nAngrist focuses on saturated models. impossible with continuous X\nOLS regressions do not implicitly assume PO framework"
  },
  {
    "objectID": "blog-unpublished/interpreting-OLS-causal-inference.html#background",
    "href": "blog-unpublished/interpreting-OLS-causal-inference.html#background",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "Background",
    "text": "Background\nOrdinary Least Squares (OLS) regression is a staple in the toolkit of data scientists and statisticians. However, its application in causal inference, especially in observational studies, requires a nuanced understanding of its limitations and the implications of its estimates. In applied research, Ordinary Least Squares (OLS) is often the workhorse method for estimating causal effects from observational data. Its popularity comes from its simplicity and the appealing property that‚Äîwhen treatment effects are homogeneous‚Äîit recovers the average treatment effect (ATE) under standard assumptions. However, in practice, treatment effects are frequently heterogeneous. This heterogeneity means that the impact of a treatment can vary across individuals, and as a result, the standard OLS estimator does not simply estimate the ATE. Instead, it implicitly assigns weights to different subpopulations (e.g., the treated and the untreated), and these weights can be quite counterintuitive.\nTwo recent papers help us understand this phenomenon in detail. Chattopadhyay and Zubizarreta (2022) focuses on deriving closed-form expressions for the implied weights of linear regression estimators. Their analysis shows how, even when treatment effects are homogeneous, the regression adjustment does not ‚Äútreat‚Äù every observation equally‚Äîit rather targets a specific covariate profile that may differ from the sample average. On the other hand, S≈Çoczy≈Ñski (2022) takes a closer look at the scenario with heterogeneous treatment effects. He demonstrates that the OLS treatment coefficient is actually a convex combination of the average treatment effects on the treated (ATT) and the untreated (ATU), and, quite surprisingly, the weights assigned to these groups are inversely related to their sample proportions.\nThis article aims to delve into the interpretation of OLS estimates for causal inference, focusing on the implied weights of linear regression and the impact of treatment effect heterogeneity. The goal of this article is to unpack these findings in detail, explain the underlying mathematical structure, and provide intuition for why OLS behaves the way it does in causal inference settings."
  },
  {
    "objectID": "blog-unpublished/interpreting-OLS-causal-inference.html#notation",
    "href": "blog-unpublished/interpreting-OLS-causal-inference.html#notation",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs start by setting up our notation in the familiar potential outcomes framework. Suppose we have a sample \\(\\{ \\,Y_i, D_i, X_i \\} \\,_{i=1}^n\\), where:\n\n\\(Y_i\\) is the observed outcome for unit \\(i\\)\n\\(Y_i(0), Y_i(1)\\) are the potential outcomes for unit \\(i\\) if in control and treatment, respectively.\n\\(D_i\\) indicates treatment (\\(D_i=1\\) for treated units, and \\(D_i=0\\) for controls)\n\\(X_i\\) is a vector of covariates\n\nThe conventional OLS regression model is written as:\n\\[Y_i=\\alpha + \\tau D_i +X_i \\beta + \\epsilon_i,\\]\nwhere \\(\\epsilon\\) is a mean-zero error term and \\(\\tau\\) is typically interpreted as the causal effect of the treatment.\nThe key causal estimants (i.e., target parameters) are:\n\nAverage Treatment Effect (ATE): \\(E\\left[Y(1)-Y(0)\\right]\\),\nAverage Treatment Effect on the Treated (ATT): \\(E\\left[Y(1)-Y(0)\\mid D=1 \\right]\\),\nAverate Treatment effect on the Control (ATC): \\(E\\left[Y(1)-Y(0) \\mid D=0 \\right]\\).\n\nLastly, it is common to introduce the propensity score \\(p(X)=E\\left[D\\mid X\\right]\\) or its best linear approximation, which plays a central role in understanding the weights that OLS assigns."
  },
  {
    "objectID": "blog-unpublished/interpreting-OLS-causal-inference.html#a-closer-look",
    "href": "blog-unpublished/interpreting-OLS-causal-inference.html#a-closer-look",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nHomogeneous Treatment Effects\nexplore how linear regression adjustments in observational studies emulate key features of randomized experiments. The main focus is on Section 3, where they derive the implied weights of various linear regression estimators.\nThe authors show that the OLS estimator of the ATE can be expressed as a difference of weighted means of the treated and control outcomes. Specifically, they provide closed-form expressions for the implied regression weights. For instance, the URI (uni-regression imputation) estimator of the ATE is given by:\n\\[\\hat{\\tau}_{\\text{OLS}}=\\sum_{i:D=1} w_i ^{\\text{URI}}Y_i - \\sum_{i:D=0} w_i^{\\text{URI}}Y_i\\]\nwhere the weights \\(w_i ^{\\text{URI}}\\) depend on the covariates and treatment indicators but not on the observed outcomes. This weighting representation shows that linear regression can be ‚Äúfit‚Äù without the outcomes, aligning it with the design stage of an observational study.\nThe paper also discusses the properties of these implied weights, such as covariate balance, representativeness, dispersion, and optimality. For example, the URI and MRI (multi-regression imputation) weights exactly balance the means of the covariates included in the model, ensuring that the regression adjustments emulate the covariate balance of a randomized experiment.\n\n\nHeterogeneous Treatment Effects\nS≈Çoczy≈Ñski‚Äôs paper tackles the same question in the world of heterogeneous treatment effects. The key result is that the OLS treatment coefficient is a convex combination of the average treatment effects on the treated (ATT) and untreated (ATU), with weights inversely related to the proportion of observations in each group.\nThe scenario becomes even more intricate when treatment effects vary across individuals. S≈Çoczy≈Ñski (2022) takes on this challenge by examining the causal interpretation of the OLS estimand when treatment effects are heterogeneous. He shows that the OLS treatment coefficient can be decomposed as a convex combination of two group-specific effects:\nMathematically, the OLS estimand can be expressed as:\n\\[\\hat{\\tau}_{\\text{OLS}}=w_1 \\times \\text{ATT} + w_0 \\times \\text{ATC},\\]\nwhere \\(w_1 = \\frac{1}{1}=f(\\rho, p(X))\\) and \\(w_0 = 1 -w_1\\).\nThis result highlights that OLS places more weight on the group with fewer observations, which can lead to substantial biases when interpreting the OLS estimand as the ATE or ATT.\nS≈Çoczy≈Ñski provides diagnostic tools to detect these biases, emphasizing that OLS might often be substantially biased for ATE, ATT, or both. He suggests using alternative estimators or diagnostic methods to avoid potential biases in applied work."
  },
  {
    "objectID": "blog-unpublished/interpreting-OLS-causal-inference.html#bottom-line",
    "href": "blog-unpublished/interpreting-OLS-causal-inference.html#bottom-line",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLinear regression remains among the most popular ways to estimate treatment effects in causal inference\nIt implicitly weights individual observations, which can be represented in closed-form expressions.\nOLS estimates can be biased when treatment effects are heterogeneous, with smaller groups receiving larger weights."
  },
  {
    "objectID": "blog-unpublished/interpreting-OLS-causal-inference.html#references",
    "href": "blog-unpublished/interpreting-OLS-causal-inference.html#references",
    "title": "Interpreting OLS Estimates for Causal Inference",
    "section": "References",
    "text": "References\nAngrist, J. D., & Krueger, A. B. (1999). Empirical strategies in labor economics. In Handbook of labor economics (Vol. 3, pp.¬†1277-1366). Elsevier.\nAngrist, J. D., & Pischke, J. S. (2009). Mostly harmless econometrics: An empiricist‚Äôs companion. Princeton university press.\nAronow, P. M., & Samii, C. (2016). Does regression produce representative estimates of causal effects?. American Journal of Political Science, 60(1), 250-267.\nChattopadhyay, A., & Zubizarreta, J. R. (2023). On the implied weights of linear regression for causal inference. Biometrika, 110(3), 615-629.\nHumphreys, M. (2009). Bounds on least squares estimates of causal effects in the presence of heterogeneous assignment probabilities. Manuscript, Columbia University.\nS≈Çoczy≈Ñski, T. (2022). Interpreting OLS estimands when treatment effects are heterogeneous: Smaller groups get larger weights. The Review of Economics and Statistics, 104(3), 501-509."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html",
    "href": "blog/hypothesis-testing-linear-ml.html",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "",
    "text": "Machine learning models are an indispensable part of data science. They are incredibly good at what they are designed for ‚Äì making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, \\(p\\)-values, or anything else related to statistical significance. Why?\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have already selected the most strongly correlated variables.\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The \\(t\\)-stats and \\(p\\)-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models ‚Äì Ridge, Elastic net, SCAD, etc."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#background",
    "href": "blog/hypothesis-testing-linear-ml.html#background",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "",
    "text": "Machine learning models are an indispensable part of data science. They are incredibly good at what they are designed for ‚Äì making excellent predictions. They fall short in assessing the strength of the relationships they find. ML models make no reference to hypothesis testing, \\(p\\)-values, or anything else related to statistical significance. Why?\nSeveral thorny challenges stand in the way. For starters, ML algorithms often scan the data multiple times to choose the best model (e.g., in selecting hyperparameters or choosing a few relevant variables). In the world of statistical inference, this is a bit like cheating since we have already selected the most strongly correlated variables.\nEven if we can account for this (which we sometimes can), there is still the issue that ML models might make mistakes. For instance, regularization might force a model to exclude a variable that, in reality, belongs to the model with only a small coefficient. The \\(t\\)-stats and \\(p\\)-values of the remaining variables are potentially contaminated and unreliable. This might seem subtle, but overcoming it has proven challenging.\nResearchers have made significant progress in assessing statistical significance in ML models in the past decade. This is exciting as it widens our understanding of how these so-very-commonly-used models work. We now have a wide variety of methods for hypothesis testing, and I will walk you through some of the most popular ones.\nThis field is often referred to as statistical inference after model selection. For simplicity, I will focus on linear models (and Lasso in particular), where we have the most exciting breakthroughs. Keep in mind that many methods generalize to other linear models ‚Äì Ridge, Elastic net, SCAD, etc."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#notation",
    "href": "blog/hypothesis-testing-linear-ml.html#notation",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Notation",
    "text": "Notation\nAs a reminder, \\(\\beta^{lasso}\\) is the solution to:\n\\[\\min_{\\beta} \\frac{1}{2} || Y-x\\beta|| ^2_2 + \\lambda ||\\beta||_1. \\]\nWe are trying to predict a vector \\(Y\\in \\mathbb{R}\\) with a set of features \\(X\\in \\mathbb{R}^{pxn}\\) with \\(p\\leq n\\), and \\(\\lambda\\) is a tuning parameter. When needed, I will use \\(j\\) to index individual columns (i.e., variables) of \\(X\\)."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#a-closer-look",
    "href": "blog/hypothesis-testing-linear-ml.html#a-closer-look",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nTwo Types of Models and Parameters\nWe first need to discuss an important subtlety. There are two distinct ways of thinking about performing hypothesis testing in ML models. The traditional view is that we have a true linear model which includes all variables:\n\\[\\begin{equation} Y=X\\beta_0+\\epsilon. \\end{equation}\\]\nWe are interested in testing whether \\(\\beta_0=0\\) ‚Äì that is, inference on the full model. This is certainly an intuitive target. The interpretation is that this model encapsulates all relevant causal variables for \\(Y\\). Importantly, even if a given variable $X_j $ is not selected, it still belongs to the model and has a meaningful interpretation.\nThere is an alternative way to think about the problem. Imagine we run a variable selection algorithm (e.g., lasso), which selects a subset \\(M=\\{1,\\dots,p\\}\\) of all available predictors (\\(X\\)), \\(M&lt;p\\) leading to the alternative model:\n\\[\\begin{equation} Y=X_M\\beta_M+u. \\end{equation}\\]\nNow we are interested in testing whether \\(\\beta_M=0\\) ‚Äì that is, inference on the selected model. Unlike the scenario above, here \\(\\beta_{Mj}\\) is interpreted as the change in \\(Y\\) for a unit change in \\(X_j\\) when all other variables in \\(X_M\\) (as opposed to all of \\(X\\)) are kept constant.\nWhich of the two targets is more intuitive?\nStatisticians argue vehemently about this. Some claim that the full model interpretation is inherently problematic. It is too na√Øve and perhaps even arrogant to think that (i) mother nature can be explained by a linear equation, (ii) we can measure and include the full set of relevant predictors. On top of this, there are also technical issues with this interpretation beyond the scope of this post.\nTo overcome these challenges, relatively recently, statisticians developed the idea of inference on the selected model. This introduces major technical challenges, however.\n\n\nThe Na√Øve Approach: What Not to Do\nFirst things first ‚Äì here is what we should not do.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRun a Lasso regression.\nRun OLS regression on the subset of selected variables.\nPerform statistical inference with the estimated \\(t\\)-stats, confidence intervals, and \\(p\\)-values.\n\n\n\nThis is bad practice. Can you see why?\nIt is simply because we ignored the fact that we already peeked at the data when we ran the Lasso regression. Lasso already chose the variables that are strongly correlated with the outcome. Intuitively, we will need to inflate the \\(p\\)-values to account for the data exploration in the first step.\nIt turns out that, in general, both the finite- and large-sample distributions of these parameters are non-Gaussian and depend on unknown parameters in weird ways. Consequently, the calculated \\(t\\)-stats and \\(p\\)-values are all wrong, and there is little hope that anything simple can be done to save this approach. And no, the standard bootstrap cannot help us either.\nBut are there special cases when this approach might work? A recent paper titled ‚ÄúIn Defense of the Indefensible: A Very Na√Øve Approach to High-Dimensional Inference‚Äù argues that under very strict assumptions on \\(X\\) and \\(\\lambda\\), this method is actually kosher. The reason it works is that in the magical world of those assumptions, the set of variables that Lasso chooses is deterministic, and not random (hence circumventing the issue described above). The resulting estimator is unbiased and asymptotically normal ‚Äì hence hypothesis testing is trivial.\nHere is what we should do instead.\n\n\nThe Classical Approach: Inference on the Full Model\nRoughly speaking, there are at least four ways we can go about doing hypothesis testing for \\(\\beta\\) in equation (1).\n\nData Split\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nSplit our data into two equal parts.\nRun a Lasso regression on the first part.\nRun OLS on the second part with the selected variables from Step 2.\nPerform inference using the computed \\(t\\)-stats, \\(p\\)-values, and confidence intervals.\n\n\n\nThis is simple and intuitive. The problem is that in small samples, the \\(p\\)-values can be quite sensitive to how we split the data in the first step. This is clearly undesirable, as we will be getting different results every time we run this algorithm for no apparent reason.\n\n\nMulti Split\nThis is a modification of the Data Split approach designed to solve the sensitivity issue and increase power.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRepeat \\(B\\) times:\n\n\nReshuffle data.\nRun the Data Split method.\nSave the \\(p\\)-values.\n\n\nAggregate the B \\(p\\)-values into a single final one for each variable.\n\n\n\nInstead of splitting the data into two parts only once, we can do it many times, and each time, we get a \\(p\\)-value for every variable. The aggregation goes a long way to solving the instability of the simple data split approach. There is a lot of clever mathematics behind it. For example, there is a complicated expression for aggregating the \\(p\\)-values rather than taking a simple average.\nSoftware Package: hdi.\n\n\nBias Correction\nThis approach tackles the problem from a very different angle. The idea is to directly remove the bias from the na√Øve Lasso procedure without any subsampling or data splitting. Somewhat magically, the resulting estimator is unbiased and asymptotically normally distributed ‚Äì statistical inference is then straightforward.\nThere are multiple versions of this idea, but the general form of these estimators is:\n\\[\\hat{\\beta}^{\\text{bias cor}} = \\hat{\\beta}^{lasso} + \\hat{\\Theta} X'\\epsilon^{lasso},\\]\nWhere \\(\\hat{\\beta}^{lasso}\\) is the lasso estimator and \\(\\epsilon^{lasso}\\) are the residuals. The missing piece is the \\(\\hat{\\Theta}\\) matrix; there are several ways to estimate it depending on the setting. In its simplest form, \\(\\hat{\\Theta}\\) is the inverse of the sample variance-covariance matrix. Other examples include the matrix, which minimizes an error term related to the bias as well as the variance of its Gaussian component. Similar bias-correction methods have been developed for Ridge regression as well.\nSoftware Package: hdi.\n\n\nBootstrap\nAs in many other complicated settings for statistical inference, the bootstrap can come to the rescue. Still, the plain vanilla bootstrap will not do. Instead, here is the general idea of the leading version of the bootstrap estimator for Lasso:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nRun a Lasso regression.\nKeep only \\(\\beta^{lasso}\\)‚Äôs larger than some magical threshold.\nCompute the associated residuals and center them around \\(0\\).\nRepeat B times:\n\n\ndraw random samples of these centered residuals,\ncompute new responses \\(\\dot{Y}\\) by adding them to the predictions \\(X'\\beta^{lasso}\\), and\nobtain \\(\\beta^{lasso}\\) coefficients from Lasso regressions on these new responses \\(\\dot{Y}\\).\n\n\nUse the distribution of the obtained coefficients to conduct statistical inference.\n\n\n\nThis idea can be generalized to other settings and, for instance, be combined with the bias-corrected estimator.\nThis wraps up our discussion of methods for performing hypothesis testing on equation (1) (i.e., the full model). We now move on to a more challenging topic ‚Äì inference on equation (2) (i.e., the selected model).\n\n\n\nThe Novel Approach: Inference on the Selected Model\n\nPoSI (Post Selection Inference)\nThe goal of the PoSI method is to construct confidence intervals that are valid regardless of the variable selection method and the selected submodel. The benefit is that we would be reaching the correct conclusion even if we did not select the true model. This luxury comes at the expense of often being too conservative (i.e., confidence intervals are ‚Äútoo wide‚Äù). Let me explain how this is done.\nTo take a step back, most confidence intervals in statistics take the form:\n\\[\\hat{\\beta} \\pm m \\times \\hat{SE}(\\hat{\\beta}).\\]\nEvery data scientist has seen a similar formula before. The question is usually one about choosing the constant \\(m\\). When we work with two-sided hypotheses tests and large samples, we often use \\(m = 1.96\\) because this is roughly the \\(97.5\\)th percentile of the \\(t\\)-distribution with many degrees of freedom. This gives a \\(2.5\\%\\) false positive error on both tails of the distribution (\\(5\\%\\) in total) and hence the associated 95% confidence intervals. The larger m, the wider or more conservative the confidence interval.\nThere are a few ways to choose the constant m in the PoSI world. Vaguely speaking, PoSI says we should select this constant to equal the \\(97.5\\)th percentile of a distribution related to the largest \\(t\\)-statistic among all possible models. This is usually approximated with Monte Carlo simulations. Interestingly, we do not need the response variable to approximate the value.\n\\[m = \\max_{\\text{ \\{all models and vars\\} }} |t|.\\]\nAnother and even more conservative choice for \\(m\\) is the Scheffe constant.\n\\[m^{scheffe} = \\sqrt{rank(X) \\times F(rank(X),  n-p)},\\]\nwhere \\(F(\\dot)\\) denotes the \\(95\\)th percentile of the \\(F\\) distribution with the respective degrees of freedom.\nUnfortunately, you guessed correctly that this method does not scale well. In some sense, it is a ‚Äúbrute force‚Äù method that scans through all possible model combinations and all variables within each model and picks the most conservative value. The authors recommend this procedure for datasets with roughly \\(p&lt;20\\). This rules out many practical applications where machine learning is most useful.\nSoftware Package: PoSI\n\n\nEPoSI (Exact PoSI)\nOk, the name of this approach is not super original. The ‚ÄúE‚Äù here stands for ‚Äúexact.‚Äù Unlike its cousin, this approach is valid only in the selected submodel. Because we cover fewer scenarios, the intervals will generally be narrower than the PoSI ones. EPoSI produces valid finite sample (as opposed to asymptotic) confidence intervals and \\(p\\)-values. Like all methods described here, the math behind this is extremely technical. So, I will give you only a high-level description of how this works.\nThe idea is first to get the conditional distribution of \\(\\beta\\) given the selected model. A bit magically, it turns out it is a truncated normal distribution. Really, who would have guessed this? Do you even remember truncated probability distributions (hint: they are just like regular PDFs but bounded from at least one side. This requires further scaling so that the density area sums to 1.)?\nTo dig one layer deeper, the authors show that the selection of Lasso predictors can be recast as a ‚Äúpolyhedral region‚Äù of the form:\n\\[ AY\\leq b. \\]\nIn English, for fixed \\(X\\) and \\(\\lambda\\), the set of alternative outcome values \\(Y^*\\) which yields the same set of selected predictors, can be expressed by the simple inequality above. In it, \\(A\\) and \\(b\\) depend do not depend on \\(Y\\). Under this new result, the distribution of \\(\\hat{\\beta}_M^{\\text{EPoSI}}\\) is now well-understood and tractable, thus enabling valid hypothesis testing.\nThen, we can use the conditional distribution function to construct a whimsical test statistic that is uniformly distributed on the \\([0,1]\\) interval. And we can finally build confidence intervals based on that statistic.\nSelective inference is currently among the hottest topic in all of statistics. There have been a myriad of extensions and improvements on the original paper. Still, this literature is painstakingly technical. What is the polyhedral selection property or a union of polytopes?\nSoftware Package: selectiveInference."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#an-example",
    "href": "blog/hypothesis-testing-linear-ml.html#an-example",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Refer to the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\nUnlike in Monte Carlo simulations, I do not know the ground truth here, so this exercise is not informative about which approaches work well and which do not. Rather, it just serves as an illustrative example.\nHere is a table displaying the number of statistically significant variables with \\(p &lt; .05\\) for various inference methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n\\(\\#\\) vars \\(p &lt; .05\\)\n7\n5\n2\n3\n2\n2\n\n\n\n\nAs expected, the naive method results in the smallest \\(p\\)-values and hence the highest number of significant predictors ‚Äì seven. Data Split knocks down two of those seven variables, resulting in five significant ones. The rest are more conservative, leaving only two or three features with \\(p &lt; .05\\).\nBelow is the table with \\(p\\)-values for all variables and each method.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: \\(p\\)-values\n\n\n\n\n\n\n\n\n\n\n\nNaive\nData Split\nMulti Split\nBias Correct.\nPoSI\nEPoSI\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.01\n1.00\n\n\nage\n0.00\n0.04\n1.00\n0.01\n1.00\n1.00\n\n\nsibsp\n0.02\n0.03\n1.00\n0.21\n1.00\n1.00\n\n\nparch\n0.32\n0.64\n1.00\n1.00\n1.00\n1.00\n\n\nfare\n0.20\n0.21\n1.00\n1.00\n1.00\n1.00\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n\n\nembarkedS\n0.00\n0.01\n1.00\n0.06\n-\n1.00\n\n\ncabinA\n0.39\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinB\n0.35\n-\n1.00\n1.00\n1.00\n1.00\n\n\ncabinD\n0.05\n0.27\n1.00\n0.44\n1.00\n-\n\n\ncabinE\n0.00\n0.64\n1.00\n0.06\n1.00\n1.00\n\n\ncabinF\n0.02\n0.52\n1.00\n0.21\n1.00\n1.00\n\n\nembarkedC\n-\n-\n1.00\n0.11\n1.00\n1.00\n\n\nembarkedQ\n-\n-\n1.00\n1.00\n1.00\n0.00\n\n\ncabinC\n-\n-\n1.00\n1.00\n1.00\n-\n\n\n\n\nYou can find the code for this exercise in this GitHub repo."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#bottom-line",
    "href": "blog/hypothesis-testing-linear-ml.html#bottom-line",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods mine through datasets to find strong correlations between the response and the features. Quantifying this strength is an open and challenging problem.\nThe naive approach to hypothesis testing is usually invalid.\nThere are two main approaches that work ‚Äì inference on the full model or on the selected model. The latter poses more technical challenges than the former.\nIf we are interested in the full model, the Multi Split approach is general enough to cover a wide range of models and settings.\nIf we believe you have to focus on the selected model, EPoSI is the state-of-the-art.\nSimulation exercises usually show no clear winner, as none of the methods consistently outperforms the rest."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#where-to-learn-more",
    "href": "blog/hypothesis-testing-linear-ml.html#where-to-learn-more",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nTaylor and Tibshirani (2015) give a non-technical introduction to the problem space along with a description of the POSI method ‚Äì a great read but focused on a single approach. Other studies both provide a relatively accessible overview of the various methods for statistical inference on the full model. For technical readers, Zhang et al.¬†(2022) provide an excellent up-to-date review of the literature, which I used extensively."
  },
  {
    "objectID": "blog/hypothesis-testing-linear-ml.html#references",
    "href": "blog/hypothesis-testing-linear-ml.html#references",
    "title": "Hypothesis Testing in Linear Machine Learning Models",
    "section": "References",
    "text": "References\nBerk, R., Brown, L., Buja, A., Zhang, K., & Zhao, L. (2013). Valid post-selection inference. The Annals of Statistics, 802-837.\nB√ºhlmann, P. (2013). Statistical significance in high-dimensional linear models. Bernoulli, 19(4), 1212-1242.\nDezeure, R., B√ºhlmann, P., Meier, L., & Meinshausen, N. (2015). High-dimensional inference: confidence intervals, p-values and R-software hdi. Statistical Science, 533-558.\nJavanmard, A., & Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15(1), 2869-2909.\nLee, J. D., Sun, D. L., Sun, Y., & Taylor, J. E. (2016). Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3), 907-927.\nLeeb, H., & P√∂tscher, B. M. (2005). Model selection and inference: Facts and fiction. Econometric Theory, 21(1), 21-59.\nLeeb, H., P√∂tscher, B. M., & Ewald, K. (2015). On various confidence intervals post-model-selection. Statistical Science, 30(2), 216-227.\nMeinshausen, N., Meier, L., & B√ºhlmann, P. (2009). P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488), 1671-1681.\nTaylor, J., & Tibshirani, R. J. (2015). Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112(25), 7629-7634.\nVan de Geer, S., B√ºhlmann, P., Ritov, Y. A., & Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. The Annals of Statistics, 42(3), 1166-1202.\nWasserman, L., & Roeder, K. (2009). High dimensional variable selection. Annals of Statistics, 37(5A), 2178.\nZhang, D., Khalili, A., & Asgharian, M. (2022). Post-model-selection inference in linear regression models: An integrated review. Statistics Surveys, 16, 86-136.\nZhang, C. H., & Zhang, S. S. (2014). Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 217-242.\nZhao, S., Witten, D., & Shojaie, A. (2021). In defense of the indefensible: A very naive approach to high-dimensional inference. Statistical Science, 36(4), 562-577."
  },
  {
    "objectID": "blog-unpublished/Q-correlated-random-effects.html",
    "href": "blog-unpublished/Q-correlated-random-effects.html",
    "title": "Understanding Correlated Random Effects Models",
    "section": "",
    "text": "In the world of panel data analysis, we often find ourselves choosing between fixed effects (FE) and random effects (RE) models. Each has its strengths and limitations. The FE model controls for all time-invariant unobserved heterogeneity but does not allow us to estimate the effects of those time-invariant covariates. On the other hand, the RE model allows for the inclusion of time-invariant variables but makes a strong assumption: that the unobserved individual-specific effects are uncorrelated with the regressors. What happens when that assumption doesn‚Äôt hold? That‚Äôs where the correlated random effects (CRE) model‚Äîor the hybrid model‚Äîsteps in.\nThis article explores the motivation behind the CRE model, its mechanics, and its advantages over traditional FE and RE models. We also provide a hands-on example using R to illustrate how it works in practice."
  },
  {
    "objectID": "blog-unpublished/Q-correlated-random-effects.html#background",
    "href": "blog-unpublished/Q-correlated-random-effects.html#background",
    "title": "Understanding Correlated Random Effects Models",
    "section": "",
    "text": "In the world of panel data analysis, we often find ourselves choosing between fixed effects (FE) and random effects (RE) models. Each has its strengths and limitations. The FE model controls for all time-invariant unobserved heterogeneity but does not allow us to estimate the effects of those time-invariant covariates. On the other hand, the RE model allows for the inclusion of time-invariant variables but makes a strong assumption: that the unobserved individual-specific effects are uncorrelated with the regressors. What happens when that assumption doesn‚Äôt hold? That‚Äôs where the correlated random effects (CRE) model‚Äîor the hybrid model‚Äîsteps in.\nThis article explores the motivation behind the CRE model, its mechanics, and its advantages over traditional FE and RE models. We also provide a hands-on example using R to illustrate how it works in practice."
  },
  {
    "objectID": "blog-unpublished/Q-correlated-random-effects.html#notation",
    "href": "blog-unpublished/Q-correlated-random-effects.html#notation",
    "title": "Understanding Correlated Random Effects Models",
    "section": "Notation",
    "text": "Notation\nLet us consider a standard panel data setup where we observe units \\(i = 1, \\dots, N\\) over time periods \\(t = 1, \\dots, T\\). The outcome is \\(y_{it}\\), and \\(x_{it}\\) is a vector of time-varying covariates. The generic random effects model is:\n\\[y_{it} = x_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\\]\nwhere \\(\\alpha_i\\) is the individual-specific effect and \\(\\varepsilon_{it}\\) is the idiosyncratic error term. The crucial RE assumption is:\n\\[\\text{Cov}(x_{it}, \\alpha_i) = 0\\]\nThe CRE model relaxes this assumption by explicitly modeling the correlation between \\(x_{it}\\) and \\(\\alpha_i\\)."
  },
  {
    "objectID": "blog-unpublished/Q-correlated-random-effects.html#a-closer-look",
    "href": "blog-unpublished/Q-correlated-random-effects.html#a-closer-look",
    "title": "Understanding Correlated Random Effects Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe CRE Model\nThe basic idea of the CRE model, popularized by Mundlak (1978) and further advocated by Schunck (2013), is to decompose the time-varying covariate \\(x_{it}\\) into two components:\n\\[x_{it} = \\bar{x}_i + (x_{it} - \\bar{x}_i)\\]\nwhere \\(\\bar{x}_i = \\frac{1}{T} \\sum_t x_{it}\\) is the individual-specific mean of \\(x_{it}\\). The model then becomes:\n\\[y_{it} = (x_{it} - \\bar{x}_i)'\\beta_w + \\bar{x}_i'\\beta_b + \\alpha_i + \\varepsilon_{it}\\]\nHere, \\(\\beta_w\\) captures the within-unit effect (similar to FE), and \\(\\beta_b\\) captures the between-unit effect.\nThe beauty of this decomposition is that it lets us keep the random effects framework while controlling for possible correlation between \\(x_{it}\\) and \\(\\alpha_i\\). The CRE model is estimated using RE methods but includes the person-level means \\(\\bar{x}_i\\) as additional regressors.\n\n\nWhen to Use CRE\nThe CRE model is useful when: - You suspect that \\(x_{it}\\) and \\(\\alpha_i\\) are correlated. - You want to retain time-invariant covariates. - You want to decompose effects into within- and between-entity components.\nIt blends the strengths of both FE and RE while relaxing the unrealistic assumptions of pure RE models."
  },
  {
    "objectID": "blog-unpublished/Q-correlated-random-effects.html#an-example",
    "href": "blog-unpublished/Q-correlated-random-effects.html#an-example",
    "title": "Understanding Correlated Random Effects Models",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs illustrate the differences among FE, RE, and CRE using R and a sample panel dataset.\n\nR\n\n\nlibrary(plm)\nlibrary(dplyr)\n\n# Simulate panel data\nset.seed(42)\nn &lt;- 100\nt &lt;- 5\ndata &lt;- data.frame(\n  id = rep(1:n, each = t),\n  time = rep(1:t, n)\n)\ndata &lt;- data %&gt;%\n  group_by(id) %&gt;%\n  mutate(\n    x = rnorm(n(), mean = id/10),\n    alpha = rnorm(1),\n    eps = rnorm(n(), sd = 1),\n    y = 1 + 0.5 * x + alpha + eps\n  )\n\npdata &lt;- pdata.frame(data, index = c(\"id\", \"time\"))\n\n# Fixed effects model\nfe_model &lt;- plm(y ~ x, data = pdata, model = \"within\")\n\n# Random effects model\nre_model &lt;- plm(y ~ x, data = pdata, model = \"random\")\n\n# Correlated Random Effects (Hybrid) model\npdata$mean_x &lt;- ave(pdata$x, pdata$id, FUN = mean)\ncre_model &lt;- plm(y ~ x + mean_x, data = pdata, model = \"random\")\n\nsummary(fe_model)\nsummary(re_model)\nsummary(cre_model)"
  },
  {
    "objectID": "blog-unpublished/Q-correlated-random-effects.html#bottom-line",
    "href": "blog-unpublished/Q-correlated-random-effects.html#bottom-line",
    "title": "Understanding Correlated Random Effects Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe CRE model allows for correlation between covariates and unobserved effects by decomposing variables into within- and between-unit components.\nIt maintains the flexibility of random effects models while addressing potential bias due to endogeneity.\nUnlike fixed effects, it allows for the estimation of time-invariant variables.\nIt is especially useful in policy evaluation and longitudinal data analysis when assumptions of standard RE models are not tenable."
  },
  {
    "objectID": "blog-unpublished/Q-correlated-random-effects.html#where-to-learn-more",
    "href": "blog-unpublished/Q-correlated-random-effects.html#where-to-learn-more",
    "title": "Understanding Correlated Random Effects Models",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nA great place to start is the original paper by Schunck (2013), which provides a very accessible overview and comparison of different panel data methods. Mundlak‚Äôs (1978) foundational work is also worth reading. If you‚Äôre using Stata, the xtreg command with cre options can replicate much of this functionality. In R, you can use plm and manipulate the covariates manually as shown in the example above."
  },
  {
    "objectID": "blog-unpublished/Q-correlated-random-effects.html#references",
    "href": "blog-unpublished/Q-correlated-random-effects.html#references",
    "title": "Understanding Correlated Random Effects Models",
    "section": "References",
    "text": "References\n\nSchunck, R. (2013). Within and between estimates in random-effects models: Advantages and drawbacks of correlated random effects and hybrid models. The Stata Journal, 13(1), 65-76.\nMundlak, Y. (1978). On the pooling of time series and cross section data. Econometrica, 46(1), 69‚Äì85."
  },
  {
    "objectID": "blog-unpublished/Q-unconditional-qreg.html",
    "href": "blog-unpublished/Q-unconditional-qreg.html",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "Quantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog-unpublished/Q-unconditional-qreg.html#background",
    "href": "blog-unpublished/Q-unconditional-qreg.html#background",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "Quantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog-unpublished/Q-unconditional-qreg.html#notation",
    "href": "blog-unpublished/Q-unconditional-qreg.html#notation",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Notation",
    "text": "Notation\nWe consider an outcome variable \\(Y\\) and a set of covariates \\(X\\). In traditional quantile regression, we estimate the conditional quantile function:\n\\[Q_\\tau(Y | X) = \\inf \\{ q : P(Y \\leq q | X) \\geq \\tau \\}.\\]\nThis tells us how the \\(\\tau\\)-th quantile of Y changes with \\(X\\). However, in many applications, we want to model the unconditional quantiles:\n\\[Q_\\tau(Y) = \\inf \\{ q : P(Y \\leq q) \\geq \\tau \\}.\\]\nUQR allows us to estimate how covariates influence these unconditional quantiles."
  },
  {
    "objectID": "blog-unpublished/Q-unconditional-qreg.html#a-closer-look",
    "href": "blog-unpublished/Q-unconditional-qreg.html#a-closer-look",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nBasics: Conditional vs.¬†Unconditional Quantiles\nThe fundamental distinction between conditional and unconditional quantile regressions is best understood through an example. Suppose we are studying wage distributions. A conditional quantile regression would estimate the effect of education on the wage quantile within a specific subgroup (e.g., workers with 5 years of experience). But what if we want to know the effect of education on the overall wage distribution? That‚Äôs where unconditional quantiles come in‚Äîthey capture the total effect of education, accounting for all pathways through which education might influence wages.\n\n\nUnconditional Quantile Regression\nFirpo et al.¬†introduced an elegant way to estimate UQR using influence functions. The influence function of a statistic measures how much that statistic changes when an observation is perturbed. The recentered influence function (RIF) for a quantile \\(Q_\\tau\\) is given by:\n\\[RIF(Y; Q_\\tau) = Q_\\tau + \\frac{\\tau - 1\\{Y \\leq Q_\\tau\\}}{f_Y(Q_\\tau)}.\\]\nHere, \\(f_Y(Q_\\tau)\\) is the density of \\(Y\\) at \\(Q_\\tau\\), which can be estimated nonparametrically. This nonparametric density estimation is often done via kernel density estimation but may be imprecise in the tails.\nFirpo et al.¬†showed that regressing \\(RIF(Y; Q_\\tau)\\) on covariates \\(X\\) via OLS provides a valid estimate of how \\(X\\) affects the \\(\\tau\\)-th quantile of \\(Y\\). This method is remarkably simple but powerful‚Äîit transforms a quantile regression problem into a standard linear regression problem.\n\n\nUnconditional Quantile Treatment Effects\nOne limitation of UQR as formulated by Firpo et al.¬†is that it assumes covariates are exogenous. But in many causal inference settings, treatment assignment is endogenous (e.g., workers self-select into training programs). Fr√∂lich and Melly (2013) extended the UQR framework to handle endogeneity using instrumental variables (IV).\nThey showed that under standard IV assumptions‚Äîrelevance and exclusion‚Äîthe unconditional quantile treatment effect (UQTE) can be estimated using a two-step approach:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nEstimate a propensity score model (or an instrumented version of \\(D\\)) to account for selection bias.\nApply RIF regression to estimate the effect of the treatment on unconditional quantiles.\n\n\n\nThis approach provides a way to estimate distributional treatment effects while addressing selection bias‚Äîa crucial tool in policy evaluation and applied econometrics.\n\n\nRank Invariance in QTEs\nA crucial assumption often invoked in the estimation of quantile treatment effects (QTEs) is rank invariance. This assumption states that units maintain their rank in the outcome distribution after receiving the treatment. In other words, if a treated unit was at the 30th percentile of the untreated outcome distribution, it would remain at the 30th percentile of the treated distribution.\nWhile this assumption simplifies identification and interpretation of QTEs, it can be highly restrictive. It rules out the possibility that treatment reshuffles individuals across the distribution‚Äîa scenario that might be not only plausible but central in many applications.\nConsider a school voucher program that offers private school access to low-income students. The effect of such a program may be heterogeneous: for high-performing students, access might enhance performance due to better environments. But for low-performing students, the same access could lead to worse outcomes due to higher academic pressure or poor fit. As a result, the program could re-rank students in the outcome distribution, violating rank invariance.\nIn such settings, assuming rank invariance could lead to misleading conclusions about who benefits and who loses from treatment. Alternative approaches, like those based on quantile treatment effect bounds (e.g., Melly, 2005; Chernozhukov & Hansen, 2005), are more robust to such violations."
  },
  {
    "objectID": "blog-unpublished/Q-unconditional-qreg.html#examples",
    "href": "blog-unpublished/Q-unconditional-qreg.html#examples",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Examples",
    "text": "Examples\n\nBitler et al.¬†(2006)\nWhen evaluating the effects of welfare reform, traditional analyses often focus on mean impacts, which can obscure critical insights into the distributional effects of policy changes. ‚Äã Quantile Treatment Effects (QTE) provide a powerful tool for understanding how reforms impact different segments of the population, revealing heterogeneity that mean impacts fail to capture. ‚Äã For example, the study ‚ÄúWhat Mean Impacts Miss: Distributional Effects of Welfare Reform Experiments‚Äù by Bitler, Gelbach, and Hoynes uses QTE to analyze Connecticut‚Äôs Jobs First program, a welfare reform initiative. The authors find that while mean impacts suggest modest income gains, QTE reveal substantial variation: earnings effects are zero at the bottom, positive in the middle, and negative at the top of the distribution before time limits take effect. ‚Äã After time limits, income effects are mixed, with gains concentrated in higher quantiles and losses at the lower end. ‚Äã This nuanced approach highlights the importance of QTE in uncovering the true breadth of policy impacts, enabling data scientists to better inform decision-making and address equity concerns in policy design.\n\n\n\nLet‚Äôs illustrate these ideas with an example in R and python. We‚Äôll use the iris dataset to estimate the effect of Sepal.Length on different quantiles of Petal.Length using UQR.\n\nRPython\n\n\nrm(list=ls())\nlibrary(quantreg)\n\n# Load dataset\ndata(iris)\n\n# Estimate unconditional quantiles\ntaus &lt;- c(0.25, 0.50, 0.75)\nq_vals &lt;- quantile(iris$Petal.Length, probs = taus)  # Estimate quantiles\nf_hat &lt;- density(iris$Petal.Length)\n\n# Compute RIF values\nrif_values &lt;- lapply(1:3, function(i) {\n  q &lt;- q_vals[i]\n  f &lt;- f_hat$y[which.min(abs(f_hat$x - q))]\n  q + ((taus[i] - (iris$Petal.Length &lt;= q)) / f)\n})\n\n# Run RIF regression\nmodels &lt;- lapply(rif_values, function(rif) lm(rif ~ Sepal.Length, data = iris))\n\n# Print results\nlapply(models, summary)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Estimate unconditional quantiles\ntaus = [0.25, 0.50, 0.75]\nq_vals = np.quantile(iris['Petal.Length'], taus)  # Estimate quantiles\nf_hat = gaussian_kde(iris['Petal.Length'])\n\n# Compute RIF values\nrif_values = []\nfor i, tau in enumerate(taus):\n    q = q_vals[i]\n    f = f_hat(q)  # Density at the quantile\n    rif = q + ((tau - (iris['Petal.Length'] &lt;= q).astype(int)) / f)\n    rif_values.append(rif)\n\n# Run RIF regression\nmodels = []\nfor rif in rif_values:\n    model = LinearRegression(fit_intercept=True)\n    model.fit(iris[['Sepal.Length']], rif)\n    models.append(model)\n\n# Print results\nfor i, model in enumerate(models):\n    print(f\"Model {i + 1}:\")\n    print(f\"Coefficient for Sepal.Length: {model.coef_[0]}\")\n    print(f\"Intercept: {model.intercept_}\")\n\n\n\nThis simple example demonstrates how to estimate the effect of a covariate on unconditional quantiles using the RIF regression approach."
  },
  {
    "objectID": "blog-unpublished/Q-unconditional-qreg.html#bottom-line",
    "href": "blog-unpublished/Q-unconditional-qreg.html#bottom-line",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nUQR allows us to estimate the effect of covariates on unconditional quantiles, capturing total effects.\nThe RIF regression method transforms a quantile regression problem into a simple linear regression.\nFr√∂lich and Melly (2013) extend UQR to address endogeneity using instrumental variables.\nThese tools are invaluable for policy evaluation and causal inference."
  },
  {
    "objectID": "blog-unpublished/Q-unconditional-qreg.html#where-to-learn-more",
    "href": "blog-unpublished/Q-unconditional-qreg.html#where-to-learn-more",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive into these methods, the foundational paper by Firpo, Fortin, and Lemieux (2009) provides a detailed introduction to UQR, while Fr√∂lich and Melly (2013) extend the framework to address endogeneity concerns. For a broader perspective on quantile regression, Koenker‚Äôs book Quantile Regression (2005) is a must-read."
  },
  {
    "objectID": "blog-unpublished/Q-unconditional-qreg.html#references",
    "href": "blog-unpublished/Q-unconditional-qreg.html#references",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "References",
    "text": "References\nAlejo, J., Favata, F., Montes-Rojas, G., & Trombetta, M. (2021). Conditional vs unconditional quantile regression models: A guide to practitioners. Econom√≠a, 44(88), 76-93.\nBitler, M. P., Gelbach, J. B., & Hoynes, H. W. (2006). What mean impacts miss: Distributional effects of welfare reform experiments. American Economic Review, 96(4), 988-1012.\nBorah, B. J., & Basu, A. (2013). Highlighting differences between conditional and unconditional quantile regression approaches through an application to assess medication adherence. Health economics, 22(9), 1052-1070.\nBorgen, N. T. (2016). Fixed effects in unconditional quantile regression. The Stata Journal, 16(2), 403-415.\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional quantile regressions. Econometrica, 77(3), 953-973.\nFr√∂lich, M., & Melly, B. (2013). Unconditional quantile treatment effects under endogeneity. Journal of Business & Economic Statistics, 31(3), 346-357.\nSasaki, Y., Ura, T., & Zhang, Y. (2022). Unconditional quantile regression with high‚Äêdimensional data. Quantitative Economics, 13(3), 955-978."
  },
  {
    "objectID": "blog-unpublished/correlated_random_effects.html",
    "href": "blog-unpublished/correlated_random_effects.html",
    "title": "Understanding Correlated Random Effects Models",
    "section": "",
    "text": "In the world of panel data analysis, we often find ourselves choosing between fixed effects (FE) and random effects (RE) models. Each has its strengths and limitations. The FE model controls for all time-invariant unobserved heterogeneity but does not allow us to estimate the effects of those time-invariant covariates. On the other hand, the RE model allows for the inclusion of time-invariant variables but makes a strong assumption: that the unobserved individual-specific effects are uncorrelated with the regressors. What happens when that assumption doesn‚Äôt hold? That‚Äôs where the correlated random effects (CRE) model‚Äîor the hybrid model‚Äîsteps in.\nThis article explores the motivation behind the CRE model, its mechanics, and its advantages over traditional FE and RE models. We also provide a hands-on example using R and python to illustrate how it works in practice."
  },
  {
    "objectID": "blog-unpublished/correlated_random_effects.html#background",
    "href": "blog-unpublished/correlated_random_effects.html#background",
    "title": "Understanding Correlated Random Effects Models",
    "section": "",
    "text": "In the world of panel data analysis, we often find ourselves choosing between fixed effects (FE) and random effects (RE) models. Each has its strengths and limitations. The FE model controls for all time-invariant unobserved heterogeneity but does not allow us to estimate the effects of those time-invariant covariates. On the other hand, the RE model allows for the inclusion of time-invariant variables but makes a strong assumption: that the unobserved individual-specific effects are uncorrelated with the regressors. What happens when that assumption doesn‚Äôt hold? That‚Äôs where the correlated random effects (CRE) model‚Äîor the hybrid model‚Äîsteps in.\nThis article explores the motivation behind the CRE model, its mechanics, and its advantages over traditional FE and RE models. We also provide a hands-on example using R and python to illustrate how it works in practice."
  },
  {
    "objectID": "blog-unpublished/correlated_random_effects.html#notation",
    "href": "blog-unpublished/correlated_random_effects.html#notation",
    "title": "Understanding Correlated Random Effects Models",
    "section": "Notation",
    "text": "Notation\nLet us consider a standard panel data setup where we observe units \\(i=1,\\dots,N\\) over time periods \\(t = 1, \\dots, T\\). The outcome is \\(y_{it}\\), and \\(x_{it}\\) is a vector of time-varying covariates. The generic random effects model is:\n\\[\ny_{it} = x_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\n\\]\nwhere \\(\\alpha_i\\) is the individual-specific effect and \\(\\varepsilon_{it}\\) is the idiosyncratic error term. The crucial RE assumption is:\n\\[\n\\text{Cov}(x_{it}, \\alpha_i) = 0\n\\]\nThe CRE model relaxes this assumption by explicitly modeling the correlation between \\(x_{it}\\) and \\(\\alpha_i\\)."
  },
  {
    "objectID": "blog-unpublished/correlated_random_effects.html#a-closer-look",
    "href": "blog-unpublished/correlated_random_effects.html#a-closer-look",
    "title": "Understanding Correlated Random Effects Models",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhy Use Correlated Random Effects?\nThe correlated random effects (CRE) model offers a middle ground between FE and RE approaches. Traditional RE models assume that unobserved heterogeneity is uncorrelated with covariates. FE models remove all unit-level heterogeneity but cannot estimate time-invariant covariates. CRE models address these limitations by including group means of time-varying covariates, decomposing variation into within and between components.\n\n\nModel Estimation and Inference\nIn the linear case:\n\\[\ny_{it} = \\beta_0 + \\beta_1 x_{it} + \\gamma \\bar{x}_i + u_i + \\varepsilon_{it},\n\\]\nwhere \\(\\bar{x}_i\\) is the individual mean of \\(x_{it}\\).\nEstimation uses RE methods but includes \\(\\bar{x}_i\\) to account for potential correlation with \\(u_i\\). This enables hypothesis testing comparing within and between effects.\n\n\nAdvantages\n\nEstimation of time-invariant variables.\nDecomposition of effects into within and between components.\nImproved efficiency under relaxed assumptions.\nDiagnostic insight into the plausibility of RE assumptions.\n\n\n\nChallenges and Issues in Practice\n\nThe random intercept assumption remains.\nNo remedy for level-2 confounding.\nCare needed with interaction terms.\nPotential bias with a small number of clusters.\n\n\n\nWhere the CRE Model Shines\nCRE models are ideal for repeated measures data where: - Both time-varying and time-invariant predictors matter. - There‚Äôs potential endogeneity between covariates and individual effects.\nApplications include policy evaluation, health outcomes research, and educational studies."
  },
  {
    "objectID": "blog-unpublished/correlated_random_effects.html#an-example",
    "href": "blog-unpublished/correlated_random_effects.html#an-example",
    "title": "Understanding Correlated Random Effects Models",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nlibrary(plm)\nlibrary(dplyr)\n\nset.seed(42)\nn &lt;- 100\nt &lt;- 5\ndata &lt;- data.frame(\n  id = rep(1:n, each = t),\n  time = rep(1:t, n)\n)\ndata &lt;- data %&gt;%\n  group_by(id) %&gt;%\n  mutate(\n    x = rnorm(n(), mean = id/10),\n    alpha = rnorm(1),\n    eps = rnorm(n(), sd = 1),\n    y = 1 + 0.5 * x + alpha + eps\n  )\n\npdata &lt;- pdata.frame(data, index = c(\"id\", \"time\"))\n\nfe_model &lt;- plm(y ~ x, data = pdata, model = \"within\")\nre_model &lt;- plm(y ~ x, data = pdata, model = \"random\")\npdata$mean_x &lt;- ave(pdata$x, pdata$id, FUN = mean)\ncre_model &lt;- plm(y ~ x + mean_x, data = pdata, model = \"random\")\n\nsummary(fe_model)\nsummary(re_model)\nsummary(cre_model)\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(42)\nn, t = 100, 5\ndf = pd.DataFrame({\n    'id': np.repeat(np.arange(1, n+1), t),\n    'time': np.tile(np.arange(1, t+1), n)\n})\ndf['x'] = df['id'] / 10 + np.random.randn(n*t)\ndf['alpha'] = np.repeat(np.random.randn(n), t)\ndf['eps'] = np.random.randn(n*t)\ndf['y'] = 1 + 0.5 * df['x'] + df['alpha'] + df['eps']\ndf['mean_x'] = df.groupby('id')['x'].transform('mean')\n\n# Short model (RE approximation)\nX_short = sm.add_constant(df[['x']])\nmodel_short = sm.OLS(df['y'], X_short).fit()\n\n# CRE model\nX_cre = sm.add_constant(df[['x', 'mean_x']])\nmodel_cre = sm.OLS(df['y'], X_cre).fit()\n\nprint(model_short.summary())\nprint(model_cre.summary())"
  },
  {
    "objectID": "blog-unpublished/correlated_random_effects.html#bottom-line",
    "href": "blog-unpublished/correlated_random_effects.html#bottom-line",
    "title": "Understanding Correlated Random Effects Models",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nCRE models relax the strict RE assumptions by modeling the correlation between unit effects and covariates.\nThey provide within and between estimates while allowing time-invariant variables.\nAppropriate for longitudinal, multilevel, and policy evaluation studies."
  },
  {
    "objectID": "blog-unpublished/correlated_random_effects.html#where-to-learn-more",
    "href": "blog-unpublished/correlated_random_effects.html#where-to-learn-more",
    "title": "Understanding Correlated Random Effects Models",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nSchunck (2013) provides a comprehensive overview of CRE models. Mundlak‚Äôs foundational work is essential for understanding the theoretical basis. Tools like R‚Äôs plm and Python‚Äôs statsmodels can implement these models with the correct transformations."
  },
  {
    "objectID": "blog-unpublished/correlated_random_effects.html#references",
    "href": "blog-unpublished/correlated_random_effects.html#references",
    "title": "Understanding Correlated Random Effects Models",
    "section": "References",
    "text": "References\n\nSchunck, R. (2013). Within and between estimates in random-effects models: Advantages and drawbacks of correlated random effects and hybrid models. The Stata Journal, 13(1), 65-76.\nMundlak, Y. (1978). On the pooling of time series and cross section data. Econometrica, 46(1), 69‚Äì85."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg-old.html",
    "href": "blog-unpublished/unconditional-qreg-old.html",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "Quantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg-old.html#background",
    "href": "blog-unpublished/unconditional-qreg-old.html#background",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "",
    "text": "Quantile regression has become a widely used tool in econometrics and statistics, thanks to its ability to model the entire distribution of an outcome variable rather than just its mean. Traditional quantile regression, however, is conditional‚Äîit models quantiles of the outcome given a set of covariates. But in many policy and causal inference applications, we are interested in changes to the unconditional distribution of the outcome variable.\nFor example, suppose we want to understand the effect of a job training program on wage inequality. A standard quantile regression would tell us how the program shifts quantiles given certain characteristics like education or experience. But we might instead want to estimate how the program shifts quantiles in the population as a whole‚Äîthis is where Unconditional Quantile Regression (UQR) comes in.\nThe key breakthrough in this space was provided by Firpo, Fortin, and Lemieux (2009), who introduced a method based on the Recentered Influence Function (RIF). This allows us to estimate the effect of covariates on unconditional quantiles using simple linear regressions. Later, Fr√∂lich and Melly (2013) extended this framework to account for endogeneity, providing a way to estimate Unconditional Quantile Treatment Effects (UQTEs) in settings where treatment is not randomly assigned.\nIn this article, we‚Äôll unpack the key ideas behind UQR, discuss how to estimate unconditional quantile treatment effects, and illustrate these concepts with an example in R and python."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg-old.html#notation",
    "href": "blog-unpublished/unconditional-qreg-old.html#notation",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Notation",
    "text": "Notation\nWe consider an outcome variable \\(Y\\) and a set of covariates \\(X\\). In traditional quantile regression, we estimate the conditional quantile function:\n\\[Q_\\tau(Y | X) = \\inf \\{ q : P(Y \\leq q | X) \\geq \\tau \\}.\\]\nThis tells us how the \\(\\tau\\)-th quantile of Y changes with \\(X\\). However, in many applications, we want to model the unconditional quantiles:\n\\[Q_\\tau(Y) = \\inf \\{ q : P(Y \\leq q) \\geq \\tau \\}.\\]\nUQR allows us to estimate how covariates influence these unconditional quantiles."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg-old.html#a-closer-look",
    "href": "blog-unpublished/unconditional-qreg-old.html#a-closer-look",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nBasics: Conditional vs.¬†Unconditional Quantiles\nThe fundamental distinction between conditional and unconditional quantile regressions is best understood through an example. Suppose we are studying wage distributions. A conditional quantile regression would estimate the effect of education on the wage quantile within a specific subgroup (e.g., workers with 5 years of experience). But what if we want to know the effect of education on the overall wage distribution? That‚Äôs where unconditional quantiles come in‚Äîthey capture the total effect of education, accounting for all pathways through which education might influence wages.\n\n\nUnconditional Quantile Regression\nFirpo et al.¬†introduced an elegant way to estimate UQR using influence functions. The influence function of a statistic measures how much that statistic changes when an observation is perturbed. The recentered influence function (RIF) for a quantile \\(Q_\\tau\\) is given by:\n\\[RIF(Y; Q_\\tau) = Q_\\tau + \\frac{\\tau - 1\\{Y \\leq Q_\\tau\\}}{f_Y(Q_\\tau)}.\\]\nHere, \\(f_Y(Q_\\tau)\\) is the density of \\(Y\\) at \\(Q_\\tau\\), which can be estimated nonparametrically. This nonparametric density estimation is often done via kernel density estimation but may be imprecise in the tails.\nFirpo et al.¬†showed that regressing \\(RIF(Y; Q_\\tau)\\) on covariates \\(X\\) via OLS provides a valid estimate of how \\(X\\) affects the \\(\\tau\\)-th quantile of \\(Y\\). This method is remarkably simple but powerful‚Äîit transforms a quantile regression problem into a standard linear regression problem.\n\n\nEstimation\nThe estimation proceeds in three steps:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nEstimate the sample quantile \\(q_{\\tau}\\).\nEstimate the density \\(f_Y(q_{\\tau})\\), typically via kernel density estimation.\nConstruct the RIF for each observation and regress it on the covariates.\n\n\n\nThe basic regression is: \\[\nRIF(Y; q_{\\tau}) = X' \\beta + \\varepsilon,\n\\]\nwhere \\(\\beta\\) now captures the effect of \\(X\\) on the \\(\\tau\\)-th unconditional quantile.\nThe most common implementation is RIF-OLS, though alternatives include RIF-Logit and nonparametric first stages (RIF-NP).\n\n\nInference and Challenges\n\nDensity Estimation: A critical step that affects the quality of inference. Poor density estimation at the quantile point can lead to noisy estimates.\nNonlinearity and Model Misspecification: RIF-OLS assumes a linear relationship between the RIF and covariates. If the true relationship is nonlinear, flexible methods (logit, nonparametric) are preferred.\nStandard Errors: Because of the multi-step estimation (quantile, density, RIF), standard error computation is more complex. Bootstrapping is commonly used.\nTreatment Effects: UQR is especially appealing for estimating treatment effects on the distribution of outcomes. When treatment is exogenous, including treatment indicators in the RIF regression yields estimates of the treatment effect at various unconditional quantiles.\n\n\n\nStrengths and Applications\nUQR shines in settings where the policy question concerns distributional effects, such as: - Wage inequality and labor economics. - Health outcomes across the full distribution. - Policy evaluation where shifting the covariate distribution is plausible.\nIt also generalizes to other distributional statistics (Gini, variance) by using the corresponding influence functions.\n\n\nUnconditional Quantile Treatment Effects\nOne limitation of UQR as formulated by Firpo et al.¬†is that it assumes covariates are exogenous. But in many causal inference settings, treatment assignment is endogenous (e.g., workers self-select into training programs). Fr√∂lich and Melly (2013) extended the UQR framework to handle endogeneity using instrumental variables (IV).\nThey showed that under standard IV assumptions‚Äîrelevance and exclusion‚Äîthe unconditional quantile treatment effect (UQTE) can be estimated using a two-step approach:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nEstimate a propensity score model (or an instrumented version of \\(D\\)) to account for selection bias.\nApply RIF regression to estimate the effect of the treatment on unconditional quantiles.\n\n\n\nThis approach provides a way to estimate distributional treatment effects while addressing selection bias‚Äîa crucial tool in policy evaluation and applied econometrics.\n\n\nRank Invariance in QTEs\nA crucial assumption often invoked in the estimation of quantile treatment effects (QTEs) is rank invariance. This assumption states that units maintain their rank in the outcome distribution after receiving the treatment. In other words, if a treated unit was at the 30th percentile of the untreated outcome distribution, it would remain at the 30th percentile of the treated distribution.\nWhile this assumption simplifies identification and interpretation of QTEs, it can be highly restrictive. It rules out the possibility that treatment reshuffles individuals across the distribution‚Äîa scenario that might be not only plausible but central in many applications.\nConsider a school voucher program that offers private school access to low-income students. The effect of such a program may be heterogeneous: for high-performing students, access might enhance performance due to better environments. But for low-performing students, the same access could lead to worse outcomes due to higher academic pressure or poor fit. As a result, the program could re-rank students in the outcome distribution, violating rank invariance.\nIn such settings, assuming rank invariance could lead to misleading conclusions about who benefits and who loses from treatment. Alternative approaches, like those based on quantile treatment effect bounds (e.g., Melly, 2005; Chernozhukov & Hansen, 2005), are more robust to such violations."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg-old.html#examples",
    "href": "blog-unpublished/unconditional-qreg-old.html#examples",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Examples",
    "text": "Examples\n\nBitler et al.¬†(2006)\nWhen evaluating the effects of welfare reform, traditional analyses often focus on mean impacts, which can obscure critical insights into the distributional effects of policy changes. ‚Äã Quantile Treatment Effects (QTE) provide a powerful tool for understanding how reforms impact different segments of the population, revealing heterogeneity that mean impacts fail to capture. ‚Äã For example, the study ‚ÄúWhat Mean Impacts Miss: Distributional Effects of Welfare Reform Experiments‚Äù by Bitler, Gelbach, and Hoynes uses QTE to analyze Connecticut‚Äôs Jobs First program, a welfare reform initiative. The authors find that while mean impacts suggest modest income gains, QTE reveal substantial variation: earnings effects are zero at the bottom, positive in the middle, and negative at the top of the distribution before time limits take effect. ‚Äã After time limits, income effects are mixed, with gains concentrated in higher quantiles and losses at the lower end. ‚Äã This nuanced approach highlights the importance of QTE in uncovering the true breadth of policy impacts, enabling data scientists to better inform decision-making and address equity concerns in policy design.\n\n\n\nLet‚Äôs illustrate these ideas with an example in R and python. We‚Äôll use the iris dataset to estimate the effect of Sepal.Length on different quantiles of Petal.Length using UQR.\n\nRPython\n\n\nrm(list=ls())\nlibrary(quantreg)\n\n# Load dataset\ndata(iris)\n\n# Estimate unconditional quantiles\ntaus &lt;- c(0.25, 0.50, 0.75)\nq_vals &lt;- quantile(iris$Petal.Length, probs = taus)  # Estimate quantiles\nf_hat &lt;- density(iris$Petal.Length)\n\n# Compute RIF values\nrif_values &lt;- lapply(1:3, function(i) {\n  q &lt;- q_vals[i]\n  f &lt;- f_hat$y[which.min(abs(f_hat$x - q))]\n  q + ((taus[i] - (iris$Petal.Length &lt;= q)) / f)\n})\n\n# Run RIF regression\nmodels &lt;- lapply(rif_values, function(rif) lm(rif ~ Sepal.Length, data = iris))\n\n# Print results\nlapply(models, summary)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression\n\n# Load dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris(as_frame=True)\niris = iris_data['data']\niris.columns = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n\n# Estimate unconditional quantiles\ntaus = [0.25, 0.50, 0.75]\nq_vals = np.quantile(iris['Petal.Length'], taus)  # Estimate quantiles\nf_hat = gaussian_kde(iris['Petal.Length'])\n\n# Compute RIF values\nrif_values = []\nfor i, tau in enumerate(taus):\n    q = q_vals[i]\n    f = f_hat(q)  # Density at the quantile\n    rif = q + ((tau - (iris['Petal.Length'] &lt;= q).astype(int)) / f)\n    rif_values.append(rif)\n\n# Run RIF regression\nmodels = []\nfor rif in rif_values:\n    model = LinearRegression(fit_intercept=True)\n    model.fit(iris[['Sepal.Length']], rif)\n    models.append(model)\n\n# Print results\nfor i, model in enumerate(models):\n    print(f\"Model {i + 1}:\")\n    print(f\"Coefficient for Sepal.Length: {model.coef_[0]}\")\n    print(f\"Intercept: {model.intercept_}\")\n\n\n\nThis simple example demonstrates how to estimate the effect of a covariate on unconditional quantiles using the RIF regression approach."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg-old.html#bottom-line",
    "href": "blog-unpublished/unconditional-qreg-old.html#bottom-line",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nUQR allows us to estimate the effect of covariates on unconditional quantiles, capturing total effects.\nThe RIF regression method transforms a quantile regression problem into a simple linear regression.\nFr√∂lich and Melly (2013) extend UQR to address endogeneity using instrumental variables.\nThese tools are invaluable for policy evaluation and causal inference."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg-old.html#where-to-learn-more",
    "href": "blog-unpublished/unconditional-qreg-old.html#where-to-learn-more",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive into these methods, the foundational paper by Firpo, Fortin, and Lemieux (2009) provides a detailed introduction to UQR, while Fr√∂lich and Melly (2013) extend the framework to address endogeneity concerns. For a broader perspective on quantile regression, Koenker‚Äôs book Quantile Regression (2005) is a must-read."
  },
  {
    "objectID": "blog-unpublished/unconditional-qreg-old.html#references",
    "href": "blog-unpublished/unconditional-qreg-old.html#references",
    "title": "Unconditional Quantile Regression and Treatment Effects",
    "section": "References",
    "text": "References\nAlejo, J., Favata, F., Montes-Rojas, G., & Trombetta, M. (2021). Conditional vs unconditional quantile regression models: A guide to practitioners. Econom√≠a, 44(88), 76-93.\nBitler, M. P., Gelbach, J. B., & Hoynes, H. W. (2006). What mean impacts miss: Distributional effects of welfare reform experiments. American Economic Review, 96(4), 988-1012.\nBorah, B. J., & Basu, A. (2013). Highlighting differences between conditional and unconditional quantile regression approaches through an application to assess medication adherence. Health economics, 22(9), 1052-1070.\nBorgen, N. T. (2016). Fixed effects in unconditional quantile regression. The Stata Journal, 16(2), 403-415.\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional quantile regressions. Econometrica, 77(3), 953-973.\nFr√∂lich, M., & Melly, B. (2013). Unconditional quantile treatment effects under endogeneity. Journal of Business & Economic Statistics, 31(3), 346-357.\nSasaki, Y., Ura, T., & Zhang, Y. (2022). Unconditional quantile regression with high‚Äêdimensional data. Quantitative Economics, 13(3), 955-978."
  },
  {
    "objectID": "blog-unpublished/unconditional_quantile_regression.html",
    "href": "blog-unpublished/unconditional_quantile_regression.html",
    "title": "Unconditional Quantile Regression and Treatment Effects: Intuition and Practice",
    "section": "",
    "text": "Quantile regression is a widely used tool to explore how covariates influence the distribution of an outcome variable. However, standard quantile regression focuses on conditional quantiles ‚Äî that is, the effect of predictors on the quantiles of the outcome given the covariates. But what if your scientific question concerns unconditional (marginal) quantiles ‚Äî the quantiles of the overall outcome distribution?\nThis is where Unconditional Quantile Regression (UQR), developed by Firpo, Fortin, and Lemieux (2009), enters the scene. Instead of modeling the conditional quantile function directly, UQR uses a clever trick: it regresses the recentered influence function (RIF) of the outcome on the covariates. This approach allows us to estimate the impact of covariates on unconditional quantiles, providing insight into how changes in the distribution of predictors shift the entire distribution of the outcome.\nThis article explains the intuition, core mechanics, estimation strategies, and practical issues of UQR, focusing especially on applications in treatment effects estimation."
  },
  {
    "objectID": "blog-unpublished/unconditional_quantile_regression.html#background",
    "href": "blog-unpublished/unconditional_quantile_regression.html#background",
    "title": "Unconditional Quantile Regression and Treatment Effects: Intuition and Practice",
    "section": "",
    "text": "Quantile regression is a widely used tool to explore how covariates influence the distribution of an outcome variable. However, standard quantile regression focuses on conditional quantiles ‚Äî that is, the effect of predictors on the quantiles of the outcome given the covariates. But what if your scientific question concerns unconditional (marginal) quantiles ‚Äî the quantiles of the overall outcome distribution?\nThis is where Unconditional Quantile Regression (UQR), developed by Firpo, Fortin, and Lemieux (2009), enters the scene. Instead of modeling the conditional quantile function directly, UQR uses a clever trick: it regresses the recentered influence function (RIF) of the outcome on the covariates. This approach allows us to estimate the impact of covariates on unconditional quantiles, providing insight into how changes in the distribution of predictors shift the entire distribution of the outcome.\nThis article explains the intuition, core mechanics, estimation strategies, and practical issues of UQR, focusing especially on applications in treatment effects estimation."
  },
  {
    "objectID": "blog-unpublished/unconditional_quantile_regression.html#notation",
    "href": "blog-unpublished/unconditional_quantile_regression.html#notation",
    "title": "Unconditional Quantile Regression and Treatment Effects: Intuition and Practice",
    "section": "Notation",
    "text": "Notation\nConsider an outcome variable ( Y ) and covariates ( X ). We are interested in the ( )-th quantile ( q_{} ) of the unconditional distribution of ( Y ).\nThe influence function for the ( )-th quantile is:\n[ IF(Y; q_{}) = , ]\nwhere ( f_Y(q_{}) ) is the density of ( Y ) at the quantile point.\nThe recentered influence function (RIF) is defined as:\n[ RIF(Y; q_{}) = q_{} + IF(Y; q_{}). ]"
  },
  {
    "objectID": "blog-unpublished/unconditional_quantile_regression.html#a-closer-look",
    "href": "blog-unpublished/unconditional_quantile_regression.html#a-closer-look",
    "title": "Unconditional Quantile Regression and Treatment Effects: Intuition and Practice",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhy Use Unconditional Quantile Regression?\nUnlike conditional quantile regression, UQR tells us how shifts in the distribution of predictors affect the overall distribution of outcomes ‚Äî not just within fixed levels of covariates. This matters in policy evaluation where interventions might change the covariate distribution itself (e.g., unionization rates, education levels).\nThe intuition is simple but powerful: by regressing ( RIF(Y; q_{}) ) on covariates ( X ), we obtain estimates of Unconditional Quantile Partial Effects (UQPEs) ‚Äî the marginal effect of shifting ( X ) on the unconditional quantiles of ( Y ).\n\n\nEstimation\nThe estimation proceeds in three steps: 1. Estimate the sample quantile ( q_{} ). 2. Estimate the density ( f_Y(q_{}) ), typically via kernel density estimation. 3. Construct the RIF for each observation and regress it on the covariates.\nThe basic regression is:\n[ RIF(Y; q_{}) = X‚Äô + , ]\nwhere ( ) now captures the effect of ( X ) on the ( )-th unconditional quantile.\nThe most common implementation is RIF-OLS, though alternatives include RIF-Logit and nonparametric first stages (RIF-NP).\n\n\nInference and Challenges\n\nDensity Estimation: A critical step that affects the quality of inference. Poor density estimation at the quantile point can lead to noisy estimates.\nNonlinearity and Model Misspecification: RIF-OLS assumes a linear relationship between the RIF and covariates. If the true relationship is nonlinear, flexible methods (logit, nonparametric) are preferred.\nStandard Errors: Because of the multi-step estimation (quantile, density, RIF), standard error computation is more complex. Bootstrapping is commonly used.\nTreatment Effects: UQR is especially appealing for estimating treatment effects on the distribution of outcomes. When treatment is exogenous, including treatment indicators in the RIF regression yields estimates of the treatment effect at various unconditional quantiles.\n\n\n\nStrengths and Applications\nUQR shines in settings where the policy question concerns distributional effects, such as: - Wage inequality and labor economics. - Health outcomes across the full distribution. - Policy evaluation where shifting the covariate distribution is plausible.\nIt also generalizes to other distributional statistics (Gini, variance) by using the corresponding influence functions."
  },
  {
    "objectID": "blog-unpublished/unconditional_quantile_regression.html#an-example",
    "href": "blog-unpublished/unconditional_quantile_regression.html#an-example",
    "title": "Unconditional Quantile Regression and Treatment Effects: Intuition and Practice",
    "section": "An Example",
    "text": "An Example\n::::\n\nR\nlibrary(dplyr)\nlibrary(quantreg)\nlibrary(KernSmooth)\n\nset.seed(123)\nn &lt;- 1000\nx &lt;- rnorm(n)\ny &lt;- 2 + 0.5 * x + rnorm(n)\nq_tau &lt;- quantile(y, 0.5)\nf_y &lt;- bkde(y)$y[which.min(abs(bkde(y)$x - q_tau))]\nrif &lt;- q_tau + (0.5 - as.numeric(y &lt;= q_tau)) / f_y\nmodel &lt;- lm(rif ~ x)\nsummary(model)\n\n\nPython\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy.stats import gaussian_kde\n\nnp.random.seed(123)\nn = 1000\nx = np.random.randn(n)\ny = 2 + 0.5 * x + np.random.randn(n)\nq_tau = np.quantile(y, 0.5)\nkde = gaussian_kde(y)\nf_y = kde.evaluate(q_tau)[0]\nrif = q_tau + (0.5 - (y &lt;= q_tau).astype(int)) / f_y\nX = sm.add_constant(x)\nmodel = sm.OLS(rif, X).fit()\nprint(model.summary())\n::::"
  },
  {
    "objectID": "blog-unpublished/unconditional_quantile_regression.html#bottom-line",
    "href": "blog-unpublished/unconditional_quantile_regression.html#bottom-line",
    "title": "Unconditional Quantile Regression and Treatment Effects: Intuition and Practice",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nUQR estimates the effects of predictors on unconditional quantiles ‚Äî a complement to conditional quantile regression.\nIt uses the recentered influence function to connect regression modeling with distributional statistics.\nCareful attention to density estimation and inference is needed.\nEspecially useful for policy analysis and distributional treatment effect estimation."
  },
  {
    "objectID": "blog-unpublished/unconditional_quantile_regression.html#where-to-learn-more",
    "href": "blog-unpublished/unconditional_quantile_regression.html#where-to-learn-more",
    "title": "Unconditional Quantile Regression and Treatment Effects: Intuition and Practice",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThe foundational paper by Firpo, Fortin, and Lemieux (2009) is the key resource for understanding UQR. For more practical applications, look into recent literature on RIF regressions in labor economics, health economics, and policy analysis."
  },
  {
    "objectID": "blog-unpublished/unconditional_quantile_regression.html#references",
    "href": "blog-unpublished/unconditional_quantile_regression.html#references",
    "title": "Unconditional Quantile Regression and Treatment Effects: Intuition and Practice",
    "section": "References",
    "text": "References\n\nFirpo, S., Fortin, N. M., & Lemieux, T. (2009). Unconditional Quantile Regressions. Econometrica, 77(3), 953‚Äì973."
  },
  {
    "objectID": "blog-unpublished/randomization-inference.html",
    "href": "blog-unpublished/randomization-inference.html",
    "title": "Randomization Inference: Exact Testing Without Parametric Assumptions",
    "section": "",
    "text": "Randomization inference offers a refreshing alternative to traditional parametric inference, providing exact control over Type I error rates without relying on large-sample approximations or strict distributional assumptions. Born out of Fisher‚Äôs famous tea-tasting experiment, the approach leverages the symmetry and structure induced by randomization itself to test hypotheses.\nThis blog post unpacks the theory and intuition behind randomization inference, drawing on the excellent review by Ritzwoller, Romano, and Shaikh (2025). We‚Äôll cover the key ideas, notation, and algorithms involved, and also touch on modern applications like two-sample tests, regression, and conformal inference. Throughout, we‚Äôll emphasize the practical considerations ‚Äî when it works, why it works, and where caution is needed."
  },
  {
    "objectID": "blog-unpublished/randomization-inference.html#background",
    "href": "blog-unpublished/randomization-inference.html#background",
    "title": "Randomization Inference: Exact Testing Without Parametric Assumptions",
    "section": "",
    "text": "Randomization inference offers a refreshing alternative to traditional parametric inference, providing exact control over Type I error rates without relying on large-sample approximations or strict distributional assumptions. Born out of Fisher‚Äôs famous tea-tasting experiment, the approach leverages the symmetry and structure induced by randomization itself to test hypotheses.\nThis blog post unpacks the theory and intuition behind randomization inference, drawing on the excellent review by Ritzwoller, Romano, and Shaikh (2025). We‚Äôll cover the key ideas, notation, and algorithms involved, and also touch on modern applications like two-sample tests, regression, and conformal inference. Throughout, we‚Äôll emphasize the practical considerations ‚Äî when it works, why it works, and where caution is needed."
  },
  {
    "objectID": "blog-unpublished/randomization-inference.html#notation",
    "href": "blog-unpublished/randomization-inference.html#notation",
    "title": "Randomization Inference: Exact Testing Without Parametric Assumptions",
    "section": "Notation",
    "text": "Notation\nLet \\(X\\) represent the observed data, generated by some unknown probability law \\(P\\). The parameter space \\(\\Omega\\) contains all possible data-generating processes, and \\(\\Omega_0 \\subset \\Omega\\) specifies the null hypothesis.\nA group \\(G\\) of transformations (e.g., permutations or sign-flips) acts on the data. Under the randomization hypothesis, the distribution of \\(X\\) is invariant under \\(G\\) if \\(P \\in \\Omega_0\\).\nFormally, under the null: \\[\ngX \\overset{d}{=} X, \\quad \\text{for all } g \\in G.\n\\]\nLet \\(T(X)\\) be a chosen test statistic."
  },
  {
    "objectID": "blog-unpublished/randomization-inference.html#a-closer-look",
    "href": "blog-unpublished/randomization-inference.html#a-closer-look",
    "title": "Randomization Inference: Exact Testing Without Parametric Assumptions",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nExact Testing via Randomization\nIf the randomization hypothesis holds, we can compute the distribution of \\(T(X)\\) by applying all transformations in \\(G\\) to the data. The p-value is simply the proportion of these transformed test statistics that are as extreme or more extreme than the observed \\(T(X)\\):\n\\[\n\\hat{p} = \\frac{1}{|G|} \\sum_{g \\in G} I\\{ T(gX) \\geq T(X) \\}.\n\\]\nBecause the null implies invariance under \\(G\\), this procedure achieves exact finite-sample control of the Type I error rate.\n\n\n\n\n\n\nAlgorithm: Randomization Test\n\n\n\n\nChoose a test statistic \\(T(X)\\).\nDefine the group \\(G\\) of transformations.\nCompute \\(T(X)\\) on the observed data.\nApply all (or a random sample of) transformations \\(g \\in G\\) to the data and recompute \\(T(gX)\\).\nCalculate the p-value as the proportion of transformed statistics as or more extreme than \\(T(X)\\).\n\n\n\n\n\nApproximate Validity and Asymptotics\nIn many real-world problems, the randomization hypothesis may not strictly hold, or the group \\(G\\) may only approximate invariance. The surprising good news is that permutation and randomization tests often remain asymptotically valid under broad conditions.\nThis validity relies on the test statistic being asymptotically pivotal ‚Äî its limiting distribution under the null does not depend on nuisance parameters. Examples include: - The studentized difference of means. - Test statistics based on ranks (e.g., Wilcoxon-Mann-Whitney).\nBy studentizing the test statistic (i.e., scaling by an estimate of its standard error), one can often restore validity even when the randomization hypothesis fails.\n\n\nWhen Things Go Wrong: Failures Without Studentization\nIf the test statistic is not pivotal, the permutation distribution might not match the true sampling distribution. This mismatch leads to inflated Type I errors and directional mistakes (Type III errors), especially when variances differ between groups.\nFor instance, testing mean differences between two groups with unequal variances using the raw difference in means (without studentization) can lead to massive over-rejection rates. Studentizing solves this problem by aligning the permutation distribution with the true sampling distribution.\n\n\nStrengths and Limitations\nRandomization inference shines when: - The randomization scheme is known and under control (e.g., in experiments). - The test statistic is carefully chosen to be pivotal. - Exact error control is desirable in finite samples.\nIt struggles when: - Covariates are correlated with treatment assignment but not accounted for. - The sample size is too small to approximate the randomization distribution well via subsampling."
  },
  {
    "objectID": "blog-unpublished/randomization-inference.html#an-example",
    "href": "blog-unpublished/randomization-inference.html#an-example",
    "title": "Randomization Inference: Exact Testing Without Parametric Assumptions",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nset.seed(123)\nn &lt;- 20\nx &lt;- rnorm(n, mean = 0)\ntest_stat &lt;- mean(x)\nn_permutations &lt;- 1000\nperms &lt;- replicate(n_permutations, mean(sample(x)))\np_value &lt;- mean(perms &gt;= test_stat)\np_value\n\n\nimport numpy as np\n\nnp.random.seed(123)\nn = 20\nx = np.random.normal(0, 1, n)\ntest_stat = np.mean(x)\nperms = [np.mean(np.random.permutation(x)) for _ in range(1000)]\np_value = np.mean([p &gt;= test_stat for p in perms])\nprint(p_value)"
  },
  {
    "objectID": "blog-unpublished/randomization-inference.html#bottom-line",
    "href": "blog-unpublished/randomization-inference.html#bottom-line",
    "title": "Randomization Inference: Exact Testing Without Parametric Assumptions",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nRandomization inference provides exact finite-sample error control when the randomization hypothesis holds.\nAsymptotic validity can often be rescued by choosing asymptotically pivotal (studentized) test statistics.\nWithout studentization, permutation tests may fail badly in the presence of unequal variances.\nRandomization tests are flexible and nonparametric, making them attractive for experimental data and beyond."
  },
  {
    "objectID": "blog-unpublished/randomization-inference.html#where-to-learn-more",
    "href": "blog-unpublished/randomization-inference.html#where-to-learn-more",
    "title": "Randomization Inference: Exact Testing Without Parametric Assumptions",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThe best starting point is the recent review by Ritzwoller, Romano, and Shaikh (2025). For foundational treatments, see Hoeffding (1952) and Lehmann & Romano‚Äôs text on nonparametric inference. The practical guide by Good (2005) on permutation tests is also highly recommended."
  },
  {
    "objectID": "blog-unpublished/randomization-inference.html#references",
    "href": "blog-unpublished/randomization-inference.html#references",
    "title": "Randomization Inference: Exact Testing Without Parametric Assumptions",
    "section": "References",
    "text": "References\n\nRitzwoller, D. M., Romano, J. P., & Shaikh, A. M. (2025). Randomization Inference: Theory and Applications.\nHoeffding, W. (1952). The large-sample power of permutation tests. Annals of Mathematical Statistics, 23(2), 169-192.\nLehmann, E. L., & Romano, J. P. (2022). Testing Statistical Hypotheses. Springer.\nGood, P. (2005). Permutation, Parametric, and Bootstrap Tests of Hypotheses. Springer."
  },
  {
    "objectID": "blog-unpublished/causal-inference-log-transformations.html",
    "href": "blog-unpublished/causal-inference-log-transformations.html",
    "title": "Causal Inference with Log Transformations",
    "section": "",
    "text": "Logarithmic transformations are among the most common tools in applied regression modeling, often motivated by their ability to linearize exponential relationships or to stabilize variance. They are also appealing in causal inference because they allow for treatment effects to be interpreted as percentage changes, a language that feels intuitive in policy discussions. But what happens when your outcome variable can be zero ‚Äî or when you use alternative transformations like \\(\\log(1 + Y)\\) or \\(\\arcsinh(Y)\\) to handle this?\nThis article walks through the core issues with log transformations in causal inference, focusing on the recent findings of Chen and Roth (2024) and insights from Huntington-Klein (2023). Along the way, we‚Äôll clarify what you can and cannot interpret as percentage effects, discuss the critical issue of scale dependence, and offer practical alternatives."
  },
  {
    "objectID": "blog-unpublished/causal-inference-log-transformations.html#background",
    "href": "blog-unpublished/causal-inference-log-transformations.html#background",
    "title": "Causal Inference with Log Transformations",
    "section": "",
    "text": "Logarithmic transformations are among the most common tools in applied regression modeling, often motivated by their ability to linearize exponential relationships or to stabilize variance. They are also appealing in causal inference because they allow for treatment effects to be interpreted as percentage changes, a language that feels intuitive in policy discussions. But what happens when your outcome variable can be zero ‚Äî or when you use alternative transformations like \\(\\log(1 + Y)\\) or \\(\\arcsinh(Y)\\) to handle this?\nThis article walks through the core issues with log transformations in causal inference, focusing on the recent findings of Chen and Roth (2024) and insights from Huntington-Klein (2023). Along the way, we‚Äôll clarify what you can and cannot interpret as percentage effects, discuss the critical issue of scale dependence, and offer practical alternatives."
  },
  {
    "objectID": "blog-unpublished/causal-inference-log-transformations.html#notation",
    "href": "blog-unpublished/causal-inference-log-transformations.html#notation",
    "title": "Causal Inference with Log Transformations",
    "section": "Notation",
    "text": "Notation\nAssume a binary treatment \\(D \\in \\{0, 1\\}\\) and an outcome \\(Y \\geq 0\\), with potential outcomes \\(Y(1)\\) and \\(Y(0)\\). The canonical average treatment effect (ATE) is:\n\\[\nATE = \\mathbb{E}[Y(1) - Y(0)].\n\\]\nWhen \\(Y\\) is strictly positive, many researchers instead report the ATE in logs:\n\\[\n\\mathbb{E}[\\log(Y(1)) - \\log(Y(0))].\n\\]\nIf \\(Y\\) can equal zero, log is undefined, so analysts often use log-like transformations like \\(\\log(1 + Y)\\) or \\(\\arcsinh(Y)\\), both of which behave like \\(\\log(Y)\\) for large \\(Y\\)."
  },
  {
    "objectID": "blog-unpublished/causal-inference-log-transformations.html#a-closer-look",
    "href": "blog-unpublished/causal-inference-log-transformations.html#a-closer-look",
    "title": "Causal Inference with Log Transformations",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Problem with Logs and Zeros\nThe main challenge arises when some individuals have \\(Y(0) = 0\\) but \\(Y(1) &gt; 0\\), or vice versa. In these cases, a percentage change isn‚Äôt well-defined at the individual level: you can‚Äôt compute a percentage increase from zero.\nChen and Roth (2024) demonstrate that for log-like transformations \\(m(Y)\\), the estimated ATE:\n\\[\n\\mathbb{E}[m(Y(1)) - m(Y(0))]\n\\]\nis not unit-invariant. In fact, the magnitude of this estimated effect can be arbitrarily manipulated simply by rescaling the units of \\(Y\\) (e.g., from dollars to cents). This violates the intuition that a percentage change should be independent of whether we‚Äôre measuring income in dollars or pennies.\nThe core insight is that log-like transformations place implicit and arbitrary weight on the extensive margin ‚Äî that is, on individuals moving from zero to nonzero outcomes ‚Äî and that this weight is sensitive to the scale of \\(Y\\).\n\n\n\nA Trilemma: Pick Two\nChen and Roth formalize a ‚Äútrilemma‚Äù: when outcomes can be zero, no treatment effect parameter can satisfy all three of the following:\n\nBe an average of individual-level treatment effects.\nBe invariant to rescaling of units.\nBe point-identified from observed data.\n\nThis means researchers must choose which property to sacrifice. If you want unit-invariance (percentage effects), you can‚Äôt use log-like ATEs. If you want point identification and interpretability, you may need to abandon simple averaging.\n\n\n\nAlternative Approaches\n\nPercentage Effects in Levels (Poisson Regression)\nOne solution is to return to level-based treatment effects, such as:\n\\[\n\\theta_{\\text{level-percentage}} = \\frac{\\mathbb{E}[Y(1) - Y(0)]}{\\mathbb{E}[Y(0)]}.\n\\]\nThis can be estimated via Poisson regression or by directly scaling the level-based ATE.\n\n\nExplicit Weighting of Margins\nIf both the extensive (zero to nonzero) and intensive (positive changes) margins are of interest, researchers can specify how much they value a zero-to-one change relative to continuous changes. This requires subjective calibration but makes assumptions transparent.\n\n\nSeparate Estimates for Intensive and Extensive Margins\nAlternatively, estimate separate treatment effects for:\n\nThe probability of having \\(Y &gt; 0\\) (extensive margin).\nThe mean of \\(\\log(Y)\\) conditional on \\(Y &gt; 0\\) (intensive margin).\n\nThis aligns with methods like Lee bounds or other partial identification strategies.\n\n\n\n\nThe Huntington-Klein Adjustment: Linear Rescaling\nHuntington-Klein (2023) offers a complementary insight: even when using standard logs, interpretation often relies on sloppy approximations (e.g., interpreting \\(p\\) units in \\(\\log(X)\\) as \\(p \\times 100\\%\\) change in \\(X\\)).\nHe proposes using logarithms with alternative bases (log(_{1+p})) where a one-unit change corresponds exactly to a \\(p\\)-percent increase. This method avoids approximation error and can be written directly into regression tables for clarity.\nWhile this doesn‚Äôt fix the zero problem per se, it improves interpretability when outcomes are positive."
  },
  {
    "objectID": "blog-unpublished/causal-inference-log-transformations.html#an-example",
    "href": "blog-unpublished/causal-inference-log-transformations.html#an-example",
    "title": "Causal Inference with Log Transformations",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nset.seed(123)\nn &lt;- 1000\nd &lt;- rbinom(n, 1, 0.5)\ny0 &lt;- rgamma(n, shape = 2, rate = 1)\ny1 &lt;- y0 + d * rgamma(n, shape = 1, rate = 0.5)\ny &lt;- ifelse(d == 1, y1, y0)\nlog1p_y &lt;- log1p(y)\nmodel &lt;- lm(log1p_y ~ d)\nsummary(model)\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(123)\nn = 1000\nd = np.random.binomial(1, 0.5, n)\ny0 = np.random.gamma(2, 1, n)\ny1 = y0 + d * np.random.gamma(1, 2, n)\ny = np.where(d == 1, y1, y0)\nlog1p_y = np.log1p(y)\nX = sm.add_constant(d)\nmodel = sm.OLS(log1p_y, X).fit()\nprint(model.summary())"
  },
  {
    "objectID": "blog-unpublished/causal-inference-log-transformations.html#bottom-line",
    "href": "blog-unpublished/causal-inference-log-transformations.html#bottom-line",
    "title": "Causal Inference with Log Transformations",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLog transformations with zeros pose serious interpretational challenges for causal inference.\nLog-like ATEs are arbitrarily sensitive to the scaling of the outcome variable.\nAlternative strategies like Poisson regression, explicit weighting of margins, or separate modeling of intensive and extensive effects provide better-grounded estimates.\nLinear rescaling (choosing alternative log bases) improves interpretation accuracy but does not solve the zero problem alone."
  },
  {
    "objectID": "blog-unpublished/causal-inference-log-transformations.html#where-to-learn-more",
    "href": "blog-unpublished/causal-inference-log-transformations.html#where-to-learn-more",
    "title": "Causal Inference with Log Transformations",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThe must-read paper on this topic is Chen and Roth (2024), which provides both the formal theory and empirical illustrations of these issues. Huntington-Klein (2023) adds valuable advice on how to interpret log coefficients cleanly. Related discussions appear in work on Poisson models, two-part models, and Lee bounds for partial identification."
  },
  {
    "objectID": "blog-unpublished/causal-inference-log-transformations.html#references",
    "href": "blog-unpublished/causal-inference-log-transformations.html#references",
    "title": "Causal Inference with Log Transformations",
    "section": "References",
    "text": "References\n\nChen, J., & Roth, J. (2024). Logs with Zeros? Some Problems and Solutions. Quarterly Journal of Economics, 139(2), 891‚Äì936.\nHuntington-Klein, N. (2023). Linear Rescaling to Accurately Interpret Logarithms. Journal of Economic Methods, 12(1), 139‚Äì147."
  },
  {
    "objectID": "blog-unpublished/weak-iv-review.html",
    "href": "blog-unpublished/weak-iv-review.html",
    "title": "Weak Instruments in Causal Inference: A Data Scientist‚Äôs Guide",
    "section": "",
    "text": "Instrumental variables (IV) methods are a cornerstone of causal inference, especially when dealing with endogeneity. But IV‚Äôs strength is only as good as the quality of its instruments. If your instruments are weak ‚Äî meaning they barely explain variation in the endogenous regressor ‚Äî all the nice properties of IV break down, sometimes spectacularly so.\nThis article provides a detailed, chronological walkthrough of the weak instruments literature, following the recent review by Keane and Neal (2024). Along the way, we‚Äôll revisit some classics (Bound et al.¬†1995, Staiger & Stock 1997), discuss critical developments like the Anderson-Rubin (AR) and Conditional Likelihood Ratio (CLR) tests, and explain why many conventional practices can lead us astray. Whether you‚Äôre doing causal inference in economics, health, or social science, understanding these pitfalls is crucial."
  },
  {
    "objectID": "blog-unpublished/weak-iv-review.html#background",
    "href": "blog-unpublished/weak-iv-review.html#background",
    "title": "Weak Instruments in Causal Inference: A Data Scientist‚Äôs Guide",
    "section": "",
    "text": "Instrumental variables (IV) methods are a cornerstone of causal inference, especially when dealing with endogeneity. But IV‚Äôs strength is only as good as the quality of its instruments. If your instruments are weak ‚Äî meaning they barely explain variation in the endogenous regressor ‚Äî all the nice properties of IV break down, sometimes spectacularly so.\nThis article provides a detailed, chronological walkthrough of the weak instruments literature, following the recent review by Keane and Neal (2024). Along the way, we‚Äôll revisit some classics (Bound et al.¬†1995, Staiger & Stock 1997), discuss critical developments like the Anderson-Rubin (AR) and Conditional Likelihood Ratio (CLR) tests, and explain why many conventional practices can lead us astray. Whether you‚Äôre doing causal inference in economics, health, or social science, understanding these pitfalls is crucial."
  },
  {
    "objectID": "blog-unpublished/weak-iv-review.html#notation",
    "href": "blog-unpublished/weak-iv-review.html#notation",
    "title": "Weak Instruments in Causal Inference: A Data Scientist‚Äôs Guide",
    "section": "Notation",
    "text": "Notation\nConsider the structural equation:\n\\[\ny = x \\beta + u, \\quad \\text{with} \\; \\text{cov}(x, u) \\neq 0.\n\\]\nHere: - \\(y\\) is the outcome. - \\(x\\) is the endogenous regressor. - \\(u\\) is the structural error term.\nWe also have an instrument \\(z\\) satisfying: - Relevance: \\(\\text{cov}(z, x) \\neq 0\\), - Exogeneity: \\(\\text{cov}(z, u) = 0\\).\nThe first-stage equation is:\n\\[\nx = z \\pi + e,\n\\]\nwhere \\(e\\) is the first-stage error."
  },
  {
    "objectID": "blog-unpublished/weak-iv-review.html#a-closer-look",
    "href": "blog-unpublished/weak-iv-review.html#a-closer-look",
    "title": "Weak Instruments in Causal Inference: A Data Scientist‚Äôs Guide",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Early Warnings: Bias and Size Distortion\nThe weak instrument problem was first highlighted by Bound et al.¬†(1995), who showed that when instruments are weak, the Two-Stage Least Squares (2SLS) estimator behaves badly: - The bias of 2SLS drifts toward the Ordinary Least Squares (OLS) bias. - The size of the 2SLS \\(t\\)-test inflates ‚Äî the \\(5\\%\\) test rejects more than \\(5\\%\\) of the time.\nStaiger and Stock (1997) proposed the famous rule of thumb: first-stage \\(F &gt; 10\\). If your first-stage \\(F\\)-statistic is below 10, trouble looms.\n\n\nSize Isn‚Äôt Everything: The Power Asymmetry Problem\nKeane and Neal (2024) show that even with strong instruments (F well above 10), 2SLS suffers from power asymmetry: - Standard errors are spuriously small when the 2SLS estimate drifts toward OLS. - Estimates near OLS appear artificially precise, leading to inflated power to detect false positives. - Meanwhile, true effects far from OLS are harder to detect.\nThis is not just a weak instrument problem ‚Äî it affects 2SLS broadly.\n\n\nThe Anderson-Rubin (AR) and Conditional Likelihood Ratio (CLR) Tests\nThe AR test, introduced in 1949 (!), avoids power asymmetry by focusing on the reduced-form relationship:\n\\[\ny = z \\gamma + \\varepsilon.\n\\]\nThe AR test is valid even with weak instruments and does not suffer from the same distortion as the 2SLS \\(t\\)-test. When there are multiple instruments, the CLR test (Moreira 2003, Kleibergen 2005) generalizes this approach.\n\n\n\n\n\n\nAlgorithm: Anderson-Rubin Test\n\n\n\n\nEstimate the first-stage regression of \\(x\\) on \\(z\\).\nCompute fitted values \\(\\hat{x}\\).\nRegress \\(y\\) on \\(\\hat{x}\\) (ignoring standard errors).\nUse the residual variance from this regression to construct the AR test statistic.\n\n\n\n\n\nWhen Is OLS Better Than 2SLS?\nA provocative insight from the recent literature is that when first-stage \\(F\\) is between \\(10\\) and \\(20\\), OLS may be closer to the truth than 2SLS, unless endogeneity is very severe.\nThe logic: If instruments barely move \\(x\\), the small amount of exogenous variation they provide may not be enough to outweigh the imprecision of 2SLS."
  },
  {
    "objectID": "blog-unpublished/weak-iv-review.html#an-example",
    "href": "blog-unpublished/weak-iv-review.html#an-example",
    "title": "Weak Instruments in Causal Inference: A Data Scientist‚Äôs Guide",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nset.seed(123)\nn &lt;- 1000\nz &lt;- rnorm(n)\nx &lt;- 0.1 * z + rnorm(n)\ny &lt;- 0.5 * x + 0.5 * rnorm(n)\n\nfirst_stage &lt;- lm(x ~ z)\nsummary(first_stage)\n\nsecond_stage &lt;- lm(y ~ fitted(first_stage))\nsummary(second_stage)\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(123)\nn = 1000\nz = np.random.randn(n)\nx = 0.1 * z + np.random.randn(n)\ny = 0.5 * x + 0.5 * np.random.randn(n)\n\nX = sm.add_constant(z)\nfirst_stage = sm.OLS(x, X).fit()\nx_hat = first_stage.fittedvalues\n\nX2 = sm.add_constant(x_hat)\nsecond_stage = sm.OLS(y, X2).fit()\nprint(second_stage.summary())"
  },
  {
    "objectID": "blog-unpublished/weak-iv-review.html#bottom-line",
    "href": "blog-unpublished/weak-iv-review.html#bottom-line",
    "title": "Weak Instruments in Causal Inference: A Data Scientist‚Äôs Guide",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nWeak instruments bias 2SLS toward OLS and inflate \\(t\\)-test size.\nPower asymmetry causes 2SLS to favor false positives near OLS.\nAR and CLR tests avoid these issues and should be preferred.\nOLS may outperform 2SLS when instruments are weak, unless endogeneity is severe."
  },
  {
    "objectID": "blog-unpublished/weak-iv-review.html#where-to-learn-more",
    "href": "blog-unpublished/weak-iv-review.html#where-to-learn-more",
    "title": "Weak Instruments in Causal Inference: A Data Scientist‚Äôs Guide",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThe recent review by Keane and Neal (2024) provides a practical and readable guide to the weak instruments literature. For deeper theory, see Bound et al.¬†(1995), Staiger & Stock (1997), and Stock & Yogo (2005). Modern software packages like ivreg in R and linearmodels in Python support these methods."
  },
  {
    "objectID": "blog-unpublished/weak-iv-review.html#references",
    "href": "blog-unpublished/weak-iv-review.html#references",
    "title": "Weak Instruments in Causal Inference: A Data Scientist‚Äôs Guide",
    "section": "References",
    "text": "References\n\nBound, J., Jaeger, D. A., & Baker, R. M. (1995). Problems with Instrumental Variables Estimation When the Correlation Between the Instruments and the Endogenous Explanatory Variable Is Weak. Journal of the American Statistical Association, 90(430), 443‚Äì450.\nStaiger, D., & Stock, J. H. (1997). Instrumental Variables Regression with Weak Instruments. Econometrica, 65(3), 557‚Äì586.\nStock, J. H., & Yogo, M. (2005). Testing for Weak Instruments in Linear IV Regression. Identification and Inference for Econometric Models.\nKeane, M. P., & Neal, T. (2024). A Practical Guide to Weak Instruments. Annual Review of Economics, 16, 185‚Äì212."
  },
  {
    "objectID": "blog-unpublished/generalized-additive-models.html",
    "href": "blog-unpublished/generalized-additive-models.html",
    "title": "Generalized Additive Models: What You Need to Know",
    "section": "",
    "text": "Generalized Additive Models (GAMs) are one of the most powerful and flexible tools in a data scientist‚Äôs toolbox for modeling complex, nonlinear relationships between covariates and an outcome. They generalize linear models by allowing smooth, nonparametric functions of the predictors while still maintaining interpretability and manageable computation. The core idea: instead of forcing relationships to be straight lines, let the data speak.\nThis article explains what you really need to know about GAMs, following the excellent review by Simon Wood (2025). We‚Äôll go over the basics of how GAMs work, how smoothness is controlled, the computational strategies involved, and key pitfalls to watch out for. We‚Äôll also walk through a code example in both R and Python to show how to fit and interpret these models in practice."
  },
  {
    "objectID": "blog-unpublished/generalized-additive-models.html#background",
    "href": "blog-unpublished/generalized-additive-models.html#background",
    "title": "Generalized Additive Models: What You Need to Know",
    "section": "",
    "text": "Generalized Additive Models (GAMs) are one of the most powerful and flexible tools in a data scientist‚Äôs toolbox for modeling complex, nonlinear relationships between covariates and an outcome. They generalize linear models by allowing smooth, nonparametric functions of the predictors while still maintaining interpretability and manageable computation. The core idea: instead of forcing relationships to be straight lines, let the data speak.\nThis article explains what you really need to know about GAMs, following the excellent review by Simon Wood (2025). We‚Äôll go over the basics of how GAMs work, how smoothness is controlled, the computational strategies involved, and key pitfalls to watch out for. We‚Äôll also walk through a code example in both R and Python to show how to fit and interpret these models in practice."
  },
  {
    "objectID": "blog-unpublished/generalized-additive-models.html#notation",
    "href": "blog-unpublished/generalized-additive-models.html#notation",
    "title": "Generalized Additive Models: What You Need to Know",
    "section": "Notation",
    "text": "Notation\nConsider an outcome variable \\(y\\) and predictors \\(x_1, x_2, \\dots, x_p\\). The simplest linear model is:\n\\[\ny = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j + \\varepsilon.\n\\]\nThe GAM replaces the linear terms \\(\\beta_j x_j\\) with smooth functions \\(f_j(x_j)\\):\n\\[\ny = \\beta_0 + \\sum_{j=1}^p f_j(x_j) + \\varepsilon.\n\\]\nMore generally, for non-Gaussian outcomes, GAMs use a link function \\(g(\\cdot)\\):\n\\[\ng(\\mathbb{E}[y]) = \\beta_0 + \\sum_{j=1}^p f_j(x_j).\n\\]\nEach \\(f_j\\) is estimated from the data and constrained to be ‚Äúsmooth‚Äù through penalization."
  },
  {
    "objectID": "blog-unpublished/generalized-additive-models.html#a-closer-look",
    "href": "blog-unpublished/generalized-additive-models.html#a-closer-look",
    "title": "Generalized Additive Models: What You Need to Know",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhat Makes a GAM?\nThe backbone of a GAM is its smooth terms. These are typically represented using splines ‚Äî basis functions that piece together polynomials smoothly at specified knots. But not just any spline will do! In GAMs, smoothness is enforced through penalty terms that discourage excessive wiggliness.\nFor example, for a cubic spline, the penalty is usually the integral of the squared second derivative:\n\\[\n\\int (f''(x))^2 \\, dx.\n\\]\nThe balance between fitting the data and keeping the function smooth is controlled by smoothing parameters (\\(\\lambda\\)). A higher \\(\\lambda\\) makes the function flatter; a lower \\(\\lambda\\) allows more flexibility.\n\n\nHow Smoothness Is Estimated\nThere are two main strategies to estimate \\(\\lambda\\):\n\nCross-validation (CV): Minimize prediction error by holding out parts of the data.\nMarginal likelihood (REML): An empirical Bayes approach that tends to perform well in practice.\n\nThe marginal likelihood approach treats the smooth functions as random effects with Gaussian priors, leading to nice frequentist properties (good coverage, calibrated uncertainty estimates).\n\n\nWhy Rank Reduction Matters\nFull spline bases can be large and computationally expensive. To address this, GAMs often use rank-reduced splines: only the leading components of the basis (those with the smallest penalties) are retained. This keeps computation tractable without sacrificing much flexibility.\nThe result: GAM fitting scales better to large datasets while preserving interpretability.\n\n\nBeyond the Mean: Location-Scale and More\nGAMs aren‚Äôt limited to modeling the mean. They can handle location, scale, and shape modeling ‚Äî meaning that the variance, skewness, or other distributional parameters can also depend on smooth functions of predictors. This generalization brings GAMs into the world of generalized additive models for location, scale, and shape (GAMLSS).\nThey can even be extended to quantile regression and non-exponential family distributions, making them incredibly versatile.\n\n\nModel Selection and Hypothesis Testing\nChoosing the right model structure ‚Äî which smooths to include, how many degrees of freedom to allow ‚Äî is a key part of using GAMs effectively. Common tools include:\n\nAkaike Information Criterion (AIC): Trade-off between goodness of fit and model complexity.\nHypothesis testing of smooth terms: Check whether each \\(f_j\\) is significantly different from zero.\n\nWood (2025) warns against naive use of Wald tests for this purpose and recommends careful use of penalization-based tests or likelihood-ratio approaches."
  },
  {
    "objectID": "blog-unpublished/generalized-additive-models.html#an-example",
    "href": "blog-unpublished/generalized-additive-models.html#an-example",
    "title": "Generalized Additive Models: What You Need to Know",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nlibrary(mgcv)\nset.seed(42)\nn &lt;- 200\nx &lt;- runif(n, 0, 10)\ny &lt;- sin(x) + rnorm(n, 0, 0.3)\nmodel &lt;- gam(y ~ s(x), method = \"REML\")\nsummary(model)\nplot(model, residuals = TRUE)\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom pygam import LinearGAM, s\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn = 200\nx = np.random.uniform(0, 10, n)\ny = np.sin(x) + np.random.normal(0, 0.3, n)\n\nX = x.reshape(-1, 1)\ngam = LinearGAM(s(0)).fit(X, y)\ngam.summary()\n\nplt.figure()\nXX = np.linspace(0, 10, 100)\nplt.plot(XX, gam.predict(XX), label=\"GAM fit\")\nplt.scatter(x, y, alpha=0.3)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blog-unpublished/generalized-additive-models.html#bottom-line",
    "href": "blog-unpublished/generalized-additive-models.html#bottom-line",
    "title": "Generalized Additive Models: What You Need to Know",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nGAMs allow flexible, nonlinear modeling while retaining interpretability.\nSmoothness is controlled by penalties, estimated via CV or marginal likelihood (REML).\nRank reduction makes GAMs computationally feasible even with large datasets.\nGAMs generalize beyond means to scale, shape, and quantile modeling."
  },
  {
    "objectID": "blog-unpublished/generalized-additive-models.html#where-to-learn-more",
    "href": "blog-unpublished/generalized-additive-models.html#where-to-learn-more",
    "title": "Generalized Additive Models: What You Need to Know",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThe recent review by Simon Wood (2025) is the most comprehensive and readable guide to modern GAMs. For practical hands-on work, Wood‚Äôs book Generalized Additive Models: An Introduction with R (2017) remains the go-to resource. For Bayesian extensions and Gaussian processes connections, see works by Rue et al.¬†(2009) and Kammann & Wand (2003)."
  },
  {
    "objectID": "blog-unpublished/generalized-additive-models.html#references",
    "href": "blog-unpublished/generalized-additive-models.html#references",
    "title": "Generalized Additive Models: What You Need to Know",
    "section": "References",
    "text": "References\n\nWood, S. N. (2025). Generalized Additive Models. Annual Review of Statistics and Its Application, 12, 497‚Äì526.\nWood, S. N. (2017). Generalized Additive Models: An Introduction with R. CRC Press."
  },
  {
    "objectID": "blog-unpublished/data-fission.html",
    "href": "blog-unpublished/data-fission.html",
    "title": "Data Fission: A New Approach to Model Selection and Inference",
    "section": "",
    "text": "When conducting statistical modeling and hypothesis testing, analysts often face the challenge of model selection bias: if we use the same data both to select a model and to conduct inference, our confidence intervals and p-values may no longer be valid. Traditionally, the go-to solution has been data splitting ‚Äî partitioning the dataset into two separate parts: one for model selection and one for inference.\nHowever, what if we don‚Äôt have the luxury of a large dataset? What if splitting would waste valuable information, especially when rare events or high-leverage points dominate? Enter data fission, a recent innovation proposed by Leiner et al.¬†(2025), which offers a more efficient, flexible, and assumption-lean alternative to data splitting.\nThis article introduces the concept of data fission, how it works, and where it outperforms traditional methods like data splitting and data carving. We‚Äôll also walk through examples and highlight its applications in linear regression, GLMs, trend filtering, and post-selection inference."
  },
  {
    "objectID": "blog-unpublished/data-fission.html#background",
    "href": "blog-unpublished/data-fission.html#background",
    "title": "Data Fission: A New Approach to Model Selection and Inference",
    "section": "",
    "text": "When conducting statistical modeling and hypothesis testing, analysts often face the challenge of model selection bias: if we use the same data both to select a model and to conduct inference, our confidence intervals and p-values may no longer be valid. Traditionally, the go-to solution has been data splitting ‚Äî partitioning the dataset into two separate parts: one for model selection and one for inference.\nHowever, what if we don‚Äôt have the luxury of a large dataset? What if splitting would waste valuable information, especially when rare events or high-leverage points dominate? Enter data fission, a recent innovation proposed by Leiner et al.¬†(2025), which offers a more efficient, flexible, and assumption-lean alternative to data splitting.\nThis article introduces the concept of data fission, how it works, and where it outperforms traditional methods like data splitting and data carving. We‚Äôll also walk through examples and highlight its applications in linear regression, GLMs, trend filtering, and post-selection inference."
  },
  {
    "objectID": "blog-unpublished/data-fission.html#notation",
    "href": "blog-unpublished/data-fission.html#notation",
    "title": "Data Fission: A New Approach to Model Selection and Inference",
    "section": "Notation",
    "text": "Notation\nAssume we observe a random variable \\(X\\) drawn from a distribution \\(P\\) with unknown parameter \\(\\theta\\). The core idea of data fission is to decompose \\(X\\) into two parts:\n\\[\nf(X), \\quad g(X)\n\\]\nsuch that: - Neither \\(f(X)\\) nor \\(g(X)\\) alone can fully reconstruct \\(X\\), - But together they determine \\(X\\), - The joint distribution of \\((f(X), g(X))\\) is known or tractable.\nFor example, if \\(X \\sim N(\\mu, \\sigma^2)\\), fission is achieved by:\n\\[\nf(X) = X + Z, \\quad g(X) = X - Z, \\quad Z \\sim N(0, \\sigma^2).\n\\]"
  },
  {
    "objectID": "blog-unpublished/data-fission.html#a-closer-look",
    "href": "blog-unpublished/data-fission.html#a-closer-look",
    "title": "Data Fission: A New Approach to Model Selection and Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhy Fission Instead of Splitting?\nThe motivation behind data fission is to keep all data points ‚Äúalive‚Äù in both selection and inference, but only share part of the information from each data point with each stage. This allows the analyst to: - Hedge their bets on influential points, - Maintain flexibility in model selection, - Retain valid inferential guarantees.\nFission can be viewed as a continuous analog to data splitting, trading off information between selection and inference smoothly, rather than discretely cutting the data in half.\n\n\n\nGaussian Example: The Simplest Case\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), one can construct independent random noise \\(Z \\sim N(0, \\sigma^2)\\) and define:\n\\[\nf(X) = X + \\tau Z, \\quad g(X) = X - \\frac{1}{\\tau} Z.\n\\]\nThe parameter \\(\\tau\\) controls how much information is allocated to \\(f(X)\\) versus \\(g(X)\\). When \\(\\tau \\to \\infty\\), \\(f(X)\\) becomes uninformative; when \\(\\tau \\to 0\\), \\(g(X)\\) becomes uninformative.\n\n\n\nGeneralized Fission via Conjugate Prior Reversal\nBeyond the Gaussian case, data fission leverages conjugate prior relationships to achieve similar decompositions:\n\nPoisson Example: If \\(X \\sim \\text{Poisson}(\\mu)\\), set \\(f(X) = Z \\sim \\text{Binomial}(X, p)\\), \\(g(X) = X - Z\\).\nBernoulli Example: If \\(X \\sim \\text{Bernoulli}(\\theta)\\), draw \\(Z \\sim \\text{Bernoulli}(p)\\) and set \\(f(X) = X(1 - Z) + (1 - X)Z\\).\n\nThese constructions maintain a tractable joint distribution between \\(f(X)\\) and \\(g(X)\\) while achieving the desired splitting of information.\n\n\n\nEfficiency: Why Fission Outperforms Splitting\nOne of the most compelling results from Leiner et al.¬†is that fission, on average, yields tighter confidence intervals and higher power than data splitting. This advantage is particularly pronounced when:\n\nThe sample size is small,\nCertain data points have disproportionate influence (high leverage),\nCovariates are fixed (non-random).\n\nThe reason: data splitting introduces randomness into which points are selected for inference, which lowers efficiency. Data fission avoids this by deterministically splitting the information within each point.\n\n\n\nApplications\nThe authors demonstrate the usefulness of data fission across several settings:\n\nPost-selection inference after multiple testing: Construct valid confidence intervals even after adaptive selection of hypotheses.\nLinear regression with feature selection: Use fissioned data to select variables with LASSO and then conduct inference without sacrificing power.\nGeneralized linear models (GLMs): Extend the approach to non-Gaussian responses like Poisson and Binomial models.\nTrend filtering and nonparametric regression: Enable valid uncertainty quantification in adaptive smoothing problems.\n\n\n\nDiscussion\nThis paper generated tremendous amount of discussion in the Journal of the American Statistical Association. Here is a brief summary of the twelve or so comments published alongisde the original paper. The discussion papers on data fission explore its potential as a novel statistical method for splitting a single data point into two components for model selection and inference. ‚Äã Many contributors praise its innovative use of Bayesian ideas and conjugate models, highlighting its advantages in selective inference and applications like clustering, Gaussian Process regression, and single-cell RNA-seq analysis. However, several critiques emerge, including concerns about its reliance on parametric assumptions, sensitivity to data-specific properties, and potential overfitting in small sample sizes. ‚Äã Some authors suggest extending data fission to nonparametric settings, leveraging tools like the Dirichlet Process, or exploring debiasing methods for complex dependencies in unsupervised learning tasks. ‚Äã\nOthers propose practical improvements, such as asymptotic guarantees for broader selection rules, new decomposition strategies for dependent data, and connections to empirical Bayes frameworks. ‚Äã Comparisons with data splitting and data carving reveal that while data fission excels in high-dimensional problems and inference tasks, it may underperform in small data contexts or when selection events are tractable. The authors of the original paper acknowledge these critiques and emphasize the need for further research into extending data fission to dependent data, improving computational accessibility, and exploring its applications in risk estimation and clustering. Overall, the discussions highlight both the promise and challenges of data fission as a versatile tool in modern statistics."
  },
  {
    "objectID": "blog-unpublished/data-fission.html#an-example",
    "href": "blog-unpublished/data-fission.html#an-example",
    "title": "Data Fission: A New Approach to Model Selection and Inference",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nset.seed(123)\nn &lt;- 500\nx &lt;- rnorm(n)\ny &lt;- 1 + 2 * x + rnorm(n)\n\n# Fission step: add independent noise\nz &lt;- rnorm(n)\ntau &lt;- 1\nf_y &lt;- y + tau * z\ng_y &lt;- y - z / tau\n\n# Model selection on f_y (e.g., LASSO, here just OLS for simplicity)\nselection_model &lt;- lm(f_y ~ x)\nselected_coef &lt;- summary(selection_model)$coefficients\n\n# Inference on g_y\ninference_model &lt;- lm(g_y ~ x)\nsummary(inference_model)\n\n\nimport numpy as np\nimport statsmodels.api as sm\n\nnp.random.seed(123)\nn = 500\nx = np.random.randn(n)\ny = 1 + 2 * x + np.random.randn(n)\n\nz = np.random.randn(n)\ntau = 1\nf_y = y + tau * z\ng_y = y - z / tau\n\nX = sm.add_constant(x)\nselection_model = sm.OLS(f_y, X).fit()\nprint(selection_model.summary())\n\ninference_model = sm.OLS(g_y, X).fit()\nprint(inference_model.summary())"
  },
  {
    "objectID": "blog-unpublished/data-fission.html#bottom-line",
    "href": "blog-unpublished/data-fission.html#bottom-line",
    "title": "Data Fission: A New Approach to Model Selection and Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nData fission splits the information within each data point rather than the data points themselves.\nIt offers higher efficiency and flexibility than traditional data splitting.\nParticularly advantageous with small samples or high-leverage points.\nExtends naturally to regression, GLMs, and nonparametric smoothing."
  },
  {
    "objectID": "blog-unpublished/data-fission.html#where-to-learn-more",
    "href": "blog-unpublished/data-fission.html#where-to-learn-more",
    "title": "Data Fission: A New Approach to Model Selection and Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThe foundational article by Leiner et al.¬†(2025) is the key resource on data fission. It offers detailed proofs, simulation studies, and practical examples. Related work on data splitting, data carving, and post-selection inference helps contextualize this method within the broader landscape of selective inference."
  },
  {
    "objectID": "blog-unpublished/data-fission.html#references",
    "href": "blog-unpublished/data-fission.html#references",
    "title": "Data Fission: A New Approach to Model Selection and Inference",
    "section": "References",
    "text": "References\n\nLeiner, J., Duan, B., Wasserman, L., & Ramdas, A. (2025). Data Fission: Splitting a Single Data Point. Journal of the American Statistical Association, 120(549), 135‚Äì146."
  },
  {
    "objectID": "blog-unpublished/double-ml.html",
    "href": "blog-unpublished/double-ml.html",
    "title": "Double/Debiased ML for Causal Inference",
    "section": "",
    "text": "Causal inference meets machine learning ‚Äî but with a twist. Many data scientists are eager to bring the power of flexible machine learning tools into their causal analyses. But here‚Äôs the catch: na√Øvely plugging ML estimates into your causal models can lead to biased results and invalid confidence intervals. Enter Double/Debiased Machine Learning (DML), a framework designed to handle these problems systematically.\nDML, pioneered by Chernozhukov, Hansen, and colleagues, combines classical econometric insights with modern machine learning to estimate causal parameters like average treatment effects (ATE) in the presence of high-dimensional or complex nuisance components. The magic lies in two key ideas: Neyman orthogonality and cross-fitting.\nThis article unpacks how DML works, what problems it solves, and how to implement it using modern tools in R and Python. Forget about functional form assumptions ‚Äî DML lets your nuisance functions be estimated by any ML method you like (random forests, boosting, neural nets, etc.) while still guaranteeing valid inference on your treatment effects."
  },
  {
    "objectID": "blog-unpublished/double-ml.html#background",
    "href": "blog-unpublished/double-ml.html#background",
    "title": "Double/Debiased ML for Causal Inference",
    "section": "",
    "text": "Causal inference meets machine learning ‚Äî but with a twist. Many data scientists are eager to bring the power of flexible machine learning tools into their causal analyses. But here‚Äôs the catch: na√Øvely plugging ML estimates into your causal models can lead to biased results and invalid confidence intervals. Enter Double/Debiased Machine Learning (DML), a framework designed to handle these problems systematically.\nDML, pioneered by Chernozhukov, Hansen, and colleagues, combines classical econometric insights with modern machine learning to estimate causal parameters like average treatment effects (ATE) in the presence of high-dimensional or complex nuisance components. The magic lies in two key ideas: Neyman orthogonality and cross-fitting.\nThis article unpacks how DML works, what problems it solves, and how to implement it using modern tools in R and Python. Forget about functional form assumptions ‚Äî DML lets your nuisance functions be estimated by any ML method you like (random forests, boosting, neural nets, etc.) while still guaranteeing valid inference on your treatment effects."
  },
  {
    "objectID": "blog-unpublished/double-ml.html#notation",
    "href": "blog-unpublished/double-ml.html#notation",
    "title": "Double/Debiased ML for Causal Inference",
    "section": "Notation",
    "text": "Notation\nConsider an independent and identically distributed (i.i.d.) sample \\(\\{ W_i \\}_{i=1}^n\\), where each \\(W_i = (Y_i, D_i, X_i)\\): - \\(Y_i\\): outcome variable, - \\(D_i\\): treatment variable (binary or continuous), - \\(X_i\\): covariates or controls.\nThe parameter of interest is a low-dimensional causal object \\(\\theta_0\\) (such as the ATE), identified through a moment condition:\n\\[\n\\mathbb{E}[m(W_i, \\theta_0, \\eta_0)] = 0,\n\\]\nwhere: - \\(m(\\cdot)\\) is a score (moment) function, - \\(\\eta_0\\) is a high-dimensional or complex nuisance parameter (e.g., propensity score, outcome regression).\nDirectly plugging in estimated \\(\\hat{\\eta}\\) can cause bias if \\(\\hat{\\eta}\\) is not perfectly estimated, especially with ML models prone to regularization or overfitting."
  },
  {
    "objectID": "blog-unpublished/double-ml.html#a-closer-look",
    "href": "blog-unpublished/double-ml.html#a-closer-look",
    "title": "Double/Debiased ML for Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Problem: Nuisance Parameters Everywhere\nIn causal inference, we often need to estimate nuisance functions like: - The propensity score \\(r(X) = \\mathbb{E}[D | X]\\), - The outcome regression \\(\\mathbb{E}[Y | D, X]\\).\nWhen we use flexible ML methods to estimate these, we risk regularization bias (from penalties or smoothing) and overfitting bias (if the same data is used both to estimate the nuisance functions and the causal parameter).\nDML solves this problem using two ideas.\n\n\nNeyman Orthogonality: Guarding Against Small Mistakes\nThe first ingredient is Neyman orthogonality. This property ensures that the score function \\(m(\\cdot)\\) is ‚Äúlocally insensitive‚Äù to small errors in the nuisance parameters. Formally, a score function \\(\\psi(W_i; \\theta, \\eta)\\) is Neyman orthogonal if:\n\\[\n\\frac{\\partial}{\\partial \\lambda} \\mathbb{E}[\\psi(W_i; \\theta_0, \\eta_0 + \\lambda (\\eta - \\eta_0))] \\Big|_{\\lambda=0} = 0.\n\\]\nIn other words, small perturbations in \\(\\eta\\) don‚Äôt affect the identifying moment condition at first order.\nFor the ATE, the doubly robust score satisfies Neyman orthogonality:\n\\[\n\\psi(W_i; \\theta, \\eta) = \\alpha(D_i, X_i)(Y_i - \\ell(D_i, X_i)) + \\ell(1, X_i) - \\ell(0, X_i) - \\theta,\n\\]\nwhere: - \\(\\alpha(D, X) = \\frac{D}{r(X)} - \\frac{1 - D}{1 - r(X)}\\), - \\(\\ell(D, X) = \\mathbb{E}[Y | D, X]\\).\nThis score remains stable even when \\(r(X)\\) or \\(\\ell(D, X)\\) are estimated imperfectly.\n\n\nCross-Fitting: Outsmarting Overfitting Bias\nThe second ingredient is cross-fitting, a clever form of sample splitting.\n\nSplit the data into \\(K\\) folds.\nEstimate nuisance functions \\(\\hat{\\eta}^{(-k)}\\) on the data excluding fold \\(k\\).\nPlug these estimates into the score function for observations in fold \\(k\\).\nCycle through all folds so that every observation is used for inference, but never for its own nuisance estimation.\n\nCross-fitting reduces dependence between nuisance estimation and target parameter estimation ‚Äî making overfitting bias negligible.\n\n\nEstimation Algorithm for Average Treatment Effects\nThe DML procedure for estimating the ATE proceeds as follows:\n\n\n\n\n\n\nAlgorithm: DML for ATE Estimation\n\n\n\n\nRandomly partition the data into \\(K\\) folds.\nFor each fold \\(k\\), estimate the propensity score \\(\\hat{r}^{(-k)}(X)\\) and outcome regressions \\(\\hat{\\ell}^{(-k)}(D, X)\\) using the data excluding fold \\(k\\).\nFor each observation in fold \\(k\\), compute the orthogonal score \\(\\psi(W_i; \\theta, \\hat{\\eta}^{(-k)})\\).\nSolve for \\(\\theta\\) such that the sample average of \\(\\psi\\) equals zero.\nEstimate variance and construct confidence intervals using standard errors based on the orthogonal scores."
  },
  {
    "objectID": "blog-unpublished/double-ml.html#an-example",
    "href": "blog-unpublished/double-ml.html#an-example",
    "title": "Double/Debiased ML for Causal Inference",
    "section": "An Example",
    "text": "An Example\n\nRPython\n\n\nlibrary(DoubleML)\nlibrary(mlr3)\nlibrary(mlr3learners)\n\nset.seed(42)\nn &lt;- 1000\nx &lt;- matrix(rnorm(n * 5), n, 5)\nd &lt;- rbinom(n, 1, plogis(x[,1]))\ny &lt;- 0.5 * d + x[,1] + rnorm(n)\n\ndata &lt;- data.frame(y = y, d = d, x)\nml_g &lt;- lrn(\"regr.rpart\")\nml_m &lt;- lrn(\"classif.rpart\", predict_type = \"prob\")\n\ndml_data &lt;- DoubleMLData$new(data, y_col = \"y\", d_cols = \"d\", x_cols = paste0(\"x\", 1:5))\ndml_plr &lt;- DoubleMLPLR$new(dml_data, ml_g, ml_m)\ndml_plr$fit()\ndml_plr$summary()\n\n\nimport numpy as np\nimport pandas as pd\nfrom doubleml import DoubleMLData, DoubleMLPLR\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nnp.random.seed(42)\nn = 1000\nX = np.random.randn(n, 5)\nD = np.random.binomial(1, p=1/(1 + np.exp(-X[:, 0])))\nY = 0.5 * D + X[:, 0] + np.random.randn(n)\n\ndata = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(5)])\ndata['d'] = D\ndata['y'] = Y\n\ndml_data = DoubleMLData.from_arrays(X=data[[f'x{i+1}' for i in range(5)]].values,\n                                    y=data['y'].values,\n                                    d=data['d'].values)\n\nml_g = RandomForestRegressor()\nml_m = RandomForestClassifier()\n\ndml_plr = DoubleMLPLR(dml_data, ml_g, ml_m)\ndml_plr.fit()\nprint(dml_plr.summary)"
  },
  {
    "objectID": "blog-unpublished/double-ml.html#bottom-line",
    "href": "blog-unpublished/double-ml.html#bottom-line",
    "title": "Double/Debiased ML for Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nDML enables valid causal inference even when nuisance functions are estimated with machine learning.\nNeyman orthogonality reduces sensitivity to small errors in nuisance estimation.\nCross-fitting prevents overfitting bias and stabilizes inference.\nDML offers a plug-and-play approach compatible with any ML method for nuisance functions."
  },
  {
    "objectID": "blog-unpublished/double-ml.html#where-to-learn-more",
    "href": "blog-unpublished/double-ml.html#where-to-learn-more",
    "title": "Double/Debiased ML for Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThe excellent review by Ahrens, Chernozhukov, Hansen, and others (2025) provides a clear and thorough introduction to DML. For implementation, the DoubleML package in R and Python offers flexible tools with built-in orthogonal scores and cross-fitting. Chernozhukov et al.¬†(2018) remains the foundational paper in this area."
  },
  {
    "objectID": "blog-unpublished/double-ml.html#references",
    "href": "blog-unpublished/double-ml.html#references",
    "title": "Double/Debiased ML for Causal Inference",
    "section": "References",
    "text": "References\n\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. Econometrics Journal, 21(1), C1‚ÄìC68.\nAhrens, A., Chernozhukov, V., Hansen, C., et al.¬†(2025). An Introduction to Double/Debiased Machine Learning."
  },
  {
    "objectID": "blog-unpublished/anytime-valid-p.html",
    "href": "blog-unpublished/anytime-valid-p.html",
    "title": "Anytime-Valid \\(p\\)-values and Online Multiple Testing: A Modern Guide",
    "section": "",
    "text": "In modern data analysis, we often don‚Äôt test all hypotheses at once. Instead, hypotheses arrive over time: an A/B test today, another one tomorrow, and so on. The problem? Traditional multiple testing corrections like the Bonferroni or Benjamini-Hochberg procedures assume you know the total number of hypotheses in advance. They were designed for batch testing, not for this streaming world.\nThis mismatch has serious consequences. If you naively apply these classical corrections each time a new test arrives, you‚Äôll either inflate your false discovery rate (FDR) or make your tests overly conservative, wasting precious power. Worse yet, if you monitor your \\(p\\)-values as data accumulate and stop when they look good (a practice called continuous monitoring or peeking), traditional \\(p\\)-values are no longer valid.\nThis is where anytime-valid \\(p\\)-values and online multiple testing methods come to the rescue. They let you monitor and make decisions at any time, without inflating Type I errors ‚Äî all while controlling error rates like the FDR.\nIn this article, we explain how anytime-valid inference works, why traditional \\(p\\)-values fail in online settings, and introduce the key algorithms developed over the last 15 years that allow principled, flexible testing when hypotheses arrive sequentially."
  },
  {
    "objectID": "blog-unpublished/anytime-valid-p.html#background",
    "href": "blog-unpublished/anytime-valid-p.html#background",
    "title": "Anytime-Valid \\(p\\)-values and Online Multiple Testing: A Modern Guide",
    "section": "",
    "text": "In modern data analysis, we often don‚Äôt test all hypotheses at once. Instead, hypotheses arrive over time: an A/B test today, another one tomorrow, and so on. The problem? Traditional multiple testing corrections like the Bonferroni or Benjamini-Hochberg procedures assume you know the total number of hypotheses in advance. They were designed for batch testing, not for this streaming world.\nThis mismatch has serious consequences. If you naively apply these classical corrections each time a new test arrives, you‚Äôll either inflate your false discovery rate (FDR) or make your tests overly conservative, wasting precious power. Worse yet, if you monitor your \\(p\\)-values as data accumulate and stop when they look good (a practice called continuous monitoring or peeking), traditional \\(p\\)-values are no longer valid.\nThis is where anytime-valid \\(p\\)-values and online multiple testing methods come to the rescue. They let you monitor and make decisions at any time, without inflating Type I errors ‚Äî all while controlling error rates like the FDR.\nIn this article, we explain how anytime-valid inference works, why traditional \\(p\\)-values fail in online settings, and introduce the key algorithms developed over the last 15 years that allow principled, flexible testing when hypotheses arrive sequentially."
  },
  {
    "objectID": "blog-unpublished/anytime-valid-p.html#notation",
    "href": "blog-unpublished/anytime-valid-p.html#notation",
    "title": "Anytime-Valid \\(p\\)-values and Online Multiple Testing: A Modern Guide",
    "section": "Notation",
    "text": "Notation\nAt time \\(t\\), you face hypothesis \\(H_t\\), with a \\(p\\)-value \\(P_t\\). You must decide whether to reject \\(H_t\\) before seeing \\(H_{t+1}\\). Importantly: - You don‚Äôt know how many hypotheses will arrive in total. - Your decision \\(R_t\\) (reject or not) may depend on past decisions \\(R_1, \\dots, R_{t-1}\\).\nThe false discovery proportion (FDP) at time \\(T\\) is:\n\\[\n\\text{FDP}(T) = \\frac{V(T)}{R(T) \\vee 1},\n\\]\nwhere \\(V(T)\\) is the number of false rejections and \\(R(T)\\) is the total number of rejections. The false discovery rate (FDR) is the expectation of this quantity:\n\\[\n\\text{FDR}(T) = \\mathbb{E}\\left[ \\frac{V(T)}{R(T) \\vee 1} \\right].\n\\]\nAn anytime-valid \\(p\\)-value is a \\(p\\)-value that remains valid no matter when you stop ‚Äî even if you peek at the data continuously."
  },
  {
    "objectID": "blog-unpublished/anytime-valid-p.html#a-closer-look",
    "href": "blog-unpublished/anytime-valid-p.html#a-closer-look",
    "title": "Anytime-Valid \\(p\\)-values and Online Multiple Testing: A Modern Guide",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhy Traditional \\(p\\)-values Fail in Sequential Testing\nTraditional \\(p\\)-values assume a fixed sample size and no interim monitoring. If you check the \\(p\\)-value repeatedly and stop as soon as it crosses a threshold, you‚Äôre implicitly running a sequential test without adjusting for multiple looks.\nThis leads to inflated Type I error rates because the chance of eventually seeing a small \\(p\\)-value grows with each additional look. The \\(p\\)-value you see is no longer uniformly distributed under the null.\nAnytime-valid \\(p\\)-values solve this by ensuring validity at every possible stopping time. Formally, for any stopping rule \\(N\\):\n\\[\n\\Pr(P_{t, N} \\leq x) \\leq x \\quad \\text{for all } x \\in [0,1].\n\\]\n\n\nThe Rise of Online Multiple Testing\nOnce we have anytime-valid \\(p\\)-values, we can build online multiple testing procedures that control the FDR over time. Here, you never know the total number of hypotheses upfront ‚Äî hypotheses just keep arriving.\nThe pioneering work of Foster and Stine (2008) introduced the concept of alpha-investing, a method that allocates an ‚Äúalpha-wealth‚Äù budget across tests. Rejections replenish this budget, allowing more liberal testing when discoveries accumulate.\nSubsequent work refined these ideas into powerful modern algorithms:\n\n1. LORD++ (Javanmard and Montanari, Ramdas et al.)\nLORD++ stands for Levels based On Recent Discovery, a monotone GAI++ (Generalized Alpha Investing) rule. It updates testing levels dynamically based on previous rejections, preventing the test levels from collapsing as more tests accumulate.\n\n\n2. SAFFRON (Ramdas et al., 2018)\nAn adaptive procedure that estimates the proportion of null hypotheses and avoids wasting testing budget on weak signals. SAFFRON focuses alpha-wealth on promising \\(p\\)-values, improving power especially when there are many non-nulls.\n\n\n3. ADDIS (Tian and Ramdas, 2019)\nADDIS stands for ADaptive algorithm that DIScards conservative nulls. It further improves power by discarding \\(p\\)-values that are unlikely to be rejected anyway (e.g., large \\(p\\)-values that suggest conservative nulls).\n\n\n\nAnytime-valid \\(p\\)-values and Confidence Sequences\nAnytime-valid \\(p\\)-values often arise from confidence sequences ‚Äî time-uniform confidence intervals that hold at every sample size. These are built using techniques like:\n\nMartingale-based betting strategies (e.g., e-processes).\nConcentration inequalities (e.g., Hoeffding‚Äôs inequality for bounded data).\n\nFor example, in A/B testing, always-valid \\(p\\)-values allow you to continuously monitor the success rate difference between two groups without inflating your Type I error."
  },
  {
    "objectID": "blog-unpublished/anytime-valid-p.html#bottom-line",
    "href": "blog-unpublished/anytime-valid-p.html#bottom-line",
    "title": "Anytime-Valid \\(p\\)-values and Online Multiple Testing: A Modern Guide",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nTraditional \\(p\\)-values are not valid under continuous monitoring or online testing.\nAnytime-valid \\(p\\)-values remain valid no matter when you stop.\nOnline multiple testing algorithms like LORD++, SAFFRON, and ADDIS enable FDR control when hypotheses arrive sequentially.\nAdaptive algorithms improve power by estimating the fraction of nulls and focusing resources on likely non-nulls."
  },
  {
    "objectID": "blog-unpublished/anytime-valid-p.html#where-to-learn-more",
    "href": "blog-unpublished/anytime-valid-p.html#where-to-learn-more",
    "title": "Anytime-Valid \\(p\\)-values and Online Multiple Testing: A Modern Guide",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThe paper by Robertson, Wason, and Ramdas (2023) offers an excellent review of the literature on online error rate control, including algorithmic details, proofs, and simulation results. For a deep dive into anytime-valid \\(p\\)-values and confidence sequences, see Howard et al.¬†(2021) and the always-valid inference literature."
  },
  {
    "objectID": "blog-unpublished/anytime-valid-p.html#references",
    "href": "blog-unpublished/anytime-valid-p.html#references",
    "title": "Anytime-Valid \\(p\\)-values and Online Multiple Testing: A Modern Guide",
    "section": "References",
    "text": "References\n\nRobertson, D. S., Wason, J. M. S., & Ramdas, A. (2023). Online Multiple Hypothesis Testing. Statistical Science, 38(4), 557‚Äì575.\nFoster, D. P., & Stine, R. A. (2008). Alpha-Investing: A Procedure for Sequential Control of Expected False Discoveries. Journal of the Royal Statistical Society: Series B, 70(2), 429‚Äì444.\nRamdas, A., et al.¬†(2018). SAFFRON: An Adaptive Algorithm for Online Control of the False Discovery Rate. Journal of the Royal Statistical Society: Series B, 80(5), 1225‚Äì1248.\nTian, J., & Ramdas, A. (2019). ADDIS: An Adaptive Discarding Algorithm for Online FDR Control. International Conference on Artificial Intelligence and Statistics (AISTATS).\nHoward, S. R., Ramdas, A., McAuliffe, J. D., & Sekhon, J. S. (2021). Time-Uniform Chernoff Bounds via Nonnegative Supermartingales. Probability Surveys, 18, 1‚Äì29."
  },
  {
    "objectID": "blog-unpublished/lasso-flavors.html",
    "href": "blog-unpublished/lasso-flavors.html",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "",
    "text": "The Lasso (Least Absolute Shrinkage and Selection Operator), introduced by Tibshirani in 1996, has become one of the go-to tools for variable selection and shrinkage in regression problems. But the classic Lasso is just the starting point. Over the years, researchers have developed many variants of Lasso, each designed to address specific limitations or tailor the method to different kinds of data structures.\nThis article provides a tour of the most popular flavors of Lasso ‚Äî from standard L1-penalized regression to modern adaptations like Adaptive Lasso, Elastic Net, Square-root Lasso, and more. For each version, we‚Äôll lay out the objective function, describe when it‚Äôs applicable, and summarize its key characteristics."
  },
  {
    "objectID": "blog-unpublished/lasso-flavors.html#background",
    "href": "blog-unpublished/lasso-flavors.html#background",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "",
    "text": "The Lasso (Least Absolute Shrinkage and Selection Operator), introduced by Tibshirani in 1996, has become one of the go-to tools for variable selection and shrinkage in regression problems. But the classic Lasso is just the starting point. Over the years, researchers have developed many variants of Lasso, each designed to address specific limitations or tailor the method to different kinds of data structures.\nThis article provides a tour of the most popular flavors of Lasso ‚Äî from standard L1-penalized regression to modern adaptations like Adaptive Lasso, Elastic Net, Square-root Lasso, and more. For each version, we‚Äôll lay out the objective function, describe when it‚Äôs applicable, and summarize its key characteristics."
  },
  {
    "objectID": "blog-unpublished/lasso-flavors.html#a-closer-look",
    "href": "blog-unpublished/lasso-flavors.html#a-closer-look",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "A Closer Look",
    "text": "A Closer Look\n\n1. Standard Lasso (Tibshirani, 1996)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right)\n\\]\n\nKey feature: Encourages sparsity by shrinking some coefficients exactly to zero.\nWhen to use: Variable selection with many predictors; handles high-dimensional data.\nCharacteristics: Shrinkage bias; struggles with highly correlated predictors.\n\n\n\n2. Adaptive Lasso (Zou, 2006)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^p w_j | \\beta_j | \\right)\n\\] where \\(w_j = 1 / |\\hat{\\beta}_j^{\\text{init}}|^\\gamma\\).\n\nKey feature: Oracle property (support recovery consistency).\nWhen to use: When initial estimates (like OLS or Ridge) are available.\nCharacteristics: Bias reduction via adaptive weights.\n\n\n\n3. Relaxed Lasso (Meinshausen, 2007)\n\nStep 1: Use Lasso for selection.\nStep 2: Refit OLS on the selected variables or apply partial shrinkage with a relaxation parameter \\(\\phi\\).\nKey feature: Separates selection from estimation.\nWhen to use: Reduce Lasso‚Äôs shrinkage bias after variable selection.\nCharacteristics: Combines selection strength of Lasso with unbiased estimation of OLS.\n\n\n\n4. Square-root Lasso / Scaled Lasso (Belloni, Chernozhukov, Wang, 2011)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{\\sqrt{n}} \\| y - X \\beta \\|_2 + \\lambda \\| \\beta \\|_1 \\right)\n\\]\n\nKey feature: Scale-invariant ‚Äî does not require estimating error variance.\nWhen to use: Unknown or heteroskedastic error variance.\nCharacteristics: Easier tuning; robust to variance misspecification.\n\n\n\n5. Elastic Net (Zou and Hastie, 2005)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\| \\beta \\|_2^2 \\right)\n\\]\n\nKey feature: Combines L1 and L2 penalties.\nWhen to use: Multicollinearity among predictors.\nCharacteristics: Handles correlated variables better than standard Lasso.\n\n\n\n6. Group Lasso (Yuan & Lin, 2006)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{g=1}^G \\| \\beta^{(g)} \\|_2 \\right)\n\\]\n\nKey feature: Selects groups of variables together.\nWhen to use: Categorical variables or grouped data structures.\nCharacteristics: Encourages sparsity at the group level, not individual coefficients.\n\n\n\n7. Fused Lasso (Tibshirani et al., 2005)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\sum_{j=2}^p | \\beta_j - \\beta_{j-1} | \\right)\n\\]\n\nKey feature: Penalizes differences between adjacent coefficients.\nWhen to use: Ordered features like time series or spatial data.\nCharacteristics: Promotes both sparsity and smoothness.\n\n\n\n8. Bayesian Lasso (Park & Casella, 2008)\n\nObjective (via prior): \\[\n\\beta_j \\sim \\text{Laplace}(0, \\lambda^{-1}).\n\\]\nKey feature: Fully Bayesian formulation with posterior inference.\nWhen to use: When uncertainty quantification (credible intervals) is desired.\nCharacteristics: Shrinkage through Laplace priors; allows full posterior analysis.\n\n\n\n9. Graphical Lasso (Friedman et al., 2008)\nObjective Function: \\[\n\\hat{\\Theta} = \\arg \\min_{\\Theta \\succ 0} \\left( -\\log \\det \\Theta + \\text{trace}(S \\Theta) + \\lambda \\| \\Theta \\|_1 \\right)\n\\]\n\nKey feature: Estimates sparse precision (inverse covariance) matrices.\nWhen to use: Gaussian graphical models (network structure estimation).\nCharacteristics: Encourages sparsity in partial correlations.\n\n\n\n10. Stability Selection (Meinshausen & B√ºhlmann, 2010)\n\nNot an objective function per se ‚Äî combines subsampling with Lasso to control false discovery rate.\nKey feature: Improves selection stability and robustness.\nWhen to use: When worried about unstable variable selection.\nCharacteristics: Reduces false positives; provides error control guarantees.\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\nVariant\nKey Feature\nMotivation\n\n\n\n\nStandard Lasso\nL1 penalty, sparsity\nVariable selection\n\n\nAdaptive Lasso\nWeighted penalties, oracle property\nBias reduction\n\n\nElastic Net\nCombines L1 and L2 penalties\nHandles multicollinearity\n\n\nGroup Lasso\nGroup-wise selection\nGrouped variables\n\n\nFused Lasso\nPenalizes differences between coefficients\nTime-series or spatial data\n\n\nSquare-root Lasso\nScale-free loss function\nUnknown error variance\n\n\nBayesian Lasso\nLaplace priors in a Bayesian framework\nPosterior inference, uncertainty\n\n\nGraphical Lasso\nPenalizes inverse covariance matrix\nGaussian graphical models\n\n\nRelaxed Lasso\nSelection and estimation separated\nBias reduction after selection\n\n\nStability Selection\nAdds subsampling for robustness\nControls false discoveries"
  },
  {
    "objectID": "blog-unpublished/lasso-flavors.html#summary-table",
    "href": "blog-unpublished/lasso-flavors.html#summary-table",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\nVariant\nKey Feature\nMotivation\n\n\n\n\nStandard Lasso\nL1 penalty, sparsity\nVariable selection\n\n\nAdaptive Lasso\nWeighted penalties, oracle property\nBias reduction\n\n\nElastic Net\nCombines L1 and L2 penalties\nHandles multicollinearity\n\n\nGroup Lasso\nGroup-wise selection\nGrouped variables\n\n\nFused Lasso\nPenalizes differences between coefficients\nTime-series or spatial data\n\n\nSquare-root Lasso\nScale-free loss function\nUnknown error variance\n\n\nBayesian Lasso\nLaplace priors in a Bayesian framework\nPosterior inference, uncertainty\n\n\nGraphical Lasso\nPenalizes inverse covariance matrix\nGaussian graphical models\n\n\nRelaxed Lasso\nSelection and estimation separated\nBias reduction after selection\n\n\nStability Selection\nAdds subsampling for robustness\nControls false discoveries"
  },
  {
    "objectID": "blog-unpublished/lasso-flavors.html#bottom-line",
    "href": "blog-unpublished/lasso-flavors.html#bottom-line",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nDifferent Lasso variants address different modeling challenges like scaling, grouping, collinearity, and bias.\nUnderstanding your data structure and inference goals helps choose the right Lasso flavor.\nMost major machine learning libraries (R: glmnet, grpreg; Python: scikit-learn, statsmodels) provide support for these variants."
  },
  {
    "objectID": "blog-unpublished/lasso-flavors.html#where-to-learn-more",
    "href": "blog-unpublished/lasso-flavors.html#where-to-learn-more",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nKey papers: Tibshirani (1996), Zou (2006), Meinshausen (2007), Yuan and Lin (2006), Belloni et al.¬†(2011), Park and Casella (2008), Friedman et al.¬†(2008), Meinshausen and B√ºhlmann (2010)."
  },
  {
    "objectID": "blog-unpublished/lasso-flavors.html#references",
    "href": "blog-unpublished/lasso-flavors.html#references",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "References",
    "text": "References\n[TO ADD]"
  },
  {
    "objectID": "blog-unpublished/lasso-theory-properties.html",
    "href": "blog-unpublished/lasso-theory-properties.html",
    "title": "Theoretical Properties of Lasso",
    "section": "",
    "text": "Lasso (Least Absolute Shrinkage and Selection Operator) is widely used not only for its variable selection ability but also for its appealing theoretical properties in high-dimensional regression. Over the past two decades, a rich literature has characterized the behavior of Lasso estimators under various conditions ‚Äî especially when the number of predictors \\(p\\) may exceed the number of observations \\(n\\).\nThis article walks through the key theoretical properties of Lasso, explaining what they mean, when they hold, and why they matter for estimation, prediction, and inference."
  },
  {
    "objectID": "blog-unpublished/lasso-theory-properties.html#background",
    "href": "blog-unpublished/lasso-theory-properties.html#background",
    "title": "Theoretical Properties of Lasso",
    "section": "",
    "text": "Lasso (Least Absolute Shrinkage and Selection Operator) is widely used not only for its variable selection ability but also for its appealing theoretical properties in high-dimensional regression. Over the past two decades, a rich literature has characterized the behavior of Lasso estimators under various conditions ‚Äî especially when the number of predictors \\(p\\) may exceed the number of observations \\(n\\).\nThis article walks through the key theoretical properties of Lasso, explaining what they mean, when they hold, and why they matter for estimation, prediction, and inference."
  },
  {
    "objectID": "blog-unpublished/lasso-theory-properties.html#a-closer-look",
    "href": "blog-unpublished/lasso-theory-properties.html#a-closer-look",
    "title": "Theoretical Properties of Lasso",
    "section": "A Closer Look",
    "text": "A Closer Look\n\n1. Convergence Rates (Estimation Consistency)\nUnder sparsity and restricted eigenvalue conditions: \\[\n\\| \\hat{\\beta} - \\beta_0 \\|_1 = O_p\\left( s \\sqrt{ \\frac{\\log p}{n} } \\right),\n\\] where \\(s\\) is the sparsity level (number of true nonzero coefficients).\nPrediction error: \\[\n\\| X ( \\hat{\\beta} - \\beta_0 ) \\|_2^2 / n = O_p\\left( s \\frac{\\log p}{n} \\right).\n\\] This is near-oracle optimal up to log factors.\n\n\n2. Support Recovery (Sparsistency)\nLasso can recover the correct set of nonzero coefficients with high probability (sparsistency) if: - The irrepresentable condition holds (Zhao and Yu, 2006), - Signal strength is sufficiently large.\nOtherwise, Lasso may fail to perfectly select the true model, especially under high correlations or weak signals.\n\n\n3. Prediction Consistency\nLasso can achieve prediction consistency: \\[\n\\mathbb{E} \\left[ \\| X (\\hat{\\beta} - \\beta_0) \\|_2^2 \\right] \\to 0 \\quad \\text{as} \\quad n \\to \\infty,\n\\] even if exact support recovery fails.\n\n\n4. Oracle Inequalities\nLasso satisfies oracle inequalities comparing its performance to an ideal estimator that knows the true support: \\[\n\\| X (\\hat{\\beta} - \\beta_0) \\|_2^2 / n \\leq C \\cdot \\inf_{\\beta: \\| \\beta \\|_0 \\leq s} \\left\\{ \\| X (\\beta - \\beta_0) \\|_2^2 / n + \\lambda^2 s \\right\\}.\n\\]\n\n\n5. Bias of the Estimates\nLasso is biased due to the L1 penalty. The bias remains unless the penalty \\(\\lambda\\) shrinks appropriately with \\(n\\).\nBias motivates the use of: - Post-Lasso OLS, - Relaxed Lasso, - Adaptive Lasso, - Debiased Lasso.\n\n\n6. Asymptotic Normality (Debiased Lasso)\nStandard Lasso does not yield asymptotically normal estimates. However, Debiased (Desparsified) Lasso allows for valid inference: \\[\n\\sqrt{n} (\\hat{\\beta}_j^{\\text{debiased}} - \\beta_{0j}) \\overset{d}{\\to} N(0, \\sigma_j^2).\n\\]\n\n\n7. Valid Inference and Confidence Intervals\nValid \\(p\\)-values and confidence intervals require adjustments: - Debiased Lasso, - Selective inference (e.g., Lee et al., 2016), - Double selection (Belloni et al., 2014).\n\n\n8. Double Robustness and Semi-parametric Efficiency\nIn causal inference settings (e.g., DML), Lasso can estimate high-dimensional nuisance components, yielding: - \\(\\sqrt{n}\\)-consistent estimates for low-dimensional targets, - Asymptotic normality under orthogonality and cross-fitting."
  },
  {
    "objectID": "blog-unpublished/lasso-theory-properties.html#summary-table",
    "href": "blog-unpublished/lasso-theory-properties.html#summary-table",
    "title": "Theoretical Properties of Lasso",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\nProperty\nAchieved by Lasso?\nConditions Required\n\n\n\n\nConvergence rates (L1/L2)\nYes, near-oracle (up to log factor)\nSparsity, restricted eigenvalues\n\n\nSupport recovery (sparsistency)\nSometimes (hard in correlated designs)\nIrrepresentable condition or compatibility\n\n\nPrediction consistency\nYes\nRestricted eigenvalues, compatibility\n\n\nOracle inequality\nYes\nStandard sparsity assumptions\n\n\nBias\nYes (biased toward zero)\nBias remains unless corrected\n\n\nAsymptotic normality\nNo (unless debiased)\nDebiasing, desparsification required\n\n\nValid inference (CI, \\(p\\)-values)\nNot directly (needs debiased Lasso)\nDebiased Lasso, post-selection inference\n\n\nDouble robustness / efficiency\nOnly when combined (e.g., DML)\nOrthogonal scores, cross-fitting"
  },
  {
    "objectID": "blog-unpublished/lasso-theory-properties.html#bottom-line",
    "href": "blog-unpublished/lasso-theory-properties.html#bottom-line",
    "title": "Theoretical Properties of Lasso",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLasso achieves strong theoretical guarantees for estimation and prediction under sparsity.\nSupport recovery requires stringent conditions, and Lasso may miss variables under collinearity.\nBias correction techniques like debiasing or post-selection OLS help achieve valid inference.\nDouble/debiased machine learning and orthogonalization frameworks leverage Lasso for high-dimensional nuisance estimation."
  },
  {
    "objectID": "blog-unpublished/lasso-theory-properties.html#where-to-learn-more",
    "href": "blog-unpublished/lasso-theory-properties.html#where-to-learn-more",
    "title": "Theoretical Properties of Lasso",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nKey papers include Tibshirani (1996), Zhao and Yu (2006), Bickel et al.¬†(2009), van de Geer et al.¬†(2014), Javanmard and Montanari (2014), and Belloni et al.¬†(2014). For debiased methods and selective inference, see recent reviews by B√ºhlmann and van de Geer."
  },
  {
    "objectID": "blog-unpublished/lasso-theory-properties.html#references",
    "href": "blog-unpublished/lasso-theory-properties.html#references",
    "title": "Theoretical Properties of Lasso",
    "section": "References",
    "text": "References\n[TO ADD]"
  },
  {
    "objectID": "blog-unpublished/pca-flavors.html",
    "href": "blog-unpublished/pca-flavors.html",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "",
    "text": "Principal Components Analysis (PCA) is one of the most widely used techniques for dimensionality reduction and unsupervised learning. Originally introduced by Karl Pearson in 1901 and later formalized by Harold Hotelling in 1933, PCA aims to find new, uncorrelated variables (the principal components) that successively maximize the variance in the data. It provides a systematic way to summarize high-dimensional datasets with fewer dimensions, making it easier to visualize, compress, or preprocess data for further modeling.\nBeyond standard PCA, many variants and extensions have been developed to handle specific challenges like sparsity, robustness to outliers, missing data, and non-linear structures. These PCA flavors extend the core idea to different contexts, making PCA a versatile workhorse in both theory and practice.\nIn this article, we‚Äôll explain the classical PCA approach thoroughly and then introduce several important PCA variants, providing their mathematical formulations and discussing when and why they are useful."
  },
  {
    "objectID": "blog-unpublished/pca-flavors.html#background",
    "href": "blog-unpublished/pca-flavors.html#background",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "",
    "text": "Principal Components Analysis (PCA) is one of the most widely used techniques for dimensionality reduction and unsupervised learning. Originally introduced by Karl Pearson in 1901 and later formalized by Harold Hotelling in 1933, PCA aims to find new, uncorrelated variables (the principal components) that successively maximize the variance in the data. It provides a systematic way to summarize high-dimensional datasets with fewer dimensions, making it easier to visualize, compress, or preprocess data for further modeling.\nBeyond standard PCA, many variants and extensions have been developed to handle specific challenges like sparsity, robustness to outliers, missing data, and non-linear structures. These PCA flavors extend the core idea to different contexts, making PCA a versatile workhorse in both theory and practice.\nIn this article, we‚Äôll explain the classical PCA approach thoroughly and then introduce several important PCA variants, providing their mathematical formulations and discussing when and why they are useful."
  },
  {
    "objectID": "blog-unpublished/pca-flavors.html#notation",
    "href": "blog-unpublished/pca-flavors.html#notation",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "Notation",
    "text": "Notation\nLet \\(X \\in \\mathbb{R}^{n \\times p}\\) be a data matrix with: - \\(n\\) observations (rows), - \\(p\\) variables (columns).\nAssume that each column of \\(X\\) has been centered (mean zero): \\[\n\\frac{1}{n} \\sum_{i=1}^n X_{ij} = 0 \\quad \\text{for all} \\ j = 1, \\dots, p.\n\\]\nThe empirical covariance matrix of \\(X\\) is: \\[\n\\Sigma = \\frac{1}{n} X^T X.\n\\]\nOur goal is to find new orthogonal directions (principal components) \\(u_1, \\dots, u_p \\in \\mathbb{R}^p\\), such that projecting the data onto these directions captures the maximum variance."
  },
  {
    "objectID": "blog-unpublished/pca-flavors.html#a-closer-look",
    "href": "blog-unpublished/pca-flavors.html#a-closer-look",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nClassical PCA: Variance Maximization and Eigen Decomposition\nThe principal components are obtained by solving the following optimization problem: \\[\n\\max_{u \\in \\mathbb{R}^p} u^T \\Sigma u \\quad \\text{subject to} \\quad \\| u \\|_2 = 1.\n\\]\nThis is a Rayleigh quotient maximization problem, whose solution is the eigenvector of \\(\\Sigma\\) corresponding to the largest eigenvalue. The eigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0\\) represent the variances explained by each principal component.\n\nSequential Extraction\nThe \\(k\\)-th principal component direction \\(u_k\\) is obtained by solving: \\[\n\\max_{u} u^T \\Sigma u, \\quad \\text{subject to} \\quad \\| u \\|_2 = 1, \\quad u^T u_j = 0 \\quad \\text{for} \\ j = 1, \\dots, k-1.\n\\]\nThe principal components themselves (the transformed data) are: \\[\nZ = X U,\n\\] where \\(U = [u_1, u_2, \\dots, u_p]\\) is the matrix of eigenvectors.\n\n\nSingular Value Decomposition (SVD) Formulation\nPCA can also be performed via Singular Value Decomposition (SVD) of the centered data matrix \\(X\\): \\[\nX = U D V^T,\n\\] where: - \\(U \\in \\mathbb{R}^{n \\times p}\\) contains the left singular vectors, - \\(D \\in \\mathbb{R}^{p \\times p}\\) is diagonal with singular values, - \\(V \\in \\mathbb{R}^{p \\times p}\\) contains the right singular vectors (principal component directions).\nThe columns of \\(V\\) are the eigenvectors of \\(\\Sigma\\), and the squared singular values \\(D^2 / n\\) are the eigenvalues.\n\n\nVariance Explained\nThe proportion of variance explained by the first \\(k\\) components is: \\[\n\\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^p \\lambda_j}.\n\\]\nPCA allows data compression by projecting onto the first \\(k\\) components while retaining most of the variance.\nWhile classical PCA provides a powerful linear dimensionality reduction tool, it has several limitations: it does not handle sparsity well, it is sensitive to outliers, and it only captures linear relationships. Over the years, many extensions of PCA have been developed to address these challenges. Below we discuss several of the most popular PCA variants.\n\n\n\nSparse PCA (Zou, Hastie, Tibshirani, 2006)\nKey Idea:\nEncourage sparsity in the principal component loading vectors to improve interpretability.\nObjective Function (simplified): \\[\n\\max_{u} \\quad u^T \\Sigma u - \\lambda \\| u \\|_1 \\quad \\text{subject to} \\quad \\| u \\|_2 = 1.\n\\]\nWhen to Use:\n\nWhen you expect only a subset of variables to be important in each component.\nUseful in high-dimensional settings like genomics, image processing, and text analysis.\n\nCharacteristics:\n\nEnhances interpretability by producing sparse loadings.\nRetains much of the variance while simplifying the component structure.\n\n\n\n\nKernel PCA (Sch√∂lkopf et al., 1998)\nKey Idea:\nMap the data into a higher-dimensional feature space using a nonlinear kernel and then apply PCA in that space.\nObjective Function:\nStandard PCA applied to the kernel matrix: \\[\nK_{ij} = k(x_i, x_j),\n\\] where \\(k(\\cdot, \\cdot)\\) is a positive-definite kernel function (e.g., RBF, polynomial).\nWhen to Use:\n\nWhen the data exhibit nonlinear structures that linear PCA cannot capture.\nPopular in pattern recognition, computer vision, and bioinformatics.\n\nCharacteristics:\n\nCaptures nonlinear relationships between variables.\nChoice of kernel critically affects performance.\n\n\n\n\nRobust PCA (Candes et al., 2011)\nKey Idea:\nDecompose the data matrix \\(X\\) into a low-rank component \\(L\\) and a sparse outlier component \\(S\\): \\[\n\\min_{L, S} \\| L \\|_* + \\lambda \\| S \\|_1 \\quad \\text{subject to} \\quad X = L + S.\n\\] where \\(\\| L \\|_*\\) is the nuclear norm (sum of singular values).\nWhen to Use:\n\nWhen the data contain gross outliers or corruptions.\nCommon in computer vision (background subtraction), video surveillance, and recommender systems.\n\nCharacteristics:\n\nMore robust to outliers than classical PCA.\nSeparates structured low-rank signals from sparse noise.\n\n\n\n\nProbabilistic PCA (Tipping and Bishop, 1999)\nKey Idea:\nReformulate PCA as a latent variable model with Gaussian noise: \\[\nx_i = W z_i + \\mu + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2 I),\n\\] where \\(z_i\\) are latent factors.\nWhen to Use:\n\nWhen modeling uncertainty and likelihood is important.\nAllows probabilistic interpretation and missing data handling.\n\nCharacteristics:\n\nMaximum likelihood estimation provides the same solution as classical PCA in the limit.\nNaturally extends to mixture models and Bayesian frameworks.\n\n\n\n\nTruncated SVD (a Computational Variant)\nKey Idea:\nUse the first \\(k\\) singular vectors from the SVD of \\(X\\) without computing the full decomposition: \\[\nX \\approx U_k D_k V_k^T.\n\\]\nWhen to Use:\n\nVery large-scale data (e.g., text mining, collaborative filtering).\nWhen speed and memory efficiency are critical.\n\nCharacteristics:\n\nOften implemented using randomized algorithms.\nComputationally efficient for sparse or massive datasets.\n\n\n\n\nNonnegative Matrix Factorization (NMF) (Lee and Seung, 1999)\nKey Idea:\nDecompose the data into nonnegative factors: \\[\nX \\approx WH, \\quad W, H \\geq 0.\n\\]\nWhen to Use:\n\nNonnegative data (e.g., image pixels, word counts).\nWhen parts-based or additive decompositions are meaningful.\n\nCharacteristics:\n\nUnlike PCA, does not produce orthogonal components.\nOften yields interpretable, parts-based representations.\n\n\n\n\nIndependent Component Analysis (ICA)\nKey Idea:\nFind components that are statistically independent, not just uncorrelated: \\[\nX = A S,\n\\] where \\(S\\) contains independent components.\nWhen to Use:\n\nWhen underlying sources are assumed to be independent (e.g., EEG signal separation).\nSuitable for blind source separation problems.\n\nCharacteristics:\n\nGoes beyond PCA by removing higher-order dependencies.\nSensitive to scaling and noise.\n\n\nThese PCA variants allow the core idea of variance decomposition to be adapted to a wide variety of practical problems, whether by enforcing sparsity, allowing for nonlinearity, handling outliers, or modeling uncertainty."
  },
  {
    "objectID": "blog-unpublished/pca-flavors.html#pca-variants-and-extensions",
    "href": "blog-unpublished/pca-flavors.html#pca-variants-and-extensions",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "PCA Variants and Extensions",
    "text": "PCA Variants and Extensions\nWhile classical PCA provides a powerful linear dimensionality reduction tool, it has several limitations: it does not handle sparsity well, it is sensitive to outliers, and it only captures linear relationships. Over the years, many extensions of PCA have been developed to address these challenges. Below we discuss several of the most popular PCA variants.\n\nSparse PCA (Zou, Hastie, Tibshirani, 2006)\nKey Idea:\nEncourage sparsity in the principal component loading vectors to improve interpretability.\nObjective Function (simplified): \\[\n\\max_{u} \\quad u^T \\Sigma u - \\lambda \\| u \\|_1 \\quad \\text{subject to} \\quad \\| u \\|_2 = 1.\n\\]\nWhen to Use:\n- When you expect only a subset of variables to be important in each component. - Useful in high-dimensional settings like genomics, image processing, and text analysis.\nCharacteristics:\n- Enhances interpretability by producing sparse loadings. - Retains much of the variance while simplifying the component structure.\n\n\n\nKernel PCA (Sch√∂lkopf et al., 1998)\nKey Idea:\nMap the data into a higher-dimensional feature space using a nonlinear kernel and then apply PCA in that space.\nObjective Function:\nStandard PCA applied to the kernel matrix: \\[\nK_{ij} = k(x_i, x_j),\n\\] where \\(k(\\cdot, \\cdot)\\) is a positive-definite kernel function (e.g., RBF, polynomial).\nWhen to Use:\n- When the data exhibit nonlinear structures that linear PCA cannot capture. - Popular in pattern recognition, computer vision, and bioinformatics.\nCharacteristics:\n- Captures nonlinear relationships between variables. - Choice of kernel critically affects performance.\n\n\n\nRobust PCA (Candes et al., 2011)\nKey Idea:\nDecompose the data matrix \\(X\\) into a low-rank component \\(L\\) and a sparse outlier component \\(S\\): \\[\n\\min_{L, S} \\| L \\|_* + \\lambda \\| S \\|_1 \\quad \\text{subject to} \\quad X = L + S.\n\\] where \\(\\| L \\|_*\\) is the nuclear norm (sum of singular values).\nWhen to Use:\n- When the data contain gross outliers or corruptions. - Common in computer vision (background subtraction), video surveillance, and recommender systems.\nCharacteristics:\n- More robust to outliers than classical PCA. - Separates structured low-rank signals from sparse noise.\n\n\n\nProbabilistic PCA (Tipping and Bishop, 1999)\nKey Idea:\nReformulate PCA as a latent variable model with Gaussian noise: \\[\nx_i = W z_i + \\mu + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2 I),\n\\] where \\(z_i\\) are latent factors.\nWhen to Use:\n- When modeling uncertainty and likelihood is important. - Allows probabilistic interpretation and missing data handling.\nCharacteristics:\n- Maximum likelihood estimation provides the same solution as classical PCA in the limit. - Naturally extends to mixture models and Bayesian frameworks.\n\n\n\nTruncated SVD (a Computational Variant)\nKey Idea:\nUse the first \\(k\\) singular vectors from the SVD of \\(X\\) without computing the full decomposition: \\[\nX \\approx U_k D_k V_k^T.\n\\]\nWhen to Use:\n- Very large-scale data (e.g., text mining, collaborative filtering). - When speed and memory efficiency are critical.\nCharacteristics:\n- Often implemented using randomized algorithms. - Computationally efficient for sparse or massive datasets.\n\n\n\nNonnegative Matrix Factorization (NMF) (Lee and Seung, 1999)\nKey Idea:\nDecompose the data into nonnegative factors: \\[\nX \\approx WH, \\quad W, H \\geq 0.\n\\]\nWhen to Use:\n- Nonnegative data (e.g., image pixels, word counts). - When parts-based or additive decompositions are meaningful.\nCharacteristics:\n- Unlike PCA, does not produce orthogonal components. - Often yields interpretable, parts-based representations.\n\n\n\nIndependent Component Analysis (ICA)\nKey Idea:\nFind components that are statistically independent, not just uncorrelated: \\[\nX = A S,\n\\] where \\(S\\) contains independent components.\nWhen to Use:\n- When underlying sources are assumed to be independent (e.g., EEG signal separation). - Suitable for blind source separation problems.\nCharacteristics:\n- Goes beyond PCA by removing higher-order dependencies. - Sensitive to scaling and noise.\n\nThese PCA variants allow the core idea of variance decomposition to be adapted to a wide variety of practical problems, whether by enforcing sparsity, allowing for nonlinearity, handling outliers, or modeling uncertainty."
  },
  {
    "objectID": "blog-unpublished/pca-flavors.html#references",
    "href": "blog-unpublished/pca-flavors.html#references",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "References",
    "text": "References\n\nBishop, C. M. (1999). ‚ÄúBayesian PCA.‚Äù Advances in Neural Information Processing Systems, 11.\nPearson, K. (1901). ‚ÄúOn Lines and Planes of Closest Fit to Systems of Points in Space.‚Äù Philosophical Magazine, 2(11), 559‚Äì572.\nHotelling, H. (1933). ‚ÄúAnalysis of a Complex of Statistical Variables into Principal Components.‚Äù Journal of Educational Psychology, 24(6), 417‚Äì441.\nJolliffe, I. T. (2002). Principal Component Analysis. Springer Series in Statistics.\nZou, H., Hastie, T., & Tibshirani, R. (2006). ‚ÄúSparse Principal Component Analysis.‚Äù Journal of Computational and Graphical Statistics, 15(2), 265‚Äì286."
  },
  {
    "objectID": "blog-unpublished/pca-flavors.html#bottom-line",
    "href": "blog-unpublished/pca-flavors.html#bottom-line",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nClassical PCA reduces dimensionality by projecting data onto orthogonal directions that maximize variance.\nMany PCA variants exist to handle real-world challenges like sparsity (Sparse PCA), outliers (Robust PCA), nonlinearity (Kernel PCA), and uncertainty (Probabilistic PCA).\nChoosing the right PCA flavor depends on the structure of your data and the goals of your analysis ‚Äî interpretability, robustness, scalability, or flexibility.\nSeveral of these extensions (e.g., Kernel PCA, NMF, ICA) relax key assumptions of traditional PCA, making them better suited for specialized applications like image analysis, genomics, and signal processing.\nUnderstanding the mathematical foundation behind these methods helps avoid misapplication and improves the quality of insights from dimensionality reduction."
  },
  {
    "objectID": "blog-unpublished/pca-flavors.html#where-to-learn-more",
    "href": "blog-unpublished/pca-flavors.html#where-to-learn-more",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a comprehensive introduction to PCA, see Principal Component Analysis by Jolliffe (2002), which remains a classic reference. The review paper by Shlens (2014), A Tutorial on Principal Component Analysis, offers an accessible and intuitive explanation of the method and its geometric interpretation. For deeper dives into specific variants, refer to Zou, Hastie, and Tibshirani (2006)"
  },
  {
    "objectID": "blog-unpublished/post-lasso-ols.html",
    "href": "blog-unpublished/post-lasso-ols.html",
    "title": "Post-Lasso OLS: Why Refit After Selection?",
    "section": "",
    "text": "When working in high-dimensional regression, where the number of covariates \\(p\\) may exceed or be comparable to the sample size \\(n\\), Lasso has become the go-to method for variable selection and estimation. Thanks to its (_1)-penalty, Lasso can zero out coefficients of irrelevant variables, effectively performing feature selection while estimating the model. However, as elegant and powerful as Lasso is, it has one well-known drawback: bias due to shrinkage.\nHere‚Äôs the big idea: what if we use Lasso to select variables, but then go back and run plain old OLS on just those selected variables? This is the core of Post-Lasso OLS (also known as OLS post-Lasso). The promise is that this approach can reduce shrinkage bias while still enjoying the selection capabilities of Lasso.\nIn this article, we‚Äôll unpack the intuition behind Post-Lasso OLS, its theoretical properties, and its practical benefits and risks. We‚Äôll also discuss when you should‚Äîand shouldn‚Äôt‚Äîuse it."
  },
  {
    "objectID": "blog-unpublished/post-lasso-ols.html#background",
    "href": "blog-unpublished/post-lasso-ols.html#background",
    "title": "Post-Lasso OLS: Why Refit After Selection?",
    "section": "",
    "text": "When working in high-dimensional regression, where the number of covariates \\(p\\) may exceed or be comparable to the sample size \\(n\\), Lasso has become the go-to method for variable selection and estimation. Thanks to its (_1)-penalty, Lasso can zero out coefficients of irrelevant variables, effectively performing feature selection while estimating the model. However, as elegant and powerful as Lasso is, it has one well-known drawback: bias due to shrinkage.\nHere‚Äôs the big idea: what if we use Lasso to select variables, but then go back and run plain old OLS on just those selected variables? This is the core of Post-Lasso OLS (also known as OLS post-Lasso). The promise is that this approach can reduce shrinkage bias while still enjoying the selection capabilities of Lasso.\nIn this article, we‚Äôll unpack the intuition behind Post-Lasso OLS, its theoretical properties, and its practical benefits and risks. We‚Äôll also discuss when you should‚Äîand shouldn‚Äôt‚Äîuse it."
  },
  {
    "objectID": "blog-unpublished/post-lasso-ols.html#notation",
    "href": "blog-unpublished/post-lasso-ols.html#notation",
    "title": "Post-Lasso OLS: Why Refit After Selection?",
    "section": "Notation",
    "text": "Notation\nConsider a linear regression model where for observations \\(i = 1, \\dots, n\\), the outcome \\(y_i\\) and covariates \\(x_i \\in \\mathbb{R}^p\\) satisfy:\n\\[\ny_i = x_i^\\top \\beta_0 + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2),\n\\]\nwith \\(\\beta_0\\) being the true, unknown coefficient vector. The number of nonzero elements in \\(\\beta_0\\), denoted by \\(s = \\|\\beta_0\\|_0\\), is assumed to be small relative to \\(n\\).\nDefine the Lasso estimator as:\n\\[\n\\hat{\\beta}^{\\text{Lasso}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right\\}.\n\\]\nThe selected model is:\n\\[\n\\hat{T} = \\text{support}(\\hat{\\beta}^{\\text{Lasso}}).\n\\]\nThe Post-Lasso OLS estimator then runs OLS on the selected variables \\(\\hat{T}\\):\n\\[\n\\tilde{\\beta} = \\arg \\min_{\\beta : \\beta_j = 0 \\text{ for } j \\notin \\hat{T}} \\frac{1}{2n} \\| y - X \\beta \\|_2^2.\n\\]"
  },
  {
    "objectID": "blog-unpublished/post-lasso-ols.html#a-closer-look",
    "href": "blog-unpublished/post-lasso-ols.html#a-closer-look",
    "title": "Post-Lasso OLS: Why Refit After Selection?",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhy Refit After Lasso?\nLasso achieves sparsity through penalization, but this same penalty introduces bias. In particular, even the coefficients of truly important variables are shrunk toward zero. While this helps with variance reduction, it may hurt estimation quality, especially when we care about unbiased coefficient estimates or accurate predictions.\nThe insight behind Post-Lasso is simple: use Lasso as a variable selector, not an estimator. Once the selection is done, we drop the penalty and refit OLS on the reduced model. This removes the shrinkage-induced bias on the selected coefficients.\n\n\nProperties and Theoretical Guarantees\nThe key results from Belloni and Chernozhukov (2013) show that:\n\nPost-Lasso OLS performs at least as well as Lasso in terms of convergence rates for prediction error.\nUnder certain conditions, Post-Lasso OLS can strictly outperform Lasso, achieving a faster rate of convergence.\nIf Lasso perfectly selects the true model (which happens under strong assumptions like well-separated signals and certain design conditions), Post-Lasso OLS becomes the oracle estimator‚Äîas if we knew the true model all along.\n\nMathematically, the prediction error for Post-Lasso satisfies:\n\\[\n\\| X ( \\tilde{\\beta} - \\beta_0 ) \\|_2^2 / n = O_p \\left( \\frac{s \\log p}{n} \\right),\n\\]\nmatching Lasso‚Äôs rate, and potentially improving upon it when model selection is good.\n\n\nWhen Does Post-Lasso Work Well?\nThe advantage of Post-Lasso depends critically on how well the first-step selection works. If Lasso misses important variables (false negatives), OLS cannot ‚Äúrecover‚Äù those missing predictors. However, if Lasso selects all relevant variables (even with some extras), Post-Lasso can effectively de-bias the estimates while tolerating mild over-selection.\nThis behavior is particularly attractive in near-sparse models, where there are a few large coefficients and many small or zero ones.\n\n\nWhen Should You Be Careful?\nDespite its appeal, Post-Lasso is not a magic bullet. There are cases where using it might backfire:\n\nSevere model selection failure: If Lasso misses key variables, Post-Lasso inherits this problem.\nHighly correlated predictors: If the design matrix has strong collinearity, selection may be unstable, leading to poor performance.\nSmall sample sizes: If \\(n\\) is small relative to the number of selected variables, the post-selection OLS may overfit.\n\nA helpful rule of thumb: Post-Lasso is most reliable when selection is conservative (including all relevant predictors, perhaps with some extras) rather than aggressive (cutting out too many)."
  },
  {
    "objectID": "blog-unpublished/post-lasso-ols.html#an-example",
    "href": "blog-unpublished/post-lasso-ols.html#an-example",
    "title": "Post-Lasso OLS: Why Refit After Selection?",
    "section": "An Example",
    "text": "An Example\nHere‚Äôs how to implement Post-Lasso OLS in both R and Python. First, use Lasso to select variables, then refit an OLS model on the selected subset.\n\nRPython\n\n\n# Load required packages\nlibrary(glmnet)\nlibrary(hdm)  # for post-lasso OLS (optional)\n\nset.seed(123)\nn &lt;- 100\np &lt;- 20\nX &lt;- matrix(rnorm(n * p), n, p)\nbeta_true &lt;- c(3, 1.5, 0, 0, 2, rep(0, p - 5))\ny &lt;- X %*% beta_true + rnorm(n)\n\n# Step 1: Fit Lasso to select variables\nlasso_fit &lt;- cv.glmnet(X, y, alpha = 1)  # alpha=1 for Lasso\ncoef_lasso &lt;- coef(lasso_fit, s = \"lambda.min\")\nselected &lt;- which(coef_lasso != 0)[-1]  # remove intercept\n\n# Step 2: Refit OLS on selected variables\nif (length(selected) &gt; 0) {\n  X_selected &lt;- X[, selected, drop = FALSE]\n  ols_fit &lt;- lm(y ~ X_selected)\n  summary(ols_fit)\n} else {\n  print(\"No variables selected by Lasso.\")\n}\n\n\nimport numpy as np\nfrom sklearn.linear_model import LassoCV, LinearRegression\n\n# Simulate data\nnp.random.seed(123)\nn, p = 100, 20\nX = np.random.randn(n, p)\nbeta_true = np.array([3, 1.5, 0, 0, 2] + [0]*(p - 5))\ny = X @ beta_true + np.random.randn(n)\n\n# Step 1: Lasso for variable selection\nlasso = LassoCV(cv=5).fit(X, y)\nselected = np.where(lasso.coef_ != 0)[0]\n\n# Step 2: Refit OLS on selected variables\nif len(selected) &gt; 0:\n    X_selected = X[:, selected]\n    ols = LinearRegression().fit(X_selected, y)\n    print(\"Selected variables:\", selected)\n    print(\"OLS coefficients:\", ols.coef_)\nelse:\n    print(\"No variables selected by Lasso.\")"
  },
  {
    "objectID": "blog-unpublished/post-lasso-ols.html#bottom-line",
    "href": "blog-unpublished/post-lasso-ols.html#bottom-line",
    "title": "Post-Lasso OLS: Why Refit After Selection?",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nPost-Lasso OLS reduces the bias introduced by Lasso‚Äôs shrinkage while retaining its variable selection benefits.\nIt matches or improves upon the prediction error rates of Lasso, especially when selection works well.\nUse Post-Lasso when you‚Äôre confident that selection includes most or all relevant variables.\nAvoid Post-Lasso if your selection step is unstable, misses important variables, or you‚Äôre working with tiny sample sizes."
  },
  {
    "objectID": "blog-unpublished/post-lasso-ols.html#where-to-learn-more",
    "href": "blog-unpublished/post-lasso-ols.html#where-to-learn-more",
    "title": "Post-Lasso OLS: Why Refit After Selection?",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nThe seminal paper by Belloni and Chernozhukov (2013), ‚ÄúLeast Squares After Model Selection in High-Dimensional Sparse Models,‚Äù provides a rigorous theoretical treatment of Post-Lasso OLS. Additional discussions on related methods can be found in the broader high-dimensional regression literature, including works on adaptive Lasso, thresholded Lasso, and debiased Lasso. For practical implementation, both the hdm package in R and the sklearn.linear_model module in Python offer accessible ways to perform Lasso and OLS refitting."
  },
  {
    "objectID": "blog-unpublished/post-lasso-ols.html#references",
    "href": "blog-unpublished/post-lasso-ols.html#references",
    "title": "Post-Lasso OLS: Why Refit After Selection?",
    "section": "References",
    "text": "References\n\nBelloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse models. Bernoulli, 19(2), 521‚Äì547.\nTibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society: Series B, 58(1), 267‚Äì288.\nZhao, P., & Yu, B. (2006). On model selection consistency of Lasso. Journal of Machine Learning Research, 7, 2541‚Äì2563."
  },
  {
    "objectID": "blog-unpublished/sensitivity-analysis.html",
    "href": "blog-unpublished/sensitivity-analysis.html",
    "title": "Sensitivity Analysis in Causal Inference",
    "section": "",
    "text": "One of the central challenges in causal inference is the problem of unobserved confounding. Even after controlling for observed covariates, we always face the nagging question: what if there‚Äôs something lurking in the background that we didn‚Äôt observe or can‚Äôt measure, and that something is driving our results?\nThis is where sensitivity analysis comes in. Developed and championed by Paul Rosenbaum and collaborators, sensitivity analysis provides a structured way to assess how robust your causal conclusions are to potential violations of the unconfoundedness assumption (also known as ignorability or selection on observables).\nIn this article, we‚Äôll walk through the basic setup of sensitivity analysis in the potential outcomes framework, focus on Rosenbaum‚Äôs classical approach for observational studies, and discuss its practical implications. While sensitivity analysis doesn‚Äôt ‚Äúsolve‚Äù the confounding problem, it makes transparent how much unobserved bias would be required to overturn your findings."
  },
  {
    "objectID": "blog-unpublished/sensitivity-analysis.html#background",
    "href": "blog-unpublished/sensitivity-analysis.html#background",
    "title": "Sensitivity Analysis in Causal Inference",
    "section": "",
    "text": "One of the central challenges in causal inference is the problem of unobserved confounding. Even after controlling for observed covariates, we always face the nagging question: what if there‚Äôs something lurking in the background that we didn‚Äôt observe or can‚Äôt measure, and that something is driving our results?\nThis is where sensitivity analysis comes in. Developed and championed by Paul Rosenbaum and collaborators, sensitivity analysis provides a structured way to assess how robust your causal conclusions are to potential violations of the unconfoundedness assumption (also known as ignorability or selection on observables).\nIn this article, we‚Äôll walk through the basic setup of sensitivity analysis in the potential outcomes framework, focus on Rosenbaum‚Äôs classical approach for observational studies, and discuss its practical implications. While sensitivity analysis doesn‚Äôt ‚Äúsolve‚Äù the confounding problem, it makes transparent how much unobserved bias would be required to overturn your findings."
  },
  {
    "objectID": "blog-unpublished/sensitivity-analysis.html#notation",
    "href": "blog-unpublished/sensitivity-analysis.html#notation",
    "title": "Sensitivity Analysis in Causal Inference",
    "section": "Notation",
    "text": "Notation\nWe use the potential outcomes framework, where for each unit \\(i = 1, \\dots, n\\): - \\(Y_i(1)\\) is the potential outcome under treatment, - \\(Y_i(0)\\) is the potential outcome under control, - \\(D_i \\in \\{0,1\\}\\) is the treatment indicator, - \\(X_i\\) is the vector of observed covariates.\nThe key parameter of interest is the average treatment effect on the treated (ATT): \\[\n\\text{ATT} = \\mathbb{E} [ Y(1) - Y(0) \\mid D = 1 ].\n\\]\nStandard causal identification relies on the unconfoundedness assumption: \\[\n(Y(1), Y(0)) \\perp D \\mid X.\n\\] In words: conditional on observed covariates \\(X\\), treatment assignment \\(D\\) is as good as random.\nHowever, what if this assumption fails? Suppose there‚Äôs an unobserved confounder \\(U\\) that affects both \\(D\\) and \\(Y\\)? Sensitivity analysis allows us to quantify how much such an unobserved confounder would need to matter to explain away the observed treatment effect."
  },
  {
    "objectID": "blog-unpublished/sensitivity-analysis.html#a-closer-look",
    "href": "blog-unpublished/sensitivity-analysis.html#a-closer-look",
    "title": "Sensitivity Analysis in Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRosenbaum‚Äôs Sensitivity Model for Binary Treatments\nPaul Rosenbaum (1987, 2002) proposed a model for sensitivity analysis in matched observational studies that formalizes the possibility of hidden bias due to unobserved confounding.\nThe key idea is to model treatment assignment probabilities with a sensitivity parameter \\(\\Gamma\\). Specifically, for any two matched individuals \\(i\\) and \\(j\\) with identical observed covariates: \\[\n\\frac{1}{\\Gamma} \\leq \\frac{\\Pr(D_i = 1 \\mid X, U)}{\\Pr(D_j = 1 \\mid X, U)} \\leq \\Gamma.\n\\]\n\nIf \\(\\Gamma = 1\\), treatment assignment is unconfounded (purely random within matched pairs).\nIf \\(\\Gamma &gt; 1\\), there could be hidden bias: even after matching on \\(X\\), units might still differ in treatment probability due to \\(U\\).\n\nThe parameter \\(\\Gamma\\) thus bounds how much the odds of receiving treatment could differ between matched units because of unobserved variables.\nRosenbaum then develops a test statistic framework to compute bounds on p-values for the null hypothesis of no treatment effect under different values of \\(\\Gamma\\).\nThe procedure asks: How large must \\(\\Gamma\\) be before the p-value crosses a critical threshold (like 0.05)? This critical value is called the sensitivity value.\n\n\n\nInterpreting \\(\\Gamma\\): The Intuition\nThink of \\(\\Gamma\\) as quantifying the strength of unmeasured confounding needed to change your conclusion. If your result holds up to a large \\(\\Gamma\\) (e.g., \\(\\Gamma = 2.5\\)), it would require quite a strong confounder to nullify the result. But if the result disappears for \\(\\Gamma = 1.05\\), your conclusion is quite fragile‚Äîeven a mild bias could explain the effect.\nIn practice, values of \\(\\Gamma\\) between 1.1 and 3 are common depending on the context, but what counts as ‚Äúlarge‚Äù depends on the subject matter.\n\n\n\nSensitivity Analysis Beyond Matching\nWhile Rosenbaum‚Äôs original framework was developed for matched designs, extensions of sensitivity analysis apply to:\n\nRegression adjustment models\nInverse probability weighting\nInstrumental variable settings\nDifference-in-differences designs\n\nThe core idea remains: quantify how much unobserved confounding could alter your results.\nIn regression contexts, the Rosenbaum bounds approach may not be directly applicable, but similar logic applies via methods like:\n\nOster‚Äôs delta method (Oster, 2019),\nAltonji, Elder, Taber approach,\nCornfield conditions (used historically in epidemiology).\n\n\n\n\nWhy Sensitivity Analysis Matters\nSensitivity analysis doesn‚Äôt claim to remove unobserved bias. Rather, it exposes the vulnerability of your conclusions. It turns the problem of unmeasured confounding from a scary unknown into a quantifiable scenario:\n\n‚ÄúYour effect holds unless there exists an unmeasured confounder that changes the odds of treatment by a factor of 2 and is strongly associated with the outcome.‚Äù\n\nThis transparency invites substantive experts to weigh in: is such a confounder plausible? If yes, proceed cautiously. If not, gain confidence in your findings."
  },
  {
    "objectID": "blog-unpublished/sensitivity-analysis.html#bottom-line",
    "href": "blog-unpublished/sensitivity-analysis.html#bottom-line",
    "title": "Sensitivity Analysis in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nSensitivity analysis quantifies how much unobserved confounding would be needed to explain away your estimated treatment effects.\nPaul Rosenbaum‚Äôs framework introduces a sensitivity parameter \\(\\Gamma\\) to model hidden bias in matched observational studies.\nA high sensitivity value suggests your results are robust; a low one suggests fragility.\nSensitivity analysis doesn‚Äôt remove bias‚Äîit makes the assumptions about bias explicit and testable."
  },
  {
    "objectID": "blog-unpublished/sensitivity-analysis.html#where-to-learn-more",
    "href": "blog-unpublished/sensitivity-analysis.html#where-to-learn-more",
    "title": "Sensitivity Analysis in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nPaul Rosenbaum‚Äôs book Observational Studies (2002, Springer) is the definitive reference on sensitivity analysis in causal inference. For a practical introduction, see his papers in Biometrika (1987) and subsequent work on sensitivity values and extensions to various designs. The R packages rbounds and sensitivitymv provide implementations for matched studies. Extensions of sensitivity analysis to regression, IV, and other settings are well-covered in recent papers by Oster (2019) and Cinelli & Hazlett (2020)."
  },
  {
    "objectID": "blog-unpublished/sensitivity-analysis.html#references",
    "href": "blog-unpublished/sensitivity-analysis.html#references",
    "title": "Sensitivity Analysis in Causal Inference",
    "section": "References",
    "text": "References\n\nRosenbaum, P. R. (1987). Sensitivity analysis for certain permutation inferences in matched observational studies. Biometrika, 74(1), 13‚Äì26.\nRosenbaum, P. R. (2002). Observational Studies (2nd ed.). Springer.\nOster, E. (2019). Unobservable selection and coefficient stability: Theory and evidence. Journal of Business & Economic Statistics, 37(2), 187‚Äì204.\nAltonji, J. G., Elder, T. E., & Taber, C. R. (2005). Selection on observed and unobserved variables: Assessing the effectiveness of Catholic schools. Journal of Political Economy, 113(1), 151‚Äì184.\nCinelli, C., & Hazlett, C. (2020). Making sense of sensitivity: Extending omitted variable bias. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(1), 39‚Äì67."
  },
  {
    "objectID": "blog-unpublished/sensitivity-analysis.html#an-example",
    "href": "blog-unpublished/sensitivity-analysis.html#an-example",
    "title": "Sensitivity Analysis in Causal Inference",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs see how to implement Rosenbaum‚Äôs sensitivity analysis in practice. Below we show how to do this in R using the rbounds package, and a simple simulation-based illustration in Python.\n\nRPython\n\n\n# Install and load the package if needed\n# install.packages(\"rbounds\")\nlibrary(rbounds)\n\n# Simulate matched pair data\nset.seed(123)\nn_pairs &lt;- 50\ntreated &lt;- rnorm(n_pairs, mean = 1)\ncontrol &lt;- rnorm(n_pairs, mean = 0.5)\ndiffs &lt;- treated - control\n\n# Perform Wilcoxon signed-rank test for matched pairs\nwilcox.test(diffs, alternative = \"greater\", exact = FALSE)\n\n# Perform Rosenbaum sensitivity analysis (Hodges-Lehmann test)\n# Gamma = 1 means no hidden bias; increase Gamma to test robustness\npsens(diffs, Gamma = 1.5, alternative = \"greater\")\n\n\n# Python does not have built-in Rosenbaum bounds yet,\n# but we can simulate how unmeasured confounding could shift results.\n\nimport numpy as np\nfrom scipy.stats import wilcoxon\n\nnp.random.seed(123)\nn_pairs = 50\ntreated = np.random.normal(1, 1, n_pairs)\ncontrol = np.random.normal(0.5, 1, n_pairs)\ndiffs = treated - control\n\n# Wilcoxon signed-rank test (no hidden bias)\nstat, p_value = wilcoxon(diffs, alternative='greater')\nprint(f\"Wilcoxon p-value (no bias): {p_value:.4f}\")\n\n# Simple sensitivity illustration: perturb the control group\nfor gamma_effect in [0.1, 0.2, 0.3]:\n    biased_control = control + gamma_effect\n    biased_diffs = treated - biased_control\n    stat, p_biased = wilcoxon(biased_diffs, alternative='greater')\n    print(f\"Gamma-effect {gamma_effect:.2f} ‚Üí p-value: {p_biased:.4f}\")"
  },
  {
    "objectID": "blog-unpublished/flavors-bootstrap.html",
    "href": "blog-unpublished/flavors-bootstrap.html",
    "title": "The Many Flavors of Bootstrap",
    "section": "",
    "text": "The bootstrap is one of those statistical ideas that feels almost too simple to work‚Äîand yet, it works beautifully. At its core, the bootstrap is about asking, ‚ÄúWhat if we could repeatedly sample from the data we already have, as if we were drawing fresh samples from the population?‚Äù This clever sleight of hand allows us to estimate variability, construct confidence intervals, and even perform hypothesis tests‚Äîall without relying heavily on strong parametric assumptions.\nBut here‚Äôs the thing: there isn‚Äôt just one bootstrap. Over the years, statisticians have developed many flavors of the bootstrap to address different challenges. Some handle small samples better. Some are designed for dependent data like time series. Others shine when the assumptions of classic bootstrapping crumble (think clustered data or heteroskedasticity).\nIn this post, we‚Äôll take a tour through the zoo of bootstrap methods: from the classic nonparametric bootstrap to the jackknife, parametric bootstrap, Bayesian bootstrap, wild bootstrap, moving block bootstrap, and more. We‚Äôll explore where each method shines, where it stumbles, and how to pick the right tool for your problem.\nNo need to worry‚ÄîI won‚Äôt just throw formulas at you (though there will be some of those). The focus here is on understanding why these methods work, not just how to mechanically apply them."
  },
  {
    "objectID": "blog-unpublished/flavors-bootstrap.html#background",
    "href": "blog-unpublished/flavors-bootstrap.html#background",
    "title": "The Many Flavors of Bootstrap",
    "section": "",
    "text": "The bootstrap is one of those statistical ideas that feels almost too simple to work‚Äîand yet, it works beautifully. At its core, the bootstrap is about asking, ‚ÄúWhat if we could repeatedly sample from the data we already have, as if we were drawing fresh samples from the population?‚Äù This clever sleight of hand allows us to estimate variability, construct confidence intervals, and even perform hypothesis tests‚Äîall without relying heavily on strong parametric assumptions.\nBut here‚Äôs the thing: there isn‚Äôt just one bootstrap. Over the years, statisticians have developed many flavors of the bootstrap to address different challenges. Some handle small samples better. Some are designed for dependent data like time series. Others shine when the assumptions of classic bootstrapping crumble (think clustered data or heteroskedasticity).\nIn this post, we‚Äôll take a tour through the zoo of bootstrap methods: from the classic nonparametric bootstrap to the jackknife, parametric bootstrap, Bayesian bootstrap, wild bootstrap, moving block bootstrap, and more. We‚Äôll explore where each method shines, where it stumbles, and how to pick the right tool for your problem.\nNo need to worry‚ÄîI won‚Äôt just throw formulas at you (though there will be some of those). The focus here is on understanding why these methods work, not just how to mechanically apply them."
  },
  {
    "objectID": "blog-unpublished/flavors-bootstrap.html#notation",
    "href": "blog-unpublished/flavors-bootstrap.html#notation",
    "title": "The Many Flavors of Bootstrap",
    "section": "Notation",
    "text": "Notation\nThroughout this article, we‚Äôll assume we have data \\(\\{Y_1, Y_2, \\dots, Y_n\\}\\), where \\(Y_i\\) are independent and identically distributed (i.i.d.) random variables drawn from some unknown distribution \\(F\\). We‚Äôre interested in estimating some parameter \\(\\theta = T(F)\\), like the mean, median, regression coefficients, or a more complicated functional.\nOur estimator of \\(\\theta\\) from the observed sample is \\(\\hat{\\theta} = T(\\hat{F}_n)\\), where \\(\\hat{F}_n\\) is the empirical distribution function that puts mass \\(1/n\\) on each observed data point.\nThe big question is: How variable is \\(\\hat{\\theta}\\)? And that‚Äôs where the bootstrap comes in."
  },
  {
    "objectID": "blog-unpublished/flavors-bootstrap.html#a-closer-look",
    "href": "blog-unpublished/flavors-bootstrap.html#a-closer-look",
    "title": "The Many Flavors of Bootstrap",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Jackknife\nLet‚Äôs start with the jackknife, developed back in the 1950s by Quenouille and popularized by Tukey. The jackknife isn‚Äôt technically a bootstrap, but it‚Äôs often the gateway drug to resampling methods.\nHere‚Äôs the idea: drop one observation at a time, recompute your estimate, and use the variability across these ‚Äúleave-one-out‚Äù estimates to approximate the variance of \\(\\hat{\\theta}\\).\nMathematically, the jackknife replicates are: \\[\n\\hat{\\theta}_{(i)} = T(\\hat{F}_{n,-i}),\n\\] where \\(\\hat{F}_{n,-i}\\) is the empirical distribution leaving out the \\(i\\)-th observation.\nThe jackknife variance estimate is: \\[\n\\hat{V}_{\\text{jack}} = \\frac{n - 1}{n} \\sum_{i=1}^n \\left( \\hat{\\theta}_{(i)} - \\bar{\\theta}_{\\text{jack}} \\right)^2,\n\\] where \\(\\bar{\\theta}_{\\text{jack}} = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\theta}_{(i)}\\).\nWhen to use it? The jackknife works well for smooth statistics like the mean or regression coefficients. But it can fail miserably for non-smooth functionals like the median or quantiles.\nStrengths: Fast, easy to implement, no randomness involved.\nWeaknesses: Limited to statistics that are smooth in the data. Doesn‚Äôt handle complex dependency structures or non-smooth parameters well.\n\nRPython\n\n\nset.seed(42)\ny &lt;- rnorm(100)\njackknife_estimates &lt;- sapply(1:length(y), function(i) mean(y[-i]))\njackknife_variance &lt;- (length(y) - 1) / length(y) * var(jackknife_estimates)\nprint(jackknife_variance)\n\n\nimport numpy as np\nnp.random.seed(42)\ny = np.random.normal(size=100)\njackknife_estimates = np.array([np.mean(np.delete(y, i)) for i in range(len(y))])\njackknife_variance = (len(y) - 1) / len(y) * np.var(jackknife_estimates, ddof=1)\nprint(jackknife_variance)\n\n\n\n\n\n\nClassic Nonparametric Bootstrap\nThe classic bootstrap, introduced by Bradley Efron in 1979, takes the idea of resampling and turns it up a notch. Instead of dropping one observation at a time, we repeatedly resample with replacement from our data to create many ‚Äúnew‚Äù datasets, each the same size as the original.\nFor each bootstrap sample \\(b = 1, \\dots, B\\): 1. Sample \\(n\\) observations with replacement. 2. Compute the statistic \\(\\hat{\\theta}^*_b = T(\\hat{F}^*_b)\\).\nThe bootstrap variance estimate is: \\[\n\\hat{V}_{\\text{boot}} = \\frac{1}{B - 1} \\sum_{b=1}^B \\left( \\hat{\\theta}^*_b - \\bar{\\theta}^* \\right)^2,\n\\] where \\(\\bar{\\theta}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*_b\\).\nWhen to use it? Whenever the sample size is moderate or large and you can safely assume i.i.d. observations.\nStrengths: Flexible, broadly applicable, works well for non-smooth statistics.\nWeaknesses: Can struggle with small samples or dependent data (like time series). Resampling with replacement assumes independence.\n\nRPython\n\n\nset.seed(42)\ny &lt;- rnorm(100)\nB &lt;- 1000\nboot_means &lt;- replicate(B, mean(sample(y, replace = TRUE)))\nboot_variance &lt;- var(boot_means)\nprint(boot_variance)\n\n\nnp.random.seed(42)\nB = 1000\nboot_means = [np.mean(np.random.choice(y, size=len(y), replace=True)) for _ in range(B)]\nboot_variance = np.var(boot_means, ddof=1)\nprint(boot_variance)\n\n\n\n\n\n\nParametric Bootstrap\nThe parametric bootstrap tweaks the classic idea. Instead of sampling from the empirical distribution \\(\\hat{F}_n\\), you assume a parametric model \\(F_\\theta\\) for the data, fit it to the sample, and then generate new data from the fitted model.\nFor example, if you assume \\(Y_i \\sim N(\\mu, \\sigma^2)\\), estimate \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}^2\\), and then generate bootstrap samples from \\(N(\\hat{\\mu}, \\hat{\\sigma}^2)\\).\nWhen to use it? When you trust your parametric model (or at least trust it more than the empirical distribution) and want to leverage that structure.\nStrengths: More efficient than nonparametric bootstrap if the model is well-specified. Can handle small samples better.\nWeaknesses: Garbage in, garbage out‚Äîif the parametric model is wrong, so are your bootstrap results.\n\nRPython\n\n\nset.seed(42)\ny &lt;- rnorm(100)\nmu_hat &lt;- mean(y)\nsigma_hat &lt;- sd(y)\nparam_boot_means &lt;- replicate(B, mean(rnorm(100, mu_hat, sigma_hat)))\nparam_boot_variance &lt;- var(param_boot_means)\nprint(param_boot_variance)\n\n\nmu_hat = np.mean(y)\nsigma_hat = np.std(y, ddof=1)\nparam_boot_means = [np.mean(np.random.normal(mu_hat, sigma_hat, size=len(y))) for _ in range(B)]\nparam_boot_variance = np.var(param_boot_means, ddof=1)\nprint(param_boot_variance)\n\n\n\n\n\n\nBayesian Bootstrap\nInvented by Rubin in 1981, the Bayesian bootstrap doesn‚Äôt resample data points directly. Instead, it puts a Dirichlet prior on the weights assigned to each observation.\nThe weights \\((w_1, \\dots, w_n)\\) are drawn from: \\[\n(w_1, \\dots, w_n) \\sim \\text{Dirichlet}(1, \\dots, 1).\n\\] Then the statistic is computed as: \\[\n\\hat{\\theta}^* = T\\left( \\sum_{i=1}^n w_i \\delta_{Y_i} \\right).\n\\]\nWhen to use it? When you‚Äôre in a Bayesian mood or want a resampling scheme without discrete resampling (i.e., no repeated observations).\nStrengths: Smooth, avoids ties from discrete resampling, easy to implement.\nWeaknesses: Interpretation may feel less intuitive if you‚Äôre used to classical frequentist bootstrap.\n\nRPython\n\n\nlibrary(MCMCpack)  # for rdirichlet\nset.seed(42)\ny &lt;- rnorm(100)\nB &lt;- 1000\nbayes_boot_means &lt;- replicate(B, {\n  weights &lt;- as.numeric(rdirichlet(1, rep(1, length(y))))\n  sum(weights * y)\n})\nvar(bayes_boot_means)\n\n\nfrom scipy.stats import dirichlet\nbayes_boot_means = []\nfor _ in range(B):\n    weights = dirichlet.rvs([1] * len(y))[0]\n    bayes_boot_means.append(np.sum(weights * y))\nnp.var(bayes_boot_means, ddof=1)\n\n\n\n\n\n\nWild Bootstrap\nThe wild bootstrap is a lifesaver when dealing with heteroskedasticity or few clusters. Instead of resampling observations, it perturbs the residuals.\nSuppose you‚Äôre estimating a regression model: \\[\nY_i = X_i \\beta + \\varepsilon_i.\n\\] The wild bootstrap generates: \\[\nY^*_i = X_i \\hat{\\beta} + v_i \\hat{\\varepsilon}_i,\n\\] where \\(v_i\\) are random variables with mean zero and variance one (e.g., Rademacher random variables taking values \\(\\pm1\\) with probability \\(0.5\\)).\nWhen to use it? Heteroskedastic models, clustered data with few clusters.\nStrengths: Handles heteroskedasticity gracefully, robust in small-sample settings.\nWeaknesses: Mostly designed for regression contexts. Choice of perturbation distribution matters.\n\nRPython\n\n\nset.seed(42)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100, sd = abs(x))\nmodel &lt;- lm(y ~ x)\nresiduals &lt;- resid(model)\npredicted &lt;- fitted(model)\nB &lt;- 1000\nwild_means &lt;- replicate(B, {\n  v &lt;- sample(c(-1, 1), length(residuals), replace = TRUE)\n  y_star &lt;- predicted + v * residuals\n  coef(lm(y_star ~ x))[2]\n})\nvar(wild_means)\n\n\nfrom sklearn.linear_model import LinearRegression\nx = np.random.normal(size=100).reshape(-1, 1)\ny = 2 * x.flatten() + np.random.normal(scale=np.abs(x.flatten()))\nmodel = LinearRegression().fit(x, y)\nresiduals = y - model.predict(x)\npredicted = model.predict(x)\nwild_boot_coefs = []\nfor _ in range(B):\n    v = np.random.choice([-1, 1], size=len(residuals))\n    y_star = predicted + v * residuals\n    coef = LinearRegression().fit(x, y_star).coef_[0]\n    wild_boot_coefs.append(coef)\nnp.var(wild_boot_coefs, ddof=1)\n\n\n\n\n\n\nMoving Block Bootstrap\nIf your data are dependent, like time series, the classic bootstrap fails because it breaks the correlation structure. The moving block bootstrap fixes this by resampling blocks of adjacent observations instead of individual data points.\nYou choose a block length \\(l\\) and create overlapping blocks of data: \\[\n\\{Y_1, \\dots, Y_l\\}, \\{Y_2, \\dots, Y_{l+1}\\}, \\dots, \\{Y_{n-l+1}, \\dots, Y_n\\}.\n\\] Then resample these blocks with replacement to form a new dataset.\nWhen to use it? Time series or spatial data with short-range dependence.\nStrengths: Maintains local dependence within blocks.\nWeaknesses: Choice of block size can be tricky; too small loses dependence, too big reduces variability.\n\nRPython\n\n\nlibrary(boot)\nset.seed(42)\ny &lt;- arima.sim(model = list(ar = 0.7), n = 100)\nblock_length &lt;- 5\nB &lt;- 1000\nblock_boot_means &lt;- tsboot(y, statistic = function(x) mean(x), R = B, l = block_length, sim = \"fixed\")\nvar(block_boot_means$t)\n\n\nfrom arch.bootstrap import MovingBlockBootstrap\nnp.random.seed(42)\ny = np.random.normal(size=100)\nblock_length = 5\nbs = MovingBlockBootstrap(block_length, y)\nboot_means = np.array([np.mean(data[0]) for data in bs.bootstrap(B)])\nnp.var(boot_means, ddof=1)\n\n\n\n\n\n\nSubsampling\nSubsampling is like bootstrap‚Äôs minimalist cousin. Instead of resampling with replacement, it draws subsamples without replacement of size \\(m &lt; n\\).\nYou then adjust for the fact that your subsamples are smaller. Subsampling doesn‚Äôt assume i.i.d. data, making it attractive for dependent or non-identically distributed data.\nWhen to use it? Dependent data, heavy-tailed distributions, or when bootstrap consistency fails.\nStrengths: Fewer assumptions than classic bootstrap.\nWeaknesses: Choosing the subsample size \\(m\\) is non-trivial. Usually requires theoretical justification.\n\nRPython\n\n\nset.seed(42)\ny &lt;- rnorm(100)\nsubsample_size &lt;- 50\nB &lt;- 1000\nsubsample_means &lt;- replicate(B, mean(sample(y, subsample_size, replace = FALSE)))\nvar(subsample_means)\n\n\nsubsample_size = 50\nsubsample_means = [np.mean(np.random.choice(y, size=subsample_size, replace=False)) for _ in range(B)]\nnp.var(subsample_means, ddof=1)"
  },
  {
    "objectID": "blog-unpublished/flavors-bootstrap.html#bottom-line",
    "href": "blog-unpublished/flavors-bootstrap.html#bottom-line",
    "title": "The Many Flavors of Bootstrap",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThe bootstrap is not a single method‚Äîit‚Äôs a whole family of techniques, each with its own sweet spot.\nThe jackknife is fast and simple but struggles with non-smooth statistics.\nThe classic bootstrap works great for i.i.d. data and smooth or non-smooth statistics, but fails with dependence or small samples.\nSpecialized bootstraps (wild, block, Bayesian, subsampling) handle heteroskedasticity, clustering, dependence, and other real-world challenges that trip up the classic approach."
  },
  {
    "objectID": "blog-unpublished/flavors-bootstrap.html#where-to-learn-more",
    "href": "blog-unpublished/flavors-bootstrap.html#where-to-learn-more",
    "title": "The Many Flavors of Bootstrap",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a thorough dive into bootstrap methods, I recommend the textbook An Introduction to the Bootstrap by Efron and Tibshirani. For time series applications, Lahiri‚Äôs Resampling Methods for Dependent Data is a gem. If you‚Äôre interested in the asymptotic theory behind these methods, consult Davison and Hinkley‚Äôs Bootstrap Methods and Their Application. There are also excellent lecture notes floating around online from advanced econometrics courses that cover these topics with a modern perspective."
  },
  {
    "objectID": "blog-unpublished/flavors-bootstrap.html#references",
    "href": "blog-unpublished/flavors-bootstrap.html#references",
    "title": "The Many Flavors of Bootstrap",
    "section": "References",
    "text": "References\nEfron, B. (1979). Bootstrap methods: Another look at the jackknife. Annals of Statistics, 7(1), 1‚Äì26.\nRubin, D. B. (1981). The Bayesian bootstrap. Annals of Statistics, 9(1), 130‚Äì134.\nLahiri, S. N. (2003). Resampling Methods for Dependent Data. Springer.\nDavison, A. C., & Hinkley, D. V. (1997). Bootstrap Methods and Their Application. Cambridge University Press."
  },
  {
    "objectID": "blog-unpublished/diff-stat-sig-not-stat-sig.html",
    "href": "blog-unpublished/diff-stat-sig-not-stat-sig.html",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "",
    "text": "We‚Äôve all seen it‚Äîtwo estimates side by side, one with a \\(p\\)-value of 0.04 and the other with a \\(p\\)-value of 0.06. Cue the celebratory confetti for the ‚Äúsignificant‚Äù effect, and a sad trombone for the ‚Äúnot significant‚Äù one. But wait! Are those results really different from each other? If one \\(p\\)-value dips below the magical 0.05 and the other doesn‚Äôt, can we say that the two estimates differ in a meaningful way?\nThe short answer is: no. And that‚Äôs the central insight of Gelman and Stern (2006), a paper that gently but firmly calls out a persistent mistake in the way statistical results are interpreted. In this article, we‚Äôre going to unpack the logic, show the math, and make sure you never fall into this trap again‚Äîespecially if you‚Äôre doing research at scale or comparing multiple models or subgroups."
  },
  {
    "objectID": "blog-unpublished/diff-stat-sig-not-stat-sig.html#background",
    "href": "blog-unpublished/diff-stat-sig-not-stat-sig.html#background",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "",
    "text": "We‚Äôve all seen it‚Äîtwo estimates side by side, one with a \\(p\\)-value of 0.04 and the other with a \\(p\\)-value of 0.06. Cue the celebratory confetti for the ‚Äúsignificant‚Äù effect, and a sad trombone for the ‚Äúnot significant‚Äù one. But wait! Are those results really different from each other? If one \\(p\\)-value dips below the magical 0.05 and the other doesn‚Äôt, can we say that the two estimates differ in a meaningful way?\nThe short answer is: no. And that‚Äôs the central insight of Gelman and Stern (2006), a paper that gently but firmly calls out a persistent mistake in the way statistical results are interpreted. In this article, we‚Äôre going to unpack the logic, show the math, and make sure you never fall into this trap again‚Äîespecially if you‚Äôre doing research at scale or comparing multiple models or subgroups."
  },
  {
    "objectID": "blog-unpublished/diff-stat-sig-not-stat-sig.html#notation",
    "href": "blog-unpublished/diff-stat-sig-not-stat-sig.html#notation",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs consider two estimated effects, say from two different subgroups or models:\n\n\\(\\hat{\\theta}_1\\): Estimate from group 1 with standard error \\(SE_1\\)\n\\(\\hat{\\theta}_2\\): Estimate from group 2 with standard error \\(SE_2\\)\n\nEach of these is associated with a \\(p\\)-value from a test of the null hypothesis \\(H_0: \\theta_i = 0\\).\nYou might be tempted to look at the individual \\(p\\)-values:\n\n\\(p_1 &lt; 0.05\\) ‚Üí statistically significant\n\\(p_2 &gt; 0.05\\) ‚Üí not statistically significant\n\nThen jump to the conclusion: ‚ÄúThe effects are different!‚Äù\nBut to formally test whether the effects differ, what you actually need is a test of:\n\\[\nH_0: \\theta_1 = \\theta_2\n\\]\nThis is not the same as testing \\(\\theta_1 = 0\\) and \\(\\theta_2 = 0\\) separately."
  },
  {
    "objectID": "blog-unpublished/diff-stat-sig-not-stat-sig.html#a-closer-look",
    "href": "blog-unpublished/diff-stat-sig-not-stat-sig.html#a-closer-look",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nThe Fallacy: ‚ÄúSignificant vs.¬†Not Significant‚Äù\nThis is the statistical equivalent of assuming that because one basketball team won a game by 1 point and another lost by 1 point, the winning team must be ‚Äúbetter‚Äù than the losing one. That doesn‚Äôt necessarily follow‚Äîit could just be noise.\nWhat we need to emphasize is this: The difference between significant and not significant is not itself statistically significant.\nIf \\(\\hat{\\theta}_1\\) is significantly different from zero, and \\(\\hat{\\theta}_2\\) is not, that tells us very little about whether \\(\\theta_1 \\neq \\theta_2\\). To evaluate that, we need to compare the two estimates directly.\n\n\nFormal Testing of Differences\nSo how do we actually test whether the estimates differ?\nThe key is to compute the standard error of the difference:\n\\[\nSE_{\\text{diff}} = \\sqrt{SE_1^2 + SE_2^2}\n\\]\nThen, compute the test statistic:\n\\[\nZ = \\frac{\\hat{\\theta}_1 - \\hat{\\theta}_2}{SE_{\\text{diff}}}\n\\]\nThis is just a standard Wald test for the difference between two independent estimates. Under the null hypothesis \\(H_0: \\theta_1 = \\theta_2\\), the \\(Z\\)-statistic is approximately standard normal, so you can compute a \\(p\\)-value from it directly.\nThe big insight? It‚Äôs entirely possible that:\n\n\\(\\hat{\\theta}_1\\) is statistically significant (\\(p_1 &lt; 0.05\\))\n\\(\\hat{\\theta}_2\\) is not (\\(p_2 &gt; 0.05\\))\nBut \\(|Z|\\) is small ‚Üí there‚Äôs no significant difference between the two!\n\nThis happens all the time in subgroup analyses, A/B tests with interactions, and model comparisons.\n\n\nIntuition and Visualization\nHere‚Äôs a helpful mental model. Imagine two normal distributions:\n\nOne centered slightly above zero (say, \\(\\hat{\\theta}_1 = 1.96\\))\nOne centered slightly below zero (say, \\(\\hat{\\theta}_2 = 1.65\\))\n\nWith standard errors of 1, the first just clears the 5% significance threshold, the second doesn‚Äôt. But the difference between the two estimates is a mere 0.31. That‚Äôs tiny relative to their standard errors, and certainly not enough to declare them ‚Äúdifferent.‚Äù\nIf you plotted the two distributions with their confidence intervals, you‚Äôd see huge overlap‚Äîso why treat them as meaningfully different?"
  },
  {
    "objectID": "blog-unpublished/diff-stat-sig-not-stat-sig.html#bottom-line",
    "href": "blog-unpublished/diff-stat-sig-not-stat-sig.html#bottom-line",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nA statistically significant result and a non-significant result do not imply a significant difference between effects.\nTo compare effects, test the difference directly using a formal hypothesis test.\nMisinterpreting significance this way leads to false conclusions in subgroup analyses and model comparisons.\nAlways report and interpret confidence intervals for comparisons‚Äînot just \\(p\\)-values."
  },
  {
    "objectID": "blog-unpublished/diff-stat-sig-not-stat-sig.html#where-to-learn-more",
    "href": "blog-unpublished/diff-stat-sig-not-stat-sig.html#where-to-learn-more",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a deeper dive, the original Gelman and Stern (2006) paper is a classic. Andrew Gelman has also written extensively on this topic in his blog, often using real-life examples from scientific publications and the media. If you‚Äôre looking for broader reading on the pitfalls of significance testing, check out ‚ÄúThe Cult of Statistical Significance‚Äù by Ziliak and McCloskey, or ‚ÄúStatistical Rethinking‚Äù by Richard McElreath for a more Bayesian angle. Lastly, brushing up on basic hypothesis testing logic in texts like Casella and Berger or Wasserman‚Äôs All of Statistics is always a good idea."
  },
  {
    "objectID": "blog-unpublished/diff-stat-sig-not-stat-sig.html#references",
    "href": "blog-unpublished/diff-stat-sig-not-stat-sig.html#references",
    "title": "Why ‚ÄòSignificant‚Äô vs.¬†‚ÄòNot Significant‚Äô Is a Statistical Trap",
    "section": "References",
    "text": "References\nGelman, A., & Stern, H. (2006). The difference between ‚Äúsignificant‚Äù and ‚Äúnot significant‚Äù is not itself statistically significant. The American Statistician, 60(4), 328‚Äì331.\nZiliak, S. T., & McCloskey, D. N. (2008). The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives. University of Michigan Press."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html",
    "href": "blog/flavors-correlation-coef.html",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "",
    "text": "Much of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford published a paper outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\nIn this article I will go over the basics of Chatterjee‚Äôs correlation measure. Before we get there, let‚Äôs first review some of the more traditional approaches in assessing bivariate relationship strength.\nFor simplicity, let‚Äôs assume away ties. Let‚Äôs also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#background",
    "href": "blog/flavors-correlation-coef.html#background",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "",
    "text": "Much of data science is concerned with learning about the relationships between different variables. The most basic tool to quantify relationship strength is the correlation coefficient. In 2021 Sourav Chatterjee of Stanford published a paper outlining a novel correlation coefficient which has ignited many discussions in the statistics community.\nIn this article I will go over the basics of Chatterjee‚Äôs correlation measure. Before we get there, let‚Äôs first review some of the more traditional approaches in assessing bivariate relationship strength.\nFor simplicity, let‚Äôs assume away ties. Let‚Äôs also set hypothesis testing aside. All test statistics I describe below have well-established asymptotic theory for hypothesis testing and calculating p-values. Thus, we can gauge not only the strength of the relationship between the variables but also the uncertainty associated with that measurement and whether or not it is statistically significant."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#a-closer-look",
    "href": "blog/flavors-correlation-coef.html#a-closer-look",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLinear Relationships\nWhen we simply say ‚Äúcorrelation‚Äù we refer to the so-called Pearson correlation coefficient. Virtually everyone working with data is familiar with it. Given a random sample \\((X_1,Y_1),\\dots,(X_n, Y_n)\\) of two random variables \\(X\\) and \\(Y\\) it is computed by:\n\\[corr^{P}(X,Y) = \\frac{ \\sum_i(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}},\\]\nwhere an upper bar denotes a sample mean.\nThis coefficient lives in the \\([-1,1]\\) interval. Larger values indicate stronger relationship between \\(X\\) and \\(Y\\), be it positive or negative. At the extreme, the Pearson coefficient will equal \\(1\\) when all observations can be perfectly lined up on a upward sloping line. Yes, you guessed it ‚Äì when it equals \\(-1\\) the line is sloping down.\nYou can easily calculate the Pearson correlation in R and python:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'pearson', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr\n\npearson_corr, pearson_pval = pearsonr(x, y)\n\n\n\nThis measure, while widely popular, suffers from a few shortcomings. First, outliers have an outsized impact in skewing its value. Sample means vulnerable to outliers are a key ingredient in the calculation, rendering the measure sensitive to data anomalies. Second, it is designed to detect only linear relationships. Two variables might have a strong but non-linear relationship which this measure will not detect. Lastly, it is not transformation-invariant, meaning that applying a monotone transformation to either of the variables will change the correlation value.\nLet‚Äôs discuss some improvements to the Pearson correlation measure. Enter Spearman correlation.\n\n\nMonotone Relationships\nSpearman correlation is the Pearson correlation among the ranks of \\(X\\) and \\(Y\\):\n\\[corr^{S}(X,Y) = corr^{P}(R(X),R(Y)),\\]\nwhere \\(R(\\cdot)\\) denotes an observation‚Äôs rank (or order) in the sample.\nSpearman correlation is thus a rank correlation measure. As such, it quantifies how well the relationship between \\(X\\) and \\(Y\\) can be described using a monotone (and not necessarily a linear) function. It is therefore a more flexible measure of association. Again, intuitively, Spearman correlation will take on a large positive value when the \\(X\\) and \\(Y\\) observations have similar ranks. This value will be negative when the ranks tend to go in opposite directions.\nSpearman correlation addresses some of the shortcomings associated with Pearson correlation. It is not easily influenced by outliers, and it does not change if we apply a monotone transformation of \\(X\\) and/or \\(Y\\). These benefits come at the expense of a loss in interpretation and potential issues when tied ranks are common, a scenario I ignore here.\nCalculating it is just as simple:\n\nRPython\n\n\ncor(x,y, method = 'pearson')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import spearmanr\n\nspearman_corr, spearman_pval = spearmanr(x, y)\n\n\n\nSpearman correlation is not the only rank correlation coefficient out there. –ê popular alternative is the Kendall rank coefficient which is computed slightly differently. Let‚Äôs define a pair of observations \\((X_i, Y_i)\\) and \\((X_j, Y_j)\\) to be agreeing (the technical term is concordant) if the differences \\((X_i - X_j)\\) and \\((Y_i - Y_j)\\) have the same sign (i.e., either both \\(X_i &gt; X_j\\) and $Y_i &gt; Y_j $or both \\(X_i &lt; X_j\\) and \\(Y_i &lt; Y_j\\) ).\nThen, Kendall‚Äôs coefficient is expressed as:\n\\[corr^{K} = \\frac{\\text{number of agreeing pairs} - \\text{number of disagreeing pairs}} {\\text{total number of pairs}}.\\]\nSo, it quantifies the degree of agreement between the ranks of \\(X\\) and \\(Y\\). Like the other coeffients described above, its range is \\([-1, 1]\\) and values away from zero indicate stronger relationship.\nAgain, this coefficient is similarly computed in R and python:\n\nRPython\n\n\ncor(x,y, method = 'kendall')\ncor.test(x,y, method = 'spearman', alternative='two.sided').\n\n\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\nkendall_corr, kendall_pval = kendalltau(x, y)\n\n\n\nKendall‚Äôs measure improves on some of the shortcomings baked in the Spearman‚Äôs coefficients ‚Äì it has a clearer interpretation and it is less sensitive to rank ties.\nHowever, none of these rank correlation coefficients can detect non-monotonic relationships. For instance, \\(X\\) and \\(Y\\) can have a parabola- or wave-like pattern when plotted against each other. We would like a correlation measure flexible enough to capture such non-linear relationships.\nThis is where Chatterjee‚Äôs coefficient comes in.\n\n\nMore General Relationships\nChatterjee recently proposed a new correlation coefficient designed to detect non-monotonic relationships. He discovered a novel estimator of a population quantity first proposed by Dette et al.¬†(2013).\nLet‚Äôs start with the formula. The Chatterjee correlation coefficient is calculated as follows:\n\\[corr^{C}(X,Y) = 1-\\frac{3\\sum_{i=1}^{n-1}|R(Y_{k:R(X_k)=i+1}) - R(Y_{k:R(X_k)=i})|}{n^2-1},\\]\nwhere n is the sample size. This looks complicated, so let‚Äôs try to simplify the numerator. Let‚Äôs sort the data in an ascending order of \\(X\\) so that we have \\((X_{(1)}, Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})\\), where \\(X_{(1)}&lt;X_{(2)}&lt;\\dots &lt;X_{(n)}\\). Also, denote \\(R(Y_i)\\) be the rank of \\(Y_{(i)}\\). Then:\n\\[corr^{C}(X,Y) = 1 - \\frac{3\\sum_{i=1}^{n-1}|R(Y_{i+1})-R(Y_i)|}{n^2-1}.\\]\nSo, this new coefficient is a scaled version of the sum of the absolute differences in the consecutive ranks of \\(Y\\) when ordered by \\(X\\). It is perhaps best to think about Chatterjee‚Äôs method as a measure of dependence and not strictly a correlation coefficient.\nThere are some major difference comapred to the previous correlation measures. Chatterjee‚Äôs correlation coefficient lies in the \\([0,1]\\) interval. It is equal to zero if and only if \\(X\\) and \\(Y\\) are independent and to one if one of them is a function of the other. Unlike the coefficients descibed above, it is not symmetric in \\(X\\) and \\(Y\\), meaning \\(corr^{C}(X,Y) \\neq corr^{C}(Y,X)\\). This is understandable since we are interested in whether \\(X\\) is a function of \\(Y\\), which does not imply the opposite. The author also develops asymptotic theory for calculating p-values although some researchers have raised concerns about the coefficient‚Äôs power.\nHere is a sample code to calculate its value:\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\nn &lt;- 1000\nx &lt;- runif(n) \ny &lt;- 5 * sin(x) + rnorm(n)\n\ndata &lt;- data.frame(x=x, y=y)\ndata$R &lt;- rank(data$y)\ndata &lt;- data[order(data$x), ]\n\n1 - 3 * sum(abs(diff(data$R))) / (n^2-1)\n&gt;[1] 0.4093024\n\n\nimport numpy as np\n\nnp.random.seed(1988)\nn = 1000\nx = np.random.uniform(size=n)\ny = 5 * np.sin(x) + np.random.normal(size=n)\n\ndata = np.array(sorted(zip(x, y), key=lambda pair: pair[0]))\nranks = np.argsort(np.argsort(data[:, 1]))  # Rank of y\nchatterjee_corr = 1 - 3 * np.sum(np.abs(np.diff(ranks))) / (n**2 - 1)\nprint(f\"Chatterjee's correlation: {chatterjee_corr:.4f}\")\n&gt; Chatterjee's correlation: 0.4050\n\n\n\nSoftware Package: XICOR.\nThere you have it. You are now well-equipped to dive deeper into your datasets and find new exciting relationships."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#bottom-line",
    "href": "blog/flavors-correlation-coef.html#bottom-line",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nThere are numerous ways of measuring association between two variables.\nThe most common methods measure only linear or monotonic relationships. These are often useful but do not capture more complex, non-linear associations.\nA new correlation measure, Chatterjee‚Äôs coeffient, is designed to go beyond monotonicty and assess more general bivariate relationships."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#where-to-learn-more",
    "href": "blog/flavors-correlation-coef.html#where-to-learn-more",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia has detailed entries on correlation, rank correlation, and Kendall‚Äôs coefficient which I found helpful. The R bloggers platform has articles exploring the Chatterjee‚Äôs correlation coefficient in detail. The more technically oriented folks will find Chatterjee‚Äôs original paper helpful."
  },
  {
    "objectID": "blog/flavors-correlation-coef.html#references",
    "href": "blog/flavors-correlation-coef.html#references",
    "title": "Nonlinear Correlations and Chatterjee‚Äôs Coefficient",
    "section": "References",
    "text": "References\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022.\nDette, H., Siburg, K. F., & Stoimenov, P. A. (2013). A Copula‚ÄêBased Non‚Äêparametric Measure of Regression Dependence. Scandinavian Journal of Statistics, 40(1), 21-41.\nShi, H., Drton, M., & Han, F. (2022). On the power of Chatterjee‚Äôs rank correlation. Biometrika, 109(2), 317-333.\nhttps://www.r-bloggers.com/2021/12/exploring-the-xi-correlation-coefficient/"
  },
  {
    "objectID": "blog/paradox-stein.html",
    "href": "blog/paradox-stein.html",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "In the realm of statistics, few findings are as counterintuitive and fascinating as Stein‚Äôs paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein‚Äôs paradox is not merely about memorizing an oddity‚Äîit‚Äôs about recognizing the delicate balance inherent in statistical decision-making.\nIn essence, Stein‚Äôs paradox asserts that when estimating the means of multiple variables simultaneously, it‚Äôs possible to achieve better results compared to relying solely on the sample averages.\nLet‚Äôs now delve deeper into this statement and unravel the underlying mechanisms."
  },
  {
    "objectID": "blog/paradox-stein.html#background",
    "href": "blog/paradox-stein.html#background",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "In the realm of statistics, few findings are as counterintuitive and fascinating as Stein‚Äôs paradox. It defies our common sense about estimation and provides a glimpse into the potential of shrinkage estimators. For aspiring and seasoned data scientists, grasping Stein‚Äôs paradox is not merely about memorizing an oddity‚Äîit‚Äôs about recognizing the delicate balance inherent in statistical decision-making.\nIn essence, Stein‚Äôs paradox asserts that when estimating the means of multiple variables simultaneously, it‚Äôs possible to achieve better results compared to relying solely on the sample averages.\nLet‚Äôs now delve deeper into this statement and unravel the underlying mechanisms."
  },
  {
    "objectID": "blog/paradox-stein.html#a-closer-look",
    "href": "blog/paradox-stein.html#a-closer-look",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Mean Squared Error (MSE)\nTo understand the paradox, let‚Äôs begin by quantify what we mean by ‚Äúbetter‚Äù estimation. The MSE of an estimator \\(\\hat{\\mu}\\) is defined as its expected squared error (or loss):\n\\[\\text{MSE}(\\hat{\\mu}) = \\mathbb{E}\\left[ ( \\hat{\\mu} - \\mu )^2 \\right],\\]\nwhere \\(\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_p)\\) is the true vector of means. All else equal, the lower MSE the better.\n\n\nMathematical Formulation\nStein‚Äôs paradox arises in the context of estimating multiple parameters simultaneously. Suppose you‚Äôre estimating the mean \\(\\mu_i\\) of several independent normal distributions:\n\\[X_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, p,\\]\nwhere \\(X_i\\) are observed values, \\(\\mu_i\\) are the unknown means, and \\(\\sigma^2\\) is known. We assume non-zero covariance between the variables.\nA natural approach is to estimate each _i using its corresponding sample mean \\(X_i\\), which is the maximum likelihood estimator (MLE). However, in dimensions \\(p \\geq 3\\), this seemingly reasonable approach is dominated by an alternative method.\nThe surprise? Shrinking the individual estimates toward a common value‚Äîsuch as the overall mean‚Äîproduces an estimator with uniformly lower expected squared error.\nThe MSE of the MLE is equal to:\n\\[R(\\hat{\\mu}_\\text{MLE}) = p\\sigma^2.\\]\nNow consider the biased(!) James-Stein (JS) estimator:\n\\[\\hat{\\mu}_\\text{JS} = \\left( 1 - \\frac{(p - 2)\\sigma^2}{\\lVert X \\rVert^2} \\right) X,\\]\nwhere \\(\\lVert X \\rVert^2 = \\sum_i X_i^2\\) is the squared norm of the observed data. The shrinkage factor \\(1 - \\frac{(p - 2)\\sigma^2}{|X|^2}\\) pulls the estimates toward zero (or any other pre-specified point).\nRemarkably, the James-Stein estimator has lower MSE than the MLE for \\(p \\geq 3\\):\n\\[\\text{MSE}(\\hat{\\mu}_{\\text{JS}}) &lt; \\text{MSE}(\\hat{\\mu}_{\\text{MLE}}).\\]\nThis is the mystery at its core.\n\n\nExplanation\nThis result holds because the James-Stein estimator balances variance reduction and bias introduction in a way that minimizes overall MSE. The MLE, in contrast, does not account for the possibility of shared structure among the parameters. The counterintuitive nature of this result stems from the independence of the \\(X_i\\)‚Äôs. Intuition suggests that pooling information across independent observations should not improve estimation, yet the James-Stein estimator demonstrates otherwise."
  },
  {
    "objectID": "blog/paradox-stein.html#an-example",
    "href": "blog/paradox-stein.html#an-example",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs emulate this paradox in R and python in a setting with \\(p=5\\).\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\n\np &lt;- 5  # Number of means\nn &lt;- 1000  # Number of simulations\nsigma &lt;- 1\nmu &lt;- rnorm(p, mean = 5, sd = 2)  # True means\n\n# results storage\nmse_mle &lt;- numeric(n)  \nmse_js &lt;- numeric(n)\n\n# create a fake dataset and compute the MSEs of both the MLE and JS estimator. \n# repeat this 1,000 times and take the average loss for each estimator.\nfor (sim in 1:n) {\n  # Simulate observations\n  X &lt;- rnorm(p, mean = mu, sd = sigma)\n\n  # MLE estimator and its MSE\n  mle &lt;- X\n  mse_mle[sim] &lt;- sum((mle - mu)^2)\n\n  # James-Stein estimator and its MSE\n  shrinkage &lt;- max(0, 1 - ((p - 2) * sigma^2) / sum(X^2))\n  js &lt;- shrinkage * X\n  mse_js[sim] &lt;- sum((js - mu)^2)\n}\n\n# print the results.\ncat(\"Average MSE of MLE:\", mean(mse_mle), \"\\n\")\n&gt; Average MSE of MLE: 5.125081 \ncat(\"Average MSE of James-Stein:\", mean(mse_js), \"\\n\")\n&gt; Average MSE of James-Stein: 5.055019 \n\n\nimport numpy as np\nnp.random.seed(1988)\n\n# Parameters\np = 5  # Number of means\nn = 1000  # Number of simulations\nsigma = 1\nmu = np.random.normal(loc=5, scale=2, size=p)  # True means\n\n# Results storage\nmse_mle = np.zeros(n)\nmse_js = np.zeros(n)\n\n# Simulate data and compute MSEs for MLE and James-Stein estimator\nfor sim in range(n):\n    # Simulate observations\n    X = np.random.normal(loc=mu, scale=sigma, size=p)\n\n    # MLE estimator and its MSE\n    mle = X\n    mse_mle[sim] = np.sum((mle - mu) ** 2)\n\n    # James-Stein estimator and its MSE\n    shrinkage = max(0, 1 - ((p - 2) * sigma**2) / np.sum(X**2))\n    js = shrinkage * X\n    mse_js[sim] = np.sum((js - mu) ** 2)\n\n# Print the results\nprint(\"Average MSE of MLE:\", np.mean(mse_mle).round(3))\n&gt; Average MSE of MLE: 4.998\nprint(\"Average MSE of James-Stein:\", np.mean(mse_js).round(3))\n&gt; Average MSE of James-Stein: 4.951\n\n\n\nIn this example, average MSE of the James-Stein estimator (\\(5.06\\)) is consistently lower than that of the MLE (\\(5.13\\)), illustrating the paradox in action."
  },
  {
    "objectID": "blog/paradox-stein.html#bottom-line",
    "href": "blog/paradox-stein.html#bottom-line",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nStein‚Äôs paradox shows that shrinkage estimators can outperform the MLE in dimensions \\(p \\geq 3\\), even when the underlying variables are independent.\nThe James-Stein estimator achieves lower MSE by balancing variance reduction and bias introduction.\nUnderstanding this result highlights the power of shrinkage techniques in high-dimensional statistics."
  },
  {
    "objectID": "blog/paradox-stein.html#references",
    "href": "blog/paradox-stein.html#references",
    "title": "Stein‚Äôs Paradox: A Simple Illustration",
    "section": "References",
    "text": "References\nEfron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press."
  },
  {
    "objectID": "blog/paradox-lord.html",
    "href": "blog/paradox-lord.html",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Lord‚Äôs paradox presents a fascinating challenge in causal inference and statistics. It highlights how different statistical methods applied to the same data can lead to contradictory conclusions. The paradox typically arises when comparing changes over time between two groups in the context of adjusting for baseline characteristics. Despite its theoretical nature, the phenomena is deeply relevant to practical data analysis, especially in observational studies where causation is challenging to establish. Let‚Äôs look at an example."
  },
  {
    "objectID": "blog/paradox-lord.html#background",
    "href": "blog/paradox-lord.html#background",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Lord‚Äôs paradox presents a fascinating challenge in causal inference and statistics. It highlights how different statistical methods applied to the same data can lead to contradictory conclusions. The paradox typically arises when comparing changes over time between two groups in the context of adjusting for baseline characteristics. Despite its theoretical nature, the phenomena is deeply relevant to practical data analysis, especially in observational studies where causation is challenging to establish. Let‚Äôs look at an example."
  },
  {
    "objectID": "blog/paradox-lord.html#a-closer-look",
    "href": "blog/paradox-lord.html#a-closer-look",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nMean Differences Over Time\nTo explore Lord‚Äôs paradox, consider the following scenario: Suppose we have two groups of individuals‚Äî\\(A\\) and \\(B\\)‚Äîwith their body weights measured at two time points: before and after a specific intervention. Let the weight at the initial time point be denoted as \\(W_{\\text{pre}}\\), and the weight at the final time point be \\(W_{\\text{post}}\\). We are interested in whether the intervention caused a change in weight between the two groups.\nOne approach to analyze this is to compute the (unadjusted) mean weight change for each group over time:\n\\[\\Delta = \\Delta^A - \\Delta^B.\\]\nIf this quantity is statistically significant, we might conclude that the intervention had a differential effect.\n\n\nControlling for Baseline Characteristics\nAn alternative approach involves adjusting for baseline weight \\(W_{\\text{pre}}\\) using, for example, a regression model:\n\\[W_{\\text{post}}=\\beta_1 + \\beta_2 G_A + \\beta_3 W_{\\text{pre}} + \\epsilon,\\]\nwhere \\(G\\) is a binary indicator for group \\(A\\) membership and \\(\\epsilon\\) is an error term. Here, \\(\\beta_2\\) captures the group difference in \\(W_{\\text{post}}\\), linearly controlling for baseline body weight.\nSurprisingly, these two approaches can yield conflicting results. For example, the former method might suggest no difference, while the regression adjustment indicates a significant group effect.\n\n\nExplanation\nThis contradiction arises because the two methods implicitly address different causal questions.\n\nMethod 1 asks: ‚ÄúDo Groups \\(A\\) and \\(B\\) gain/lose different amounts of weight?‚Äù\nMethod 2 asks: ‚ÄúGiven the same initial weight, does any of the groups end up at different final weights?‚Äù The regression approach adjusts for baseline differences, assuming \\(W_{\\text{pre}}\\) is a confounder.\n\nThe key insight is that the choice of analysis reflects underlying assumptions about the causal structure of the data. If \\(W_{\\text{pre}}\\) is affected by group membership (e.g., due to selection bias), then adjusting for it may introduce bias rather than remove it. However, in practice we most often should control for baseline characteristics as they are critical in balancing the treatment and control groups.\n\n\nThe Simpson‚Äôs Paradox Once Again\nI recently illustrated the more commonly discussed Simpson‚Äôs paradox. Interestingly, a 2008 paper claims that two phenomena are closely related, with the Lord‚Äôs paradox being a ‚Äúcontinuous version‚Äù of Simpson‚Äôs paradox."
  },
  {
    "objectID": "blog/paradox-lord.html#an-example",
    "href": "blog/paradox-lord.html#an-example",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs look at some code illustrating Lord‚Äôs paradox in R and python. We start with simulating a dataset where two groups have identical distributions of \\(W_{\\text{pre}}\\) and \\(W_{\\text{post}}\\), yet differing relationships between the two variables.\n\nRPython\n\n\nrm(list=ls())\nset.seed(1988)\nn &lt;- 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup &lt;- factor(rep(c(\"A\", \"B\"), each = n/2))\n\n# Initial weight (pre). # Group A starts with higher average weight\nweight_pre &lt;- numeric(n)\nweight_pre[group == \"A\"] &lt;- rnorm(n/2, mean = 75, sd = 10)\nweight_pre[group == \"B\"] &lt;- rnorm(n/2, mean = 65, sd = 10)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain &lt;- rnorm(n, mean = 10, sd = 5)\nweight_post &lt;- weight_pre + gain\n\n# Create data frame\ndata &lt;- data.frame(group = group, pre = weight_pre, post = weight_post)\n\n# Analysis 1: Compare change scores between groups`\nt.test(post - pre ~ group, data = data)\n&gt; p-value = 0.6107\n\n# Analysis 2: Regression Adjustment`\nmodel &lt;- lm(post ~ group + pre, data = data)\nsummary(model)\n&gt; p-value = 0.08428742\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Set seed for reproducibility\nnp.random.seed(1988)\nn = 1000\n\n# Simulate data for two groups (e.g., male/female students)\ngroup = np.array([\"A\"] * (n // 2) + [\"B\"] * (n // 2))\n\n# Initial weight (pre). Group A starts with higher average weight\nweight_pre = np.zeros(n)\nweight_pre[group == \"A\"] = np.random.normal(loc=75, scale=10, size=n // 2)\nweight_pre[group == \"B\"] = np.random.normal(loc=65, scale=10, size=n // 2)\n\n# Final weight (post). Both groups improve by the same amount on average\ngain = np.random.normal(loc=10, scale=5, size=n)\nweight_post = weight_pre + gain\n\n# Create DataFrame\ndata = pd.DataFrame({\"group\": group, \"pre\": weight_pre, \"post\": weight_post})\n\n# Analysis 1: Compare change scores between groups\ndata[\"change\"] = data[\"post\"] - data[\"pre\"]\ngroup_a_change = data[data[\"group\"] == \"A\"][\"change\"]\ngroup_b_change = data[data[\"group\"] == \"B\"][\"change\"]\nt_stat, p_value = ttest_ind(group_a_change, group_b_change)\nprint(f\"Analysis 1: p-value = {p_value:.4f}\")\n\n# Analysis 2: Regression Adjustment\nmodel = smf.ols(\"post ~ group + pre\", data=data).fit()\nprint(model.summary())\n\n\n\nThe former approach shows lack of statistically significant differences in the body weight after the intervention between the two groups (\\(p\\)-value =$ 0.6107\\(). The results from the latter method do show meaningful differences (\\)p$-value = \\(0.0842\\)).\nThis illustrates the core of Lord‚Äôs paradox ‚Äì the statistical approach chosen can lead to different interpretations of the same underlying phenomenon."
  },
  {
    "objectID": "blog/paradox-lord.html#bottom-line",
    "href": "blog/paradox-lord.html#bottom-line",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLord‚Äôs paradox underscores the importance of aligning statistical methods with causal assumptions.\nDifferent methods answer different questions and may yield contradictory results if applied blindly.\nCareful consideration of the data-generating process and the role of potential confounders is crucial in choosing the appropriate analytical approach."
  },
  {
    "objectID": "blog/paradox-lord.html#references",
    "href": "blog/paradox-lord.html#references",
    "title": "Lord‚Äôs Paradox: A Simple Illustration",
    "section": "References",
    "text": "References\nLord, E. M. (1967). A paradox in the interpretation of group comparisons. Psychological Bulletin, 68, 304‚Äì305. doi:10.1037/h0025105\nLord, F. M. (1969). Statistical adjustments when comparing preexisting groups. Psychological Bulletin, 72, 336‚Äì337. doi:10.1037/h0028108\nLord, E. M. (1975). Lord‚Äôs paradox. In S. B. Anderson, S. Ball, R. T. Murphy, & Associates, Encyclopedia of Educational Evaluation (pp.¬†232‚Äì236). San Francisco, CA: Jossey-Bass.\nTu, Y. K., Gunnell, D., & Gilthorpe, M. S. (2008). Simpson‚Äôs Paradox, Lord‚Äôs Paradox, and Suppression Effects are the same phenomenon‚Äìthe reversal paradox. Emerging themes in epidemiology, 5, 1-9."
  },
  {
    "objectID": "blog/paradox-simpson.html",
    "href": "blog/paradox-simpson.html",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Simpson‚Äôs paradox is one of the most counterintuitive phenomena in data analysis. It describes situations where a trend observed within groups disappears‚Äîor even reverses‚Äîwhen the data is aggregated. The underlying cause is often a confounding variable that distorts the overall trend. Let‚Äôs examine a concrete example."
  },
  {
    "objectID": "blog/paradox-simpson.html#background",
    "href": "blog/paradox-simpson.html#background",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "",
    "text": "Simpson‚Äôs paradox is one of the most counterintuitive phenomena in data analysis. It describes situations where a trend observed within groups disappears‚Äîor even reverses‚Äîwhen the data is aggregated. The underlying cause is often a confounding variable that distorts the overall trend. Let‚Äôs examine a concrete example."
  },
  {
    "objectID": "blog/paradox-simpson.html#a-closer-look",
    "href": "blog/paradox-simpson.html#a-closer-look",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nAn Example\nImagine two hospitals, \\(A\\) and \\(B\\), treating patients for a particular condition with two treatment options, \\(T_1\\) and \\(T_2\\). Hospital \\(A\\), located in a higher-income neighborhood, primarily receives healthier patients, while Hospital \\(B\\), in a lower-income neighborhood, tends to treat sicker patients. The effectiveness of the treatments is measured as improvement in a continuous health score.\nWe are interested in examining whether one of the treatment options leads to better health outcomes. Consider the following data gathered across both hospitals.\n\n\nHealth improvement by hospital and treatment type. Both treatments \\(T_1\\) and \\(T_2\\) are equally effective within each hospital.\n\n\nHospital\nTreatment\nHealth Improvement\nN\n\n\n\n\nA\n\\(T_1\\)\n20\n90\n\n\nA\n\\(T_2\\)\n20\n10\n\n\nB\n\\(T_1\\)\n10\n10\n\n\nB\n\\(T_2\\)\n10\n90\n\n\n\n\nLet‚Äôs now look at what happens when we combine the data from both hospitals.\n\n\nHealth improvement by treatment type. Treatment \\(T_1\\) is more effective overall.\n\n\nTreatment\nN\nHealth Improvement\n\n\n\n\n\\(T_1\\)\n100\n19 = 20 * .9 + 10 * .1\n\n\n\\(T_2\\)\n100\n11 = 10 * .9 + 20 * .1\n\n\n\n\nWithin each hospital, the data shows that both treatments are equally effective. However, combining the data across both hospitals reveals that treatment \\(T_1\\) appears to be significantly more effective overall. Why does this happen?\nThe confounding variable here is the underlying health status of patients. Hospital \\(A\\) treats mostly healthier patients, while Hospital \\(B\\) handles more severe cases. This difference in patient distribution influences the overall success rates of the treatments, even though both treatments perform identically within each hospital.\n\n\nA Visualization\nTo illustrate, imagine a scatter plot where each dot represents a patient. The color of the dot indicates the treatment they received, and the horizontal lines represent the average health improvement for each group. The vertical axis depicts the outcome variable (health improvement).\n\nRPython\n\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\n# Set random seed for reproducibility\nset.seed(4904)\n\n# Define data dimensions\nn_a &lt;- 200\nn_b &lt;- 20\n\n# Create the dataset\ndata &lt;- data.frame(\n  Hospital = rep(c(\"Hospital A\", \"Hospital B\"), each = n_a + n_b),\n  Treatment = c(\n    rep(\"Treatment 1\", n_a), rep(\"Treatment 2\", n_b),  # Hospital A\n    rep(\"Treatment 1\", n_b), rep(\"Treatment 2\", n_a)   # Hospital B\n  ),\n  Improvement = c(\n    rnorm(n_a, mean = 20, sd = 2), rnorm(n_b, mean = 20, sd = 2),  # Hospital A\n    rnorm(n_b, mean = 10, sd = 2), rnorm(n_a, mean = 10, sd = 2)   # Hospital B\n  ),\n  Aggregated = \"Overall\"\n)\n\n# Calculate mean improvement for each treatment and hospital\nmean_improvement &lt;- data %&gt;%\n  group_by(Hospital, Treatment) %&gt;%\n  summarize(Mean_Improvement = mean(Improvement), .groups = \"drop\")\n\n# Calculate overall mean improvement for each treatment\noverall_mean &lt;- data %&gt;%\n  group_by(Treatment) %&gt;%\n  summarize(Mean_Improvement = mean(Improvement), .groups = \"drop\")\n\n# Create the disaggregated plot\nplot1 &lt;- ggplot(data, aes(x = Hospital, y = Improvement, color = Treatment)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.7) +\n  geom_hline(data = mean_improvement, aes(yintercept = Mean_Improvement, color = Treatment), linetype = \"solid\") +\n  labs(\n    title = \"By Hospital\",\n    x = NULL,\n    y = \"Health Improvement\",\n    color = \"Treatment\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Create the aggregated plot\nplot2 &lt;- ggplot(data, aes(x = Aggregated, y = Improvement, color = Treatment)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.7) +\n  geom_hline(data = overall_mean, aes(yintercept = Mean_Improvement, color = Treatment), linetype = \"solid\") +\n  labs(\n    title = \"Overall\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")\n\n# Combine the plots with a common legend and title\nfinal_plot &lt;- (plot1 + plot2) +\n  plot_annotation(\n    title = \"Simpson's Paradox: Treatment Effectiveness Within Hospitals and Overall\",\n    theme = theme(plot.title = element_text(hjust = 0.5))\n  ) &\n  theme(legend.position = \"bottom\")\n\n# Display the combined plot\nprint(final_plot)\n\n\n# load libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.random.seed(1988)\n\n# Define data dimensions\nn_a = 200\nn_b = 20\n\n# Create the dataset\nhospital = ['Hospital A'] * (n_a + n_b) + ['Hospital B'] * (n_a + n_b)\ntreatment = (\n    ['Treatment 1'] * n_a + ['Treatment 2'] * n_b +  # Hospital A\n    ['Treatment 1'] * n_b + ['Treatment 2'] * n_a   # Hospital B\n)\nimprovement = (\n    list(np.random.normal(20, 2, n_a)) + list(np.random.normal(20, 2, n_b)) +  # Hospital A\n    list(np.random.normal(10, 2, n_b)) + list(np.random.normal(10, 2, n_a))   # Hospital B\n)\naggregated = ['Overall'] * (2 * (n_a + n_b))\n\ndata = pd.DataFrame({\n    'Hospital': hospital,\n    'Treatment': treatment,\n    'Improvement': improvement,\n    'Aggregated': aggregated\n})\n\n# Calculate mean improvement for each treatment and hospital\nmean_improvement = data.groupby(['Hospital', 'Treatment'], as_index=False)['Improvement'].mean()\nmean_improvement.rename(columns={'Improvement': 'Mean_Improvement'}, inplace=True)\n\n# Calculate overall mean improvement for each treatment\noverall_mean = data.groupby(['Treatment'], as_index=False)['Improvement'].mean()\noverall_mean.rename(columns={'Improvement': 'Mean_Improvement'}, inplace=True)\n\n# Create the disaggregated plot\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.stripplot(data=data, x='Hospital', y='Improvement', hue='Treatment', jitter=0.2, alpha=0.7, dodge=True)\nfor _, row in mean_improvement.iterrows():\n    plt.axhline(y=row['Mean_Improvement'], color=sns.color_palette('Set1')[0 if row['Treatment'] == 'Treatment 1' else 1], linestyle='solid')\nplt.title(\"By Hospital\")\nplt.xlabel(None)\nplt.ylabel(\"Health Improvement\")\nplt.legend(title=\"Treatment\", loc='upper right')\nplt.grid(True)\n\n# Create the aggregated plot\nplt.subplot(1, 2, 2)\nsns.stripplot(data=data, x='Aggregated', y='Improvement', hue='Treatment', jitter=0.2, alpha=0.7, dodge=True)\nfor _, row in overall_mean.iterrows():\n    plt.axhline(y=row['Mean_Improvement'], color=sns.color_palette('Set1')[0 if row['Treatment'] == 'Treatment 1' else 1], linestyle='solid')\nplt.title(\"Overall\")\nplt.xlabel(None)\nplt.ylabel(None)\nplt.legend([], [], frameon=False)\nplt.grid(True)\n\n# Add a common title\nplt.suptitle(\"Simpson's Paradox: Treatment Effectiveness Within Hospitals and Overall\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\nIn the aggregated data, \\(T_1\\) shows a higher average improvement, creating the illusion of greater effectiveness. But when disaggregated by hospital, the averages for \\(T_1\\) and \\(T_2\\) are identical."
  },
  {
    "objectID": "blog/paradox-simpson.html#where-to-learn-more",
    "href": "blog/paradox-simpson.html#where-to-learn-more",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nStart with the Wikipedia entry, where you will find all necessary additional resources."
  },
  {
    "objectID": "blog/paradox-simpson.html#bottom-line",
    "href": "blog/paradox-simpson.html#bottom-line",
    "title": "Simpson‚Äôs Paradox: A Simple Illustration",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nSimpson‚Äôs paradox manifests when an observable pattern within groups disappears if the data is aggregated.\nIt is a reminder of the critical role confounding variables play in data analysis.\nIt underscores the importance of stratifying data by meaningful subgroups and carefully considering the context before drawing conclusions from aggregated statistics."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html",
    "href": "blog/flavors-ml-methods-ci.html",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "",
    "text": "The most exciting trend in causal inference over the last decade has been the infusion of machine learning (ML) techniques. Supervised machine learning is designed to find complex patterns in data and as such, it is merely occupied with prediction. Causal inference, on the other hand, pays close attention to statistical precision and inference based on asymptotic properties like consistency and normality. The two worlds are, thus, fundamentally different.\nIt should be no surprise then that machine learning and causal inference do not naturally speak to each other, and some modifications are required to marry them. The good news is recent innovations have led to a bunch of ways in which ML models can be used in isolating causal effects especially in settings with many covariates (also called ‚Äúhigh dimensional‚Äù settings).\nIn this article, I will briefly describe the ways in which we can use ML when looking for causal relationships. I will indeed be concise, and I will avoid diving deeper into technical details. This blog post will look more like a laundry list with references to papers and software packages than a tutorial."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#background",
    "href": "blog/flavors-ml-methods-ci.html#background",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "",
    "text": "The most exciting trend in causal inference over the last decade has been the infusion of machine learning (ML) techniques. Supervised machine learning is designed to find complex patterns in data and as such, it is merely occupied with prediction. Causal inference, on the other hand, pays close attention to statistical precision and inference based on asymptotic properties like consistency and normality. The two worlds are, thus, fundamentally different.\nIt should be no surprise then that machine learning and causal inference do not naturally speak to each other, and some modifications are required to marry them. The good news is recent innovations have led to a bunch of ways in which ML models can be used in isolating causal effects especially in settings with many covariates (also called ‚Äúhigh dimensional‚Äù settings).\nIn this article, I will briefly describe the ways in which we can use ML when looking for causal relationships. I will indeed be concise, and I will avoid diving deeper into technical details. This blog post will look more like a laundry list with references to papers and software packages than a tutorial."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#notation",
    "href": "blog/flavors-ml-methods-ci.html#notation",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Notation",
    "text": "Notation\nIt is helpful to quickly summarize some features of the potential outcome framework. Imagine we have a i.i.d. random sample of a binary treatment indicator \\(D\\), outcome variable \\(Y\\) and a vector of covariates \\(X\\). Assume the potential outcomes \\(Y(0)\\) and \\(Y(1)\\) are unrelated to the binary treatment status \\(D\\) which is often referred to as the unconfoundedness or ignorability.\nA common estimand of interest is the Average Treatment Effect (ATE)\n\\[ATE = E[Y(1) - Y(0)],\\]\nwhere \\(Y(d)\\) is the potential outcome under treatment regime \\(D=d\\). Another popular estimand is the Conditional ATE (CATE),\n\\[CATE(X) = E[Y(1) - Y(0) | X],\\]\nwhich is the ATE for a particular group of units with a fixed covariates level (e.g., women, men, new users, etc.).\nThe ATE can be expressed in at least three useful ways:\n\\[\\begin{align*} ATE & = \\mathbf{E} \\left[ \\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(outcome model only)} \\\\ & = \\mathbf{E}\\left[ \\frac{YD}{e(X)} - \\frac{Y(1-D)}{1-e(X)} \\right] \\hspace{1cm} \\text{(prop. score model only)} \\\\ & = \\mathbf{E} \\left[ \\frac{[Y-\\mu(1,X)D]}{e(X)} - \\frac{[Y-\\mu(0,X)](1-D)}{1-e(X)} \\right] \\\\ & + \\mathbf{E} \\left[\\mu(1, X) - \\mu(0,X) \\right] \\hspace{1cm} \\text{(both models)} \\end{align*}\\]\nwhere\n\\[\\mu(D,X) = \\mathbf{E}[Y|D,X]\\]\nis the outcome model and\n\\[e(x)=\\mathbf{E}[D|X]\\]\nis the propensity score.\nThis formulation is helpful because it naturally splits the types of treatment effect estimation methods into three separate categories ‚Äì (i) those that require only estimation of \\(\\mu(D,X)\\), (ii) those that use only \\(e(X)\\), and (iii) those that need both.\nOne can think of the propensity score (PS) and the outcome models as nuisance functions ‚Äì ones that are not of direct interest but play a part in treatment effect estimation. ML methods are attractive candidates for estimating these nuisance functions flexibly."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#a-closer-look",
    "href": "blog/flavors-ml-methods-ci.html#a-closer-look",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nCovariate Balancing Methods\nUnder the ignorability assumption, all confounding bias comes from differences in the covariates \\(X\\) between the treatment and the control groups. Intuitively, balancing these is enough to guarantee unbiasedness. One line of research develops methods to do exactly that ‚Äì directly equate covariates between the two groups of interest. These approaches circumvent estimation of the two nuisance functions mentioned above.\nThese are inspired by the ML view of data analysis framed as an optimization problem. Examples include Entropy Balancing, Genetic Matching, Stable Weights, and Residual Balancing. The last approach combines balancing with a regression adjustment to reduce extrapolation when estimating the counterfactuals for the treatment group. Some of these methods were designed with a low dimensional setting in mind, but they still carry the spirit of ML type of thinking.\nSoftware Packages: MatchIt, Ebal, BalanceHD.\n\n\nML Methods for the Propensity Score Model\nPropensity score methods rely on correctly specifying the PS model. In low-dimensional settings, it is possible to estimate it nonparametrically. In practice, however, this is unrealistic when data scientists have access to continuous or even bunch of discrete covariates. Can ML methods come to the rescue?\nIn principle, yes. A major challenge in this context, however, is the choice of a loss function. In the ML world loss functions target measures of fit (e.g., Root Mean Squared Error, log likelihood, etc.) but these would be problematic here as they do not aim at balancing covariates important to reduce bias. Thus, these methods do not perform very well unless used with much caution.\nImai and Ratkovic (2014) propose a PS method that directly balances covariates. Another choice is the Boosted CART implementation. As its name suggests, it iteratively forms a bunch of tree models and averages them, but with an appropriately chosen loss function. A series of simulation studies analyze the performance of various ML algorithms used to estimate the PS, but overall, these methods are nowadays dominated by some of the doubly robust approaches described below.\nSoftware Packages: TWANG, CBPS.\n\n\nML Methods for the Outcome Model\nWe can also estimate treatment effects directly by modelling the outcome variable. This also requires correct model specification, and even then, it is prone to extrapolation in finite samples. Examples include Bayesian Additive Regression Trees (BART) and other ensemble methods.\nBelloni et al.¬†(2014) show that the set of features optimal when estimating the outcome model, is not necessarily optimal for estimating treatment effects. The issue is omitting a variable that is correlated with the treatment even if its correlation with the outcome is only modest, can introduce considerable bias. Moreover, typically, the rate of convergence in this context when using ML models will be slower than \\(\\sqrt{n}\\), meaning that you will need much more data to get a good treatment effect estimate.\nOverall, there is no statistical theory of why ML methods should work well here, but some methods tend to perform well empirically. This brings us to the doubly robust approach.\nSoftware Packages: BART, rBART, BayesTree.\n\n\nML Methods for Both Models & Doubly Robust Methods\nMethods combining models for both the propensity score and the outcome have long been advocated. Intuitively, the propensity score can be seen as a balancing step after which regression adjustment can remove any remaining bias. Imbens (2015), for instance, promotes this type of thinking in matching methods specifically.\nDoubly robust (DR) estimators use both nuisance models and have the amazing property of being consistent even if only one of the two models is correctly specified. You can think of the bias term as a product of the biases in the two nuisance models ‚Äì if one of them is equal to zero, the entire term vanishes. Additionally, if both models are correctly specified, some methods are semiparametrically efficient, (i.e., ‚Äúthe best‚Äù in a large class of flexible models). A simple DR method is the Augmented Inverse Probability Weighting (AIPW) estimator which in linear models comes down to running weighted OLS regression of \\(Y\\) on \\(D\\) and \\(X\\) with the estimated (inverse) propensity score as weights.\nThere is more good news. Amazingly, DR methods can still converge at a rate \\(\\sqrt{N}\\) even if the underlying nuisance models converge at slower rates. The formal requirement is that the nuisance models must belong to something called a Donsker class. In simple words, they should not be too complex and prone to overfit.\nOne line of research has developed the Double ML framework. This work has been so influential that it deserves a blog post of its own. Without going into technical details, the authors of the original paper show that na√Øve application of ML methods when estimating both nuisance functions results in two types of biases ‚Äì regularization and overfitting. DoubleML makes use of something called Neyman orthogonalization (think of the Frisch‚ÄìWaugh‚ÄìLovell theorem) to remove the former, and sample splitting to avoid the latter. In simple settings, this method combines the residuals from regressions of \\(Y\\) on \\(X\\) and \\(Y\\) on \\(D\\), but in general it can take more complicated forms.\nAnother line of research has developed the Double Post Lasso approach. The idea here is simpler ‚Äì use Lasso to select covariates relevant to the outcome regression and then again to select ones relevant to the propensity score. Lastly, use OLS to regress \\(Y\\) on the union of the covariates selected previously. This procedure removes confounding or regularization bias that might be present in methods using ML models to estimate only one of the nuisance models.\nSoftware Packages: DoubleML, hdm, dlsr.\n\n\nHeterogeneous Treatment Effect Estimation\nMining for heterogeneous treatment effects has been a particularly fruitful field for ML methods. A leading example is the causal tree method developed by Athey and Imbens (2016). It resembles the traditional CART algorithm, but it uses a different criterion for splitting the data: instead of focusing on Mean Squared Error (MSE) for the outcome, it uses MSE for treatment effect. The result is a decision tree, in which units in each leaf have similar treatment effects. The method also features ‚Äúhonest‚Äù sample splitting for obtaining variance estimates ‚Äì one half of the data is used to determine the optimal tree, and the other half to estimate the treatment effects.\nBuilding on this idea, Wager and Athey (2018) propose a random forest-based method which generates a bunch of causal trees and averages the results to induce smoothness in the treatment effect‚Äôs function. Magically, the authors show the predictions are asymptotically normal and centered around the true value for each unit! This is exciting as it allows for standard methods for statistical inference.\nOther methods include BART, a Bayesian version of random forests mentioned above, Imai and Ratkovic (2013) who propose adding treatment indicators interacted with covariates in a LASSO regression to determine which variables are important for treatment effect heterogeneity. Similarly, Tian et al.¬†(2014) suggest modifying the covariates in a straightforward way and running a regression of the outcome on the modified variables without an intercept. Other methods include the \\(R\\)-learner of Nie and Wager (2021) which relies on estimating the two nuisance models described above and using a special loss function. K√ºnzel et al.¬†(2019) propose a \\(X\\)-learner metaalogrithm.\nSoftware Packages: FindIt, rlearner, grf, causalToolbox.\n\n\nOthers\nA by-product of estimating treatment effect heterogeneity is that we can determine which units should be treated. Intuitively, if the treatment effect is close to zero (or even negative) for some users, there is not much to be gained from the exposure. Kitagawa and Tetenov (2018) analyze a setting with limited complexity, and Athey and Wager (2021) develop the DoubleML framework discussed above for choosing whom to treat. ML has also been used for variance reduction in randomized experiments via regression adjustments. See, for instance, Wager et al.¬†(2016), Bloniarz et al.¬†(2016), and List et al.¬†(2022)."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#bottom-line",
    "href": "blog/flavors-ml-methods-ci.html#bottom-line",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMachine learning methods are slowly becoming an indispensable part of data scientists‚Äô toolkit for estimating causal relationships. There is an abundance of methods aiding practitioners in both ATE and CATE estimation.\nDoubly robust approaches offer better theoretical guarantees than methods relying on estimating either the outcome or the propensity score models.\nThe leading approaches for estimating ATEs are Double ML and Double Post Lasso.\nThe leading approach for estimating CATEs is the causal forest method."
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#where-to-learn-more",
    "href": "blog/flavors-ml-methods-ci.html#where-to-learn-more",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nMore technical data scientists will find the following review papers useful:\n\nAthey and Imbens (2019)\nAthey and Imbens (2017)\nVarian (2014)\nKreif and DiazOrdaz (2019)\nMullainathan and Spiess (2017)\nHu (2023)\n\nThere are a few major Python frameworks for using ML in causal inference estimation. More practically-oriented folks might like their documentation:\n\nCausalML\nEconML\nDoubleML"
  },
  {
    "objectID": "blog/flavors-ml-methods-ci.html#references",
    "href": "blog/flavors-ml-methods-ci.html#references",
    "title": "An Overview of Machine Learning Methods in Causal Inference",
    "section": "References",
    "text": "References\nAthey, S., & Imbens, G. (2016). Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27), 7353-7360.\nAthey, S., & Imbens, G. W. (2017). The state of applied econometrics: Causality and policy evaluation. Journal of Economic perspectives, 31(2), 3-32.\nAthey, S., & Imbens, G. W. (2019). Machine learning methods that economists should know about. Annual Review of Economics, 11, 685-725.\nAthey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 80(4), 597-623.\nAthey, S., & Wager, S. (2021). Policy learning with observational data. Econometrica, 89(1), 133-161.\nAustin, P. C. (2012). Using ensemble-based methods for directly estimating causal effects: an investigation of tree-based G-computation. Multivariate behavioral research, 47(1), 115-135.\nBelloni, A., Chernozhukov, V., & Hansen, C. (2014). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2), 608-650.\nBloniarz, A., Liu, H., Zhang, C. H., Sekhon, J. S., & Yu, B. (2016). Lasso adjustments of treatment effect estimates in randomized experiments. Proceedings of the National Academy of Sciences, 113(27), 7383-7390.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal.\nDiamond, A., & Sekhon, J. S. (2013). Genetic matching for estimating causal effects: A general multivariate matching method for achieving balance in observational studies. Review of Economics and Statistics, 95(3), 932-945.\nHahn, P. R., Murray, J. S., & Carvalho, C. M. (2020). Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion). Bayesian Analysis, 15(3), 965-1056.\nHainmueller, J. (2012). Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20(1), 25-46.\nHill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240.\nImai, K., & Ratkovic, M. (2013). Estimating treatment effect heterogeneity in randomized program evaluation. Annals of Applied Statistics\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B: Statistical Methodology, 243-263.\nImbens, G. W. (2015). Matching methods in practice: Three examples. Journal of Human Resources, 50(2), 373-419.\nKitagawa, T., & Tetenov, A. (2018). Who should be treated? empirical welfare maximization methods for treatment choice. Econometrica, 86(2), 591-616.\nKreif, N., & DiazOrdaz, K. (2019). Machine learning in policy evaluation: new tools for causal inference. arXiv preprint arXiv:1903.00402.\nK√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165.\nLee, B. K., Lessler, J., & Stuart, E. A. (2010). Improving propensity score weighting using machine learning. Statistics in medicine, 29(3), 337-346.\nList, J. A., Muir, I., & Sun, G. K. (2022). Using Machine Learning for Efficient Flexible Regression Adjustment in Economic Experiments (No.¬†w30756). National Bureau of Economic Research.\nMcCaffrey, D. F., Ridgeway, G., & Morral, A. R. (2004). Propensity score estimation with boosted regression for evaluating causal effects in observational studies. Psychological methods, 9(4), 403.\nMullainathan, S., & Spiess, J. (2017). Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2), 87-106.\nNie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319.\nRobins, J. M., Rotnitzky, A., & Zhao, L. P. (1994). Estimation of regression coefficients when some regressors are not always observed. Journal of the American statistical Association, 89(427), 846-866.\nSetoguchi, S., Schneeweiss, S., Brookhart, M. A., Glynn, R. J., & Cook, E. F. (2008). Evaluating uses of data mining techniques in propensity score estimation: a simulation study. Pharmacoepidemiology and drug safety, 17(6), 546-555.\nTian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method for estimating interactions between a treatment and a large number of covariates. Journal of the American Statistical Association, 109(508), 1517-1532.\nVarian, H. R. (2014). Big data: New tricks for econometrics. Journal of Economic Perspectives, 28(2), 3-28.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nWager, S., Du, W., Taylor, J., & Tibshirani, R. J. (2016). High-dimensional regression adjustments in randomized experiments. Proceedings of the National Academy of Sciences, 113(45), 12673-12678.\nWestreich, D., Lessler, J., & Funk, M. J. (2010). Propensity score estimation: neural networks, support vector machines, decision trees (CART), and meta-classifiers as alternatives to logistic regression. Journal of clinical epidemiology, 63(8), 826-833.\nWyss, R., Ellis, A. R., Brookhart, M. A., Girman, C. J., Jonsson Funk, M., LoCasale, R., & St√ºrmer, T. (2014). The role of prediction modeling in propensity score estimation: an evaluation of logistic regression, bCART, and the covariate-balancing propensity score. American journal of epidemiology, 180(6), 645-655.\nZivich, P. N., & Breskin, A. (2021). Machine learning for causal inference: on the use of cross-fit estimators. Epidemiology (Cambridge, Mass.), 32(3), 393.\nZubizarreta, J. R. (2015). Stable weights that balance covariates for estimation with incomplete outcome data. Journal of the American Statistical Association, 110(511), 910-922."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html",
    "href": "blog/flavors-gradient-boosting.html",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "",
    "text": "Gradient boosting has emerged as one of the most powerful techniques for predictive modeling. In its simplest form, we can think of gradient boosting like having a team of detectives working in sequence, where each new detective specifically focuses on solving the cases where their predecessors stumbled. Each detective contributes their findings to the investigation, with earlier ones catching obvious clues and later ones piecing together the subtle evidence that was initially missed. The final solution emerges from combining all their work.\nWhile the term ‚Äúgradient boosting‚Äù is often used generically, there are notable differences among its implementations‚Äîeach with unique strengths, weaknesses, and practical applications. Understanding these nuances is essential for advanced data scientists aiming to choose the best method for their specific datasets and computational constraints.\nThis article aims to provide a brief overview of the most popular gradient boosting methods, delving into their mathematical foundations and unique characteristics. By the end, you will have a clearer understanding of when and how to use each method to achieve the best results in your predictive modeling tasks. At the end of the article, I‚Äôll provide a step-by-step Python example that implements these algorithms."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#background",
    "href": "blog/flavors-gradient-boosting.html#background",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "",
    "text": "Gradient boosting has emerged as one of the most powerful techniques for predictive modeling. In its simplest form, we can think of gradient boosting like having a team of detectives working in sequence, where each new detective specifically focuses on solving the cases where their predecessors stumbled. Each detective contributes their findings to the investigation, with earlier ones catching obvious clues and later ones piecing together the subtle evidence that was initially missed. The final solution emerges from combining all their work.\nWhile the term ‚Äúgradient boosting‚Äù is often used generically, there are notable differences among its implementations‚Äîeach with unique strengths, weaknesses, and practical applications. Understanding these nuances is essential for advanced data scientists aiming to choose the best method for their specific datasets and computational constraints.\nThis article aims to provide a brief overview of the most popular gradient boosting methods, delving into their mathematical foundations and unique characteristics. By the end, you will have a clearer understanding of when and how to use each method to achieve the best results in your predictive modeling tasks. At the end of the article, I‚Äôll provide a step-by-step Python example that implements these algorithms."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#notation",
    "href": "blog/flavors-gradient-boosting.html#notation",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Notation",
    "text": "Notation\nI assume a mathematical familiarity with machine learning (ML) basics and some minimal previous exposure to gradient boosting. If you need a refresher, grab any introductory ML textbook.\nBefore diving into the specifics of each method, let‚Äôs establish some common notation that will be used throughout this article:\n\n\\(\\mathbf{X}\\): Covariates/features matrix\n\\(\\mathbf{y}\\): Outcome/target variable\n\\(f(x)\\): Predictive model\n\\(L(y, \\hat{y})\\): Loss function\n\\(\\hat{y}\\): Predicted outcome/target value\n\\(\\gamma\\): Learning rate\n\\(n\\): Number of observations/instances\n\\(M\\): Number of algorithm iterations."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#a-closer-look",
    "href": "blog/flavors-gradient-boosting.html#a-closer-look",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nRefresher on Gradient Boosting\nGradient boosting is an ensemble machine learning technique that builds models sequentially, each new model attempting to correct the errors of the previous ones. The general idea is to minimize a loss function by adding weak learners (typically short decision trees) in a stage-wise manner to arrive at a single strong learner. This iterative process allows the model to improve its performance gradually, making it highly effective for complex datasets. Gradient boosting methods are versatile in that they can be used both for regression and classifications problems. In the most common case when the weak learners are decision trees, the algorithm is known as gradient tree boosting.\nMathematically, the model is built as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m = 1\\) to \\(M\\):\n\nCompute the pseudo-residuals: \\(r_{im} = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}\\) for \\(i=1,\\dots,n\\). In regression tasks this is simply \\(y-\\hat{y}\\).\nFit a weak learner such as a tree, \\(h_m(x)\\), on \\(\\{(x_i, r_{im}) \\}_{i=1}^n.\\)\nCompute \\(\\gamma\\) by solving: \\(\\gamma=\\arg\\min\\sum_i L(y_i, f_{m-1}(x_i)+\\gamma h_m(x_i)).\\)\nUpdate the model: \\(f_m(x) = f_{m-1}(x) + \\gamma h_m(x).\\)\n\nThe final model is \\(f_M(x).\\)\n\n\n\nThis is the most generic recipe for a gradient boosting algorithm. Let‚Äôs now focus on more specific implementations of this idea.\n\n\nAdaBoost\nAdaBoost, short for Adaptive Boosting, is one of the earliest boosting algorithms. It adjusts the weights of incorrectly classified observations so that subsequent learners focus more on difficult ones. In other words, it works by learning from its mistakes ‚Äì after each round of predictions, it identifies which observartions it got wrong and pays special attention to them (i.e., gives them a higher weight) in the next round. Thus, AdaBoost is not strictly speaking a gradient boosting algorithm, in the modern sense of the term.\nLet‚Äôs consider a binary classification problem where the outcome variable takes values in \\(\\{ -1, 1\\}\\). Here is the general AdaBoost algorithm:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nInitialize weights \\(w_i = \\frac{1}{n}\\) for \\(i = 1, \\ldots, n.\\)\nFor \\(m = 1 \\dots M\\):\n\nTrain a weak learner \\(h_m(x)\\) using weights \\(w_i\\).\nCompute the weighted error: \\(\\epsilon_m = \\frac{\\sum_{i=1}^{n} w_i \\mathbb{I}(y_i \\neq h_m(x_i))}{\\sum_{i=1}^{n} w_i}\\), where \\(\\mathbb{I}(\\cdot)\\) is the indicator function.\nCompute the model weight:$ _m = ()$.\nUpdate and \\(w_i^{m+1}= w_i^{m} \\exp(-\\alpha_m y_i h_m(x_i))\\), and \\(w_i^{m+1} = \\frac{ w_i^{m+1}}{\\sum_j  w_j^{m+1}}\\) for \\(i=1,\\dots,n\\).\n\nThe final model is \\(f(x)=sign \\left( \\sum_m \\alpha_m h_m(x) \\right)\\).\n\n\n\nAdaBoost is simple to implement while being relatively resistant to overfitting, making it especially effective for problems with clean, well-structured data. Its adaptive nature means it can identify and focus on the most challenging observations. However, AdaBoost has notable weaknesses: it‚Äôs highly sensitive to noisy data and outliers (since it increases weights on misclassified examples), can perform poorly when working with insufficient training data, and tends to be computationally intensive compared to newer, simpler algorithms.\nSoftware Packages: adabag, gbm, scikit-learn.\n\n\nXGBoost\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting designed for speed and performance. It revolutionizes the method by using the Newton-Raphson method in function space, setting it apart from traditional gradient boosting‚Äôs simpler gradient descent approach. At its core, XGBoost leverages both the first and second derivatives of the loss function through a second-order Taylor approximation. This sophisticated approach enables faster convergence and more accurate predictions by making better-informed decisions about how to improve the model at each step. When building decision trees, XGBoost considers both the expected improvement and the uncertainty of potential splits, while simultaneously applying regularization to prevent overfitting.\nWith some simplifications, the regression version of the XGBoost is implemented as follows:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nInitialize the model with a constant value: \\(f_0(x) = \\arg\\min_{\\gamma} \\sum_i L(y_i, \\gamma)\\)\nFor \\(m=1, \\dots M\\):\n\nCompute the gradients \\[grad_m(x_i)=\\left( \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right)_{f(x)=f_{m-1}(x)}\\] and hessians \\[hess_m(x_i)=\\left( \\frac{\\partial^2 L(y_i, f(x_i))}{\\partial^2 f(x_i)^2}\\right)_{f(x)=f_{m-1}(x)}\\] of the loss function for \\(i=1,\\dots,n\\).\nFit a weak learner such as a tree, \\(h_m(x)\\) on$ {(x_i, ) }_{i=1}^n$.\nUpdate the model \\(f_m(x)=f_{(m-1)} - \\alpha h_m(x).\\)\n\nThe final model is \\(f_M(x)=\\sum_m f_m(x)\\).\n\n\n\nXGBoost is ideal for large datasets and competitions where model performance is critical. It is also a good choice when you need a highly optimized and scalable solution as it is highly efficient, supports parallel and distributed computing. Nevertheless, this algorithm can be complex to tune, and may require significant computational resources for very large datasets.\nSoftware Packages: xgboost.\n\n\nCatBoost\nCatBoost, short for Categorical Boosting, is a gradient boosting library that handles categorical features efficiently. It uses ordered boosting to reduce overfitting and supports GPU training, making it both powerful and versatile. CatBoost modifies the standard gradient boosting algorithm by incorporating two novel techniques ‚Äì ordered boosting and target statistics for categorical features. Let‚Äôs examine each one in turn.\nRather than requiring preprocessing like one-hot encoding, CatBoost encodes categorical values based on the distribution of the target variable without introducing data leakage. This is achieved by encoding each data point as if it were unseen, preventing overfitting. Additionally, ordered boosting is a method that builds each new tree while treating each data point as ‚Äúout-of-fold‚Äù for itself. This helps reduce overfitting, particularly in small datasets, by preventing over-reliance on individual observations during training.\nSoftware Packages: catboost.\n\n\nLightGBM\nLightGBM shares many of XGBoost‚Äôs benefits, such as support for sparse data, parallel training, multiple loss functions, regularization, and early stopping, but it also introduces a bunc of new features and improvements.\nFirst, rather than growing trees level-wise (row by row) as in most implementations, LightGBM grows trees leaf-wise, selecting the leaf that provides the greatest reduction in loss. Second, it does not use the typical sorted-based approach for finding split points, which sorts feature values to locate the best split. Instead, it relies on an efficient histogram-based method that significantly improves both speed and memory efficiency. Third, LightGBM incorporates the so-called Gradient-Based One-Side Sampling, which speeds up training by focusing on the most informative samples. Lastly, the algorithm uses Exclusive Feature Bundling which can group features to reduce dimensionality, enabling faster and more accurate model training.\nSoftware Packages: lightgbm.\n\n\nChallenges\nGradinet boosting methods comes with a few common practical challenges. When a predicitve model learns the training data too precisely, it may perform poorly on new, unseen data ‚Äì a problem called overfitting. To combat this, various regularization methods are used to add helpful constraints to the learning process. A main parameter that regulates the model‚Äôs learning precision is the number of boosting rounds (\\(M\\)), which determines how many base models are created. While using more rounds reduces training errors, it also increases overfitting risk. To find the optimal \\(M\\) value, it‚Äôs common to use cross validation. The depth of decision trees is another important control parameter in tree boosting. Deeper trees can capture more complex patterns but are more prone to memorizing the training data rather than learning generalizable patterns.\nThe improved predictive performance of gradient boosting relative to simpler models comes at a cost of reduced model transparency. While a single decision tree‚Äôs reasoning can be easily traced and understood, tracking the decision-making process across numerous trees becomes extremely complex and challenging. Gradient boosting is an excellent example of the inherent tradeoff between model simplicity and performance.\nLet‚Äôs now look at how can we use these methods in practice."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#an-example",
    "href": "blog/flavors-gradient-boosting.html#an-example",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "An Example",
    "text": "An Example\nHere is some sample python code illustrating the implementation of each algorithm described above on a common dataset. Let‚Äôs look at it in detail.\n# loading the necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\n# we load the data and split it into training and test parts.\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# define and implement the boosting algorithms. \nclassifiers = {\n    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n    \"XGBoost\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42),\n    \"LightGBM\": LGBMClassifier(n_estimators=100, random_state=42),\n    \"CatBoost\": CatBoostClassifier(n_estimators=100, verbose=0, random_state=42)\n}\n\n# save results\nresults = {}\nfor name, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = accuracy\nFinally, we print the accuracy results:\n\nfor name, accuracy in results.items():\n    print(f\"{name}: {accuracy:.4f}\")\n\n# results:\n&gt;AdaBoost: 0.9737\n&gt;XGBoost: 0.9561\n&gt;LightGBM: 0.9649\n&gt;CatBoost: 0.9649\nOverall each method performed reasonably well, with acuracy ranging from \\(95.6\\)% to \\(97.4\\)%. Interestingly, Adaboost outperformed the other more complex algorithms, at least in the in terms of accuracy.\nAnd that‚Äôs it. You are now familiar with the most popular implementations of gradient boosting along with their advantages and weaknesses. You also know how to employ them in practice. Have fun incorporating XGboost and the like into your predictive modeling tasks."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#bottom-line",
    "href": "blog/flavors-gradient-boosting.html#bottom-line",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nGradient boosting is a powerful ensemble technique for predictive modeling that comes in a variety of flavors.\nAdaBoost focuses on misclassified instances by adjusting weights.\nXGBoost introduces regularization and optimization for speed and performance.\nCatBoost efficiently handles categorical features and reduces overfitting.\nLightGBM enjoys many of XGBoost‚Äôs strenghts while introducing a few novelties including a different way of building the underlying weak learners.\nCommon practical challenges when implementing gradient boosting include overfitting, decreased interpretability and computational costs."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#where-to-learn-more",
    "href": "blog/flavors-gradient-boosting.html#where-to-learn-more",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great startint point, and it‚Äôs a resource I used extensively when preparing this article. ‚ÄúThe Elements of Statistical Learning‚Äù by Hastie, Tibshirani, and Friedman is a comprehensive guide that covers the theoretical foundations of machine learning, including gradient boosting. It is the de facto bible for statistical ML. While this book is phenomenal, it can be challenging for less technical practitioners for which I recommend its lighther versions, ‚ÄúAn Introduction to Statistical Learning‚Äù with R and Python code. All these books are available for free online. Lastly, if you want to dive even deeper into any of the algortihms descibe above, consider studying the papers in the References section below."
  },
  {
    "objectID": "blog/flavors-gradient-boosting.html#references",
    "href": "blog/flavors-gradient-boosting.html#references",
    "title": "Gradient Boosting Methods: A Brief Overview",
    "section": "References",
    "text": "References\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.¬†785-794).\nDorogush, A. V., Ershov, V., & Gulin, A. (2018). CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363.\nFreund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139.\nFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The elements of statistical learning: data mining, inference, and prediction.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An introduction to statistical learning: With applications in python. Springer Nature.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2013). An introduction to statistical learning: With applications in R. Springer Nature.\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ‚Ä¶ & Liu, T. Y. (2017). Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30.\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: unbiased boosting with categorical features. Advances in neural information processing systems, 31."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html",
    "href": "blog/flavors-multiple-testing.html",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "",
    "text": "The abundance of data around us is a major factor making the data science field so attractive. It enables all kinds of impactful, interesting, or fun analyses. I admit this is what got me into statistics and causal inference in the first place. However, this luxury has downsides ‚Äì it might require deeper methodological knowledge and attention.\nIf we are interested in measuring statistical significance, the standard frequentist framework relies on testing a single hypothesis on any given dataset. Once we start using the same data multiple times ‚Äì i.e., testing several hypotheses at once ‚Äì as we often do, we might have to make some additional adjustments.\nIn this article, I will walk you through the most popular multiple hypotheses (MH) testing corrections. Common examples of when these come to play include estimating the impact of several variables on a single outcome or estimating the effect of an intervention on several subgroups of users to mine for heterogeneous treatment effects. Both scenarios are ubiquitous in most data analysis projects.\nTo illustrate the problem, imagine we are testing seven hypotheses at the \\(5\\%\\) significance level. Then, the probability that we incorrectly reject at least one of these hypotheses is:\n\\[ 1 - (1-.05)^7 = .30. \\]\nThat is, roughly one in three times, we will reach a wrong conclusion.\nVaguely speaking, MH adjustments can be viewed as inflating the calculated p-values and hence decreasing the probability that we reject any given hypothesis. In other words, we are accounting for using the data multiple times by making the definition of statistical significance more stringent."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#background",
    "href": "blog/flavors-multiple-testing.html#background",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "",
    "text": "The abundance of data around us is a major factor making the data science field so attractive. It enables all kinds of impactful, interesting, or fun analyses. I admit this is what got me into statistics and causal inference in the first place. However, this luxury has downsides ‚Äì it might require deeper methodological knowledge and attention.\nIf we are interested in measuring statistical significance, the standard frequentist framework relies on testing a single hypothesis on any given dataset. Once we start using the same data multiple times ‚Äì i.e., testing several hypotheses at once ‚Äì as we often do, we might have to make some additional adjustments.\nIn this article, I will walk you through the most popular multiple hypotheses (MH) testing corrections. Common examples of when these come to play include estimating the impact of several variables on a single outcome or estimating the effect of an intervention on several subgroups of users to mine for heterogeneous treatment effects. Both scenarios are ubiquitous in most data analysis projects.\nTo illustrate the problem, imagine we are testing seven hypotheses at the \\(5\\%\\) significance level. Then, the probability that we incorrectly reject at least one of these hypotheses is:\n\\[ 1 - (1-.05)^7 = .30. \\]\nThat is, roughly one in three times, we will reach a wrong conclusion.\nVaguely speaking, MH adjustments can be viewed as inflating the calculated p-values and hence decreasing the probability that we reject any given hypothesis. In other words, we are accounting for using the data multiple times by making the definition of statistical significance more stringent."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#notation",
    "href": "blog/flavors-multiple-testing.html#notation",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Notation",
    "text": "Notation\nWe are interested in testing a bunch of m null hypotheses about a population parameter \\(\\beta\\).\nAs a running example, you can think of \\(\\beta\\) as the causal impact of a new product feature on user engagement and \\(m\\) as indexing some geographical regions such as cities. We are interested in whether the new feature is more impactful in some cities than others. We will denote these hypotheses with \\(H_1, H_2, \\dots, H_m\\) and refer to their associated p-values with \\(p_1, p_2, \\dots, p_m\\).\nWe use \\(\\alpha\\) to denote the probability of a Type 1 error ‚Äì rejecting a true null (i.e., a false positive). In technical jargon, we refer to \\(\\alpha\\) as test size. We often choose \\(\\alpha=.05\\), meaning that we are allowing a 5% chance that we will make such an error. This corresponds to the 95% confidence intervals that we often see.\nNext, statistical power is the probability of correctly rejecting a false null hypothesis (i.e., a true positive). This is a desirable property ‚Äì the higher it is, the better. While this terminology does not describe the entire hypothesis testing framework, it does cover what is necessary to continue reading the article."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#a-closer-look",
    "href": "blog/flavors-multiple-testing.html#a-closer-look",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nWhen is MH Control NOT Necessary\nMultiple hypothesis adjustments are not required when:\n\nwe are interested in a purely predictive model, such as in machine learning settings,\nwe have an independent dataset for each correlated hypothesis,\nthe dependent variables we are testing are not correlated. An example is testing the impact of a new feature or intervention on a set of outcomes unrelated to each other ‚Äì e.g., health and education variables. In such cases, it is common to adjust for MH separately within the health- and education-related hypotheses.\n\nBefore we dive into the methods, let‚Äôs establish some basic terminology and notation.\n\n\nFWER vs FDR\nThere are two distinct types of targets that data scientists are after. The first one was introduced in the 1950s by the famous statistician John Tukey and is called Familywise Error Rate (FWER). FWER is the probability of making at least one type 1 error:\n\\[ FWER = \\mathbb{P} (\\text{At Least 1 Type 1 Error}). \\]\nIn the example from the introduction, \\(FWER = 0.3\\).\nControlling the FWER is good, but sometimes it is not great. It does what it is supposed to do ‚Äì limit the number of false positives, but it is often too conservative. In practice, too few variables remain significant after such FWER adjustments, especially when testing many (as opposed to just a few) hypotheses.\nTo correct this, in the 1990s, Yoav Benjamini and Yosef Hochberg, conceptualized controlling the so-called False Discovery Rate (FDR). FDR is the expected proportion of false positives:\n\\[ FDR = \\mathbb{E} \\left[ \\frac{\\text{\\# False Positives}}{\\text{\\# False Positives + \\# True Positives} } \\right]. \\]\nTake a minute to notice the significant difference between these two approaches. FWER controls the probability of having at least one false positive at \\(\\alpha\\). At the same time, FDR is okay with having several false positives as long as they are (on average) less than \\(\\alpha\\) of all hypotheses.\nFWER is more conservative than FDR, resulting in fewer significant variables. In our example, applying an FWER adjustment as opposed to an FDR one will lead to fewer statistically significant differences in the impact of the new feature across various cities.\nLet‚Äôs go over the most popular ways to control the FWER.\n\n\nFWER Control\n\nBonferroni‚Äôs Procedure\nYou might have heard of this one. The procedure is extremely simple ‚Äì it goes like this:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nreject \\(H_i\\) if \\(p_i \\times m  \\leq \\alpha\\) for all \\(i\\).\n\n\n\nIn words, we multiply all \\(p\\)-values by m and reject the hypotheses with adjusted \\(p\\)-values that are smaller than or equal to \\(\\alpha\\).\nThe Bonferroni adjustment is often considered to control the FWER, but it actually controls an even more conservative target ‚Äì the per-family error rate (PFER). PFER is the sum of probabilities of Type 1 error for all the hypotheses.\nYou correctly guessed that PFER is considered too strict. Although it is common in practice, there are better alternatives. While the Bonferroni correction is simple and elegant, it is always dominated by the Holm-Bonferroni procedure, so there is really no reason ever to use it.\n\n\nHolm & Bonferroni‚Äôs Procedure\nThis adjustment offers greater statistical power than the Bonferroni method regardless of the test size without significantly compromising on simplicity. In algorithmic language:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m.\\)\nReject the null hypothesis \\(H_1,\\dots H_{k-1}\\), where \\(k\\) is the smallest index for which \\(p_i \\times (m+1-k) \\leq \\alpha\\).\n\n\n\nMuch like the Bonferroni method with a multiplication factor of (\\(m+1-k\\)) instead of \\(m\\).\nThese two procedures control the FWER by assuming a kind of a ‚Äòworst-case‚Äô scenario in which the \\(p\\)-values are close to being independent of each other. To see why this is the case, imagine the extreme opposite scenario in which there is perfect positive dependence. Then, there is effectively one test statistic, and FWER adjustment is unnecessary.\nIn practice, however, the \\(p\\)-values are often correlated, and exploiting this dependence can result in even more powerful FWER corrections. This can be achieved with various resampling methods, such as the bootstrap or permutation procedures.\n\n\nWestfall & Young‚Äòs Procedure\nThe Westfall & Young method, developed in the 1990s, was the first resampling procedure controlling the FWER. It uses the bootstrap to account for the dependence structure among the \\(p\\)-values and improves on Holm-Bonferroni‚Äôs method.\nIt goes roughly like this:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(m\\) \\(p\\)-values in ascending order ‚Äì \\(p_1, \\dots, p_M\\).\nBegin with a \\(COUNTER_i = 0\\) for each hypothesis \\(i\\).\nTake a bootstrap sample and compute the \\(m\\) \\(p\\)-values (\\(p^*_1, \\dots, p^*_m\\))\nDefine the successive minima by starting with the largest \\(p\\)-value (\\(\\tilde{p}_M=p^*_M\\)) and for each subsequent hypothesis \\(i\\) taking the minimum between \\(\\tilde{p}_{i+1}\\) and \\(p^*_i\\). This gives a list of \\(p\\)-values \\(\\tilde{p}_1, \\dots, \\tilde{p}_m\\).\nIf \\(\\tilde{p}_i \\leq p_i\\) add 1 unit to \\(COUNTER_i\\) for each \\(i\\).\nRepeat steps 3-5 above \\(B\\) times and compute the fraction \\(\\hat{p}_i = COUNT_i/B\\). This gives a list of \\(p\\)-values \\(\\hat{p}_1, \\dots, \\hat{p}_m\\).\nEnforce monotonicity by starting with \\(\\hat{p}_1\\) and for each subsequent hypothesis \\(i\\) taking the maximum between \\(\\hat{p}_{i-1}\\) and \\(\\hat{p}_i\\). This final vector presents the FWER-adjusted \\(p\\)-values.\nReject all null hypotheses \\(i\\) for which \\(\\hat{p}\\leq \\alpha\\).\n\n\n\nYes, this is a complicated procedure. But it does offer substantial improvements in power, especially if the hypotheses are positively correlated.\nImportantly, Westfall and Young‚Äôs method rests on a critical assumption called ‚Äúsubset pivotality.‚Äù Roughly speaking, this condition requires that the joint distribution of any subset of test statistics does not depend on whether the remaining hypotheses are true or not.\nSoftware Package: multtest.\n\n\nRomano & Wolf‚Äòs Procedure(s)\nThis procedure improves on Westfall and Young‚Äôs method by not requiring a pivotality assumption. Here I am describing the method from Romano and Wolf‚Äôs 2005 Econometrica paper, although the authors have developed a few closely related FWER adjustments. This is the rough algorithm for it:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the test statistics in descending order, \\(t_1 \\geq t_2 \\geq, \\dots, t_m\\).\nTake B bootstrap samples, and for each hypothesis i compute the empirical distribution function of the successive maximum test statistic random variable. That is, the random variable of the maximum test statistic between \\(t_i, t_{i+1}, \\dots, t_m\\). Call this distribution \\(c(i)\\).\nReject \\(H_i\\) if \\(t_i\\) is greater than the 1-quantile of \\(c(1)\\).\nDenote by h the number of rejected hypotheses. If \\(h=0\\) stop. Otherwise, for \\(H_{h+1},\\dots H_s\\):\nReject \\(H_i\\) if \\(t_i &gt; c(h+1)\\)\nIterate until no further hypotheses are rejected.\n\n\n\nAnd you thought the Westfall and Young correction was tricky. The good news is that there are software packages for all these adjustments, so you won‚Äôt have to program them yourself. Arguably, this one of the best ways to control FWER; does not rest on pivotality assumptions. However, it can be difficult to explain to non-technical audiences; difficult to explain to anyone, really.\nSoftware Package: hdm.\nThis wraps up the descriptions of the methods controlling the FWER.\n\n\n\nFDR Control\nAs a reminder, FDR procedures have greater power at the cost of increased rates of type 1 errors compared to FWER adjustments. This is the dominant framework in the modern literature on MH adjustment.\n\nBenjamini & Hochberg‚Äòs Procedure\nThe Benjamini-Hochberg (BH) method is simple:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m}{k} \\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\n\n\nBH is valid when the tests are independent and in some, but not all, dependence scenarios. In general, FDR control is tricky with arbitrary dependence.\nThis adjustment has an interesting geometric interpretation:\n\nplot \\(p\\) versus \\(k\\),\ndraw a line through the origin with a slope equal to \\(frac{\\alpha}{m}\\),\nreject all hypotheses associated with the \\(p\\)-values on the left up to the last point below this line.\n\n\n\nBenjamini & Yekutieli‚Äòs Procedure\nBenjamini and Yekutieli (BY) developed a refinement of the BH procedure, which adds a magical constant \\(c(m)\\):\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nOrder the \\(p\\)-values in ascending order ‚Äì \\(p_1, p_2, \\dots, p_m\\).\nFind the largest integer \\(k\\) such that \\(p_k \\times \\frac{m.c(m)}{k}\\leq \\alpha\\).\nReject the null hypotheses \\(H_1, \\dots, H_k\\).\n\n\n\nThe obvious question is in choosing the optimal value for \\(c(m)\\). You can view BH as a special case of BY with \\(c(m)=1\\). The authors show that the following choice works under any arbitrary dependence assumption:\n\\[ c(m) = \\sum_{i=1}^m \\frac{1}{i} \\approx log(m) + 0.57721 + \\frac{1}{2m}. \\]\nFascinating. For most applications, this simple procedure provides a credible way of taming the false positives rate. There are improvements to this method, but it feels like we are entering a world in which we get theoretical gains with only limited practical implications.\nThe main way to improve on BH/BY is to know (or estimate) the share of false hypotheses among the ones we are testing. This, however, goes beyond the scope of the article.\n\n\nKnockoffs\nWhat if we would like to combine variable selection and control the FDR? BH and BY do not really tell us how to do that. Enter knockoffs. The knockoffs approach is the kid on the block, and it does more than just FDR control.\nThe idea behind the knockoffs is novel and unrelated to any of the methods I discussed above. Knockoffs are copies of the variables with specific properties. In particular, for each feature/covariate variable \\(X\\), we create a knockoff copy \\(\\tilde{X}\\) such that the correlation structure between any two knockoff variables is the same as between their original counterparts. These knockoffs are constructed without looking at the outcome variable; hence, they are unrelated to it by design. Consequently, we can gauge whether each variable is statistically significant by comparing its test statistic to that of the one for its knockoff copy.\nKnockoffs are among the most active fields of research in statistics, with new papers coming out daily (ok, maybe weekly). These methods come in two flavors.\nThe fixed-design knockoff filter controls FDR for low-dimensional linear models regardless of the dependency structure of the features. This is great, but it does not work in high dimensions where variable selection is usually most needed.\nThe model-X knockoff filter solves this issue but requires exact knowledge of the joint distribution of the features. This is often infeasible in practice; even estimating that distribution in high dimensions can be really challenging as the curse of dimensionality lays its hammer on the data scientist.\nThe knockoff idea presents a big step forward because it solves two statistical problems at the same time ‚Äì selecting among a wide set of predictors and controlling the FDR. I am really excited to see where the current research will take us next.\nSoftware Package: knockoff.\n\n\nOther Modern Methods\nThere is certainly no shortage of statistical methods in this field. Each week, there is at least one new paper dealing with some aspects of FDR or FWER control. Knockoffs, in particular, have gotten a lot of attention recently, with researchers extending the idea to all kinds of settings and models.\nI will briefly mention some of the newer methods that have caught my eye in recent months. Many of these control the FDR asymptotically, and some are valid under general dependency structures among the test statistics.\nOne recent variation of the knockoffs idea uses Gaussian Mirrors; while others rely on subsampling approaches ‚Äì Data Splitting (based on estimating two independent regression coefficients after splitting the dataset in half) and Data Aggregation. Clever ideas to improve power include using covariate information to optimally weigh hypotheses in the BH framework or incorporating them into FDR regressions.\nTime will show whether any of these newcomers will come to dominate in practice."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#an-example",
    "href": "blog/flavors-multiple-testing.html#an-example",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "An Example",
    "text": "An Example\nI used the popular Titanic dataset (\\(n=889\\)) to illustrate some of the methods I discussed above. Check the Kaggle website carefully for the descriptions of each variable. The outcome/response variable, survived, indicated whether the passenger survived the disaster (mean=\\(0.382\\)), while the predictors included demographic characteristics (e.g., age, gender) as well as some information about the travel ticket (e.g., cabin number, fare).\n\n\n\nNumber of Statistically Significant Variables\n\n\nHere is a table of the \\(p\\)-values for each feature and various MH adjustments.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaw\nBonferroni\nHolm\nRom.-Wolf\nBenj.-Hoch.\nBenj.-Yek.\n\n\n\n\n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nage\n0.00\n0.02\n0.01\n0.01\n0.00\n0.01\n\n\nsibsp\n0.02\n0.34\n0.18\n0.16\n0.04\n0.14\n\n\nparch\n0.32\n1.00\n1.00\n0.77\n0.40\n1.00\n\n\nfare\n0.22\n1.00\n1.00\n0.66\n0.30\n0.98\n\n\nmale\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nembarkedC\n0.01\n0.23\n0.15\n0.14\n0.04\n0.13\n\n\nembarkedQ\n0.05\n0.76\n0.35\n0.29\n0.09\n0.28\n\n\ncabinA\n0.39\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinB\n0.37\n1.00\n1.00\n0.77\n0.42\n1.00\n\n\ncabinC\n0.92\n1.00\n1.00\n0.92\n0.92\n1.00\n\n\ncabinD\n0.06\n0.89\n0.36\n0.29\n0.09\n0.30\n\n\ncabinE\n0.01\n0.07\n0.05\n0.05\n0.01\n0.05\n\n\ncabinF\n0.02\n0.31\n0.18\n0.16\n0.04\n0.14\n\n\n\nEight variables were statistically significant without any MH adjustment. As expected, the FWER adjustments lead to fewer significant variables than the FDR ones. Interestingly, there is a noticeable difference between the two FDR methods ‚Äì BH and BY, with the latter being more conservative.\nYou can find the code for this analysis in this GitHub repository."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#bottom-line",
    "href": "blog/flavors-multiple-testing.html#bottom-line",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMultiple testing corrections are likely necessary when you are using a single dataset multiple times (i.e., testing multiple hypotheses).\nTwo major frameworks exist ‚Äì FWER and FDR control. The former is often considered too conservative, while the latter is the dominant way researchers and practitioners think about correcting for MH testing.\nIn most settings, the Benjamini-Yekuiteli approach offers a great balance between statistical power and technical simplicity.\nKnockoffs are a novel and exciting approach to FDR control."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#where-to-learn-more",
    "href": "blog/flavors-multiple-testing.html#where-to-learn-more",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nWikipedia is a great resource on the classic approaches to tackling MH issues (both for FDR and FWER control), but it lacks material on modern methodologies. Emmanuel Cand√®s‚Äô website features an accessible introduction to the world of knockoffs. Clarke et al.¬†(2020) strip down many technicalities and provide accessible descriptions of both Westfall and Young‚Äôs, as well as Romano and Wolf‚Äôs methods. Korthauer et al.¬†(2019) compare some of the more recent approaches to controlling the FDR, which are beyond the scope of this blog post."
  },
  {
    "objectID": "blog/flavors-multiple-testing.html#references",
    "href": "blog/flavors-multiple-testing.html#references",
    "title": "Multiple Testing: Methods Overview (Part 1)",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2015). Controlling the false discovery rate via knockoffs. The Annals of Statistics, 43(5), 2055-2085.\nBenjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of Statistics, 1165-1188.\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300.\nCandes, E., Fan, Y., Janson, L., & Lv, J. (2018). Panning for gold:‚Äômodel‚ÄêX‚Äôknockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3), 551-577.\nClarke, D., Romano, J. P., & Wolf, M. (2020). The Romano‚ÄìWolf multiple-hypothesis correction in Stata. The Stata Journal, 20(4), 812-843.\nDai, C., Lin, B., Xing, X., & Liu, J. S. (2022). False discovery rate control via data splitting. Journal of the American Statistical Association, 1-38.\nKorthauer, K., Kimes, P. K., Duvallet, C., Reyes, A., Subramanian, A., Teng, M., ‚Ä¶ & Hicks, S. C. (2019). A practical guide to methods controlling false discoveries in computational biology. Genome biology, 20(1), 1-21.\nRomano, J. P., & Wolf, M. (2005). Stepwise multiple testing as formalized data snooping. Econometrica, 73(4), 1237-1282.\nRomano, J. P., & Wolf, M. (2005). Exact and approximate stepdown methods for multiple hypothesis testing. Journal of the American Statistical Association, 100(469), 94-108.\nWestfall, P. H., & Young, S. S. (1993). Resampling-based multiple testing: Examples and methods for p-value adjustment (Vol. 279). John Wiley & Sons.\nXing, X., Zhao, Z., & Liu, J. S. (2021). Controlling false discovery rate using gaussian mirrors. Journal of the American Statistical Association, 1-20."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html",
    "href": "blog/flavors-het-treatment-effects.html",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "",
    "text": "Numerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods ‚Äì the so-called \\(S\\)-, \\(T\\)-, \\(X\\)- and \\(R\\)-learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as the lasso, gradient boosting, or even neural networks. In some clever sense, \\(STXR\\) is the \\(ABC\\) of heterogeneous treatment effect estimation.\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The CausalML Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#background",
    "href": "blog/flavors-het-treatment-effects.html#background",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "",
    "text": "Numerous tales illustrate the inadequacy of the average to capture meaningful quantities. Statisticians love these. In my favorite one the protagonist places her head in a burning oven and her feet in an ice bucket before declaring she is fine on average. Looking beyond the average is often quite important.\nThis is especially salient in policy evaluation where we design interventions, programs or product features aimed at improving the outcomes for specific groups of students, voters, users, etc. Heterogeneous treatment effects in these cases might be even more important than the overall average one.\nThe Machine Learning (ML) toolbox offers among the most powerful methods to detect such heterogeneity in formalized, data-driven ways. Today I will describe four model-agnostic methods ‚Äì the so-called \\(S\\)-, \\(T\\)-, \\(X\\)- and \\(R\\)-learners (i.e., estimators). By model-free I mean that these methods work with any predictive models such as the lasso, gradient boosting, or even neural networks. In some clever sense, \\(STXR\\) is the \\(ABC\\) of heterogeneous treatment effect estimation.\nMy goal is to give a high-level overview, without getting lost in technical details. Enthusiastic data scientists can jump to the Where to Learn More section below to dive deep into the theory when needed. The CausalML Python package implements many of these methods, so I encourage everyone who has never used it to go ahead and try them."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#notation",
    "href": "blog/flavors-het-treatment-effects.html#notation",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Notation",
    "text": "Notation\nAs usual, let‚Äôs begin by setting some mathematical notation. I use D to denote a binary treatment indicator, \\(Y\\) is the observed outcome and \\(X\\) is a covariate of interest. The potential outcomes under each treatment state are \\(Y(0)\\) and \\(Y(1)\\), and \\(p\\) is the (conditional) probability of assignment into the treatment (i.e., the propensity score).\nThe average treatment effect is then the mean difference in potential outcomes across all units:\n\\[ATE = E[Y(1)-E(0)].\\]\nInterest is, instead, in the ATE for units with values \\(X=x\\) which I refer to as the heterogeneous treatment effect, \\(HTE(X)\\):\n\\[HTE(X) = E[Y(1)-E(0)|X].\\]\nIt is also helpful to define the conditional outcome functions under each treatment state:\n\\[\\mu(X,d) = E[Y(d)|X].\\]\nIt then follows that \\(HTE(X)\\) can also be expressed as:\n\\[HTE(X) = \\mu(X,1) - \\mu(X,0).\\]"
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#a-closer-look",
    "href": "blog/flavors-het-treatment-effects.html#a-closer-look",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "A Closer Look",
    "text": "A Closer Look\nI will now briefly describe the four learners for heterogeneous treatment effects in ascending order of complexity.\n\n\\(S\\)(ingle) Learner\nThe idea behind the \\(S\\)-learner is to estimate a single outcome function \\(\\mu(X,D)\\) and then calculate \\(HTE(X)\\) by taking the difference in the predicted values between the units in the treatment and control groups.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nUse the entire sample to estimate \\(\\hat{\\mu}(X,D)\\).\nCompute \\(\\hat{HTE}(X)=\\hat{\\mu}(X,1)-\\hat{\\mu}(X,0)\\).\n\n\n\nThis is intuitive and fine. But the problem is that the treatment variable D might be excluded in step 1 when it is not highly correlated with the outcome. (Note that in observational data this does not necessarily imply a null treatment effect.) In this case, we cannot even move to step 2.\n\n\n\\(T\\)(wo) Learner\nThe \\(T\\)-learner solves the above problem by forcing the response models to include \\(D\\). The idea is to first estimate two separate (conditional) outcome functions ‚Äì one for the treatment and one for the control and proceed similarly.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for \\(\\hat{\\mu}(X,1)\\).\nThen \\(\\hat{HTE}(X)=\\hat{\\mu}(X,1)- \\hat{\\mu}(X,0)\\)\n\n\n\nThis is better, but still not great. A potential problem is when there are different number of observations in the treatment and control groups. For instance, in the common case where the treatment group is much smaller, their \\(HTE(X)\\) will be estimated much less precisely than the that of the control group. When combining the two to arrive at a final estimate of \\(HTE(X)\\) the former should get a smaller weight than the latter. This is because the ML algorithms optimize for learning the \\(\\mu(\\cdot)\\) functions, and not the \\(HTE(X)\\) function directly.\n\n\n\\(X\\) Learner\nThe \\(X\\)-learner is designed to overcome the above concern. The procedure starts similarly to the \\(T\\)-learner but then weighs differently the \\(HTE(X)\\)‚Äôs for the treatment and control groups.\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nUse the observations in the control group to estimate \\(\\hat{\\mu}(X,0)\\) and the ones in the treatment effect for \\(\\hat{\\mu}(X,1)\\).\nEstimate the unit-level treatment effect for the observations in the control group, \\(\\hat{\\mu}(X,1)-Y\\), and for the treatment group, \\(Y-\\hat{\\mu}(X,0)\\).\nCombine both estimates by weighing them using the predicted propensity score, \\(\\hat{p}\\): \\[HTE(X)=\\hat{p} \\times HTE(X|D=1) + (1-\\hat{p})\\times HTE(X|D=0).\\]\n\n\n\nHere \\(\\hat{p}\\) balances the uncertainty associated with the \\(HTE(X)\\)‚Äôs in each group and hence, this approach is particularly effective when there is a significant difference in the number of units between the two groups.\n\n\n\\(R\\)(obinson) Learner\nTo avoid getting too deep into technical details, I will not be describing the entire algorithm. The \\(R\\)-learner models both the outcome, and the propensity score and begins by computing unit-level predicted outcomes and treatment probabilities using cross-fitting and leave-one-out estimation. The key innovation is plugging these into a novel loss function featuring squared deviations of these predictions as well as a regularization term. This ensures ‚Äúoptimality‚Äù in learning the treatment effect heterogeneity."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#bottom-line",
    "href": "blog/flavors-het-treatment-effects.html#bottom-line",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nML methods offer a promising way of determining which groups of units experience differential response to treatments.\nI summarized four such model-agnostic methods ‚Äì the \\(S\\)-, \\(T\\)-, \\(X\\)-, and \\(R\\)-learners.\nCompared to the simpler \\(S\\)- and \\(T\\)- learners, the \\(X\\)- and \\(R\\)-learners solve some common issues and are more attractive options in most settings."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#where-to-learn-more",
    "href": "blog/flavors-het-treatment-effects.html#where-to-learn-more",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nI have previously written on how ML can be useful in causal inference, more generally and how we can use the Lasso to estimate heterogeneous treatment effects. Hu (2022) offers a detailed summary of a bunch of ML methods for HTE estimation. A Statistical Odds and Ends blog post describes the \\(S\\)-, \\(T\\)- and \\(X\\)-learners and contains useful advice on when each of them is preferrable. Chapter 21 of Causal Inference for the Brave and True also discusses this material and provides useful examples."
  },
  {
    "objectID": "blog/flavors-het-treatment-effects.html#references",
    "href": "blog/flavors-het-treatment-effects.html#references",
    "title": "The Alphabet of Learners for Heterogeneous Treatment Effects",
    "section": "References",
    "text": "References\nHu, A. (2022). Heterogeneous treatment effects analysis for social scientists: A review. Social Science Research, 102810.\nK√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10), 4156-4165. Nie,\nXie, N., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319."
  },
  {
    "objectID": "blog-unpublished/prop-score-cont-var.html",
    "href": "blog-unpublished/prop-score-cont-var.html",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "",
    "text": "Propensity scores are one of the most celebrated ideas in modern causal inference. They elegantly reduce a high-dimensional covariate adjustment problem into a one-dimensional balancing act. But here‚Äôs the catch: the canonical setup assumes a binary treatment. Treated or not. Drug or placebo. Exposed or unexposed. That‚Äôs great, but real-world interventions are rarely so black-and-white.\nWhat if your treatment is a dosage? Or a level of exposure? Or some index that varies continuously, like air pollution levels, advertising intensity, or participation hours in a program? That‚Äôs when we enter the more nuanced world of continuous treatment causal inference, and our trusty binary propensity score needs a serious upgrade.\nIn this article, we‚Äôll journey from the classic binary case through discrete levels of treatment, and ultimately arrive at the continuous treatment setting. Along the way, we‚Äôll unpack the intuition and math behind generalized propensity scores, density estimation, and what it really means to estimate a dose-response function. We‚Äôll arm you with the tools to think clearly and rigorously about treatment effects when there are more than just two treatment arms‚Äîor infinitely many."
  },
  {
    "objectID": "blog-unpublished/prop-score-cont-var.html#background",
    "href": "blog-unpublished/prop-score-cont-var.html#background",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "",
    "text": "Propensity scores are one of the most celebrated ideas in modern causal inference. They elegantly reduce a high-dimensional covariate adjustment problem into a one-dimensional balancing act. But here‚Äôs the catch: the canonical setup assumes a binary treatment. Treated or not. Drug or placebo. Exposed or unexposed. That‚Äôs great, but real-world interventions are rarely so black-and-white.\nWhat if your treatment is a dosage? Or a level of exposure? Or some index that varies continuously, like air pollution levels, advertising intensity, or participation hours in a program? That‚Äôs when we enter the more nuanced world of continuous treatment causal inference, and our trusty binary propensity score needs a serious upgrade.\nIn this article, we‚Äôll journey from the classic binary case through discrete levels of treatment, and ultimately arrive at the continuous treatment setting. Along the way, we‚Äôll unpack the intuition and math behind generalized propensity scores, density estimation, and what it really means to estimate a dose-response function. We‚Äôll arm you with the tools to think clearly and rigorously about treatment effects when there are more than just two treatment arms‚Äîor infinitely many."
  },
  {
    "objectID": "blog-unpublished/prop-score-cont-var.html#notation",
    "href": "blog-unpublished/prop-score-cont-var.html#notation",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs set up some notation that will help us keep our thoughts clean as we move through different levels of treatment granularity.\nLet: - \\(Y \\in \\mathbb{R}\\): observed outcome. - \\(T \\in \\mathbb{R}\\): treatment variable, which can be binary, discrete, or continuous. - \\(X \\in \\mathbb{R}^p\\): vector of observed pre-treatment covariates. - \\(Y(t)\\): potential outcome if the individual were assigned treatment level \\(t\\).\nOur goal is to estimate a treatment effect function, like: - For binary: \\(\\mathbb{E}[Y(1) - Y(0)]\\), - For continuous: \\(\\mathbb{E}[Y(t)]\\) for all \\(t \\in \\mathbb{R}\\), also known as the dose-response function.\nThe key object of interest generalizes accordingly: - In the binary case: the propensity score is \\(e(X) = \\Pr(T=1 \\mid X)\\). - In the continuous case: the generalized propensity score (GPS) is the conditional density \\(r(t, X) = f_{T|X}(t \\mid X)\\)."
  },
  {
    "objectID": "blog-unpublished/prop-score-cont-var.html#a-closer-look",
    "href": "blog-unpublished/prop-score-cont-var.html#a-closer-look",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nBinary Treatment: The Classic Setup\nIn the binary world, Rosenbaum and Rubin (1983) showed that adjusting for the propensity score \\(e(X)\\) is sufficient to remove confounding, under the assumption of strong ignorability: \\[\nY(1), Y(0) \\perp T \\mid X.\n\\]\nThey also proved that \\(Y(1), Y(0) \\perp T \\mid e(X)\\), which means we can reduce the dimensionality of covariate adjustment from \\(p\\) to 1. Score!\nPropensity scores can be estimated via logistic regression or any predictive ML method, and treatment effects can be estimated via matching, inverse probability weighting (IPW), or regression adjustment using \\(e(X)\\).\n\n\nDiscrete Treatment: More Than Two Levels\nWhat if treatment takes on three or more levels? Say, a low, medium, and high dose of a medication?\nIn that case, we generalize the propensity score into a vector: one probability for each treatment level, conditional on covariates: \\[\ne_j(X) = \\Pr(T = j \\mid X), \\quad j = 1, \\dots, K.\n\\]\nThe adjustment strategy is similar: match or weight across strata of these multinomial probabilities to balance covariates. Some methods treat this as a multi-class classification problem. But what if treatment isn‚Äôt just levels 1, 2, or 3, but a smooth continuum?\n\n\nContinuous Treatment: The Generalized Propensity Score\nNow we hit the interesting case. Suppose \\(T \\in \\mathbb{R}\\) and can take on many (even infinite) values. Instead of estimating a probability, we estimate a density. The generalized propensity score is defined as:\n\\[\nr(t, X) = f_{T|X}(t \\mid X),\n\\]\nthe conditional density of the treatment given covariates. This is a continuous analogue of the classic propensity score.\nJust like before, we assume weak unconfoundedness (Hirano and Imbens, 2004):\n\\[\nY(t) \\perp T \\mid X \\quad \\text{for all } t.\n\\]\nAnd just like in the binary case, conditioning on the GPS balances covariates, but now at each level of \\(t\\). Hirano and Imbens showed that adjusting for \\(r(T, X)\\) is sufficient for identification of the dose-response function \\(\\mu(t) = \\mathbb{E}[Y(t)]\\).\n\n\nEstimating the Dose-Response Function\nThe general workflow goes like this:\n\n\n\n\n\n\nAlgorithm:\n\n\n\n\nEstimate the GPS: Fit a model for the conditional density \\(f_{T|X}(t \\mid X)\\). This could be a normal model, or a more flexible density estimator.\nModel the outcome given treatment and GPS: Fit a model for \\(\\mathbb{E}[Y \\mid T=t, R=r]\\), where \\(R = r(t, X)\\) is the estimated GPS.\nAverage over the population: For a fixed value \\(t\\), compute: \\[\\hat{\\mu}(t) = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}(t, \\hat{r}(t, X_i)),\\] where \\(\\hat{m}\\) is the estimated conditional expectation of \\(Y\\) given \\(T\\) and \\(R\\).\n\n\n\nThis approach estimates the full dose-response curve \\(t \\mapsto \\mu(t)\\), giving you a complete picture of how outcomes evolve with different levels of treatment.\n\n\nA Note on Density Estimation\nDensity estimation is the crux of this whole approach. You‚Äôre estimating \\(f_{T|X}(t \\mid X)\\), which can be tricky. If you assume normality:\n\\[T \\mid X \\sim \\mathcal{N}(\\mu(X), \\sigma^2(X)),\\]\nthen you can fit a regression model for \\(T\\) and use the residuals to compute the density. For more flexibility, kernel density estimation or machine learning methods like normalizing flows can be used to approximate the conditional distribution of \\(T \\mid X\\).\nKeep in mind: poor density estimation leads to poor GPS, which leads to biased treatment effect estimates. Garbage in, garbage out.\nWhile density estimation is the cornerstone of the GPS approach, it becomes increasingly challenging as the number of covariates grows‚Äîa phenomenon often called the ‚Äúcurse of dimensionality.‚Äù In high-dimensional spaces issues include data sparistiy making local density estimation unreliable, parametric assumptions also become increasingly restircive. To address these challenges, consider dimensionality reduction techniques before density estimation, semi-parametric approaches that make assumptions on the functional form but allow flexibility in other aspects, or Bayesian nonparametric methods that adapt to the complexity of the data."
  },
  {
    "objectID": "blog-unpublished/prop-score-cont-var.html#covariate-balancing-generalized-propensity-scores-cbgps",
    "href": "blog-unpublished/prop-score-cont-var.html#covariate-balancing-generalized-propensity-scores-cbgps",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Covariate Balancing Generalized Propensity Scores (CBGPS)",
    "text": "Covariate Balancing Generalized Propensity Scores (CBGPS)\nCBGPS, introduced by Fong, Hazlett, and Imai (2018), directly optimizes covariate balance rather than focusing on accurately modeling the treatment mechanism. It estimates weights \\(w_i\\) by solving a set of moment conditions that ensure covariates are uncorrelated with treatment after weighting: \\[ \\sum_{i=1}^n w_i(T_i - \\bar{T})X_i = 0 \\]\nThis approach avoids sequential modeling and estimation, potentially reducing bias from model misspecification. It‚Äôs particularly valuable when the treatment mechanism is complex or difficult to model accurately.\n\nDiagnostics\nAssessing GPS quality requires thorough diagnostics focused on three key areas: covariate balance checks, model diagnostics, and sensitivity analysis. For covariate balance, practitioners should evaluate whether correlations between covariates and treatment approach zero after GPS adjustment, check balance within treatment strata, and create visual plots of standardized differences. GPS model quality can be verified through residual analysis against covariates and treatments, Q-Q plots to assess distributional assumptions, and cross-validation to evaluate predictive performance. Sensitivity testing should include trimming extreme GPS values, comparing results across different model specifications, and conducting placebo tests on outcomes that should be unaffected by treatment. While perfect balance may be unattainable, these diagnostics build confidence in causal estimates by revealing substantial improvements over unadjusted comparisons and identifying potential estimation issues."
  },
  {
    "objectID": "blog-unpublished/prop-score-cont-var.html#an-example",
    "href": "blog-unpublished/prop-score-cont-var.html#an-example",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "An Example",
    "text": "An Example\nLet‚Äôs try estimating a dose-response function using simulated data.\n\nRPython\n\n\nlibrary(MASS)\nlibrary(np)\n\n# Simulate data\nset.seed(1988)\nn &lt;- 500\nX &lt;- matrix(rnorm(n * 3), n, 3)\nT &lt;- 0.5 * X[,1] - 0.3 * X[,2] + rnorm(n)\nY &lt;- 2 + 3 * T - T^2 + 0.5 * X[,1] + rnorm(n)\n\n# Estimate GPS via kernel regression\ngps_model &lt;- npcdensbw(xdat = data.frame(X), ydat = T)\ngps &lt;- npcdens(bws = gps_model, xdat = data.frame(X), ydat = T)$condens\n\n# Estimate outcome model\ndf &lt;- data.frame(Y = Y, T = T, GPS = gps)\nfit &lt;- lm(Y ~ poly(T, 2) + GPS + T*GPS, data = df)\n\n# Estimate dose-response\nt_vals &lt;- seq(min(T), max(T), length.out = 100)\ngps_vals &lt;- predict(np::npudens(~ T + X1 + X2 + X3, data = data.frame(T = t_vals, X1 = X[,1], X2 = X[,2], X3 = X[,3])))\npreds &lt;- predict(fit, newdata = data.frame(T = t_vals, GPS = gps_vals))\nplot(t_vals, preds, type = 'l', lwd = 2)\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KernelDensity\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Simulate data\nnp.random.seed(1988)\nn = 500\nX = np.random.randn(n, 3)\nT = 0.5 * X[:, 0] - 0.3 * X[:, 1] + np.random.randn(n)\nY = 2 + 3 * T - T**2 + 0.5 * X[:, 0] + np.random.randn(n)\n\n# Estimate GPS: assume normality\nfrom sklearn.linear_model import LinearRegression\ngps_model = LinearRegression().fit(X, T)\nmu = gps_model.predict(X)\nresid = T - mu\nsigma = np.std(resid)\ngps = norm.pdf(T, loc=mu, scale=sigma)\n\n# Fit outcome model\ndf = pd.DataFrame({'Y': Y, 'T': T, 'GPS': gps})\ndf['T2'] = T**2\ndf['T_GPS'] = T * gps\nX_outcome = df[['T', 'T2', 'GPS', 'T_GPS']]\nfit = LinearRegression().fit(X_outcome, df['Y'])\n\n# Estimate dose-response\nt_vals = np.linspace(T.min(), T.max(), 100)\nmu_vals = gps_model.predict(X)\ndose_response = []\nfor t in t_vals:\n    gps_t = norm.pdf(t, loc=mu_vals, scale=sigma)\n    X_pred = np.column_stack((np.repeat(t, n), np.repeat(t**2, n), gps_t, t * gps_t))\n    preds = fit.predict(X_pred)\n    dose_response.append(np.mean(preds))\n\nplt.plot(t_vals, dose_response)\nplt.title(\"Estimated Dose-Response Function\")\nplt.xlabel(\"Treatment (T)\")\nplt.ylabel(\"Expected Outcome\")\nplt.show()"
  },
  {
    "objectID": "blog-unpublished/prop-score-cont-var.html#bottom-line",
    "href": "blog-unpublished/prop-score-cont-var.html#bottom-line",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nPropensity score methods can be extended beyond binary treatments to continuous treatments using generalized propensity scores.\nThe GPS is the conditional density of treatment given covariates: \\(f_{T‚à£X}(t\\mid X)\\).\nYou estimate the dose-response function by modeling outcomes as a function of both treatment and GPS, then averaging.\nEstimating the GPS well is crucial‚Äîgarbage density estimates lead to poor causal conclusions."
  },
  {
    "objectID": "blog-unpublished/prop-score-cont-var.html#where-to-learn-more",
    "href": "blog-unpublished/prop-score-cont-var.html#where-to-learn-more",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nIf you‚Äôre interested in mastering this topic, I recommend starting with Hirano and Imbens‚Äô 2004 Econometrica paper, which formally introduces the generalized propensity score framework. From there, take a look at more recent work in causal machine learning and semiparametric methods (like those in the DoubleML or EconML libraries), which handle continuous treatments and flexible outcome models. Also, keep an eye on newer approaches using generative models and normalizing flows for high-quality density estimation‚Äîan exciting frontier for continuous causal inference."
  },
  {
    "objectID": "blog-unpublished/prop-score-cont-var.html#references",
    "href": "blog-unpublished/prop-score-cont-var.html#references",
    "title": "Propensity Scores with a Continuous Twist: Estimating Treatment Effects Beyond Binary",
    "section": "References",
    "text": "References\nBrown, D. W., Greene, T. J., Swartz, M. D., Wilkinson, A. V., & DeSantis, S. M. (2021). Propensity score stratification methods for continuous treatments. Statistics in medicine, 40(5), 1189-1203.\nHirano, K., & Imbens, G. W. (2004). The propensity score with continuous treatments. Econometrica, 73(2), 731‚Äì748.\nImai, K., & van Dyk, D. A. (2004). Causal inference with general treatment regimes: Generalizing the propensity score. Journal of the American Statistical Association.\nFong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score for a continuous treatment: Application to the efficacy of political advertisements. The Annals of Applied Statistics, 12(1), 156-177.\nKluve, J., Schneider, H., Uhlendorff, A., & Zhao, Z. (2012). Evaluating continuous training programmes by using the generalized propensity score. Journal of the Royal Statistical Society Series A: Statistics in Society, 175(2), 587-617.\nMcCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). A tutorial on propensity score estimation for multiple treatments using generalized boosted models. Statistics in medicine, 32(19), 3388-3414.\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika."
  },
  {
    "objectID": "blog-unpublished/flavors-lasso.html",
    "href": "blog-unpublished/flavors-lasso.html",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "",
    "text": "The Lasso (Least Absolute Shrinkage and Selection Operator), introduced by Tibshirani in 1996, has become one of the go-to tools for variable selection and shrinkage in regression problems. But the classic Lasso is just the starting point. Over the years, researchers have developed many variants of Lasso, each designed to address specific limitations or tailor the method to different kinds of data structures.\nThis article provides a tour of the most popular flavors of Lasso ‚Äî from standard L1-penalized regression to modern adaptations like Adaptive Lasso, Elastic Net, Square-root Lasso, and more. For each version, we‚Äôll lay out the objective function, describe when it‚Äôs applicable, and summarize its key characteristics."
  },
  {
    "objectID": "blog-unpublished/flavors-lasso.html#background",
    "href": "blog-unpublished/flavors-lasso.html#background",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "",
    "text": "The Lasso (Least Absolute Shrinkage and Selection Operator), introduced by Tibshirani in 1996, has become one of the go-to tools for variable selection and shrinkage in regression problems. But the classic Lasso is just the starting point. Over the years, researchers have developed many variants of Lasso, each designed to address specific limitations or tailor the method to different kinds of data structures.\nThis article provides a tour of the most popular flavors of Lasso ‚Äî from standard L1-penalized regression to modern adaptations like Adaptive Lasso, Elastic Net, Square-root Lasso, and more. For each version, we‚Äôll lay out the objective function, describe when it‚Äôs applicable, and summarize its key characteristics."
  },
  {
    "objectID": "blog-unpublished/flavors-lasso.html#a-closer-look",
    "href": "blog-unpublished/flavors-lasso.html#a-closer-look",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "A Closer Look",
    "text": "A Closer Look\n\n1. Standard Lasso (Tibshirani, 1996)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right)\n\\]\n\nKey feature: Encourages sparsity by shrinking some coefficients exactly to zero.\nWhen to use: Variable selection with many predictors; handles high-dimensional data.\nCharacteristics: Shrinkage bias; struggles with highly correlated predictors.\n\n\n\n2. Adaptive Lasso (Zou, 2006)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^p w_j | \\beta_j | \\right)\n\\] where \\(w_j = 1 / |\\hat{\\beta}_j^{\\text{init}}|^\\gamma\\).\n\nKey feature: Oracle property (support recovery consistency).\nWhen to use: When initial estimates (like OLS or Ridge) are available.\nCharacteristics: Bias reduction via adaptive weights.\n\n\n\n3. Relaxed Lasso (Meinshausen, 2007)\n\nStep 1: Use Lasso for selection.\nStep 2: Refit OLS on the selected variables or apply partial shrinkage with a relaxation parameter \\(\\phi\\).\nKey feature: Separates selection from estimation.\nWhen to use: Reduce Lasso‚Äôs shrinkage bias after variable selection.\nCharacteristics: Combines selection strength of Lasso with unbiased estimation of OLS.\n\n\n\n4. Square-root Lasso / Scaled Lasso (Belloni, Chernozhukov, Wang, 2011)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{\\sqrt{n}} \\| y - X \\beta \\|_2 + \\lambda \\| \\beta \\|_1 \\right)\n\\]\n\nKey feature: Scale-invariant ‚Äî does not require estimating error variance.\nWhen to use: Unknown or heteroskedastic error variance.\nCharacteristics: Easier tuning; robust to variance misspecification.\n\n\n\n5. Elastic Net (Zou and Hastie, 2005)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\| \\beta \\|_2^2 \\right)\n\\]\n\nKey feature: Combines L1 and L2 penalties.\nWhen to use: Multicollinearity among predictors.\nCharacteristics: Handles correlated variables better than standard Lasso.\n\n\n\n6. Group Lasso (Yuan & Lin, 2006)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{g=1}^G \\| \\beta^{(g)} \\|_2 \\right)\n\\]\n\nKey feature: Selects groups of variables together.\nWhen to use: Categorical variables or grouped data structures.\nCharacteristics: Encourages sparsity at the group level, not individual coefficients.\n\n\n\n7. Fused Lasso (Tibshirani et al., 2005)\nObjective Function: \\[\n\\hat{\\beta} = \\arg \\min_{\\beta} \\left( \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\sum_{j=2}^p | \\beta_j - \\beta_{j-1} | \\right)\n\\]\n\nKey feature: Penalizes differences between adjacent coefficients.\nWhen to use: Ordered features like time series or spatial data.\nCharacteristics: Promotes both sparsity and smoothness.\n\n\n\n8. Bayesian Lasso (Park & Casella, 2008)\n\nObjective (via prior): \\[\n\\beta_j \\sim \\text{Laplace}(0, \\lambda^{-1}).\n\\]\nKey feature: Fully Bayesian formulation with posterior inference.\nWhen to use: When uncertainty quantification (credible intervals) is desired.\nCharacteristics: Shrinkage through Laplace priors; allows full posterior analysis.\n\n\n\n9. Graphical Lasso (Friedman et al., 2008)\nObjective Function: \\[\n\\hat{\\Theta} = \\arg \\min_{\\Theta \\succ 0} \\left( -\\log \\det \\Theta + \\text{trace}(S \\Theta) + \\lambda \\| \\Theta \\|_1 \\right)\n\\]\n\nKey feature: Estimates sparse precision (inverse covariance) matrices.\nWhen to use: Gaussian graphical models (network structure estimation).\nCharacteristics: Encourages sparsity in partial correlations.\n\n\n\n10. Stability Selection (Meinshausen & B√ºhlmann, 2010)\n\nNot an objective function per se ‚Äî combines subsampling with Lasso to control false discovery rate.\nKey feature: Improves selection stability and robustness.\nWhen to use: When worried about unstable variable selection.\nCharacteristics: Reduces false positives; provides error control guarantees.\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\nVariant\nKey Feature\nMotivation\n\n\n\n\nStandard Lasso\nL1 penalty, sparsity\nVariable selection\n\n\nAdaptive Lasso\nWeighted penalties, oracle property\nBias reduction\n\n\nElastic Net\nCombines L1 and L2 penalties\nHandles multicollinearity\n\n\nGroup Lasso\nGroup-wise selection\nGrouped variables\n\n\nFused Lasso\nPenalizes differences between coefficients\nTime-series or spatial data\n\n\nSquare-root Lasso\nScale-free loss function\nUnknown error variance\n\n\nBayesian Lasso\nLaplace priors in a Bayesian framework\nPosterior inference, uncertainty\n\n\nGraphical Lasso\nPenalizes inverse covariance matrix\nGaussian graphical models\n\n\nRelaxed Lasso\nSelection and estimation separated\nBias reduction after selection\n\n\nStability Selection\nAdds subsampling for robustness\nControls false discoveries"
  },
  {
    "objectID": "blog-unpublished/flavors-lasso.html#bottom-line",
    "href": "blog-unpublished/flavors-lasso.html#bottom-line",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nDifferent Lasso variants address different modeling challenges like scaling, grouping, collinearity, and bias.\nUnderstanding your data structure and inference goals helps choose the right Lasso flavor.\nMost major machine learning libraries (R: glmnet, grpreg; Python: scikit-learn, statsmodels) provide support for these variants."
  },
  {
    "objectID": "blog-unpublished/flavors-lasso.html#where-to-learn-more",
    "href": "blog-unpublished/flavors-lasso.html#where-to-learn-more",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nKey papers: Tibshirani (1996), Zou (2006), Meinshausen (2007), Yuan and Lin (2006), Belloni et al.¬†(2011), Park and Casella (2008), Friedman et al.¬†(2008), Meinshausen and B√ºhlmann (2010)."
  },
  {
    "objectID": "blog-unpublished/flavors-lasso.html#references",
    "href": "blog-unpublished/flavors-lasso.html#references",
    "title": "Lasso Flavors: Variants, Objective Functions, and When to Use Them",
    "section": "References",
    "text": "References\n[TO ADD]"
  },
  {
    "objectID": "blog-unpublished/flavors-pca.html",
    "href": "blog-unpublished/flavors-pca.html",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "",
    "text": "Principal Components Analysis (PCA) is one of the most widely used techniques for dimensionality reduction and unsupervised learning. Originally introduced by Karl Pearson in 1901 and later formalized by Harold Hotelling in 1933, PCA aims to find new, uncorrelated variables (the principal components) that successively maximize the variance in the data. It provides a systematic way to summarize high-dimensional datasets with fewer dimensions, making it easier to visualize, compress, or preprocess data for further modeling.\nBeyond standard PCA, many variants and extensions have been developed to handle specific challenges like sparsity, robustness to outliers, missing data, and non-linear structures. These PCA flavors extend the core idea to different contexts, making PCA a versatile workhorse in both theory and practice.\nIn this article, we‚Äôll explain the classical PCA approach thoroughly and then introduce several important PCA variants, providing their mathematical formulations and discussing when and why they are useful."
  },
  {
    "objectID": "blog-unpublished/flavors-pca.html#background",
    "href": "blog-unpublished/flavors-pca.html#background",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "",
    "text": "Principal Components Analysis (PCA) is one of the most widely used techniques for dimensionality reduction and unsupervised learning. Originally introduced by Karl Pearson in 1901 and later formalized by Harold Hotelling in 1933, PCA aims to find new, uncorrelated variables (the principal components) that successively maximize the variance in the data. It provides a systematic way to summarize high-dimensional datasets with fewer dimensions, making it easier to visualize, compress, or preprocess data for further modeling.\nBeyond standard PCA, many variants and extensions have been developed to handle specific challenges like sparsity, robustness to outliers, missing data, and non-linear structures. These PCA flavors extend the core idea to different contexts, making PCA a versatile workhorse in both theory and practice.\nIn this article, we‚Äôll explain the classical PCA approach thoroughly and then introduce several important PCA variants, providing their mathematical formulations and discussing when and why they are useful."
  },
  {
    "objectID": "blog-unpublished/flavors-pca.html#notation",
    "href": "blog-unpublished/flavors-pca.html#notation",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "Notation",
    "text": "Notation\nLet \\(X \\in \\mathbb{R}^{n \\times p}\\) be a data matrix with: - \\(n\\) observations (rows), - \\(p\\) variables (columns).\nAssume that each column of \\(X\\) has been centered (mean zero): \\[\n\\frac{1}{n} \\sum_{i=1}^n X_{ij} = 0 \\quad \\text{for all} \\ j = 1, \\dots, p.\n\\]\nThe empirical covariance matrix of \\(X\\) is: \\[\n\\Sigma = \\frac{1}{n} X^T X.\n\\]\nOur goal is to find new orthogonal directions (principal components) \\(u_1, \\dots, u_p \\in \\mathbb{R}^p\\), such that projecting the data onto these directions captures the maximum variance."
  },
  {
    "objectID": "blog-unpublished/flavors-pca.html#a-closer-look",
    "href": "blog-unpublished/flavors-pca.html#a-closer-look",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nClassical PCA: Variance Maximization and Eigen Decomposition\nThe principal components are obtained by solving the following optimization problem: \\[\n\\max_{u \\in \\mathbb{R}^p} u^T \\Sigma u \\quad \\text{subject to} \\quad \\| u \\|_2 = 1.\n\\]\nThis is a Rayleigh quotient maximization problem, whose solution is the eigenvector of \\(\\Sigma\\) corresponding to the largest eigenvalue. The eigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0\\) represent the variances explained by each principal component.\n\nSequential Extraction\nThe \\(k\\)-th principal component direction \\(u_k\\) is obtained by solving: \\[\n\\max_{u} u^T \\Sigma u, \\quad \\text{subject to} \\quad \\| u \\|_2 = 1, \\quad u^T u_j = 0 \\quad \\text{for} \\ j = 1, \\dots, k-1.\n\\]\nThe principal components themselves (the transformed data) are: \\[\nZ = X U,\n\\] where \\(U = [u_1, u_2, \\dots, u_p]\\) is the matrix of eigenvectors.\n\n\nSingular Value Decomposition (SVD) Formulation\nPCA can also be performed via Singular Value Decomposition (SVD) of the centered data matrix \\(X\\): \\[\nX = U D V^T,\n\\] where: - \\(U \\in \\mathbb{R}^{n \\times p}\\) contains the left singular vectors, - \\(D \\in \\mathbb{R}^{p \\times p}\\) is diagonal with singular values, - \\(V \\in \\mathbb{R}^{p \\times p}\\) contains the right singular vectors (principal component directions).\nThe columns of \\(V\\) are the eigenvectors of \\(\\Sigma\\), and the squared singular values \\(D^2 / n\\) are the eigenvalues.\n\n\nVariance Explained\nThe proportion of variance explained by the first \\(k\\) components is: \\[\n\\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^p \\lambda_j}.\n\\]\nPCA allows data compression by projecting onto the first \\(k\\) components while retaining most of the variance.\nWhile classical PCA provides a powerful linear dimensionality reduction tool, it has several limitations: it does not handle sparsity well, it is sensitive to outliers, and it only captures linear relationships. Over the years, many extensions of PCA have been developed to address these challenges. Below we discuss several of the most popular PCA variants.\n\n\n\nSparse PCA (Zou, Hastie, Tibshirani, 2006)\nKey Idea:\nEncourage sparsity in the principal component loading vectors to improve interpretability.\nObjective Function (simplified): \\[\n\\max_{u} \\quad u^T \\Sigma u - \\lambda \\| u \\|_1 \\quad \\text{subject to} \\quad \\| u \\|_2 = 1.\n\\]\nWhen to Use:\n\nWhen you expect only a subset of variables to be important in each component.\nUseful in high-dimensional settings like genomics, image processing, and text analysis.\n\nCharacteristics:\n\nEnhances interpretability by producing sparse loadings.\nRetains much of the variance while simplifying the component structure.\n\n\n\n\nKernel PCA (Sch√∂lkopf et al., 1998)\nKey Idea:\nMap the data into a higher-dimensional feature space using a nonlinear kernel and then apply PCA in that space.\nObjective Function:\nStandard PCA applied to the kernel matrix: \\[\nK_{ij} = k(x_i, x_j),\n\\] where \\(k(\\cdot, \\cdot)\\) is a positive-definite kernel function (e.g., RBF, polynomial).\nWhen to Use:\n\nWhen the data exhibit nonlinear structures that linear PCA cannot capture.\nPopular in pattern recognition, computer vision, and bioinformatics.\n\nCharacteristics:\n\nCaptures nonlinear relationships between variables.\nChoice of kernel critically affects performance.\n\n\n\n\nRobust PCA (Candes et al., 2011)\nKey Idea:\nDecompose the data matrix \\(X\\) into a low-rank component \\(L\\) and a sparse outlier component \\(S\\): \\[\n\\min_{L, S} \\| L \\|_* + \\lambda \\| S \\|_1 \\quad \\text{subject to} \\quad X = L + S.\n\\] where \\(\\| L \\|_*\\) is the nuclear norm (sum of singular values).\nWhen to Use:\n\nWhen the data contain gross outliers or corruptions.\nCommon in computer vision (background subtraction), video surveillance, and recommender systems.\n\nCharacteristics:\n\nMore robust to outliers than classical PCA.\nSeparates structured low-rank signals from sparse noise.\n\n\n\n\nProbabilistic PCA (Tipping and Bishop, 1999)\nKey Idea:\nReformulate PCA as a latent variable model with Gaussian noise: \\[\nx_i = W z_i + \\mu + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2 I),\n\\] where \\(z_i\\) are latent factors.\nWhen to Use:\n\nWhen modeling uncertainty and likelihood is important.\nAllows probabilistic interpretation and missing data handling.\n\nCharacteristics:\n\nMaximum likelihood estimation provides the same solution as classical PCA in the limit.\nNaturally extends to mixture models and Bayesian frameworks.\n\n\n\n\nTruncated SVD (a Computational Variant)\nKey Idea:\nUse the first \\(k\\) singular vectors from the SVD of \\(X\\) without computing the full decomposition: \\[\nX \\approx U_k D_k V_k^T.\n\\]\nWhen to Use:\n\nVery large-scale data (e.g., text mining, collaborative filtering).\nWhen speed and memory efficiency are critical.\n\nCharacteristics:\n\nOften implemented using randomized algorithms.\nComputationally efficient for sparse or massive datasets.\n\n\n\n\nNonnegative Matrix Factorization (NMF) (Lee and Seung, 1999)\nKey Idea:\nDecompose the data into nonnegative factors: \\[\nX \\approx WH, \\quad W, H \\geq 0.\n\\]\nWhen to Use:\n\nNonnegative data (e.g., image pixels, word counts).\nWhen parts-based or additive decompositions are meaningful.\n\nCharacteristics:\n\nUnlike PCA, does not produce orthogonal components.\nOften yields interpretable, parts-based representations.\n\n\n\n\nIndependent Component Analysis (ICA)\nKey Idea:\nFind components that are statistically independent, not just uncorrelated: \\[\nX = A S,\n\\] where \\(S\\) contains independent components.\nWhen to Use:\n\nWhen underlying sources are assumed to be independent (e.g., EEG signal separation).\nSuitable for blind source separation problems.\n\nCharacteristics:\n\nGoes beyond PCA by removing higher-order dependencies.\nSensitive to scaling and noise.\n\n\nThese PCA variants allow the core idea of variance decomposition to be adapted to a wide variety of practical problems, whether by enforcing sparsity, allowing for nonlinearity, handling outliers, or modeling uncertainty."
  },
  {
    "objectID": "blog-unpublished/flavors-pca.html#bottom-line",
    "href": "blog-unpublished/flavors-pca.html#bottom-line",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nClassical PCA reduces dimensionality by projecting data onto orthogonal directions that maximize variance.\nMany PCA variants exist to handle real-world challenges like sparsity (Sparse PCA), outliers (Robust PCA), nonlinearity (Kernel PCA), and uncertainty (Probabilistic PCA).\nChoosing the right PCA flavor depends on the structure of your data and the goals of your analysis ‚Äî interpretability, robustness, scalability, or flexibility.\nSeveral of these extensions (e.g., Kernel PCA, NMF, ICA) relax key assumptions of traditional PCA, making them better suited for specialized applications like image analysis, genomics, and signal processing.\nUnderstanding the mathematical foundation behind these methods helps avoid misapplication and improves the quality of insights from dimensionality reduction."
  },
  {
    "objectID": "blog-unpublished/flavors-pca.html#where-to-learn-more",
    "href": "blog-unpublished/flavors-pca.html#where-to-learn-more",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a comprehensive introduction to PCA, see Principal Component Analysis by Jolliffe (2002), which remains a classic reference. The review paper by Shlens (2014), A Tutorial on Principal Component Analysis, offers an accessible and intuitive explanation of the method and its geometric interpretation. For deeper dives into specific variants, refer to Zou, Hastie, and Tibshirani (2006)"
  },
  {
    "objectID": "blog-unpublished/flavors-pca.html#references",
    "href": "blog-unpublished/flavors-pca.html#references",
    "title": "Principal Components Analysis (PCA) and Its Variants",
    "section": "References",
    "text": "References\n\nBishop, C. M. (1999). ‚ÄúBayesian PCA.‚Äù Advances in Neural Information Processing Systems, 11.\nPearson, K. (1901). ‚ÄúOn Lines and Planes of Closest Fit to Systems of Points in Space.‚Äù Philosophical Magazine, 2(11), 559‚Äì572.\nHotelling, H. (1933). ‚ÄúAnalysis of a Complex of Statistical Variables into Principal Components.‚Äù Journal of Educational Psychology, 24(6), 417‚Äì441.\nJolliffe, I. T. (2002). Principal Component Analysis. Springer Series in Statistics.\nZou, H., Hastie, T., & Tibshirani, R. (2006). ‚ÄúSparse Principal Component Analysis.‚Äù Journal of Computational and Graphical Statistics, 15(2), 265‚Äì286."
  },
  {
    "objectID": "blog-unpublished/flavors-var-selection.html",
    "href": "blog-unpublished/flavors-var-selection.html",
    "title": "The Many Flavors of Variable Selection",
    "section": "",
    "text": "If you‚Äôve ever worked with high-dimensional data, you‚Äôve probably run into this problem: there are just too many variables, and not all of them matter. Some are noise, some are collinear, and a few are the real signal you care about. So how do we separate the wheat from the chaff?\nWelcome to the world of variable selection.\nOver the years, statisticians and machine learning researchers have cooked up a rich menu of methods for variable selection‚Äîeach with its own philosophy, strengths, and blind spots. Some use penalty terms to shrink coefficients (like Lasso and Ridge). Others cleverly exploit data geometry (like Principal Components Analysis). Some work their magic through randomized constructions (like Knockoffs). And a few rely on stepwise or greedy search strategies (like Forward Selection or Least Angle Regression).\nIn this post, we‚Äôre going to take a guided tour through these different approaches‚Äîwhat they do, when to use them, and why they work. We‚Äôll also highlight their limitations, because no method is perfect. The goal is not to crown a winner but to help you recognize which tool fits your problem.\nThis won‚Äôt be a recipe book with code (though that might come in a future post). Instead, we‚Äôll focus on the ideas and intuition behind these methods. Think of this as your field guide to variable selection."
  },
  {
    "objectID": "blog-unpublished/flavors-var-selection.html#background",
    "href": "blog-unpublished/flavors-var-selection.html#background",
    "title": "The Many Flavors of Variable Selection",
    "section": "",
    "text": "If you‚Äôve ever worked with high-dimensional data, you‚Äôve probably run into this problem: there are just too many variables, and not all of them matter. Some are noise, some are collinear, and a few are the real signal you care about. So how do we separate the wheat from the chaff?\nWelcome to the world of variable selection.\nOver the years, statisticians and machine learning researchers have cooked up a rich menu of methods for variable selection‚Äîeach with its own philosophy, strengths, and blind spots. Some use penalty terms to shrink coefficients (like Lasso and Ridge). Others cleverly exploit data geometry (like Principal Components Analysis). Some work their magic through randomized constructions (like Knockoffs). And a few rely on stepwise or greedy search strategies (like Forward Selection or Least Angle Regression).\nIn this post, we‚Äôre going to take a guided tour through these different approaches‚Äîwhat they do, when to use them, and why they work. We‚Äôll also highlight their limitations, because no method is perfect. The goal is not to crown a winner but to help you recognize which tool fits your problem.\nThis won‚Äôt be a recipe book with code (though that might come in a future post). Instead, we‚Äôll focus on the ideas and intuition behind these methods. Think of this as your field guide to variable selection."
  },
  {
    "objectID": "blog-unpublished/flavors-var-selection.html#notation",
    "href": "blog-unpublished/flavors-var-selection.html#notation",
    "title": "The Many Flavors of Variable Selection",
    "section": "Notation",
    "text": "Notation\nSuppose we observe data \\((Y, X)\\), where \\(Y \\in \\mathbb{R}^n\\) is the outcome vector and \\(X \\in \\mathbb{R}^{n \\times p}\\) is the matrix of predictors (covariates, features, regressors‚Äîpick your favorite term). We‚Äôre interested in estimating a relationship like: \\[\nY = X \\beta + \\varepsilon,\n\\] where \\(\\beta \\in \\mathbb{R}^p\\) is the vector of coefficients and \\(\\varepsilon\\) is the error term.\nIn high-dimensional settings, \\(p\\) may be large‚Äîpossibly even larger than \\(n\\). The core task of variable selection is to identify which components of \\(\\beta\\) are nonzero (or, more generally, which features matter for predicting \\(Y\\))."
  },
  {
    "objectID": "blog-unpublished/flavors-var-selection.html#a-closer-look",
    "href": "blog-unpublished/flavors-var-selection.html#a-closer-look",
    "title": "The Many Flavors of Variable Selection",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nLasso (aka ‚Ñì‚ÇÅ Regularization)\nLasso introduced the big idea of sparsity. It penalizes the sum of the absolute values of the coefficients: \\[\n\\hat{\\beta}^{\\text{lasso}} = \\arg\\min_{\\beta} \\left\\{ \\| Y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1 \\right\\}.\n\\] The magic of the ‚Ñì‚ÇÅ penalty is that it can shrink some coefficients exactly to zero, performing variable selection as part of the estimation.\nWhen to use it? When you believe that only a subset of predictors are relevant and want an interpretable model.\nStrengths: Sparse solutions, automatic variable selection, computationally efficient.\nWeaknesses: Can struggle with groups of correlated predictors (tends to pick one arbitrarily), biased estimates due to shrinkage.\n\nRPython\n\n\nlibrary(glmnet)\ndata(iris)\nX &lt;- as.matrix(iris[, c(\"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")])\nY &lt;- iris$Sepal.Length\nfit &lt;- cv.glmnet(X, Y, alpha = 1)\ncoef(fit, s = \"lambda.min\")\n\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n\niris = load_iris(as_frame=True).frame\nX = iris[['sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\ny = iris['sepal length (cm)']\nlasso = LassoCV(cv=5).fit(X, y)\nlasso.coef_\n\n\n\n\n\n\nRidge Regression (aka ‚Ñì‚ÇÇ Regularization)\nLet‚Äôs start with Ridge regression, one of the oldest forms of regularization. Ridge doesn‚Äôt exactly select variables‚Äîit shrinks them. The idea is to add a penalty on the size of the coefficients: \\[\n\\hat{\\beta}^{\\text{ridge}} = \\arg\\min_{\\beta} \\left\\{ \\| Y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2 \\right\\}.\n\\] This discourages large coefficients but never forces them exactly to zero.\nWhen to use it? When multicollinearity is a problem or when you prefer stability over sparsity. Ridge is especially good when many small effects contribute to the outcome.\nStrengths: Stabilizes estimates, handles multicollinearity gracefully, works even when \\(p &gt; n\\).\nWeaknesses: Does not produce sparse solutions; all coefficients remain in the model.\n\nRPython\n\n\nfit_ridge &lt;- cv.glmnet(X, Y, alpha = 0)\ncoef(fit_ridge, s = \"lambda.min\")\n\n\nfrom sklearn.linear_model import RidgeCV\nridge = RidgeCV(alphas=np.logspace(-6, 6, 13), cv=5).fit(X, y)\nridge.coef_\n\n\n\n\n\n\nElastic Net\nElastic Net combines the penalties of Ridge and Lasso: \\[\n\\hat{\\beta}^{\\text{EN}} = \\arg\\min_{\\beta} \\left\\{ \\| Y - X \\beta \\|_2^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\| \\beta \\|_2^2 \\right\\}.\n\\] It keeps the sparsity of Lasso but borrows Ridge‚Äôs ability to handle correlated predictors.\nWhen to use it? When predictors are correlated, and you want both sparsity and stability.\nStrengths: Handles groups of correlated variables better than Lasso alone.\nWeaknesses: Adds an extra tuning parameter to balance the ‚Ñì‚ÇÅ and ‚Ñì‚ÇÇ penalties.\n\nRPython\n\n\nfit_enet &lt;- cv.glmnet(X, Y, alpha = 0.5)\ncoef(fit_enet, s = \"lambda.min\")\n\n\nfrom sklearn.linear_model import ElasticNetCV\nenet = ElasticNetCV(cv=5).fit(X, y)\nenet.coef_\n\n\n\n\n\n\nPrincipal Components Regression (PCR)\nPrincipal Components Analysis (PCA) finds linear combinations of the original variables that explain the most variance. In Principal Components Regression, we regress \\(Y\\) on the top \\(k\\) principal components of \\(X\\) instead of on the original variables.\nWhen to use it? When predictors are highly correlated or when dimensionality reduction is needed before regression.\nStrengths: Reduces dimensionality, handles multicollinearity.\nWeaknesses: Components may be hard to interpret; variable selection is indirect since it selects combinations of variables, not individual variables.\n\nRPython\n\n\nlibrary(pls)\npcr_model &lt;- pcr(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris, scale = TRUE, validation = \"CV\")\nsummary(pcr_model)\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nreg = LinearRegression().fit(X_pca, y)\nreg.coef_\n\n\n\n\n\n\nKnockoffs\nKnockoffs, introduced by Barber and Cand√®s (2015), is a clever framework for variable selection with false discovery rate (FDR) control. The method constructs ‚Äúknockoff copies‚Äù of each feature‚Äîartificial variables that mimic the correlation structure of the real ones but are known to be null. Then it tests whether the real variables outperform their knockoffs.\nWhen to use it? When you care about valid statistical guarantees like FDR control.\nStrengths: Controls FDR rigorously; applicable even in high-dimensional settings.\nWeaknesses: Requires construction of knockoff variables, which can be challenging for non-Gaussian designs.\n\nRPython\n\n\n# Knockoff example requires knockoff package and Gaussian design\n# Skipping implementation here due to complexity\n\n\n# Requires knockpy package and Gaussian assumption\n# Skipping detailed implementation here\n\n\n\n\n\n\nSCAD (Smoothly Clipped Absolute Deviation)\nSCAD is a non-convex penalty designed to overcome the bias problem of Lasso. The SCAD penalty behaves like Lasso for small coefficients but applies less shrinkage to larger ones, reducing bias: \\[\nP'_\\lambda(\\beta) = \\lambda \\left[ I(|\\beta| \\leq \\lambda) + \\frac{(a \\lambda - |\\beta|)_+}{(a - 1)\\lambda} I(|\\beta| &gt; \\lambda) \\right],\n\\] where \\(a &gt; 2\\) is typically set to 3.7.\nWhen to use it? When you want sparsity without the strong bias of Lasso.\nStrengths: Less biased than Lasso, still encourages sparsity.\nWeaknesses: Non-convex optimization problem; computationally more demanding.\n\nRPython\n\n\nlibrary(ncvreg)\nscad_fit &lt;- ncvreg(X, Y, penalty = \"SCAD\")\ncoef(scad_fit, lambda = scad_fit$lambda.min)\n\n\n# SCAD is not widely available in sklearn, typically uses specialized packages like pyglmnet or custom implementation\n\n\n\n\n\n\nLeast Angle Regression (LAR)\nLAR is a stepwise procedure that adds variables to the model one at a time, moving in the direction of the most correlated predictor. It‚Äôs closely related to the path-following algorithm for Lasso.\nWhen to use it? When you want a fast, interpretable selection process similar to forward selection.\nStrengths: Computationally efficient, provides the full regularization path.\nWeaknesses: Like Lasso, can behave poorly with correlated predictors.\n\nRPython\n\n\nlibrary(lars)\nlar_model &lt;- lars(X, Y, type = \"lar\")\nprint(lar_model)\n\n\nfrom sklearn.linear_model import Lars\nlar = Lars().fit(X, y)\nlar.coef_\n\n\n\n\n\n\nFOCI (Feature Ordering by Conditional Independence)\nFOCI is a recent, information-theoretic method that orders features by how much conditional mutual information they contribute to the outcome. It‚Äôs model-free and does not assume a particular parametric form.\nWhen to use it? When you suspect nonlinear relationships or want model-agnostic feature screening.\nStrengths: Handles nonlinearities, no need for parametric models.\nWeaknesses: More computationally intensive; newer and less widely used in practice.\n\n\n\nStepwise Selection (Forward, Backward, Both)\nThe classic workhorse of variable selection, stepwise procedures iteratively add or remove variables based on some criterion like AIC, BIC, or p-values.\nWhen to use it? For smaller problems where computational cost is low and interpretability is key.\nStrengths: Simple, interpretable, available in every stats package.\nWeaknesses: Can be unstable, prone to overfitting, ignores model uncertainty.\n\nRPython\n\n\nlibrary(MASS)\nfull_model &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris)\nstep_model &lt;- stepAIC(full_model, direction = \"both\")\nsummary(step_model)\n\n\n# Stepwise selection is not built into sklearn; can be implemented manually or with statsmodels\n# Skipping for brevity"
  },
  {
    "objectID": "blog-unpublished/flavors-var-selection.html#bottom-line",
    "href": "blog-unpublished/flavors-var-selection.html#bottom-line",
    "title": "The Many Flavors of Variable Selection",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nLasso, Ridge, and Elastic Net are the go-to penalized regression methods, with Lasso giving sparsity, Ridge providing stability, and Elastic Net blending the two.\nKnockoffs offer strong statistical guarantees like FDR control but require careful implementation.\nNon-convex penalties like SCAD address Lasso‚Äôs bias issue but at a computational cost.\nPCA-based methods reduce dimensionality but don‚Äôt directly select variables.\nModern approaches like FOCI expand the toolkit to nonlinear and information-theoretic settings."
  },
  {
    "objectID": "blog-unpublished/flavors-var-selection.html#where-to-learn-more",
    "href": "blog-unpublished/flavors-var-selection.html#where-to-learn-more",
    "title": "The Many Flavors of Variable Selection",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a great introduction to penalized regression methods, The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman is a classic. For a deeper dive into FDR control and Knockoffs, see Barber and Cand√®s (2015). The SCAD penalty was introduced by Fan and Li (2001), and the literature on FOCI is still developing, but the original papers provide a solid starting point. If you‚Äôre curious about the algorithmic side, Trevor Hastie‚Äôs lectures on LAR and variable selection are highly recommended."
  },
  {
    "objectID": "blog-unpublished/flavors-var-selection.html#references",
    "href": "blog-unpublished/flavors-var-selection.html#references",
    "title": "The Many Flavors of Variable Selection",
    "section": "References",
    "text": "References\nBarber, R. F., & Cand√®s, E. J. (2015). Controlling the false discovery rate via knockoffs. Annals of Statistics, 43(5), 2055‚Äì2085.\nFan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456), 1348‚Äì1360.\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer."
  },
  {
    "objectID": "blog-unpublished/flavors-prop-score-methods.html",
    "href": "blog-unpublished/flavors-prop-score-methods.html",
    "title": "The Many Flavors of Propensity Score Methods for Causal Inference",
    "section": "",
    "text": "If matching is the hammer in the causal inference toolbox, the propensity score is the blueprint that helps you decide where to swing it. Introduced by Rosenbaum and Rubin in 1983, the propensity score‚Äîthe probability of receiving treatment given observed covariates‚Äîhas become the workhorse for handling confounding in observational studies.\nBut here‚Äôs the thing: the propensity score itself is just the starting point. There are many ways to use propensity scores. You can match on them, stratify your sample, weight your observations, or plug them into doubly robust estimators that combine modeling of both the treatment and the outcome. You can tweak how you weight the units‚Äîdownweighting those with extreme scores or focusing on the region where treated and control groups overlap.\nIn this post, we‚Äôll explore the many flavors of propensity score methods: when to use them, how they work, and what their pros and cons are. The focus is on intuition, math, and practice‚Äînot code."
  },
  {
    "objectID": "blog-unpublished/flavors-prop-score-methods.html#background",
    "href": "blog-unpublished/flavors-prop-score-methods.html#background",
    "title": "The Many Flavors of Propensity Score Methods for Causal Inference",
    "section": "",
    "text": "If matching is the hammer in the causal inference toolbox, the propensity score is the blueprint that helps you decide where to swing it. Introduced by Rosenbaum and Rubin in 1983, the propensity score‚Äîthe probability of receiving treatment given observed covariates‚Äîhas become the workhorse for handling confounding in observational studies.\nBut here‚Äôs the thing: the propensity score itself is just the starting point. There are many ways to use propensity scores. You can match on them, stratify your sample, weight your observations, or plug them into doubly robust estimators that combine modeling of both the treatment and the outcome. You can tweak how you weight the units‚Äîdownweighting those with extreme scores or focusing on the region where treated and control groups overlap.\nIn this post, we‚Äôll explore the many flavors of propensity score methods: when to use them, how they work, and what their pros and cons are. The focus is on intuition, math, and practice‚Äînot code."
  },
  {
    "objectID": "blog-unpublished/flavors-prop-score-methods.html#notation",
    "href": "blog-unpublished/flavors-prop-score-methods.html#notation",
    "title": "The Many Flavors of Propensity Score Methods for Causal Inference",
    "section": "Notation",
    "text": "Notation\nWe‚Äôre back in the familiar causal inference setup:\n\n\\(D_i \\in \\{0, 1\\}\\): treatment indicator.\n\\(X_i\\): observed covariates.\n\\(Y_i(1), Y_i(0)\\): potential outcomes.\n\nThe propensity score is: \\[\ne(X_i) = \\mathbb{P}(D_i = 1 \\mid X_i).\n\\]\nThe key result from Rosenbaum and Rubin (1983): \\[\n(Y(1), Y(0)) \\perp D \\mid e(X),\n\\] meaning that, conditional on the propensity score, treatment assignment is as good as random."
  },
  {
    "objectID": "blog-unpublished/flavors-prop-score-methods.html#a-closer-look",
    "href": "blog-unpublished/flavors-prop-score-methods.html#a-closer-look",
    "title": "The Many Flavors of Propensity Score Methods for Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nNearest Neighbor Matching on Propensity Score\nThis is often the first method people try after estimating the propensity score. Once \\(e(X)\\) is estimated, treated units are matched to control units with the closest propensity scores (nearest neighbor). You can match one-to-one, one-to-many, with or without replacement.\nWhen to use it? When the number of controls is large enough to find good matches for treated units.\nStrengths: Simple and intuitive; reduces high-dimensional matching to one dimension.\nWeaknesses: Balance on the propensity score doesn‚Äôt guarantee balance on covariates; sensitive to poor matches.\n\n\n\nCaliper Matching\nCaliper matching adds a threshold: only match treated and control units if their propensity scores are within a specified distance (the caliper). Often the caliper is set to 0.2 times the standard deviation of the logit of the propensity score, following Rosenbaum and Rubin‚Äôs recommendation.\nWhen to use it? To avoid bad matches in nearest neighbor matching.\nStrengths: Prevents extreme mismatches; improves balance.\nWeaknesses: May discard treated units if no control is close enough.\n\n\n\nStratification (Subclassification) on the Propensity Score\nHere, the range of propensity scores is divided into \\(K\\) strata (often quintiles), and treatment effects are estimated within each stratum, then averaged across strata.\nWhen to use it? When matching isn‚Äôt feasible or you prefer a more aggregate approach.\nStrengths: Easy to implement, balances on average within strata.\nWeaknesses: Coarse adjustment; may not fully eliminate bias within strata.\n\n\n\nInverse Probability Weighting (IPW)\nIPW turns the propensity score into weights: \\[\nw_i = \\frac{D_i}{e(X_i)} + \\frac{1 - D_i}{1 - e(X_i)}.\n\\] This reweights the sample so that treated and control groups resemble each other on observed covariates.\nWhen to use it? When you want to use the whole dataset and avoid discarding units.\nStrengths: Simple and fully utilizes all observations.\nWeaknesses: Sensitive to extreme propensity scores near 0 or 1, which can lead to huge weights and unstable estimates.\n\n\n\nAugmented IPW (AIPW) / Doubly Robust Estimators\nAIPW combines IPW with outcome modeling (regression adjustment). The key appeal: if either the propensity score model or the outcome model is correct (but not necessarily both), the estimator is consistent. This is called the doubly robust property.\nThe AIPW estimator for the ATE looks like: \\[\n\\hat{\\tau}_{\\text{AIPW}} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{D_i (Y_i - \\hat{m}_1(X_i))}{e(X_i)} - \\frac{(1 - D_i) (Y_i - \\hat{m}_0(X_i))}{1 - e(X_i)} + \\hat{m}_1(X_i) - \\hat{m}_0(X_i) \\right],\n\\] where \\(\\hat{m}_d(X)\\) is the predicted outcome for treatment group \\(d\\).\nWhen to use it? When you want robust estimation and are unsure about model correctness.\nStrengths: Doubly robust consistency. Efficient use of data.\nWeaknesses: Computational complexity; requires both models to be estimated.\n\n\n\nCovariate Balancing Propensity Score (CBPS)\nCBPS, introduced by Imai and Ratkovic (2014), directly estimates the propensity score while optimizing covariate balance. Instead of fitting a logistic regression and then checking balance, CBPS ensures balance is achieved as part of the estimation process.\nWhen to use it? When standard propensity score estimation leads to poor balance.\nStrengths: Good balance without iterative tuning; works directly toward the matching goal.\nWeaknesses: More complex to implement; less widely available in standard packages.\n\n\n\nOverlap Weights\nOverlap weighting focuses on the region of common support‚Äîwhere treated and control units both exist‚Äîby assigning weights: \\[\nw_i = D_i (1 - e(X_i)) + (1 - D_i) e(X_i).\n\\] This downweights units with extreme scores near 0 or 1 and emphasizes comparability.\nWhen to use it? When you want to avoid extrapolation and focus on the units where treatment and control overlap.\nStrengths: Naturally avoids instability from extreme weights; targets the ‚Äúoverlap population.‚Äù\nWeaknesses: Estimates effects for the overlap population, not necessarily ATE or ATT.\n\n\n\nEntropy Balancing\nEntropy balancing directly reweights the control group so that the moments of the covariates (mean, variance, etc.) match exactly between treated and control groups. Instead of matching or stratifying, this solves a constrained optimization problem that minimizes the Kullback-Leibler divergence of weights subject to balance constraints.\nWhen to use it? When balance is hard to achieve with traditional weighting.\nStrengths: Guarantees exact balance on chosen covariate moments. Fully utilizes the data.\nWeaknesses: Requires specifying which moments to balance; can be sensitive to that choice."
  },
  {
    "objectID": "blog-unpublished/flavors-prop-score-methods.html#bottom-line",
    "href": "blog-unpublished/flavors-prop-score-methods.html#bottom-line",
    "title": "The Many Flavors of Propensity Score Methods for Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nMatching methods (nearest neighbor, caliper) are intuitive and interpretable but can discard data.\nStratification offers simplicity and full data use but may not fully balance covariates.\nIPW uses all data but can suffer from instability due to extreme weights.\nAIPW gives the best of both worlds with double robustness.\nCBPS directly targets balance in the propensity score estimation.\nOverlap weights avoid the problem of extreme scores and focus on the common support.\nEntropy balancing guarantees exact covariate balance via weighting without matching or stratification."
  },
  {
    "objectID": "blog-unpublished/flavors-prop-score-methods.html#where-to-learn-more",
    "href": "blog-unpublished/flavors-prop-score-methods.html#where-to-learn-more",
    "title": "The Many Flavors of Propensity Score Methods for Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor the original introduction to propensity scores, see Rosenbaum and Rubin‚Äôs (1983) landmark paper. Imai and Ratkovic‚Äôs (2014) work on CBPS is a must-read for understanding balance-focused estimation. The textbook Causal Inference for Statistics, Social, and Biomedical Sciences by Imbens and Rubin (2015) provides excellent coverage of these methods. There are also great tutorials and vignettes in R packages like MatchIt, twang, and WeightIt."
  },
  {
    "objectID": "blog-unpublished/flavors-prop-score-methods.html#references",
    "href": "blog-unpublished/flavors-prop-score-methods.html#references",
    "title": "The Many Flavors of Propensity Score Methods for Causal Inference",
    "section": "References",
    "text": "References\n\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41‚Äì55.\nImai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243‚Äì263.\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nHainmueller, J. (2012). Entropy balancing for causal effects. Political Analysis, 20(1), 25‚Äì46.\n\nPropensity Score Methods Applied to the Iris Dataset\nThe following examples apply several popular propensity score methods to the Iris dataset using both R and Python. For demonstration, we define an artificial binary treatment (D) based on petal length. The outcome variable is Sepal.Length, and the predictors are the remaining covariates.\nPropensity Score Estimation (Logistic Regression)\n\nRPython\n\n\nlibrary(MatchIt)\ndata(iris)\niris$D &lt;- ifelse(iris$Petal.Length &gt; 3, 1, 0)\nps_model &lt;- glm(D ~ Sepal.Width + Petal.Width, data = iris, family = binomial)\nsummary(ps_model)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n\niris = load_iris(as_frame=True).frame\niris['D'] = (iris['petal length (cm)'] &gt; 3).astype(int)\nX = iris[['sepal width (cm)', 'petal width (cm)']]\ny = iris['D']\nmodel = LogisticRegression().fit(X, y)\nmodel.coef_, model.intercept_\n\n\n\nNearest Neighbor Matching\n\nRPython\n\n\nmatchit_nn &lt;- matchit(D ~ Sepal.Width + Petal.Width, data = iris, method = \"nearest\")\nsummary(matchit_nn)\n\n\nfrom causalinference import CausalModel\ncm = CausalModel(iris[['sepal width (cm)', 'petal width (cm)']].values, iris['D'].values, iris['sepal length (cm)'].values)\ncm.est_via_matching()\ncm.estimates\n\n\n\nCaliper Matching\n\nRPython\n\n\nmatchit_caliper &lt;- matchit(D ~ Sepal.Width + Petal.Width, data = iris, method = \"nearest\", caliper = 0.2)\nsummary(matchit_caliper)\n\n\n# caliper matching in Python is not built-in; would require manual implementation or use DoWhy or related packages\n\n\n\nStratification (Subclassification)\n\nRPython\n\n\nmatchit_strat &lt;- matchit(D ~ Sepal.Width + Petal.Width, data = iris, method = \"subclass\", subclass = 5)\nsummary(matchit_strat)\n\n\n# Stratification would require binning the propensity score and estimating within strata manually\n\n\n\n\nRPython\n\n\niris$ps &lt;- predict(ps_model, type = \"response\")\niris$weights &lt;- ifelse(iris$D == 1, 1 / iris$ps, 1 / (1 - iris$ps))\nsummary(iris$weights)\n\n\niris['ps'] = model.predict_proba(X)[:,1]\niris['weights'] = np.where(iris['D'] == 1, 1 / iris['ps'], 1 / (1 - iris['ps']))\niris['weights'].describe()\n\n\n\nAugmented IPW (AIPW) / Doubly Robust\n\nRPython\n\n\n# Requires additional outcome modeling and manual implementation\n# Use packages like AIPW or drtmle for robust estimators\n\n\n# Requires DoWhy, EconML, or custom implementation for AIPW\n\n\n\nCovariate Balancing Propensity Score (CBPS)\n\nRPython\n\n\nlibrary(CBPS)\ncbps_fit &lt;- CBPS(D ~ Sepal.Width + Petal.Width, data = iris)\nsummary(cbps_fit)\n\n\n# CBPS not readily available in sklearn; typically done via R or custom implementation\n\n\n\nOverlap Weights\n\nRPython\n\n\niris$overlap_weights &lt;- ifelse(iris$D == 1, 1 - iris$ps, iris$ps)\nsummary(iris$overlap_weights)\n\n\niris['overlap_weights'] = np.where(iris['D'] == 1, 1 - iris['ps'], iris['ps'])\niris['overlap_weights'].describe()\n\n\n\nEntropy Balancing\n\nRPython\n\n\nlibrary(ebal)\ncontrol_idx &lt;- which(iris$D == 0)\ntreated_idx &lt;- which(iris$D == 1)\nX_control &lt;- iris[control_idx, c(\"Sepal.Width\", \"Petal.Width\")]\neg &lt;- ebalance(Treatment = rep(0, length(control_idx)), X = as.matrix(X_control), target.margins = colMeans(iris[treated_idx, c(\"Sepal.Width\", \"Petal.Width\")]))\nsummary(eg)\n\n\n# Entropy balancing in Python requires specialized packages or manual convex optimization"
  },
  {
    "objectID": "blog-unpublished/flavors-matching-methods.html",
    "href": "blog-unpublished/flavors-matching-methods.html",
    "title": "The Many Flavors of Matching for Causal Inference",
    "section": "",
    "text": "If you‚Äôve worked on causal inference in observational data, you‚Äôve likely faced the fundamental challenge: treated and control groups often look very different. Matching methods aim to fix that. The idea is beautifully intuitive‚Äîlet‚Äôs compare treated units to similar control units and mimic the conditions of a randomized experiment as best as we can.\nBut here‚Äôs the twist: there isn‚Äôt just one way to define ‚Äúsimilar.‚Äù Should we look for exact matches? Should we match on covariates directly or on some summary score like the propensity score? Should we optimize the matches globally or locally? Over the years, researchers have developed a vibrant ecosystem of matching methods, each with its own philosophy, strengths, and quirks.\nIn this article, we‚Äôll walk through the most popular matching strategies for causal inference. We‚Äôll talk about what each method does, when to use it, and where it might lead you astray. The focus is on the math, intuition, and practical aspects‚Äînot on the code.\nWhether you‚Äôre doing matching for the first time or looking to expand your toolkit, this guide is for you."
  },
  {
    "objectID": "blog-unpublished/flavors-matching-methods.html#background",
    "href": "blog-unpublished/flavors-matching-methods.html#background",
    "title": "The Many Flavors of Matching for Causal Inference",
    "section": "",
    "text": "If you‚Äôve worked on causal inference in observational data, you‚Äôve likely faced the fundamental challenge: treated and control groups often look very different. Matching methods aim to fix that. The idea is beautifully intuitive‚Äîlet‚Äôs compare treated units to similar control units and mimic the conditions of a randomized experiment as best as we can.\nBut here‚Äôs the twist: there isn‚Äôt just one way to define ‚Äúsimilar.‚Äù Should we look for exact matches? Should we match on covariates directly or on some summary score like the propensity score? Should we optimize the matches globally or locally? Over the years, researchers have developed a vibrant ecosystem of matching methods, each with its own philosophy, strengths, and quirks.\nIn this article, we‚Äôll walk through the most popular matching strategies for causal inference. We‚Äôll talk about what each method does, when to use it, and where it might lead you astray. The focus is on the math, intuition, and practical aspects‚Äînot on the code.\nWhether you‚Äôre doing matching for the first time or looking to expand your toolkit, this guide is for you."
  },
  {
    "objectID": "blog-unpublished/flavors-matching-methods.html#notation",
    "href": "blog-unpublished/flavors-matching-methods.html#notation",
    "title": "The Many Flavors of Matching for Causal Inference",
    "section": "Notation",
    "text": "Notation\nLet‚Äôs set up the basic framework. Suppose we have \\(n\\) units indexed by \\(i = 1, \\dots, n\\). Each unit has:\n\nA binary treatment indicator \\(D_i \\in \\{0, 1\\}\\), where \\(D_i = 1\\) for treated units and \\(D_i = 0\\) for controls.\nA vector of observed covariates \\(X_i\\).\nPotential outcomes \\(Y_i(1)\\) and \\(Y_i(0)\\), where \\(Y_i(1)\\) is the outcome if treated, and \\(Y_i(0)\\) if untreated.\n\nOur goal is to estimate treatment effects like the Average Treatment Effect (ATE) or the Average Treatment Effect on the Treated (ATT): \\[\n\\text{ATT} = \\mathbb{E}[Y(1) - Y(0) \\mid D = 1].\n\\] The core idea behind matching is to find comparable untreated units for each treated unit so we can approximate \\(Y(0)\\) for the treated group."
  },
  {
    "objectID": "blog-unpublished/flavors-matching-methods.html#a-closer-look",
    "href": "blog-unpublished/flavors-matching-methods.html#a-closer-look",
    "title": "The Many Flavors of Matching for Causal Inference",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nExact Matching\nThis is the simplest and most stringent approach: match treated and control units exactly on all covariates \\(X\\). If a treated unit has \\(X = x\\), we look for control units with the exact same \\(X = x\\).\nWhen to use it? When covariates are discrete and there aren‚Äôt too many of them.\nStrengths: Conceptually clear, no modeling assumptions, perfect balance on matched covariates.\nWeaknesses: Infeasible with continuous variables or many covariates (the curse of dimensionality); can lead to lots of unmatched units.\n\n\n\nMahalanobis Distance Matching\nInstead of exact equality, Mahalanobis matching uses a distance metric: \\[\nd_M(X_i, X_j) = \\sqrt{(X_i - X_j)^\\top S^{-1} (X_i - X_j)},\n\\] where \\(S\\) is the sample covariance matrix of \\(X\\).\nIt scales and accounts for correlations among covariates, unlike Euclidean distance.\nWhen to use it? When covariates are continuous and moderately low-dimensional.\nStrengths: Handles continuous variables well, accounts for correlation between covariates.\nWeaknesses: Sensitive to high dimensionality; doesn‚Äôt work well with mixed discrete and continuous variables.\n\n\n\nPropensity Score Matching (PSM)\nRosenbaum and Rubin‚Äôs famous result tells us that if treatment assignment is unconfounded given \\(X\\), it‚Äôs also unconfounded given the propensity score: \\[\ne(X) = \\mathbb{P}(D = 1 \\mid X).\n\\] PSM matches treated and control units with similar estimated propensity scores, reducing the dimensionality problem.\nWhen to use it? When the number of covariates is large or mostly continuous.\nStrengths: Reduces a multivariate problem to a one-dimensional one. Flexible.\nWeaknesses: Balance on the propensity score does not guarantee balance on covariates. Sensitive to propensity model misspecification.\n\n\n\nCoarsened Exact Matching (CEM)\nCEM strikes a balance between exact matching and flexibility. It bins continuous covariates into coarsened categories (e.g., age groups, income brackets) and then does exact matching on these coarsened values.\nWhen to use it? When you can meaningfully coarsen your covariates and want to retain interpretability.\nStrengths: Improves balance while avoiding the curse of dimensionality. You control the coarsening.\nWeaknesses: Requires subjective choices about how to coarsen. Coarsening too much can reduce matching quality; coarsening too little can lead to few matches.\n\n\n\nOptimal Matching\nOptimal matching minimizes the total distance across all matched pairs. You can think of it as solving a global optimization problem: \\[\n\\min_{\\text{matching}} \\sum_{(i, j) \\in \\text{pairs}} d(X_i, X_j).\n\\] The distance can be Mahalanobis, Euclidean, or something else.\nWhen to use it? When you care about global match quality rather than greedy, local matching.\nStrengths: Finds globally optimal matches (not just nearest neighbor).\nWeaknesses: Computationally intensive for large datasets. Sensitive to distance metric choice.\n\n\n\nNearest Neighbor Matching (NNM)\nThis is the workhorse of matching: for each treated unit, find the control unit(s) with the smallest distance (usually on covariates or propensity scores).\nVariants: - With or without replacement: Can controls be matched multiple times? - One-to-one or one-to-many matching: How many controls per treated?\nWhen to use it? The default starting point for matching; especially with propensity scores.\nStrengths: Simple and fast. Flexible to different distance metrics.\nWeaknesses: Greedy approach may lead to poor global balance. Matching without replacement can worsen balance.\n\n\n\nGenetic Matching\nGenetic matching uses a genetic algorithm to search for weights on covariates that improve balance. Think of it as automating the process of choosing the ‚Äúright‚Äù distance metric.\nIt iteratively adjusts weights to minimize imbalance (measured by standardized mean differences) across covariates.\nWhen to use it? When achieving good balance is critical and standard matching methods struggle.\nStrengths: Excellent balance across covariates; data-driven weighting.\nWeaknesses: Computationally heavy. May require tuning. Randomness in the genetic algorithm introduces some variability.\n\n\n\nCaliper Matching\nCaliper matching puts a strict limit on how far apart matched units can be (e.g., match only if the propensity score difference is less than 0.1). Often used as a modification to nearest neighbor matching.\nWhen to use it? When you want to avoid poor matches with big distance gaps.\nStrengths: Prevents bad matches. Easy to implement alongside other methods.\nWeaknesses: May leave some treated units unmatched if no control falls within the caliper.\n\n\n\nFull Matching\nFull matching creates matched sets where each set contains at least one treated and at least one control unit. It can balance treated and control groups fully across the sample while minimizing a global imbalance measure.\nWhen to use it? When you don‚Äôt want to discard units and want balance across the full sample.\nStrengths: Uses all data, flexible matching ratios.\nWeaknesses: More complex to implement and analyze. Interpretability can suffer."
  },
  {
    "objectID": "blog-unpublished/flavors-matching-methods.html#bottom-line",
    "href": "blog-unpublished/flavors-matching-methods.html#bottom-line",
    "title": "The Many Flavors of Matching for Causal Inference",
    "section": "Bottom Line",
    "text": "Bottom Line\n\nExact matching is great for small, discrete covariate spaces but quickly becomes infeasible otherwise.\nMahalanobis matching handles continuous covariates but struggles with high dimensions.\nPropensity score matching reduces dimensionality but needs careful balance checks and trimming.\nCEM offers a middle ground by coarsening covariates before matching.\nOptimal and nearest neighbor matching differ in global vs.¬†local matching priorities.\nGenetic matching automates the search for good weighting but at a computational cost.\nCaliper matching helps prevent poor matches by enforcing distance thresholds.\nFull matching balances the entire sample without discarding units."
  },
  {
    "objectID": "blog-unpublished/flavors-matching-methods.html#where-to-learn-more",
    "href": "blog-unpublished/flavors-matching-methods.html#where-to-learn-more",
    "title": "The Many Flavors of Matching for Causal Inference",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nFor a thorough introduction to matching methods, start with Matching Methods for Causal Inference by Elizabeth Stuart (2010). The book Causal Inference for Statistics, Social, and Biomedical Sciences by Imbens and Rubin (2015) provides excellent coverage of matching and its theoretical underpinnings. Rosenbaum‚Äôs Observational Studies remains the classic for in-depth discussions on design and sensitivity analysis. The MatchIt R package documentation is also a goldmine for practical implementation details."
  },
  {
    "objectID": "blog-unpublished/flavors-matching-methods.html#references",
    "href": "blog-unpublished/flavors-matching-methods.html#references",
    "title": "The Many Flavors of Matching for Causal Inference",
    "section": "References",
    "text": "References\n\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41‚Äì55.\nStuart, E. A. (2010). Matching methods for causal inference: A review and a look forward. Statistical Science, 25(1), 1‚Äì21.\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nRosenbaum, P. R. (2002). Observational Studies. Springer."
  }
]