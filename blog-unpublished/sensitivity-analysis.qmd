---
title: "Sensitivity Analysis in Causal Inference"
date: "2025-00-00"
categories: [causal inference, sensitivity analysis]
---

## Background

One of the central challenges in causal inference is the problem of **unobserved confounding**. Even after controlling for observed covariates, we always face the nagging question: what if there’s something lurking in the background that we didn’t observe or can’t measure, and that something is driving our results?

This is where **sensitivity analysis** comes in. Developed and championed by Paul Rosenbaum and collaborators, sensitivity analysis provides a structured way to assess **how robust your causal conclusions are to potential violations of the unconfoundedness assumption** (also known as ignorability or selection on observables).

In this article, we’ll walk through the basic setup of sensitivity analysis in the potential outcomes framework, focus on Rosenbaum’s classical approach for observational studies, and discuss its practical implications. While sensitivity analysis doesn’t “solve” the confounding problem, it makes transparent how much unobserved bias would be required to overturn your findings.

## Notation

We use the **potential outcomes framework**, where for each unit $i = 1, \dots, n$:
- $Y_i(1)$ is the potential outcome under treatment,
- $Y_i(0)$ is the potential outcome under control,
- $D_i \in \{0,1\}$ is the treatment indicator,
- $X_i$ is the vector of observed covariates.

The key parameter of interest is the **average treatment effect on the treated (ATT)**:
$$
\text{ATT} = \mathbb{E} [ Y(1) - Y(0) \mid D = 1 ].
$$

Standard causal identification relies on the **unconfoundedness assumption**:
$$
(Y(1), Y(0)) \perp D \mid X.
$$
In words: conditional on observed covariates $X$, treatment assignment $D$ is as good as random.

However, what if this assumption fails? Suppose there’s an **unobserved confounder** $U$ that affects both $D$ and $Y$? Sensitivity analysis allows us to **quantify how much such an unobserved confounder would need to matter** to explain away the observed treatment effect.

## A Closer Look

### Rosenbaum's Sensitivity Model for Binary Treatments

Paul Rosenbaum (1987, 2002) proposed a model for sensitivity analysis in matched observational studies that formalizes the possibility of hidden bias due to unobserved confounding.

The key idea is to **model treatment assignment probabilities** with a sensitivity parameter $\Gamma$. Specifically, for any two matched individuals $i$ and $j$ with identical observed covariates:
$$
\frac{1}{\Gamma} \leq \frac{\Pr(D_i = 1 \mid X, U)}{\Pr(D_j = 1 \mid X, U)} \leq \Gamma.
$$

- If $\Gamma = 1$, treatment assignment is unconfounded (purely random within matched pairs).

- If $\Gamma > 1$, there could be hidden bias: even after matching on $X$, units might still differ in treatment probability due to $U$.

The parameter $\Gamma$ thus bounds how much the odds of receiving treatment could differ between matched units because of unobserved variables.

Rosenbaum then develops a **test statistic framework** to compute bounds on p-values for the null hypothesis of no treatment effect under different values of $\Gamma$.

The procedure asks: *How large must $\Gamma$ be before the p-value crosses a critical threshold (like 0.05)?* This critical value is called the **sensitivity value**.

---

### Interpreting $\Gamma$: The Intuition

Think of $\Gamma$ as quantifying the strength of unmeasured confounding needed to change your conclusion. If your result holds up to a large $\Gamma$ (e.g., $\Gamma = 2.5$), it would require quite a strong confounder to nullify the result. But if the result disappears for $\Gamma = 1.05$, your conclusion is quite fragile—even a mild bias could explain the effect.

In practice, values of $\Gamma$ between 1.1 and 3 are common depending on the context, but what counts as "large" depends on the subject matter.

---

### Sensitivity Analysis Beyond Matching

While Rosenbaum’s original framework was developed for matched designs, extensions of sensitivity analysis apply to:

- **Regression adjustment models**
- **Inverse probability weighting**
- **Instrumental variable settings**
- **Difference-in-differences designs**

The core idea remains: quantify how much unobserved confounding could alter your results.

In regression contexts, the **Rosenbaum bounds** approach may not be directly applicable, but similar logic applies via methods like:

- **Oster's delta method** (Oster, 2019),
- **Altonji, Elder, Taber approach**,
- **Cornfield conditions** (used historically in epidemiology).

---

### Why Sensitivity Analysis Matters

Sensitivity analysis doesn’t claim to remove unobserved bias. Rather, it **exposes the vulnerability of your conclusions**. It turns the problem of unmeasured confounding from a scary unknown into a quantifiable scenario:

> “Your effect holds unless there exists an unmeasured confounder that changes the odds of treatment by a factor of 2 *and* is strongly associated with the outcome.”

This transparency invites substantive experts to weigh in: is such a confounder plausible? If yes, proceed cautiously. If not, gain confidence in your findings.

## An Example

Let’s see how to implement Rosenbaum’s sensitivity analysis in practice. Below we show how to do this in R using the `rbounds` package, and a simple simulation-based illustration in Python.

::::{.panel-tabset}

### R

```r
# Install and load the package if needed
# install.packages("rbounds")
library(rbounds)

# Simulate matched pair data
set.seed(123)
n_pairs <- 50
treated <- rnorm(n_pairs, mean = 1)
control <- rnorm(n_pairs, mean = 0.5)
diffs <- treated - control

# Perform Wilcoxon signed-rank test for matched pairs
wilcox.test(diffs, alternative = "greater", exact = FALSE)

# Perform Rosenbaum sensitivity analysis (Hodges-Lehmann test)
# Gamma = 1 means no hidden bias; increase Gamma to test robustness
psens(diffs, Gamma = 1.5, alternative = "greater")

```

### Python

```python
# Python does not have built-in Rosenbaum bounds yet,
# but we can simulate how unmeasured confounding could shift results.

import numpy as np
from scipy.stats import wilcoxon

np.random.seed(123)
n_pairs = 50
treated = np.random.normal(1, 1, n_pairs)
control = np.random.normal(0.5, 1, n_pairs)
diffs = treated - control

# Wilcoxon signed-rank test (no hidden bias)
stat, p_value = wilcoxon(diffs, alternative='greater')
print(f"Wilcoxon p-value (no bias): {p_value:.4f}")

# Simple sensitivity illustration: perturb the control group
for gamma_effect in [0.1, 0.2, 0.3]:
    biased_control = control + gamma_effect
    biased_diffs = treated - biased_control
    stat, p_biased = wilcoxon(biased_diffs, alternative='greater')
    print(f"Gamma-effect {gamma_effect:.2f} → p-value: {p_biased:.4f}")
```

::::

---

## Bottom Line

- Sensitivity analysis quantifies how much unobserved confounding would be needed to explain away your estimated treatment effects.
- Paul Rosenbaum’s framework introduces a sensitivity parameter $\Gamma$ to model hidden bias in matched observational studies.
- A high sensitivity value suggests your results are robust; a low one suggests fragility.
- Sensitivity analysis doesn’t remove bias—it makes the assumptions about bias explicit and testable.

## Where to Learn More

Paul Rosenbaum’s book *Observational Studies* (2002, Springer) is the definitive reference on sensitivity analysis in causal inference. For a practical introduction, see his papers in *Biometrika* (1987) and subsequent work on sensitivity values and extensions to various designs. The R packages `rbounds` and `sensitivitymv` provide implementations for matched studies. Extensions of sensitivity analysis to regression, IV, and other settings are well-covered in recent papers by Oster (2019) and Cinelli & Hazlett (2020).

## References

- Rosenbaum, P. R. (1987). Sensitivity analysis for certain permutation inferences in matched observational studies. *Biometrika*, 74(1), 13–26.
- Rosenbaum, P. R. (2002). *Observational Studies* (2nd ed.). Springer.
- Oster, E. (2019). Unobservable selection and coefficient stability: Theory and evidence. *Journal of Business & Economic Statistics*, 37(2), 187–204.
- Altonji, J. G., Elder, T. E., & Taber, C. R. (2005). Selection on observed and unobserved variables: Assessing the effectiveness of Catholic schools. *Journal of Political Economy*, 113(1), 151–184.
- Cinelli, C., & Hazlett, C. (2020). Making sense of sensitivity: Extending omitted variable bias. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 82(1), 39–67.
