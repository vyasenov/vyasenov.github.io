---
title: "Causal Inference with Log Transformations"
date: "2025-00-00"
categories: [causal-inference, log-transformations]
---

## Background

Logarithmic transformations are among the most common tools in applied regression modeling, often motivated by their ability to linearize exponential relationships or to stabilize variance. They are also appealing in causal inference because they allow for treatment effects to be interpreted as **percentage changes**, a language that feels intuitive in policy discussions. But what happens when your outcome variable can be zero — or when you use alternative transformations like $\log(1 + Y)$ or $\arcsinh(Y)$ to handle this?

This article walks through the core issues with log transformations in causal inference, focusing on the recent findings of Chen and Roth (2024) and insights from Huntington-Klein (2023). Along the way, we'll clarify what you can and cannot interpret as percentage effects, discuss the critical issue of scale dependence, and offer practical alternatives.

## Notation

Assume a binary treatment $D \in \{0, 1\}$ and an outcome $Y \geq 0$, with potential outcomes $Y(1)$ and $Y(0)$. The canonical average treatment effect (ATE) is:

$$
ATE = \mathbb{E}[Y(1) - Y(0)].
$$

When $Y$ is strictly positive, many researchers instead report the **ATE in logs**:

$$
\mathbb{E}[\log(Y(1)) - \log(Y(0))].
$$

If $Y$ can equal zero, log is undefined, so analysts often use **log-like transformations** like $\log(1 + Y)$ or $\arcsinh(Y)$, both of which behave like $\log(Y)$ for large $Y$.

## A Closer Look

### The Problem with Logs and Zeros

The main challenge arises when some individuals have $Y(0) = 0$ but $Y(1) > 0$, or vice versa. In these cases, a **percentage change** isn’t well-defined at the individual level: you can’t compute a percentage increase from zero.

Chen and Roth (2024) demonstrate that for log-like transformations $m(Y)$, the estimated ATE:

$$
\mathbb{E}[m(Y(1)) - m(Y(0))]
$$

is **not unit-invariant**. In fact, the magnitude of this estimated effect can be arbitrarily manipulated simply by rescaling the units of $Y$ (e.g., from dollars to cents). This violates the intuition that a percentage change should be independent of whether we’re measuring income in dollars or pennies.

The core insight is that log-like transformations place implicit and arbitrary weight on the **extensive margin** — that is, on individuals moving from zero to nonzero outcomes — and that this weight is sensitive to the scale of $Y$.

---

### A Trilemma: Pick Two

Chen and Roth formalize a "trilemma": when outcomes can be zero, no treatment effect parameter can satisfy all three of the following:

1. Be an average of individual-level treatment effects.
2. Be invariant to rescaling of units.
3. Be point-identified from observed data.

This means researchers must choose which property to sacrifice. If you want unit-invariance (percentage effects), you can’t use log-like ATEs. If you want point identification and interpretability, you may need to abandon simple averaging.

---

### Alternative Approaches

#### Percentage Effects in Levels (Poisson Regression)

One solution is to return to **level-based** treatment effects, such as:

$$
\theta_{\text{level-percentage}} = \frac{\mathbb{E}[Y(1) - Y(0)]}{\mathbb{E}[Y(0)]}.
$$

This can be estimated via **Poisson regression** or by directly scaling the level-based ATE.

#### Explicit Weighting of Margins

If both the extensive (zero to nonzero) and intensive (positive changes) margins are of interest, researchers can specify how much they value a zero-to-one change relative to continuous changes. This requires subjective calibration but makes assumptions transparent.

#### Separate Estimates for Intensive and Extensive Margins

Alternatively, estimate separate treatment effects for:

- The probability of having $Y > 0$ (extensive margin).
- The mean of $\log(Y)$ conditional on $Y > 0$ (intensive margin).

This aligns with methods like **Lee bounds** or other partial identification strategies.

---

### The Huntington-Klein Adjustment: Linear Rescaling

Huntington-Klein (2023) offers a complementary insight: even when using standard logs, **interpretation often relies on sloppy approximations** (e.g., interpreting $p$ units in $\log(X)$ as $p \times 100\%$ change in $X$).

He proposes using **logarithms with alternative bases** (log\(_{1+p}\)) where a one-unit change corresponds exactly to a $p$-percent increase. This method avoids approximation error and can be written directly into regression tables for clarity.

While this doesn't fix the zero problem per se, it improves interpretability when outcomes are positive.

## An Example

::::{.panel-tabset}

### R

```r
set.seed(123)
n <- 1000
d <- rbinom(n, 1, 0.5)
y0 <- rgamma(n, shape = 2, rate = 1)
y1 <- y0 + d * rgamma(n, shape = 1, rate = 0.5)
y <- ifelse(d == 1, y1, y0)
log1p_y <- log1p(y)
model <- lm(log1p_y ~ d)
summary(model)
```

### Python

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm

np.random.seed(123)
n = 1000
d = np.random.binomial(1, 0.5, n)
y0 = np.random.gamma(2, 1, n)
y1 = y0 + d * np.random.gamma(1, 2, n)
y = np.where(d == 1, y1, y0)
log1p_y = np.log1p(y)
X = sm.add_constant(d)
model = sm.OLS(log1p_y, X).fit()
print(model.summary())
```

::::

## Bottom Line

- Log transformations with zeros pose serious interpretational challenges for causal inference.

- Log-like ATEs are arbitrarily sensitive to the scaling of the outcome variable.

- Alternative strategies like Poisson regression, explicit weighting of margins, or separate modeling of intensive and extensive effects provide better-grounded estimates.

- Linear rescaling (choosing alternative log bases) improves interpretation accuracy but does not solve the zero problem alone.

## Where to Learn More

The must-read paper on this topic is Chen and Roth (2024), which provides both the formal theory and empirical illustrations of these issues. Huntington-Klein (2023) adds valuable advice on how to interpret log coefficients cleanly. Related discussions appear in work on Poisson models, two-part models, and Lee bounds for partial identification.

## References

- Chen, J., & Roth, J. (2024). Logs with Zeros? Some Problems and Solutions. *Quarterly Journal of Economics*, 139(2), 891–936.

- Huntington-Klein, N. (2023). Linear Rescaling to Accurately Interpret Logarithms. *Journal of Economic Methods*, 12(1), 139–147.
