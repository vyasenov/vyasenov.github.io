---
title: "Theoretical Properties of Lasso"
date: "2025-00-00"
categories: [lasso, statitical inference]
---

## Background

Lasso (Least Absolute Shrinkage and Selection Operator) is widely used not only for its variable selection ability but also for its appealing theoretical properties in high-dimensional regression. Over the past two decades, a rich literature has characterized the behavior of Lasso estimators under various conditions — especially when the number of predictors $p$ may exceed the number of observations $n$.

This article walks through the key theoretical properties of Lasso, explaining what they mean, when they hold, and why they matter for estimation, prediction, and inference.

## A Closer Look

### 1. Convergence Rates (Estimation Consistency)

Under sparsity and restricted eigenvalue conditions:
$$
\| \hat{\beta} - \beta_0 \|_1 = O_p\left( s \sqrt{ \frac{\log p}{n} } \right),
$$
where $s$ is the sparsity level (number of true nonzero coefficients).

Prediction error:
$$
\| X ( \hat{\beta} - \beta_0 ) \|_2^2 / n = O_p\left( s \frac{\log p}{n} \right).
$$
This is near-oracle optimal up to log factors.

### 2. Support Recovery (Sparsistency)

Lasso can recover the correct set of nonzero coefficients with high probability (sparsistency) if:
- The **irrepresentable condition** holds (Zhao and Yu, 2006),
- Signal strength is sufficiently large.

Otherwise, Lasso may fail to perfectly select the true model, especially under high correlations or weak signals.

### 3. Prediction Consistency

Lasso can achieve prediction consistency:
$$
\mathbb{E} \left[ \| X (\hat{\beta} - \beta_0) \|_2^2 \right] \to 0 \quad \text{as} \quad n \to \infty,
$$
even if exact support recovery fails.

### 4. Oracle Inequalities

Lasso satisfies oracle inequalities comparing its performance to an ideal estimator that knows the true support:
$$
\| X (\hat{\beta} - \beta_0) \|_2^2 / n \leq C \cdot \inf_{\beta: \| \beta \|_0 \leq s} \left\{ \| X (\beta - \beta_0) \|_2^2 / n + \lambda^2 s \right\}.
$$

### 5. Bias of the Estimates

Lasso is biased due to the L1 penalty. The bias remains unless the penalty $\lambda$ shrinks appropriately with $n$.

Bias motivates the use of:
- Post-Lasso OLS,
- Relaxed Lasso,
- Adaptive Lasso,
- Debiased Lasso.

### 6. Asymptotic Normality (Debiased Lasso)

Standard Lasso does not yield asymptotically normal estimates. However, **Debiased (Desparsified) Lasso** allows for valid inference:
$$
\sqrt{n} (\hat{\beta}_j^{\text{debiased}} - \beta_{0j}) \overset{d}{\to} N(0, \sigma_j^2).
$$

### 7. Valid Inference and Confidence Intervals

Valid $p$-values and confidence intervals require adjustments:
- Debiased Lasso,
- Selective inference (e.g., Lee et al., 2016),
- Double selection (Belloni et al., 2014).

### 8. Double Robustness and Semi-parametric Efficiency

In causal inference settings (e.g., DML), Lasso can estimate high-dimensional nuisance components, yielding:
- $\sqrt{n}$-consistent estimates for low-dimensional targets,
- Asymptotic normality under orthogonality and cross-fitting.

## Summary Table

| Property                    | Achieved by Lasso?               | Conditions Required                             |
|-----------------------------|----------------------------------|--------------------------------------------------|
| Convergence rates (L1/L2)    | Yes, near-oracle (up to log factor) | Sparsity, restricted eigenvalues                 |
| Support recovery (sparsistency) | Sometimes (hard in correlated designs) | Irrepresentable condition or compatibility     |
| Prediction consistency       | Yes                             | Restricted eigenvalues, compatibility            |
| Oracle inequality            | Yes                             | Standard sparsity assumptions                    |
| Bias                         | Yes (biased toward zero)        | Bias remains unless corrected                    |
| Asymptotic normality         | No (unless debiased)            | Debiasing, desparsification required             |
| Valid inference (CI, $p$-values) | Not directly (needs debiased Lasso) | Debiased Lasso, post-selection inference         |
| Double robustness / efficiency | Only when combined (e.g., DML)  | Orthogonal scores, cross-fitting                 |

## Bottom Line

- Lasso achieves strong theoretical guarantees for estimation and prediction under sparsity.

- Support recovery requires stringent conditions, and Lasso may miss variables under collinearity.

- Bias correction techniques like debiasing or post-selection OLS help achieve valid inference.

- Double/debiased machine learning and orthogonalization frameworks leverage Lasso for high-dimensional nuisance estimation.

## Where to Learn More

Key papers include Tibshirani (1996), Zhao and Yu (2006), Bickel et al. (2009), van de Geer et al. (2014), Javanmard and Montanari (2014), and Belloni et al. (2014). For debiased methods and selective inference, see recent reviews by Bühlmann and van de Geer.

## References

[TO ADD]

