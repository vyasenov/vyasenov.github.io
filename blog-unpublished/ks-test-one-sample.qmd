---
title: "The Kolmogorov–Smirnov Test as a Goodness-of-fit"
date: "2025-00-00"
categories: [statistical inference, hypothesis testing]
---

## Background

The Kolmogorov–Smirnov (KS) test is a staple in the statistical toolbox for checking how well data fit a hypothesized distribution. It’s nonparametric, straightforward to compute, and widely implemented in `R`, `Python`, and just about every statistical software you can think of. But—and this is a big but—using the KS test naively can lead to some serious misinterpretations, especially when parameters are estimated from the data.

This article is based on the 2024 paper by Zeimbekakis, Schifano, and Yan, which takes a hard look at the common misuses of the one-sample KS test. We’ll walk through what the KS test is supposed to do, when it goes wrong, and how to think more clearly about assessing goodness-of-fit.

## Notation

Let $X_1, \dots, X_n$ be i.i.d. random variables with unknown distribution function $F$. We want to test whether $F = F_0$, for some known distribution function $F_0$.

The empirical distribution function (EDF) is:
$$F_n(x) = \frac{1}{n} \sum_{i=1}^n I(X_i \leq x)$$

The KS statistic is:
$$D_n = \sup_{x \in \mathbb{R}} |F_n(x) - F_0(x)|$$

The null distribution of $D_n$ is known under the assumption that $F_0$ is fully specified, i.e., no parameters have been estimated from the data.

## A Closer Look

### What the KS Test Measures

The KS test is sensitive to discrepancies in the cumulative distribution function. Intuitively, it’s measuring the largest vertical distance between the EDF and the hypothesized CDF $F_0$. This gives you a global measure of discrepancy, not a local one—so it’s less powerful for detecting issues like tail misspecification or multimodality.

The KS test gives equal weight to all observations when computing the maximum deviation, making it less effective at detecting differences in distribution tails compared to other tests like Anderson-Darling. This is important because in many applications, tail behavior is critically important, such as in risk modeling or extreme value analysis.

With small samples, the test has limited power to detect distributional differences, while with very large samples, it may detect statistically significant but practically trivial deviations from the hypothesized distribution.

### When Things Go Wrong: Parameter Estimation

Here’s the catch: the null distribution of the KS statistic assumes $F_0$ is fully known. But in practice, people often use the test to evaluate model fit *after* estimating parameters—e.g., fitting a normal distribution by MLE and then checking fit with KS.

That invalidates the test.

Why? Because the theoretical distribution of $D_n$ changes when parameters are estimated. The true distribution of the test statistic becomes conditional on the data, and the critical values are no longer accurate. This leads to an inflated Type I error rate: you're more likely to incorrectly reject the null.

### Better Alternatives

When parameters are estimated, we need modified procedures:

- **Lilliefors test**: An adaptation of the KS test that adjusts the null distribution when testing for normality with estimated parameters.
- **Parametric bootstrap**: Simulate the null distribution of the test statistic by repeatedly fitting the model and computing $D_n$ on simulated data.
- **Other GOF tests**: Anderson-Darling and Cramér-von Mises tests have versions that handle estimated parameters more gracefully.

## An Example

:::: {.panel-tabset}

### R

```r
set.seed(42)
x <- rnorm(100, mean = 5, sd = 2)
ks.test(x, "pnorm", mean = mean(x), sd = sd(x)) # misuse!

# Better: use Lilliefors test
library(nortest)
lillie.test(x)
```

### Python

```python
import numpy as np
from scipy.stats import kstest, norm
from statsmodels.stats.diagnostic import lilliefors

np.random.seed(42)
x = np.random.normal(loc=5, scale=2, size=100)

# Misuse: parameters estimated from x
kstest(x, 'norm', args=(np.mean(x), np.std(x, ddof=1)))

# Better: use Lilliefors test
stat, pval = lilliefors(x)
print(f"Lilliefors test: statistic={stat}, p-value={pval}")
```

::::

## Bottom Line

- The KS test assumes no parameters are estimated—violating this leads to invalid inference.

- Estimating parameters from the same data used in the test inflates Type I error.

- Use alternatives like the Lilliefors test or bootstrap methods when parameters are estimated.

- The KS test is less sensitive in the tails—be cautious with it as a global fit measure.

## Where to Learn More

If you're regularly checking distributional assumptions, it's worth diving into goodness-of-fit tests in more depth. The *American Statistician* paper by Zeimbekakis et al. (2024) is a great read. Also consider books like *Goodness-of-Fit Techniques* by D’Agostino and Stephens, or *All of Statistics* by Larry Wasserman for more intuitive overviews.

## References

Zeimbekakis, A., Schifano, E. D., & Yan, J. (2024). On Misuses of the Kolmogorov–Smirnov Test for One-Sample Goodness-of-Fit. *The American Statistician*, 78(4), 481-487.

