---
title: "Randomization Inference: Exact Testing Without Parametric Assumptions"
date: "2025-00-00"
categories: [causal inference, randomized experiments]
---

## Background

Randomization inference offers a refreshing alternative to traditional parametric inference, providing exact control over Type I error rates without relying on large-sample approximations or strict distributional assumptions. Born out of Fisher's famous tea-tasting experiment, the approach leverages the symmetry and structure induced by randomization itself to test hypotheses. 

This blog post unpacks the theory and intuition behind randomization inference, drawing on the excellent review by Ritzwoller, Romano, and Shaikh (2025). We'll cover the key ideas, notation, and algorithms involved, and also touch on modern applications like two-sample tests, regression, and conformal inference. Throughout, we'll emphasize the practical considerations — when it works, why it works, and where caution is needed.

## Notation

Let $X$ represent the observed data, generated by some unknown probability law $P$. The parameter space $\Omega$ contains all possible data-generating processes, and $\Omega_0 \subset \Omega$ specifies the null hypothesis.

A group $G$ of transformations (e.g., permutations or sign-flips) acts on the data. Under the **randomization hypothesis**, the distribution of $X$ is invariant under $G$ if $P \in \Omega_0$.

Formally, under the null:
$$
gX \overset{d}{=} X, \quad \text{for all } g \in G.
$$

Let $T(X)$ be a chosen test statistic.

## A Closer Look

### Exact Testing via Randomization

If the randomization hypothesis holds, we can compute the distribution of $T(X)$ by applying all transformations in $G$ to the data. The p-value is simply the proportion of these transformed test statistics that are as extreme or more extreme than the observed $T(X)$:

$$
\hat{p} = \frac{1}{|G|} \sum_{g \in G} I\{ T(gX) \geq T(X) \}.
$$

Because the null implies invariance under $G$, this procedure achieves exact finite-sample control of the Type I error rate.

::: {.callout-note title="Algorithm: Randomization Test"}
1. Choose a test statistic $T(X)$.
2. Define the group $G$ of transformations.
3. Compute $T(X)$ on the observed data.
4. Apply all (or a random sample of) transformations $g \in G$ to the data and recompute $T(gX)$.
5. Calculate the p-value as the proportion of transformed statistics as or more extreme than $T(X)$.
:::

### Approximate Validity and Asymptotics

In many real-world problems, the randomization hypothesis may not strictly hold, or the group $G$ may only approximate invariance. The surprising good news is that permutation and randomization tests often remain **asymptotically valid** under broad conditions.

This validity relies on the test statistic being **asymptotically pivotal** — its limiting distribution under the null does not depend on nuisance parameters. Examples include:
- The studentized difference of means.
- Test statistics based on ranks (e.g., Wilcoxon-Mann-Whitney).

By **studentizing** the test statistic (i.e., scaling by an estimate of its standard error), one can often restore validity even when the randomization hypothesis fails.

### When Things Go Wrong: Failures Without Studentization

If the test statistic is not pivotal, the permutation distribution might not match the true sampling distribution. This mismatch leads to inflated Type I errors and directional mistakes (Type III errors), especially when variances differ between groups.

For instance, testing mean differences between two groups with unequal variances using the raw difference in means (without studentization) can lead to massive over-rejection rates. Studentizing solves this problem by aligning the permutation distribution with the true sampling distribution.

### Strengths and Limitations

Randomization inference shines when:
- The randomization scheme is known and under control (e.g., in experiments).
- The test statistic is carefully chosen to be pivotal.
- Exact error control is desirable in finite samples.

It struggles when:
- Covariates are correlated with treatment assignment but not accounted for.
- The sample size is too small to approximate the randomization distribution well via subsampling.

---

## An Example

::::{.panel-tabset}

### R

```r
set.seed(123)
n <- 20
x <- rnorm(n, mean = 0)
test_stat <- mean(x)
n_permutations <- 1000
perms <- replicate(n_permutations, mean(sample(x)))
p_value <- mean(perms >= test_stat)
p_value
```

### Python

```python
import numpy as np

np.random.seed(123)
n = 20
x = np.random.normal(0, 1, n)
test_stat = np.mean(x)
perms = [np.mean(np.random.permutation(x)) for _ in range(1000)]
p_value = np.mean([p >= test_stat for p in perms])
print(p_value)
```

::::

## Bottom Line

- Randomization inference provides exact finite-sample error control when the randomization hypothesis holds.

- Asymptotic validity can often be rescued by choosing asymptotically pivotal (studentized) test statistics.

- Without studentization, permutation tests may fail badly in the presence of unequal variances.

- Randomization tests are flexible and nonparametric, making them attractive for experimental data and beyond.

## Where to Learn More

The best starting point is the recent review by Ritzwoller, Romano, and Shaikh (2025). For foundational treatments, see Hoeffding (1952) and Lehmann & Romano's text on nonparametric inference. The practical guide by Good (2005) on permutation tests is also highly recommended.

## References

- Ritzwoller, D. M., Romano, J. P., & Shaikh, A. M. (2025). Randomization Inference: Theory and Applications.

- Hoeffding, W. (1952). The large-sample power of permutation tests. *Annals of Mathematical Statistics*, 23(2), 169-192.

- Lehmann, E. L., & Romano, J. P. (2022). *Testing Statistical Hypotheses*. Springer.

- Good, P. (2005). *Permutation, Parametric, and Bootstrap Tests of Hypotheses*. Springer.
